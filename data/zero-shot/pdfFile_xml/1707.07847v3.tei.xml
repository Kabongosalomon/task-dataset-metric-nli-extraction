<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<email>at.luu@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><surname>Cheung</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3159652.3159664</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Deep Learning</term>
					<term>Learning to Rank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attentionbased matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind HyperQA is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>are as follows: Firstly, representations of questions and answers are first learned via a neural encoder such as the long short-term memory (LSTM) <ref type="bibr" target="#b9">[10]</ref> network or convolutional neural network (CNN). Secondly, these representations of questions and answers are composed by an interaction function to produce an overall matching score.</p><p>The design of the interaction function between question and answer representations lives at the heart of deep learning QA research. While it is simply possible to combine QA representations with simple feed forward neural networks or other composition functions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, a huge bulk of recent work is concerned with designing novel word interaction layers that model the relationship between the words in the QA pairs. For example, similarity matrix based matching <ref type="bibr" target="#b29">[30]</ref>, soft attention alignment <ref type="bibr" target="#b16">[17]</ref> and attentive pooling <ref type="bibr" target="#b3">[4]</ref> are highly popular techniques for improving the performance of neural ranking models. Apparently, it seems to be well-established that grid-based matching is essential to good performance. Notably, these new innovations come with trade-offs such as huge computational cost that lead to significantly longer training times and also a larger memory footprint. Additionally, it is good to consider that the base neural encoder employed also contributes to the computational cost of these neural ranking models, e.g., LSTM networks are known to be over-parameterized and also incur a parameter and runtime cost of quadratic scale. It also seems to be a well-established fact that a neural encoder (such as the LSTM, Gated Recurrent Unit (GRU), CNN, etc.) must be first selected for learning individual representations of questions and answers and is generally treated as mandatory for good performance.</p><p>In this paper, we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K-90K parameters (as opposed to millions). Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space. Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially. Intuitively, this makes it suitable for learning embeddings that reflect a natural hierarchy (e.g., networks, text, etc.) which we believe might benefit neural ranking models for QA. Notably, our work is inspired by the recently incepted Poincar? embeddings <ref type="bibr" target="#b15">[16]</ref> which demonstrates the effectiveness of inducing a structural (hierarchical) bias in the embedding space for improved generalization. In our early empirical experiments, we discovered that a simple feed forward neural network trained in Hyperbolic space is capable of outperforming more sophisticated models on several standard benchmark datasets. We believe that this can be attributed to two reasons. Firstly, latent hierarchies are prominent in QA. Aside from the natural hierarchy of questions and answers, conceptual hierarchies also exist.</p><p>Secondly, natural language is inherently hierarchical which can be traced to power law distributions such as Zipf's law <ref type="bibr" target="#b20">[21]</ref>. The key contributions in this paper are as follows:</p><p>? We propose a new neural ranking model for ranking question answer pairs. For the first time, our proposed model, HyperQA, performs matching of questions and answers in Hyperbolic space. To the best of our knowledge, we are the first to model QA pairs in Hyperbolic space. While hyperbolic geometry and embeddings have been explored in the domains of complex networks or graphs <ref type="bibr" target="#b12">[13]</ref>, our work is the first to investigate the suitability of this metric space for question answering. ? HyperQA is an extremely fast and parameter efficient model that achieves very competitive results on multiple QA benchmarks such as TrecQA, WikiQA and YahooCQA. The efficiency and speed of HyperQA are attributed by the fact that we do not use any sophisticated neural encoder and have no complicated word interaction layer. In fact, HyperQA is a mere single layered neural network with only 90K parameters. Very surprisingly, HyperQA actually outperforms many state-of-the-art models such as Attentive Pooling BiLSTMs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref> and Multi-Perspective CNNs <ref type="bibr" target="#b7">[8]</ref>. We believe that this allows us to reconsider whether many of these complex word interaction layers are really necessary for good performance. ? We conduct extensive qualitative analysis of both the learned QA embeddings and word embeddings. We discover several interesting properties of QA embeddings in Hyperbolic space. Due to its compositional nature, we find that our model learns to self-organize not only at the QA level but also at the word-level. Our qualitative studies enable us to gain a better intuition pertaining to the good performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many prior works have established the fact that there are mainly two key ingredients to a powerful neural ranking model. First, an effective neural encoder and second, an expressive word interaction layer. The first ingredient is often treated as a given, i.e., the top performing models always use a neural encoder such as the CNN or LSTM. In fact, many top performing models adopt convolutional encoders for sentence representation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37]</ref>. The usage of recurrent models is also notable <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The key component in which many recent models differ at is at the interaction layer. Early works often combined QA embeddings 'as it is', i.e., representations are learned first and then combined. For example, Yu et al. <ref type="bibr" target="#b35">[36]</ref> used CNN representations as feature inputs to a logistic regression model. The end-to-end CNN-based model of Severyn and Moschitti <ref type="bibr" target="#b22">[23]</ref> combines the CNN encoded representations of question and answer using a multi-layered perceptron (MLP). Recently, a myriad of composition functions have been proposed as well, e.g., tensor layers in Qiu et al. <ref type="bibr" target="#b18">[19]</ref> and holographic layers in Tay et al. <ref type="bibr" target="#b25">[26]</ref>.</p><p>It has been recently fashionable to model the relationships between question and answer using similarity matrices. Intuitively, this enables more fine-grained matching across words in question and answer sentences. The Multi-Perspective CNN (MP-CNN) <ref type="bibr" target="#b6">[7]</ref> compared two sentences via a wide diversity of pooling functions and filter widths aiming to capture 'multi-perspectives' between two sentences. The attention based neural matching (aNMM) model of Yang et al. <ref type="bibr" target="#b31">[32]</ref> performed soft-attention alignment by first measuring the pairwise word similarity between each word in question and answer. The attentive pooling models of Santos et al. <ref type="bibr" target="#b3">[4]</ref> (AP-BiLSTM and AP-CNN) utilized this soft-attention alignment to learn weighted representations of question and answer that are dependent of each other. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> extended AP-CNN to 3D tensor-based attentive pooling (AI-CNN). A recent work, the Cross Temporal Recurrent Network (CTRN) <ref type="bibr" target="#b26">[27]</ref> proposed a pairwise gating mechanism for joint learning of QA pairs.</p><p>Unfortunately, these models actually introduce a prohibitive computational cost to the model usually for a very marginal performance gain. Notably, it is easy to see that similarity matrix based matching incurs a computational cost of quadratic scale. Representation ability such as dimension size of word or CNN/RNN embeddings are naturally also quite restricted, i.e., increasing any of these dimensions can cause computation or memory requirements to explode. Moreover, it is not uncommon for models such as AI-CNN or AP-BiLSTM to spend more than 30 minutes on a single epoch on QA datasets that are only medium sized. Let us not forget that these models still have to be extensively tuned which aggravates the impracticality problem posed by some of these models.</p><p>In this paper, we seek a new paradigm for neural ranking for QA. While many recent works try to out-stack each other with new layers, we strip down our network instead. Our work is inspired by the very recent Poincar? embeddings <ref type="bibr" target="#b15">[16]</ref> which demonstrates the superiority and efficiency of generalization in Hyperbolic space. Moreover, this alleviates many overfitting and complexity issues that Euclidean embeddings might face especially if the data has intrinsic hierarchical structure. It is good to note that power-law distributions, such as Zipf's law, have been known to be from innate hierarchical structure <ref type="bibr" target="#b20">[21]</ref>. Specifically, the defining characteristic of Hyperbolic space is a much quicker expansion relative to that of Euclidean space which makes naturally equipped for modeling hierarchical structure. The concept of Hyperbolic spaces has been applied to domains such as complex network modeling <ref type="bibr" target="#b12">[13]</ref>, social networks <ref type="bibr" target="#b28">[29]</ref> and geographic routing <ref type="bibr" target="#b11">[12]</ref>.</p><p>There are several key geometric intuitions regarding Hyperbolic spaces. Firstly, the concept of distance and area is warped in Hyperbolic spaces. Specifically, each tile in <ref type="figure" target="#fig_0">Figure 1</ref>(a) is of equal area in Hyperbolic space but diminishes towards zero in Euclidean space towards the boundary. Secondly, Hyperbolic spaces are conformal, i.e., angles in Hyperbolic spaces and Euclidean spaces are identical. In <ref type="figure" target="#fig_0">Figure 1</ref>(b), the arcs on the curve are parallel lines that are orthogonal to the boundary. Finally, hyperbolic spaces can be regarded as larger spaces relative to Euclidean spaces due to the fact that the concept of relative distance can be expressed much better, i.e., not only does the distance between two vectors encode information but also where a vector is placed in Hyperbolic space. This enables efficient representation learning.</p><p>In Nickel et al. <ref type="bibr" target="#b15">[16]</ref>, the authors applied the hyperbolic distance (specifically, the Poincar? distance) to model taxonomic entities and graph nodes. Notably, our work, to the best of our knowledge, is the only work that learns QA embeddings in Hyperbolic space. Moreover, questions and answers introduce an interesting layer of complexity to the problem since QA embeddings are in fact compositions of their constituent word embeddings. On the other hand, nodes in a graph and taxonomic entities in <ref type="bibr" target="#b15">[16]</ref> are already at its most abstract form, i.e., symbolic objects. As such, we believe it would be interesting to investigate the impacts of QA in Hyperbolic space in lieu of the added compositional nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED APPROACH</head><p>This section outlines the overall architecture of our proposed model. Similar to many neural ranking models for QA, our network has 'two' sides with shared parameters, i.e., one for question and another for answer. However, since we optimize for a pairwise ranking loss, the model takes in a positive (correct) answer and a negative (wrong) answer and aims to maximize the margin between the scores of the correct QA pair and the negative QA pair. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the overall model architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>Our model accepts three sequences as an input, i.e., the question (denoted as q), the correct answer (denoted as a) and a randomly sampled corrupted answer (denoted as a ? ). Each sequence consists of M words where M q and M a are predefined maximum sequence lengths for questions and answers respectively. Each word is represented as a one-hot vector (representing a word in the vocabulary).</p><p>As such, this layer is a look-up layer that converts each word into a low-dimensional vector by indexing onto the word embedding matrix. In our implementation, we initialize this layer with pretrained word embeddings <ref type="bibr" target="#b17">[18]</ref>. Note that this layer is not updated during training. Instead, we utilize a projection layer that learns a task-specific projection of the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projection Layer</head><p>In order to learn a task-specific representation for each word, we utilize a projection layer. The projection layer is essentially a single layered neural network that is applied to each word in all three sequences.</p><formula xml:id="formula_0">x = ? (W p z + b p ) (1) where W p ? R d ?n , z ? R n , x ? R d</formula><p>and ? is a non-linear function such as the rectified linear unit (ReLU). The output of this layer is a sequence of d dimensional embeddings for each sequence (question, positive answer and negative answer). Note that the parameters of this projection layer are shared for both question and answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning QA Representations</head><p>In order to learn question and answer representations, we simply take the sum of all word embeddings in the sequence.</p><formula xml:id="formula_1">y * = M * i=1 x * i<label>(2)</label></formula><p>where * = {q, a, a ? }. M is the predefined max sequence length (specific to question and answer) and</p><formula xml:id="formula_2">x 1 , x 2 . . . x M are d-dimensional</formula><p>embeddings of the sequence. This is essentially the neural bagof-words (NBoW) representation. Unlike popular neural encoders such as LSTM or CNN, the NBOW representation does not add any parameters and is much more efficient. Additionally, we constrain the question and answer embeddings to the unit ball before passing to the next layer, i.e., ?y * ? ? 1. This is easily done via y * = y * ?y * ? when ?y * ? &gt; 1. Note that this projection of QA embeddings onto the unit ball is mandatory and absolutely crucial for HyperQA to even work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hyperbolic Representations of QA Pairs</head><p>Neural ranking models are mainly characterized by the interaction function between question and answer representations. In our work, we mainly adopt the hyperbolic 1 distance function to model the relationships between questions and answers. Formally, let</p><formula xml:id="formula_3">B d = {x ? R d | ?x ? &lt; 1}</formula><p>be the open d-dimensional unit ball, our model corresponds to the Riemannian manifold (B d , ? x ) and is equipped with the Riemannian metric tensor given as follows:</p><formula xml:id="formula_4">? x = ( 2 1 ? ?x ? 2 ) 2 ? E<label>(3)</label></formula><p>where ? E is the Euclidean metric tensor. The hyperbolic distance function between question and answer is defined as:</p><formula xml:id="formula_5">d(q, a) = arcosh(1 + 2 ?q ? a? 2 (1 ? ?q? 2 )(1 ? ?a? 2 ) )<label>(4)</label></formula><p>where ?.? denotes the Euclidean norm and q, a ? R d are the question and answer embeddings respectively. Note that arcosh is the inverse hyperbolic cosine function, i.e., arcoshx = ln(x + (x 2 ? 1)). Notably, d(q, a) changes smoothly with respect to the position of q and a which enables the automatic discovery of latent hierarchies. As mentioned earlier, the distance increases exponentially as the norm of the vectors approaches 1. As such, the latent hierarchies of QA embeddings are captured through the norm of the vectors. From a geometric perspective, the origin can be seen as the root of a tree that branches out towards the boundaries of the hyperbolic ball. This self-organizing ability of the hyperbolic distance is visually and qualitatively analyzed in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Gradient Derivation.</head><p>Amongst the other models of Hyperbolic geometry, the hyperbolic Poincar? distance is differentiable. Let The partial derivate w.r.t to ? is defined as:</p><formula xml:id="formula_6">?d(?, x) ?? = 4 ? ? 2 ? 1 ( ?x ? 2 ? 2??, x? + 1 ? 2 ? ? x ? ) (5) where ? = 1 ? ?? ? 2 , ? = 1 ? ?x ? 2 and ? = 1 + 2 ? ? ?? ? x ? 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Similarity Scoring Layer</head><p>Finally, we pass the hyperbolic distance through a linear transformation described as follows:</p><formula xml:id="formula_7">s(q, a) = w f d(q, a) + b f<label>(6)</label></formula><p>where w f ? R 1 and b f ? R 1 are scalar parameters of this layer. The performance of this layer is empirically motivated by its performance and was selected amongst other variants such as exp(?d(q, a)), non-linear activations such as sigmoid function or the raw hyperbolic distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Optimization and Learning</head><p>This section describes the optimization and learning process of HyperQA. Our model learns via a pairwise ranking loss, which is well suited for metric-based learning algorithms.</p><p>3.6.1 Pairwise Hinge Loss. Our network minimizes the pairwise hinge loss which is defined as follows:</p><formula xml:id="formula_8">L = (q,a)?? q (q,a ? ) ? q max(0, s(q, a) + ? ? s(q, a ? ))<label>(7)</label></formula><p>where ? q is the set of all QA pairs for question q, s(q, a) is the score between q and a, and ? is the margin which controls the extent of discrimination between positive QA pairs and corrupted QA pairs. The adoption of the pairwise hinge loss is motivated by the good empirical results demonstrated in Rao et al. <ref type="bibr" target="#b19">[20]</ref>. Additionally, we also adopt the mix sampling strategy for sampling negative samples as described in their work.</p><p>3.6.2 Gradient Conversion. Since our network learns in Hyperbolic space, parameters have to be learned via stochastic Riemannian optimization methods such as RSGD <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_9">? t +1 = ? ? t (??? R ?(? t ))<label>(8)</label></formula><p>where ? ? t denotes a retraction onto B at ? . ? is the learning rate and ? R ?(? t ) is the Riemannian gradient with respect to ? t . Fortunately, the Riemannian gradient can be easily derived from the Euclidean gradient in this case <ref type="bibr" target="#b2">[3]</ref>. In order to do so, we can simply scale the Euclidean gradient by the inverse of the metric tensor ? ?1 ? . Overall, the final gradients used to update the parameters are:</p><formula xml:id="formula_10">? R = (1 ? ?? t ? 2 ) 2 4 ? E<label>(9)</label></formula><p>Due to the lack of space, we refer interested readers to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> for more details. For practical purposes, we simply utilize the automatic gradient feature of TensorFlow but convert the gradients with Equation <ref type="formula" target="#formula_10">(9)</ref> before updating the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>This section describes our empirical evaluation and its results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In the spirit of experimental rigor, we conduct our empirical evaluation based on four popular and well-studied benchmark datasets for question answering.</p><p>? YahooCQA -This is a benchmark dataset for communitybased question answering that was collected from Yahoo Answers. In this dataset, the answer lengths are relatively longer than TrecQA and WikiQA. Therefore, we filtered answers that have more than 50 words and less than 5 characters. The train-dev-test splits for this dataset are provided by <ref type="bibr" target="#b25">[26]</ref>. ? WikiQA -This is a recently popular benchmark dataset <ref type="bibr" target="#b32">[33]</ref> for open-domain question answering based on factual questions from Wikipedia and Bing search logs. ? SemEvalCQA -This is a well-studied benchmark dataset from SemEval-2016 Task 3 Subtask A (CQA). This is a real world dataset obtained from Qatar Living Forums. In this dataset, there are ten answers in each question 'thread' which are marked as 'Good', 'Potentially Useful' or ''Bad'. We treat 'Good' as positive and anything else as negative labels. ? TrecQA -This is the benchmark dataset provided by Wang et al. <ref type="bibr" target="#b30">[31]</ref>. This dataset was collected from TREC QA tracks 8-13 and is comprised of factoid based questions which mainly answer the 'who', 'what', 'where', 'when' and 'why' types of questions. There are two versions, namely clean and raw, as noted by <ref type="bibr" target="#b19">[20]</ref> which we evaluate our models on.</p><p>Statistics pertaining to each dataset is given in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Baselines</head><p>In this section, we introduce the baselines for comparison.</p><p>? YahooCQA -The key competitors of this dataset are the Neural Tensor LSTM (NTN-LSTM) and HD-LSTM from Tay et al. <ref type="bibr" target="#b25">[26]</ref> along with their implementation of the Convolutional Neural Tensor Network <ref type="bibr" target="#b18">[19]</ref>, vanilla CNN model, and the Okapi BM-25 <ref type="bibr" target="#b21">[22]</ref> benchmark. Additionally, we also report our own implementations of QA-BiLSTM, QA-CNN, AP-BiLSTM and AP-CNN on this dataset based on our experimental setup. ? WikiQA -The key competitors of this dataset are the Paragraph Vector (PV) and PV + Cnt models <ref type="bibr" target="#b13">[14]</ref> of Le and Mikolv, CNN + Cnt model from Yu et al. <ref type="bibr" target="#b35">[36]</ref> and LCLR (Yih et al.) <ref type="bibr" target="#b34">[35]</ref>. These three baselines are reported in the original Wik-iQA paper <ref type="bibr" target="#b32">[33]</ref> which also include variations that include handcrafted features. Additional strong baselines include QA-BiLSTM, QA-CNN from <ref type="bibr" target="#b3">[4]</ref> along with AP-BiLSTM and AP-CNN which are attentive pooling improvements of the former. Finally, we also report the Pairwise Ranking MP-CNN from Rao et al. <ref type="bibr" target="#b19">[20]</ref>.  <ref type="bibr" target="#b19">[20]</ref>. Additionally and due to long standing nature of this dataset, there have been a huge number of works based on traditional feature engineering approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> which we also report. For the clean version of this dataset, we also compare with AP-CNN and QA-BiLSTM/CNN <ref type="bibr" target="#b3">[4]</ref>.</p><p>Since the training splits are standard, we are able to directly report the results from the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Protocol</head><p>This section describes the key evaluation protocol / metrics and implementation details of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Metrics.</head><p>We adopt a dataset specific evaluation protocol in which we follow the prior work in their evaluation protocols. Specifically, TrecQA and WikiQA adopt the Mean Reciprocal Rank (MRR) and MAP (Mean Average Precision) metrics which are commonplace in IR research. On the other hand, YahooCQA and Se-mEvalCQA evaluate on MAP and Precision@1 (abbreviated P@1) which is determined based on whether the top predicted answer is the ground truth. For all competitor methods, we report the performance results from the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Training Time &amp; Parameter</head><p>Size. Additionally, we report the parameter size and runtime (seconds per epoch) of selected models. We selectively re-implement some of the key competitors with the best performance and benchmark their training time on our machine/GPU (a single Nvidia GTX1070). For reporting the parameter size and training time, we try our best to follow the hyperparameters stated in the original papers. As such, the same model can have different training time and parameter size on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Hyperparameters.</head><p>HyperQA is implemented in Tensor-Flow. We adopt the AdaGrad <ref type="bibr" target="#b4">[5]</ref> optimizer with initial learning rate tuned amongst {0.2, 0.1, 0.05, 0.01}. The batch size is tuned amongst {50, 100, 200}. Models are trained for 25 epochs and the model parameters are saved each time the performance on the validation set is topped. The dimension of the projection layer is tuned amongst {100, 200, 300, 400}. L2 regularization is tuned amongst {0.001, 0.0001, 0.00001}. The negative sampling rate is tuned from 2 to 8. Finally, the margin ? is tuned amongst {1, 2, 5, 10, 20}. For TrecQA, WikiQA and YahooCQA, we initialize the embedding layer with GloVe <ref type="bibr" target="#b17">[18]</ref> and use the version with d = 300 and trained on 840 billion words. For SemEvalCQA, we train our own Skipgram model using the unannotated corpus provided by the task. In this case, the embedding dimension is tuned amongst {100, 200, 300}. Embeddings are not updated during training. For the SemEvalCQA dataset, we concatenated the raw QA embeddings before passing into the final layer since we found that it improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>In this section, we present our empirical results on all datasets. For all reported results, the best result is in boldface and the second best is underlined. <ref type="table" target="#tab_2">Table 2</ref> reports our results on the WikiQA dataset. Firstly, we observe that HyperQA outperforms a myriad of complex neural architectures. Notably, we obtain a clear performance gain of 2% ? 3% in terms of MAP/MRR against models such as AP-CNN or AP-BiLSTM. Our model also outperforms MP-CNN which is severely equipped with parameterized word matching mechanisms. We achieve competitive results relative to the Rank MP-CNN. Finally, HyperQA is extremely efficient and fast, clocking 2s per epoch compared to 33s per epoch for Rank MP-CNN. The parameter cost is also 90K vs 10 million which is a significant improvement.   <ref type="table" target="#tab_3">Table 3</ref> reports the experimental results on YahooCQA. First, we observe that HyperQA outperforms AP-BiLSTM and AP-CNN significantly. Specifically, we outperform AP-BiLSTM, the runner-up model by 6% in terms of MRR and 10% in terms of MAP. Notably, HyperQA is 32 times faster than AP-BiLSTM and has 20 times less parameters. Our approach shows that complicated attentive pooling mechanisms are not necessary for good performance.   <ref type="table" target="#tab_4">Table 4</ref> reports the experimental results on SemEvalCQA. Our proposed approach achieves highly competitive performance on this dataset. Specifically, we have obtained the best P@1 performance overall, outperforming the state-of-the-art AI-CNN model by 3% in terms of P@1. The performance of our model on MAP is marginally short from the best performing model. Notably, AI-CNN has benefited from external handcrafted features. As such, comparing AI-CNN (w/o features) with HyperQA shows that our proposed model is a superior neural ranking model. Next, we draw the readers attention to the time cost of AI-CNN. The training time per epoch is ? 3250s per epoch which is about 300 times longer than our model. AI-CNN is extremely cost prohibitive, i.e., attentive pooling is already very expensive and yet AI-CNN performs 3D attentive pooling. Evidently, its performance can be easily superseded in a much smaller training time and parameter cost. This raises questions about the effectiveness of the 3D attentive pooling mechanism.   <ref type="table" target="#tab_5">Table 5</ref> reports the results on TrecQA (raw). HyperQA achieves very competitive performance on both MAP and MRR metrics. Specifically, HyperQA outperforms the basic CNN model of (S&amp;M) by 2% ? 3% in terms of MAP/MRR. Moreover, the CNN (S&amp;M) model uses handcrafted   features which HyperQA does not require. Similarly, the aNMM model and HD-LSTM also benefit from additional features but are outperformed by HyperQA. HyperQA also outperforms MP-CNN but is around 10 times faster and has 100 times less parameters. MP-CNN consists of a huge number of filter banks and utilizes heavy parameterization to match multiple perspectives of questions and answers. On the other hand, our proposed HyperQA is merely a single layered neural network with 90K parameters and yet outperforms MP-CNN. Similarly, <ref type="table" target="#tab_6">Table 6</ref> reports the results on TrecQA (clean). Similarly, HyperQA also outperforms MP-CNN, AP-CNN and QA-CNN. On both datasets, the performance of HyperQA is competitive to Rank MP-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Experimental Results on WikiQA.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Overall analysis.</head><p>Overall, we summarize the key findings of our experiments.</p><p>? It is possible to achieve very competitive performance with small parameterization, and no word matching or interaction layers. HyperQA outperforms complex models such as MP-CNN and AP-BiLSTM on multiple datasets. ? The relative performance of HyperQA is significantly better on large datasets, e.g., YahooCQA (253K training pairs) as opposed to smaller ones like WikiQA (5.9K training pairs). We believe that this is due to the fact that Hyperbolic space is seemingly larger than Euclidean space. ? HyperQA is extremely fast and trains at 10 ? 20 times faster than complex models like MP-CNN. Note that if CPUs are used instead of GPUs (which speed convolutions up significantly), this disparity would be significantly larger.</p><p>? Our proposed approach does not require handcrafted features and yet outperforms models that benefit from them. This is evident on all datasets, i.   In this section, we study the effects of the QA embedding size on performance. <ref type="figure" target="#fig_7">Figure 3</ref> describes the relationship between QA embedding size (d) and MAP on the WikiQA dataset. Additionally, we include a simple baseline (CosineQA) which is exactly the same as HyperQA but uses cosine similarity instead of hyperbolic distance. The MAP scores of three other reported models (MP-CNN, CNN-Cnt and PV-Cnt) are also reported for reference. Firstly, we notice the disparity between HyperQA and CosineQA in terms of performance. This is also observed across other datasets but is not reported due to the lack of space. While CosineQA maintains a stable performance throughout embedding size, the performance of HyperQA rapidly improves at d &gt; 150. In fact, the performance of HyperQA at d = 150 (45K parameters) is already similar to the Multi-Perspective CNN <ref type="bibr" target="#b6">[7]</ref> which contains 10 million parameters. Moreover, the performance of HyperQA outperforms MP-CNN with d = 250-300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effects of QA Embedding Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND ANALYSIS</head><p>This section delves into qualitative analysis of our model and aims to investigate the following research questions:</p><p>(1) RQ1: Is there any hierarchical structure learned in the QA embeddings? How are QA embeddings organized in the final embedding space of HyperQA?</p><p>(2) RQ2: What are the impacts of embedding compositional embeddings in hyperbolic space? Is there an impact on the constituent word embeddings? (3) RQ3: Are we able to derive any insight about how word interaction and matching happens in HyperQA?   <ref type="bibr" target="#b27">[28]</ref>. QA embeddings are extracted from the network as discussed in Section 3.3. We observe that question embeddings form a 'sphere' over answer embeddings. Contrastingly, this is not exhibited when the cosine similarity is used as shown in <ref type="figure" target="#fig_8">Figure 4(b)</ref>. It is important to note that these are embeddings from the test set which have not been trained and therefore the model is not explicitly told whether a particular textual input is a question or answer. This demonstrates the innate ability of HyperQA to self-organize and learn latent hierarchies which directly answers RQ1. Additionally, <ref type="figure">Figure 5</ref>(a) shows a histogram of the vector norms of question and answer embeddings. We can clearly see that questions in general have a higher vector norm 2 and are at a different hierarchical level from answers. In order to further understand what the model is doing, we delve deeper into the visualization at word-level.  <ref type="figure">Figure 5</ref>: Histogram plots of embedding norms. <ref type="bibr" target="#b1">2</ref> We extract QA embeddings right before the constraining / normalization layer.    <ref type="table" target="#tab_11">Table 9</ref> shows some examples of words at each hierarchical level of the sphere on TrecQA. Recall that the vector norms 3 allow us to infer the distance of the word embedding from the origin which depicts its hierarchical level in our context. Interestingly, we found that HyperQA exhibits self-organizing ability even at word-level. Specifically, we notice that the words closer to the origin are common words such as 'to', 'and' which do not have much semantic values for QA problems. At the middle of the hierarchy (?w ? ? 3), we notice that there are more verbs. Finally, as we move towards the surface of the 'sphere', the words become rarer and reflect more domain-specific words such as 'ebay' and 'spielberg'. Moreover, we also found many names and proper nouns occurring at this hierarchical level. Additionally, we also observe that words such as 'where' or 'what' have relatively high vector norms and located quite high up in the hierarchy. This is in concert with <ref type="figure" target="#fig_8">Figure 4</ref> which shows the question embeddings form a sphere around the answer embeddings. At last, we parsed QA pairs word-by-word according to hierarchical level (based on their vector norm). <ref type="table" target="#tab_10">Table 8</ref> reports the outcome of this experiment where H 1 ? H 5 are hierarchical levels based on vector norms. First, we find that questions often start with the overall context and drill down into more specific query words. Take the first sample in <ref type="table" target="#tab_10">Table 8</ref> for example, it begins at a top level with 'burger king' and then drills down progressively to 'what is gross sales?'. Similarly in the second example, it begins with 'florence nightingale' and drills down to 'famous' at H3 in which a match is being found with 'nursing' in the same hierarchical level. Overall, based on our qualitative analysis, we observe that, HyperQA builds two hierarchical structures at the word-level (in vector space) towards the middle which strongly facilitates word-level matching. <ref type="bibr" target="#b2">3</ref> Note that word embeddings are not constrained to ?x ? &lt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of QA Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Word Embeddings</head><p>Pertaining to answers, it seems like the model builds a hierarchy by splitting on conjunctive words ('and'), i.e., the root node of the tree starts by conjunctive words at splits sentences into semantic phrases. Overall, <ref type="figure" target="#fig_11">Figure 6</ref> depicts our key intuitions regarding the inner workings of HyperQA which explains both RQ2 and RQ3. This is also supported by <ref type="figure">Figure 5</ref>(b) which shows the majority of the word norms are clustered with ?w ? ? 3. This would be reasonable considering that the leaf nodes of both question and answer hierarchies would reside in the middle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Question</head><p>Starts with the main context and branches to more specific query words.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed a new neural ranking model for question answering. Our proposed HyperQA achieves very competitive performance on four well-studied benchmark datasets. Our model is light-weight, fast and efficient, outperforming many state-of-the-art models with complex word interaction layers, attentive mechanisms or rich neural encoders. Our model only has 40K-90K parameters as opposed to millions of parameters which plague many competitor models. Moreover, we derive qualitative insights pertaining to our model which enable us to further understand its inner workings. Finally, we observe that the superior generalization of our model (despite small parameters) can be attributed to self-organizing properties of not only question and answer embeddings but also word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualizations of Hyperbolic space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our proposed model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>SemEvalCQA -The key competitors of this dataset are the CNN-based ARC-I/II architecture by Hu et al. [11], the Attentive Pooling CNN [4], Kelp [6] a feature engineering based SVM method, ConvKN [1] a combination of convolutional tree kernels with CNN and finally AI-CNN (Attentive Interactive CNN) [37], a tensor-based attentive pooling neural model. A comparison with AI-CNN (with features) is also included. ? TrecQA -The key competitors on the dataset are mainly the CNN model of Severyn and Moschitti (S&amp;M) [23], the Attention-based Neural Matching Model (aNMM) of Yang et al. [32], HD-LSTM (Tay et al.) [26] and Multi-Perspective CNN (MP-CNN) [7] proposed by He et al. Lastly, we also compare with the pairwise ranking adaption of MP-CNN (Rao et al.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Effects of QA embedding size on WikiQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of QA embeddings on the test set of TrecQA. Projected to 3 dimensions using t-SNE. Blue lines depict mapping of question embeddings to their correct answers. HyperQA learns 'sphere' shaped structure where questions embeddings surround the answer embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 (</head><label>4</label><figDesc>a) shows a visualization of QA embeddings on the test set TrecQA projected in 3-dimensional space using t-SNE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Dual hierarchical matching of question and answer at word-level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">YahooCQA WikiQA SemEvalCQA TrecQA</cell></row><row><cell>Train Qns</cell><cell>50.1K</cell><cell>94</cell><cell>4.8K</cell><cell>1229</cell></row><row><cell>Dev Qns</cell><cell>6.2K</cell><cell>65</cell><cell>224</cell><cell>82</cell></row><row><cell>Test Qns</cell><cell>6.2K</cell><cell>68</cell><cell>327</cell><cell>100</cell></row><row><cell>Train Pairs</cell><cell>253K</cell><cell>5.9K</cell><cell>36K</cell><cell>53</cell></row><row><cell>Dev Pairs</cell><cell>31.7K</cell><cell>1.1K</cell><cell>2.4K</cell><cell>1.1K</cell></row><row><cell>Test Pairs</cell><cell>31.7K</cell><cell>1.4K</cell><cell>3.2K</cell><cell>1.5K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on WikiQA.</figDesc><table /><note>4.4.2 Experimental Results on YahooCQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on YahooCQA.</figDesc><table /><note>4.4.3 Experimental Results on SemEvalCQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on SemEvalCQA. 4.4.4 Experimental Results on TrecQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on TrecQA (raw). Feature engineering and deep learning approaches are separated by the middle line.</figDesc><table><row><cell>Model</cell><cell>MAP</cell><cell cols="3">MRR # Params Time</cell></row><row><cell cols="3">QA-LSTM / CNN (Santos et al.) 0.728 0.832</cell><cell>-</cell><cell>-</cell></row><row><cell>AP-CNN (Santos et al.)</cell><cell cols="2">0.753 0.851</cell><cell>-</cell><cell>-</cell></row><row><cell>MP-CNN (He et al.)</cell><cell cols="2">0.777 0.836</cell><cell>10M</cell><cell>141</cell></row><row><cell>Rank MP-CNN (Rao et al.)</cell><cell cols="2">0.801 0.877</cell><cell>10M</cell><cell>130s</cell></row><row><cell>HyperQA</cell><cell cols="2">0.784 0.865</cell><cell>90K</cell><cell>12s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Experimental results on TrecQA (clean)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>e., HyperQA outperforms CNN model with features (TrecQA and WikiQA) and AI-CNN + features on SemEvalCQA.</figDesc><table><row><cell>Ours against</cell><cell>Performance</cell><cell>Params</cell><cell>Speed</cell></row><row><cell>AP-BiLSTM</cell><cell>1-7% better</cell><cell>20x less</cell><cell>32 x faster</cell></row><row><cell>AP-CNN</cell><cell>1-12% better</cell><cell>Same</cell><cell>3x faster</cell></row><row><cell>AI-CNN</cell><cell>Competitive</cell><cell>3x less</cell><cell>300x faster</cell></row><row><cell>MP-CNN</cell><cell>1-2% better</cell><cell cols="2">100x less 10x faster</cell></row><row><cell cols="4">Rank MP-CNN Competitive 100x less 10x faster</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Overall comparison of HyperQA against other state-of-the-art models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Analysis of QA pairs with respect to hierarchical level (H1-H5) based on vector norms. Self-organizing hierarchical structure facilitates better word level matching. Most informative word matches are marked in bold. Some words might be omitted from the answer due to lack of space. First two examples are from TrecQA and the third is from WikiQA.</figDesc><table><row><cell cols="2">?w ? Words (w)</cell></row><row><cell>0-1</cell><cell>to, and, an, on, in, of, its, the, had, or, go</cell></row><row><cell>1-2</cell><cell>be, a, was, up, put, said, but</cell></row><row><cell>2-3</cell><cell>judging, returning, volunteered, managing, meant, cited</cell></row><row><cell>3-4</cell><cell>responsibility, engineering, trading, prosecuting</cell></row><row><cell>4-5</cell><cell>turkish, autonomous, cowboys, warren, seven, what</cell></row><row><cell>5-6</cell><cell>ebay, magdalena, spielberg, watson, nova</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Examples of words in each hierarchical level of the sphere based on vector norms. Smaller norms are closer to the core/origin.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While there exist multiple models of Hyperbolic geometry such as the Beltrami-Klein model or the Hyperboloid model, we adopt the Poincar? ball / disk due to its ease of differentiability and freedom from constraints<ref type="bibr" target="#b15">[16]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ConvKN at SemEval-2016 Task 3: Answer and Question Selection for Question Answering on Arabic and English Fora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barr?n-Cede?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><forename type="middle">R</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Al-Obaidli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bridging the lexical chasm: statistical approaches to answer-finding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><forename type="middle">O</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.1145/345508.345576</idno>
		<idno>SIGIR. 192-199. DOI</idno>
		<ptr target="http://dx.doi.org/10.1145/345508.345576" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic Gradient Descent on Riemannian Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.2013.2254619</idno>
		<ptr target="http://dx.doi.org/10.1109/TAC.2013.2254619" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Attentive Pooling Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?cero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="1116" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06-02" />
			<biblScope unit="page" from="1011" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network Architectures for Matching Natural Language Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geographic Routing Using Hyperbolic Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFCOM.2007.221</idno>
		<ptr target="http://dx.doi.org/10.1109/INFCOM.2007.221" />
	</analytic>
	<monogr>
		<title level="m">INFO-COM 2007. 26th IEEE International Conference on Computer Communications, Joint Conference of the IEEE Computer and Communications Societies</title>
		<meeting><address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hyperbolic Geometry of Complex Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><forename type="middle">V</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari?n</forename><surname>Bogu??</surname></persName>
		</author>
		<idno>abs/1006.5169</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>ICML-14</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Siamese Recurrent Architectures for Learning Sentence Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Poincar? Embeddings for Learning Hierarchical Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno>abs/1705.08039</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Decomposable Attention Model for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional Neural Tensor Network Architecture for Community-Based Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="1305" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983323.2983872</idno>
		<ptr target="http://dx.doi.org/10.1145/2983323.2983872" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical organization in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erzs?bet</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">26112</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Text REtrieval Conference</title>
		<meeting>The Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11-02" />
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767738</idno>
		<ptr target="http://dx.doi.org/10.1145/2766462.2767738" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08-09" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A syntax-aware re-ranker for microblog retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609511</idno>
		<ptr target="http://dx.doi.org/10.1145/2600428.2609511" />
	</analytic>
	<monogr>
		<title level="m">The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;14</title>
		<meeting><address><addrLine>Gold Coast , QLD, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080790</idno>
		<ptr target="http://dx.doi.org/10.1145/3077136.3080790" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-07" />
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cross Temporal Recurrent Networks for Ranking Question Answer Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1711.07656</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning a Parametric Embedding by Preserving Local Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelfth International Conference on Artificial Intelligence and Statistics<address><addrLine>Clearwater Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04-16" />
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metric embedding, hyperbolic space, and social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhash</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2835" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-28" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
	<note>EMNLP-CoNLL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983323.2983818</idno>
		<ptr target="http://dx.doi.org/10.1145/2983323.2983818" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-24" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">WikiQA: A Challenge Dataset for Open-Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Answer Extraction as Sequence Tagging with Tree Edit Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-09" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Question Answering Using Enhanced Lexical Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep Learning for Answer Sentence Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<idno>abs/1412.1632</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentive Interactive Neural Networks for Answer Selection in Community Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3525" to="3531" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
