<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
							<email>raoyongming95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyan</forename><surname>Liu</surname></persName>
							<email>liuzuyan19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point clouds captured in real-world applications are often incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoderdecoder architecture for point cloud completion. By representing the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometryaware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-ofthe-art methods by a large margin on both the new benchmarks and the existing ones. Code is available at https: //github.com/yuxumin/PoinTr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent developments in 3D sensors largely boost researches in 3D computer vision. One of the most commonly used 3D data format is the point cloud, which requires less memory to store but convey detailed 3D shape <ref type="figure">Figure 1</ref>: PoinTr is designed for point cloud completion task. It takes the downsampled partial point clouds as inputs (gray points), and predicts the missing parts and upsamples the known parts simultaneously (blue points). We propose to formulate the point cloud completion task as a set-to-set translation task and use a transformer encoder-decoder architecture to learn the complex dependencies among the point groups. Furthermore, we design two new benchmarks with more diverse tasks (i.e., upsampling and completion of point cloud), more diverse categories (i.e., from 8 categories to 55 categories), more diverse viewpoints (i.e., from 8 viewpoints to all possible viewpoints) and more diverse levels of incompleteness (i.e., missing 25% to 75% points of the groundtruth point clouds) to better reflect the real-world scenarios and promote future research. information. However, point cloud data from existing 3D sensors are not always complete and satisfactory because of inevitable self-occlusion, light reflection, limited sensor resolution, etc. Therefore, recovering complete point clouds from partial and sparse raw data becomes an indispensable task with ever-growing significance.</p><p>Over the years, researchers have tried many approaches to tackle this problem in the realm of deep learning. Early attempts on point cloud completion <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b22">20,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b44">42]</ref> try to migrate mature methods from 2D completion tasks to 3D point clouds by voxelization and 3D convolutions. However, these methods suffer from a heavy computational cost that grows cubically as the spatial resolution increases. With the success of PointNet and PointNet++ <ref type="bibr" target="#b29">[27,</ref><ref type="bibr" target="#b30">28]</ref>, directly processing 3D coordinates becomes the mainstream of point cloud based 3D analysis. The technique is further applied to many pioneer works <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b16">14,</ref><ref type="bibr" target="#b33">31]</ref> in point cloud completion task, in which an encoder-decoder based architecture is designed to generate complete point clouds. However, the bottleneck of such methods lies in the max-pooling operation in the encoding phase, where fine-grained information is lost and can hardly be recovered in the decoding phase.</p><p>Reconstructing complete point cloud is a challenging problem since the structural information required in the completion task runs counter to the unordered and unstructured nature of point cloud data. Therefore, learning structural features and long-range correlations among local parts of the point cloud becomes the key ingredient towards better point cloud completion. In this paper, we propose to adopt Transformers <ref type="bibr" target="#b41">[39]</ref>, one of the most successful architecture in Natural Language Processing (NLP), to learn the structural information of pairwise interactions and global correlations for point cloud completion. Our model, named PoinTr, is characterized by five key components: 1) Encoder-Decoder Architecture: We adopt the encoderdecoder architecture to convert point cloud completion as a set-to-set translation problem. The self-attention mechanism of transformers models all pairwise interactions between elements in the encoder, while the decoder reasons about the missing elements based on the learnable pairwise interactions among features of the input point cloud and queries; 2) Point Proxy: We represent the set of point clouds in a local region as a feature vector called Point Proxy. The input point cloud is convert to a sequence of Point Proxies, which are used as the inputs of our transformer model; 3) Geometry-aware Transformer Block: To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we design a geometryaware block that models the geometric relations explicitly; 4) Query Generator: We use dynamic queries instead of fixed queirs in the decoder, which are generated by a query generation module that summarizes the features produced by the encoder and represents the initial sketch of the missing points; 5) Multi-Scale Point Cloud Generation: We devise a multi-scale point generation module to recover the missing point cloud in a coarse-to-fine manner.</p><p>As another contribution, we argue that existing benchmarks are not representative enough to cover real-world scenarios of incompleted point clouds. Therefore, we introduce two more challenging benchmarks that contain more diverse tasks (i.e., joint upsampling and completion of point cloud), more object categories (i.e., from 8 categories to 55 categories), more diverse views points (i.e., from 8 viewpoints to all possible viewpoints) and more diverse level of incompleteness (i.e., missing 25% to 75% points of the ground-truth point clouds). We evaluate our method on both the new benchmarks and the widely used PCN dataset <ref type="bibr" target="#b53">[51]</ref> and KITTI benchmark <ref type="bibr" target="#b12">[10]</ref>. Experiments demonstrate that PointTr outperforms previous state-of-the-art methods on all benchmarks by a large margin. The main contributions of this paper are summarized in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Shape Completion. Traditional methods for 3D shape completion tasks often adopt voxel grids or distance fields to describe 3D objects <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b35">33]</ref>. Based on such structured 3D representations, the powerful 3D convolutions are used and achieve a great success in the tasks of 3D reconstruction <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b13">11]</ref> and shape completion <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b48">46]</ref>. However, this group of methods suffers from heavy memory consumption and computational burden. Although these issues are further alleviated by methods based on sparse representations <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b14">12]</ref>, the quantization operation in these methods still cause a significant loss in detailed information. Different from the above methods, researchers gradually start to use unstructured point clouds as the representation of 3D objects, given the small memory consumption and strong ability to represent fine-grained details. Nevertheless, the migration from structured 3D data understanding to point clouds analysis is non-trivial, since the commonly used convolution operator is no longer suitable for unordered points clouds. PointNet and its variants <ref type="bibr" target="#b29">[27,</ref><ref type="bibr" target="#b30">28]</ref> are the pioneer work to directly process 3D coordinates and inspire the researches in many downstream tasks. In the realm of point cloud completion, PCN <ref type="bibr" target="#b53">[51]</ref> is the first learning-based architecture, which proposes an Encoder-Decoder framework and adopts a FoldingNet to map the 2D points onto a 3D surface by mimicking the deformation of a 2D plane. After PCN, many other methods <ref type="bibr" target="#b39">[37,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b20">18]</ref> spring up, pursuing point clouds completion in higher resolution with better robustness.</p><p>Transformers. Transformers <ref type="bibr" target="#b41">[39]</ref> are first introduced as an attention-based framework in Natural Language Processing (NLP). Transformer models often utilize the encoderdecoder architecture and are characterized by both selfattention and cross-attention mechanisms. Transformer models have proven to be very helpful to the tasks that involve long sequences thanks to the self-attention mechanism. The cross-attention mechanism in the decoder exploit the encoder information to learn the attention map of query features, which making transformers powerful in generation tasks. By taking the advantages of both self-attention and cross-attention mechanisms, transformers have a strong capability to handle long sequence input and enhance infor-mation communications between the encoder and the decoder. In the past few years, transformers have dominated the tasks that take long sequences as input and gradually replaced RNNs <ref type="bibr" target="#b42">[40]</ref> in many domains. Now they begin their journey in computer vision <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b32">30</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The overall framework of PoinTr is illustrated in <ref type="figure">Figure</ref> 2. We will introduce our method in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Set-to-Set Translation with Transformers</head><p>The primary goal of our method is to leverage the impressive sequence-to-sequence generation ability of transformer architecture for point cloud completion tasks. We propose to first convert the point cloud to a set of feature vectors, point proxies, that represent the local regions in the point clouds (we will describe in Section 3.2). By analogy to the language translation pipeline, we model point cloud completion as a set-to-set translation task, where the transformers take the point proxies of the partial point clouds as the inputs and produce the point proxies of the missing parts. Specifically, given the set of point proxies F = {F 1 , F 2 , ..., F N } that represents the partial point cloud, we model the process of point cloud completion as a set-to-set translation problem:</p><formula xml:id="formula_0">V = M E (F), H = M D (Q, V),<label>(1)</label></formula><p>where M E and M D are the encoder and decoder models, V = {V 1 , V 2 , ..., V N } are the output features of the encoder, Q = {Q 1 , Q 2 , ..., Q M } are the dynamic queries for the decoder, H = {H 1 , H 2 , ..., H M } are the predicted point proxies of the missing point cloud, and M is the number of the predicted point proxies. The recent success in NLP tasks like text translation and question answering <ref type="bibr" target="#b10">[8]</ref> have clearly demonstrated the effectiveness of transformers to solve this kind of problem. Therefore, we propose to adopt a transformer-based encoder-decoder architecture to solve the point cloud completion problem. The encoder-decoder architecture consists of L E and L D multi-head self-attention layers <ref type="bibr" target="#b41">[39]</ref> in the encoder and decoder, respectively. The self-attention layer in the encoder first updates proxy features with both long-range and short-range information. Then a feed forward network (FFN) further updates the proxy features with an MLP architecture. The decoder utilizes self-attention and crossattention mechanisms to learn structural knowledge. The self-attention layer enhances the local features with global information, while the cross-attention layer explores the relationship between queries and outputs of the encoder. To predict the point proxies of the missing parts, we propose to use dynamic query embeddings, which makes our decoder more flexible and adjustable for different types of objects and their missing information. More details about the transformer architecture can be found in the supplementary material and <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b41">39]</ref>.</p><p>Note that benefiting from the self-attention mechanism in transformers, the features learned by the transformer network are invariant to the order of point proxies, which is also the basis of using transformers to process point clouds. Considering the strong ability to capture data relationships, we expect the transformer architecture to be a promising alternative for deep learning on point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Point Proxy</head><p>The Transformers in NLP take as input a 1D sequence of word embeddings <ref type="bibr" target="#b41">[39]</ref>. To make 3D point clouds suitable for transformers, the first step is to convert the point cloud to a sequence of vectors. A trivial solution is directly feeding the sequence of xyz coordinates to the transformers. However, since the computational complexity of the transformers is quadratic to the sequence length, this solution will lead to an unacceptable cost. Therefore, we propose to represent the original point cloud as a set of point proxies.</p><p>Multi-Head Self-Attention A point proxy represents a local region of the point clouds. Inspired by the set abstraction operation in <ref type="bibr" target="#b30">[28]</ref>, we first conduct furthest point sample (FPS) to locate a fixed number N of point centers {q 1 , q 2 , ..., q N } in the partial point cloud. Then, we use a light-weight DGCNN <ref type="bibr" target="#b46">[44]</ref> with hierarchical downsampling to extract the feature of the point centers from the input point cloud. The point proxy F i is a feature vector that captures the local structure around q i , which can be computed as:</p><formula xml:id="formula_1">Fi = F i + ?(qi),<label>(2)</label></formula><p>where F i is the feature of point q i that is extracted using the DGCNN model, and ? is another MLP to capture the location information of the point proxy. The first term represents the semantic patterns of the local region, and the second term is inspired by the position embedding <ref type="bibr" target="#b5">[3]</ref> operation in transformers, which explicitly encodes the global location of the point proxy. The detailed architecture of the feature extraction model can be found in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometry-aware Transformer Block</head><p>One of the key challenges of applying transformers for vision tasks is the self-attention mechanism in transformers lacks some inductive biases inherent to conventional vision models like CNNs and point cloud networks which explicitly model the structures of vision data. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we design a geometryaware block that models the geometric relations, which can be a plug-and-play module to incorporate with the attention blocks in any transformer architectures. The details of the proposed block are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Different from the self-attention module that uses the feature similarity to capture the semantic relation, we propose to use kNN model to capture the geometric relation in the point cloud. Given the query coordinates p Q , we query the features of the nearest keys according to the key coordinates p k . Then we follow the practice of DGCNN <ref type="bibr" target="#b46">[44]</ref> to learn the local geometric structures by feature aggregation with a linear layer followed by the max pooing operation. The geometric feature and semantic feature are then concatenated and mapped to the original dimensions to form the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Query Generator</head><p>The queries Q serve as the initial state of the predicted proxies. To make sure the queries correctly reflect the sketch of the completed point cloud, we propose a query generator module to generate the query embeddings dynamically conditioned on the encoder outputs. Specifically, we first summarize V with a linear projection to higher dimensions followed by the max pooing operation. Similar to <ref type="bibr" target="#b53">[51]</ref>, we use a linear projection layer to directly generate M ? 3 dimension features that can be reshaped as the M coordinates {c 1 , c 2 , ..., c M }. Lastly, we concatenate the global feature of the encoder and the coordinates, and use an MLP to produce the query embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Multi-Scale Point Cloud Generation</head><p>The goal of our encoder-decoder network is to predict the missing parts of incomplete point clouds. However, we can only get predictions for missing proxies from the transformer decoder. Therefore, we propose a multi-scale point cloud generation framework to recover missing point clouds at full resolution. To reduce redundant computations, we reuse the M coordinates produced by the query generator as the local centers of the missing point cloud. Then, we utilize a FoldingNet <ref type="bibr" target="#b52">[50]</ref> f to recover detailed local shapes centered at the predicted proxies:</p><formula xml:id="formula_2">Pi = f (Hi) + ci, i = 1, 2, ..., M.<label>(3)</label></formula><p>where P i is the set of neighboring points centered at c i . Following previous work <ref type="bibr" target="#b18">[16]</ref>, we only predict the missing parts of the point cloud and concatenate them with the input point cloud to obtain the complete objects. Both predicted proxies and recovered point clouds are supervised during the training process, and the detailed loss function will be introduced in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Optimization</head><p>The loss function for point cloud completion should provide a quantitative measurement for the quality of output. However, since the point clouds are unordered, many loss functions that directly measure the distance between two points (i.e. 2 distance) are unsuitable. Fan et al. <ref type="bibr" target="#b11">[9]</ref> introduce two metrics that are invariant to the permutation of points, which are Chamfer Distance (CD) and Earth Mover's Distance (EMD). We adopt Chamfer Distance as our loss function for its O(N log N ) complexity. We use C to represent the n C local centers and P to represent n P points of the completed point cloud. Give the ground-truth completed point cloud G, the loss functions for these two predictions can be written as:</p><formula xml:id="formula_3">J0 = 1 nC c?C min g?G c ? g + 1 nG g?G min c?C g ? c , J1 = 1 nP p?P min g?G p ? g + 1 nG g?G min p?P g ? p .</formula><p>Note that we also concatenate the predicted local centers and the centers of the input point cloud to form the local centers of the whole object C. We directly use the highresolution point cloud G to supervise the sparse point cloud C to encourage them to have similar distributions. Our final objective function is the sum of these two objectives J = J 0 + J 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the new benchmarks for diverse point cloud completion and the evaluation metric. Then, we show the results of both our method and several baseline methods on our new benchmarks. Lastly, we demonstrate the effectiveness of our model on the widely used PCN dataset and KITTI benchmark. We also provide ablation study and visual analysis of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks for Diverse Point Completion</head><p>We choose to generate the samples in our benchmarks based on the synthetic dataset, ShapeNet <ref type="bibr" target="#b49">[47]</ref>, because it contains the complete object models that cannot be obtained from real-world datasets like ScanNet <ref type="bibr" target="#b7">[5]</ref> and S3DIS <ref type="bibr" target="#b4">[2]</ref>. What makes our benchmarks distinct is that our benchmarks contain more object categories, more incomplete patterns and more viewpoints. Besides, we pay more attention to the ability of networks to deal with the objects from novel categories that do not appear in the training set.</p><p>ShapeNet-55 Benchmark: In this benchmark, we use all the objects in ShapeNet from 55 categories. Most existing datasets for point cloud completion like PCN <ref type="bibr" target="#b53">[51]</ref> only consider a relatively small number of categories (e.g., 8 categories in PCN). However, the incompleted point clouds from the real-world scenarios are much more diverse. Therefore, we propose to evaluate the point cloud completion models on all 55 categories in ShapeNet to more comprehensively test the ability of models with a more diverse dataset. We split the original ShapeNet using the 80-20 strategy: we randomly sample 80% objects from each category to form the training set and use the rest for evaluation. As a result, we get 41,952 models for training and 10,518 models for testing. For each object, we randomly sample 8,192 points from the surface to obtain the point cloud.</p><p>ShapeNet-34 Benchmark: In this benchmark, we want to explore another important issue in point cloud completion: the performance on novel categories. We believe it is necessary to build a benchmark for this task to better evaluate the generalization performance of models. We first split the origin ShapeNet into two parts: 21 unseen categories and 34 seen categories. In the seen categories, we randomly sample 100 objects from each category to construct a test set of the seen categories (3,400 objects in total) and leave the rest as the training set, resulting in 46,765 object models for training. We also construct another test set consisting of 2,305 objects from 21 novel categories. We evaluate the performance on both the seen and unseen categories to show the generalization ability of models. Training and Evaluation: In both benchmarks, the partial point clouds for training are generated online. We sample 2048 points from the object as the input and 8192 points as the ground truth. In order to mimic the real-world situation, we first randomly select a viewpoint and then remove the n furthest points from the viewpoint to obtain a training partial point cloud. Although the projection method proposed in <ref type="bibr" target="#b53">[51]</ref> is a better approximation to real scans, our strategy is more flexible and efficient. Our experiments on KITTI also show the model learned on our dataset works well when finetuning to real-world scans. Besides, our strategy also ensures the diversity of our training samples in the aspect of viewpoints. During training, n is randomly chosen from 2048 to 6144 (25% to 75% of the complete point cloud), resulting in different level of incompleteness. We then downsample the remaining point clouds to 2048 points as the input data for models.</p><p>During evaluation, we fix 8 view points and n is set to 2048, 4096 or 6144 (25%, 50% or 75% of the whole point cloud) for convenience. According to the value of n, we divide the test samples into three difficulty degrees, simple, moderate and hard in our experiments. In the following experiments, we will report the performance for each method in simple, moderate and hard to show the ability of each network to deal with tasks at difficulty levels. In addition, we use the average performance under three difficulty degrees to report the overall performance (Avg).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>We follow the existing works <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b50">48]</ref> to use the mean Chamfer Distance as the evaluation metric, which can measure distance between the prediction point cloud and ground-truth in set-level. For each prediction, the Chamfer Distance between the prediction point set P and the groundtruth point set G is calculated by:</p><formula xml:id="formula_4">dCD(P, G) = 1 |P| p?P min g?G p ? g + 1 |G| g?G min p?P g ? p</formula><p>Following the previous methods, we use two versions of Chamfer distance as the evaluation metric to compare the performance with existing works. CD-1 uses L1-norm to <ref type="table">Table 1</ref>: Results of our methods and state-of-the-art methods on ShapeNet-55. We report the detailed results for each method on 10 categories and the overall results on 55 categories for three difficulty degrees. We use CD-S, CD-M and CD-H to represent the CD results under the Simple, Moderate and Hard settings. We also provide results under the F-Score@1% metric.  calculate the distance between two points, while CD-2 uses L2-norm instead. We also follow <ref type="bibr" target="#b38">[36]</ref> to adopt F-Score as another evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on ShapeNet-55</head><p>We first conduct experiments on ShapeNet-55, which consists of objects from 55 categories. To compare with existing methods, We implement FoldingNet <ref type="bibr" target="#b52">[50]</ref>, PCN <ref type="bibr" target="#b53">[51]</ref>, TopNet <ref type="bibr" target="#b39">[37]</ref>, PFNet <ref type="bibr" target="#b18">[16]</ref> and GRNet <ref type="bibr" target="#b50">[48]</ref> on our benchmark according to their open-source code and use the best hyper-parameters in their papers for fair comparisons. We first investigate how the existing methods and our method perform when there are objects from more categories. The last four columns in <ref type="table">Table 1</ref> show that our PoinTr can better cope with different situations with diverse viewpoints, diverse object categories, diverse incomplete patterns and diverse incompleteness levels. We achieve 0.58, 0.6 and 0.69 improvement in CD-2 (multiplied by 1000) under three settings (simple, moderate and hard) comparing with the SOTA method GRNet <ref type="bibr" target="#b50">[48]</ref>. PFNet <ref type="bibr" target="#b18">[16]</ref>, which proposes to directly predict the missing parts of objects, fail in our benchmarks due to the high diversity. We further report the performance on categories with sufficient and insufficient samples. We only sample 10 categories out from 55 categories to show the results due to the limited space, in which Table, chair, Airplane,Car and Sofa contain more than 2500 samples in the training set while Birdhouse, Bag, Remote, Keyboard and Rocket contain less than 80 samples. We also provide the detailed results for all 55 categories in our supplementary material. We place the categories with sufficient samples at the first five columns and the categories with insufficient samples in the following five columns in <ref type="table">Table 1</ref>. The average CD results for three difficulty degrees are also reported. Surprisingly, there is no obvious difference between the results on these two kinds of categories. However, except for our PoinTr and SOTA method GRNet, the imbalance in the number of training samples lead to a relatively high CD in the categories with insufficient samples. Besides, our model achieves 0.46 F-Score on ShapeNet-55, while the state-of-the-art GRNet only obtain 0.24 F-Score. These results clearly demonstrate the effectiveness of PoinTr under the more diverse setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on ShapeNet-34</head><p>On ShapeNet-34, we also conduct experiments for our method and other five state-of-the-art methods. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. For the 34 seen categories, we can see our method outperforms all the other methods. For the 21 unseen categories, we using the networks that are trained on the 34 seen categories to evaluate the performance on the novel objects from the other 21 categories that do not appear in the training phase. We see our method also achieves the best performance in this more challenging setting. Comparing with the results of seen categories, we see in the simple   setting (25% of point cloud will be removed), the performance drop of our method is less than 0.3. But when the difficulty level increases, the performance gap between seen categories and unseen categories significantly increases. We also visualize the results in <ref type="figure" target="#fig_2">Figure 4</ref> to show the effectiveness of our method on the unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on the Existing Benchmarks</head><p>Apart from the experiments on the two newly proposed challenging benchmarks, we also conduct the experiments on the existing benchmarks including the PCN dataset <ref type="bibr" target="#b53">[51]</ref>   and KITTI benchmark <ref type="bibr" target="#b12">[10]</ref>.</p><p>Results on the PCN Dataset. The PCN dataset <ref type="bibr" target="#b53">[51]</ref> is one of the most widely used benchmark datasets for the point cloud completion task. To verify the effectiveness of our method on existing benchmarks and compare it with more state-of-the-art methods, we conducted experiments on this dataset following the standard protocol and evaluation metric used in previous work <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b45">43]</ref>. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. We see our method largely improves the previous methods and establishes the new state-of-theart on this dataset.</p><p>Results on KITTI Benchmark. To show the performance of our method in real-world scenarios, we follow <ref type="bibr" target="#b50">[48]</ref> to finetune our trained model on ShapeNetCars <ref type="bibr" target="#b53">[51]</ref> and evaluate the performance of our model on KITTI dataset, which contains the incomplete point clouds of cars in the realworld scenes from LiDAR scans. We report the Fidelity and MMD metrics in <ref type="table" target="#tab_4">Table 5</ref> and show some reconstruction results in <ref type="figure">Figure 5</ref>. Our method achieves better qualitative and quantitative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Model Design Analysis</head><p>To examine the effectiveness of our designs, we conduct a detailed ablation study on the key components of PoinTr. The results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. The baseline model A is the vanilla transformer model for point cloud completion, which uses the encoder-decoder architecture with the standard transformer blocks. In this model, we form the point proxies directly from the point cloud using a singlelayer DGCNN model. We then add the query generator   between the encoder and decoder (model B). We see the query generator improve the baseline by 0.34 in Chamfer distance. When using DGCNN to extract features from the input point cloud (model C), we observe a significant im-provement to 8.69. By adding the geometric block to all the transformer blocks (model D), we see the performance can be further improved, which clearly demonstrates the effectiveness of the geometric structures learned by the block. We find that only adding the geometric block to the first transformer block in both encoder and decoder can lead to a slightly better performance (model E), which indicates the role of geometric block is to introduce the inductive bias and a single layer is sufficient while adding more blocks may result in over-fitting. Besides, we see our method can achieve over 0.74 F-Score on the PCN dataset while obtaining only 0.46 F-Score on our ShapeNet-55, which also suggests our new datatset is much more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative Results</head><p>In <ref type="figure" target="#fig_5">Figure 6</ref>, we show some completion results for all methods and find our method perform better. For example, the input data in (a) nearly lose all the geometric information and can be hardly recognized as an airplane. In this case, other methods can only roughly complete the shape with unsatisfactory geometry details, while our method can still complete the point cloud with higher fidelity. These results show our method has a stronger ability to recover details and is more robust to various incomplete patterns. More results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a new architecture, PoinTr, to convert the point cloud completion task into a set to set translation tasks. With several technical innovations, we successfully applied the transformer model to this task and achieved state-of-the-art performance. Moreover, we proposed two more challenging benchmarks for more diverse point cloud completion. Extending our transformer architecture to other 3D tasks can an interesting future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Our proposed method PointTr is implemented with Py-Torch <ref type="bibr" target="#b28">[26]</ref>. We utilize AdamW optimizer <ref type="bibr" target="#b23">[21]</ref> to train the network with initial learning rate as 0.0005 and weight decay as 0.0005. In all of our experiments, we set the depth of the encoder and decoder in our transformer to 6 and 8 and set k of kNN operation to 16 and 8 for the DGCNN feature extractor and the geometry-aware block respectively. We use 6 head attention for all transformer blocks and set their hidden dimensions to 384. On the PCN dataset, the network takes 2048 points as inputs and is required to complete the other 14336 points. We set the batch size to 54 and train the model for 300 epochs with the continuous learning rate decay of 0.9 for every 20 epochs. We set N to 128 and M to 224. On ShapeNet-55/34, the model takes 2048 points as inputs and is required to complete the other 6144 points. We set the batch size to 128 and train the model for 200 epochs with the continuous learning rate decay of 0.76 for every 20 epochs. We set N to 128 and M to 96.</p><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Technical Details on Transformers</head><p>Encoder-Decoder Architecture. The overall architecture of the transformer encoder-decoder networks is illustrated in <ref type="figure" target="#fig_7">Figure 7</ref>. The point proxies are passed through the transformer encoder with N multi-head self-attention layers and feed-forward network layers. Then, the decoder receives the generated query embeddings and encoder memory, and produces the final set of predicted point proxies that represents the missing part of the point cloud through N multihead self-attention layers, decoder-encoder attention layers and feed-forward network layers. We set N to 6 in all our experiments following common practice <ref type="bibr" target="#b41">[39]</ref>.</p><p>Multi-head Attention. Multi-head attention mechanism allows the network to jointly attend to information from different representation subspaces at different positions <ref type="bibr" target="#b41">[39]</ref>. Speciacally, given the input values V , keys K and queries Q, the multi-head attention is computed by: where W O the weights of the output linear layer and each head feature can be obtained by:</p><formula xml:id="formula_5">MultiHead(Q, K, V ) = W O Concat(head 1 , ..., head h ),</formula><formula xml:id="formula_6">head i = softmax( QW Q i (KW K i ) T ? d k )V W V i where W Q i , W K i and W V i</formula><p>are the linear layers that project the inputs to different subspaces and d k is the dimension of the input features.</p><p>Feed-forward network (FFN). Following <ref type="bibr" target="#b41">[39]</ref>, we use two linear layers with ReLU activations and dropout as the feed-forward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed Experimental Results</head><p>Detailed results on ShapeNet-55: In <ref type="table">Table 6</ref>, we report the detailed results for FoldingNet <ref type="bibr" target="#b52">[50]</ref>, PCN <ref type="bibr" target="#b53">[51]</ref>, Top-Net <ref type="bibr" target="#b39">[37]</ref>, PFNet <ref type="bibr" target="#b18">[16]</ref>, GRNet <ref type="bibr" target="#b50">[48]</ref> and the proposed method on ShapeNet-55. Each row in the table stands for a category of object. We test each method under three settings: simple, moderate and hard.</p><p>Detailed results on ShapeNet-34: In <ref type="table">Table 7</ref>, we report the detailed results for the novel objects from 21 categories in ShapeNet-34. Each row in the table stands for a category of object. We test each method under the three settings: simple, moderate and hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Complexity Analysis</head><p>Our method achieves the best performance on both our newly proposed diverse benchmarks and the existing benchmarks. We provide the detailed complexity analysis of our method in <ref type="table" target="#tab_6">Table 8</ref>. We report the number of parameters and theoretical computation cost (FLOPs) of our method and airplane 1. <ref type="bibr" target="#b38">36</ref>  other five methods. We also provide the average Chamfer distances of all categories in ShapeNet-55 and unseen categories in ShapeNet34 as references. We can see our method achieves the best performance while using relatively low parameters and FLOPs among the methods in the table, which shows our method offers a decent trade-off between cost and performance.  we show the input partial point clouds and the predicted centers. Based on predicted point proxies, we can easily predicted the accurate point centers and then complete the point clouds, as shown in Line (b). We show the ground-truth point cloud in Line (c) for comparisons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Comparisons of the vanilla transformer block and the proposed geometry-aware transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Point cloud completion results on some objects from novel categories. We show the input point cloud and the ground truth as well as the predictions of GRNet and our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 Figure 5 :</head><label>25</label><figDesc>Qualitative results on the KITTI dataset. In order to better show the shape of the car, we provide two views of the same point cloud in each case. Our method can recover the complete point cloud of a car with more accurate boundaries and details (e.g. tires of cars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F o l d i n</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on ShapeNet-55. All methods above takes the point clouds in the first line as inputs and generate complete point clouds. Our methods can complete the point clouds with higher fidelity, which clearly shows the effectiveness of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>employ a lightweight DGCNN [44] model to extract the point proxy features. To reduce the computational cost, we hierarchically downsample the original input point cloud to N = 128 center points and use several DGCNN layers to capture local geometric relationships. The detailed network architecture is: Linear(C in = 3, C out = 8) ? DGCNN(C in = 8, C out = 32, K = 8, N out = 2048) ? DGCNN(C in = 32, C out = 64, K = 8, N out = 512) ? DGCNN(C in = 64, C out = 64, K = 8, N out = 512) ? DGCNN(C in = 64, C out = 128, K = 8, N out = 128), where C in and C out are the numbers of channels of input and output features, N out is the number of points after FPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The overall architecture of the transformer encoderdecoder networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of predicted points proxies. In Line (a),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table Chair</head><label>Chair</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Airplane Car Sofa</cell><cell>Bird house</cell><cell>Bag Remote</cell><cell>Key board</cell><cell>Rocket CD-S CD-M CD-H CD-Avg F1</cell></row><row><cell cols="2">FoldingNet [50] 2.53 2.81</cell><cell>1.43</cell><cell cols="3">1.98 2.48 4.71 2.79 1.44</cell><cell cols="2">1.24 1.48 2.67 2.66 4.05</cell><cell>3.12 0.082</cell></row><row><cell>PCN [51]</cell><cell>2.13 2.29</cell><cell>1.02</cell><cell cols="3">1.85 2.06 4.50 2.86 1.33</cell><cell cols="2">0.89 1.32 1.94 1.96 4.08</cell><cell>2.66 0.133</cell></row><row><cell>TopNet [37]</cell><cell>2.21 2.53</cell><cell>1.14</cell><cell cols="3">2.18 2.36 4.83 2.93 1.49</cell><cell cols="2">0.95 1.32 2.26 2.16</cell><cell>4.3</cell><cell>2.91 0.126</cell></row><row><cell>PFNet [16]</cell><cell>3.95 4.24</cell><cell>1.81</cell><cell cols="3">2.53 3.34 6.21 4.96 2.91</cell><cell cols="2">1.29 2.36 3.83 3.87 7.97</cell><cell>5.22 0.339</cell></row><row><cell>GRNet [48]</cell><cell>1.63 1.88</cell><cell>1.02</cell><cell cols="3">1.64 1.72 2.97 2.06 1.09</cell><cell cols="2">0.89 1.03 1.35 1.71 2.85</cell><cell>1.97 0.238</cell></row><row><cell>PoinTr</cell><cell>0.81 0.95</cell><cell>0.44</cell><cell cols="3">0.91 0.79 1.86 0.93 0.53</cell><cell cols="2">0.38 0.57 0.58 0.88 1.79</cell><cell>1.09 0.464</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of our methods and state-of-the-art methods on ShapeNet-34. We report the results of 34 seen categories and 21 unseen categories in three difficulty degrees. We use CD-S, CD-M and CD-H to represent the CD results under the Simple, Moderate and Hard settings. We also provide results under the F-Score@1% metric.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">34 seen categories</cell><cell></cell><cell></cell><cell cols="3">21 unseen categories</cell><cell></cell></row><row><cell></cell><cell>CD-S</cell><cell>CD-M</cell><cell>CD-H</cell><cell>CD-Avg</cell><cell>F1</cell><cell>CD-S</cell><cell>CD-M</cell><cell>CD-H</cell><cell>CD-Avg</cell><cell>F1</cell></row><row><cell>FoldingNet [50]</cell><cell>1.86</cell><cell>1.81</cell><cell>3.38</cell><cell>2.35</cell><cell>0.139</cell><cell>2.76</cell><cell>2.74</cell><cell>5.36</cell><cell>3.62</cell><cell>0.095</cell></row><row><cell>PCN [51]</cell><cell>1.87</cell><cell>1.81</cell><cell>2.97</cell><cell>2.22</cell><cell>0.154</cell><cell>3.17</cell><cell>3.08</cell><cell>5.29</cell><cell>3.85</cell><cell>0.101</cell></row><row><cell>TopNet [37]</cell><cell>1.77</cell><cell>1.61</cell><cell>3.54</cell><cell>2.31</cell><cell>0.171</cell><cell>2.62</cell><cell>2.43</cell><cell>5.44</cell><cell>3.50</cell><cell>0.121</cell></row><row><cell>PFNet [16]</cell><cell>3.16</cell><cell>3.19</cell><cell>7.71</cell><cell>4.68</cell><cell>0.347</cell><cell>5.29</cell><cell>5.87</cell><cell>13.33</cell><cell>8.16</cell><cell>0.322</cell></row><row><cell>GRNet [48]</cell><cell>1.26</cell><cell>1.39</cell><cell>2.57</cell><cell>1.74</cell><cell>0.251</cell><cell>1.85</cell><cell>2.25</cell><cell>4.87</cell><cell>2.99</cell><cell>0.216</cell></row><row><cell>PoinTr</cell><cell>0.76</cell><cell>1.05</cell><cell>1.88</cell><cell>1.23</cell><cell>0.421</cell><cell>1.04</cell><cell>1.67</cell><cell>3.44</cell><cell>2.05</cell><cell>0.384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the PCN dataset. We use the L1 Chamfer Distance to compare with other methods. ] 14.31 9.49 15.80 12.61 15.55 16.41 15.97 13.65 14.99 AtlasNet [13] 10.85 6.37 11.94 10.10 12.06 12.37 12.99 10.33 10.61 PCN [51] 9.64 5.50 22.70 10.63 8.70 11.00 11.34 11.68 8.59 TopNet [37] 12.15 7.61 13.31 10.90 13.82 14.44 14.78 11.22 11.</figDesc><table><row><cell cols="2">CD-1 (? 1000) Avg</cell><cell>Air Cab</cell><cell>Car</cell><cell>Cha Lam</cell><cell>Sof</cell><cell>Tab</cell><cell>Wat</cell></row><row><cell cols="8">FoldingNet [5012</cell></row><row><cell>MSN [17]</cell><cell cols="5">10.0 5.6 11.9 10.3 10.2 10.7 11.6</cell><cell>9.6</cell><cell>9.9</cell></row><row><cell>GRNet [48]</cell><cell cols="7">8.83 6.45 10.37 9.45 9.41 7.96 10.51 8.44 8.04</cell></row><row><cell>PMP-Net [45]</cell><cell cols="7">8.73 5.65 11.24 9.64 9.51 6.95 10.83 8.72 7.25</cell></row><row><cell>CRN [43]</cell><cell cols="7">8.51 4.79 9.97 8.31 9.49 8.94 10.69 7.81 8.05</cell></row><row><cell>PoinTr</cell><cell cols="7">8.38 4.75 10.47 8.68 9.39 7.75 10.93 7.78 7.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the PCN dataset. We investigate dif-</figDesc><table><row><cell cols="4">ferent designs including query generator (Query), DGCNN feature</cell></row><row><cell cols="4">extractor (DGCNN) and Geometry-aware Blocks (Geometry).</cell></row><row><cell cols="4">Model Query DGCNN Geometry CD-1 F-Score@1%</cell></row><row><cell>A</cell><cell></cell><cell>9.43</cell><cell>67.82</cell></row><row><cell>B</cell><cell></cell><cell>9.09</cell><cell>0.713</cell></row><row><cell>C</cell><cell></cell><cell>8.69</cell><cell>0.736</cell></row><row><cell>D</cell><cell>all</cell><cell>8.44</cell><cell>0.741</cell></row><row><cell>E</cell><cell>1 st</cell><cell>8.38</cell><cell>0.745</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on LiDAR scans from KITTI dataset under the Fidelity and MMD metrics. CD-2 (? 1000) AtlasNet [13] PCN [51] FoldingNet [50] TopNet [37] MSN [17] NSFA [52] PFNet [16] CRN [43] GRNet [48] PoinTr</figDesc><table><row><cell>Fidelity ?</cell><cell>1.759</cell><cell>2.235</cell><cell>7.467</cell><cell>5.354</cell><cell>0.434</cell><cell>1.281</cell><cell>1.137</cell><cell>1.023</cell><cell>0.816</cell><cell>0.000</cell></row><row><cell>MMD ?</cell><cell>2.108</cell><cell>1.366</cell><cell>0.537</cell><cell>0.636</cell><cell>2.259</cell><cell>0.891</cell><cell>0.792</cell><cell>0.872</cell><cell>0.568</cell><cell>0.526</cell></row><row><cell>( a )</cell><cell>( b )</cell><cell>( c )</cell><cell>( d )</cell><cell>( e )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I n p u t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1.28 1.7 0.9 0.89 1.32 1.02 0.99 1.48 1.35 1.44 2.69 0.87 0.87 1.27 0.27 0.38 0.69 trash bin 2.93 2.9 5.03 2.16 2.18 5.15 2.51 2.32 5.03 4.03 3.39 9.63 1.69 2.01 3.48 0.8 1.15 2.15 .24 3.25 1.56 1.66 3.26 1.84 1.85 3.48 2.27 2.83 5.52 1.13 1.38 2.29 0.48 0.67 1.33 earphone 6.37 6.48 9.14 3.13 2.94 7.56 4.36 4.47 8.36 15.07 17.5 33.37 1.78 2.18 5.33 0.81 1.38 3.78 faucet 4.46 4.39 7.2 3.21 3.48 7.52 3.61 3.59 7.25 5.68 6.79 14.29 1.81 2.32 4.91 0.71 1.42 3.49 filecabinet 2.59 2.48 3.76 2.02 1.97 4.14 2.41 2.12 4.12 3.72 3.57 7.13 1.46 1.71 2.89 0.63 0.84 1.69 guitar 0.65 0.6 1.25 0.42 0.38 1.23 0.57 0.47 1.42 0.74 0.89 5.41 0.44 0.48 0.76 0.14 0.21 0.42 helmet 5.39 5.37 7.96 3.76 4.18 7.53 4.36 4.55 7.73 9.55 8.41 15.44 2.33 3.18 6.03 0.99 1.93 4.22 jar 3.65 3.87 6.51 2.57 2.82 6.0 3.03 3.17 7.03 5.44 5.56 11.87 1.72 2.37 4.37 0.77 1.33 2.87 .09 3.14 1.65 1.61 2.92 1.93 1.76 3.39 2.65 2.53 4.84 1.35 1.45 2.32 0.56 0.67 1.14 stove 2.69 2.63 3.99 2.07 2.02 4.72 2.44 2.16 4.84 4.03 3.71 7.15 1.46 1.72 3.22 0.63 0.92 1.73 table 2.23 2.15 3.21 1.56 1.5 3.36 1.78 1.65 3.21 3.03 3.11 5.74 1.15 1.33 2.33 0.46 0.64 1.31 .69 2.49 1.46 1.39 2.4 1.53 1.42 2.67 2.1 2.13 4.58 1.09 1.12 1.65 0.41 0.62 1.07 washer 3.47 3.2 4.89 2.42 2.31 6.08 2.92 2.53 6.53 5.55 4.11 7.04 1.72 2.05 4.19 0.75 1.06 2.44 mean 2.68 2.66 4.06 1.96 1.98 4.09 2.26 2.17 4.31 3.84 3.88 8.03 1.35 1.63 2.86 0.58 0.88 1.8 bag 2.15 2.27 3.99 2.48 2.46 3.94 2.08 1.95 4.36 3.88 4.42 9.67 1.47 1.88 3.45 0.96 1.34 2.08 basket 2.37 2.2 4.87 2.79 2.51 4.78 2.46 2.11 5.18 4.47 4.55 14.46 1.78 1.94 4.18 1.04 1.4 2.9 birdhouse 3.27 3.15 5.62 3.53 3.47 5.31 3.17 2.97 5.89 .98 11.49 7.21 7.14 10.94 4.68 4.23 9.17 14.11 14.86 28.23 3.29 4.87 13.02 1.4 2.74 8.35 .96 12.77 7.88 6.59 16.53 5.33 4.83 11.67 20.31 23.21 39.49 4.29 4.16 10.3 2.03 5.1 10.69 helmet 4.86 5.04 8.86 6.15 6.41 9.16 4.89 4.86 8.73 8.78 10.07 21.2 3.06 4.38 10.27 1.</figDesc><table><row><cell>bag basket bathtub bed bench birdhouse bookshelf bottle bowl bus cabinet camera can cap car cellphone chair clock keyboard dishwasher display 2.15 2knife 2.31 2.38 3.67 2.11 2.04 4.44 2.36 2.23 4.21 3.63 3.66 2.98 2.77 4.8 2.21 2.1 4.55 2.62 2.43 5.71 4.74 3.88 8.47 1.65 1.84 3.15 0.73 0.88 1.82 7.6 1.41 1.7 2.97 0.53 0.74 1.51 2.68 2.66 4.0 2.11 2.09 3.94 2.49 2.25 4.33 3.64 3.5 5.74 1.46 1.73 2.73 0.64 0.94 1.68 4.24 4.08 5.65 2.86 3.07 5.54 3.13 3.1 5.71 4.44 5.36 9.14 1.64 2.03 3.7 0.76 1.1 2.26 1.94 1.77 2.36 1.31 1.24 2.14 1.56 1.39 2.4 2.17 2.16 4.11 1.03 1.09 1.71 0.38 0.52 0.94 4.06 4.18 5.88 3.29 3.53 6.69 3.73 3.98 6.8 3.96 5.0 9.66 1.87 2.4 4.71 0.98 1.49 3.13 3.04 3.03 3.91 2.7 2.7 4.61 3.11 2.87 4.87 3.19 3.47 5.72 1.42 1.71 2.78 0.71 1.06 1.93 1.7 1.91 4.02 1.25 1.43 4.61 1.56 1.66 4.02 2.37 2.89 10.03 1.05 1.44 2.67 0.37 0.74 1.5 2.79 2.6 4.23 2.05 1.83 3.66 2.33 1.98 4.82 4.3 3.97 8.76 1.6 1.77 2.99 0.68 0.78 1.44 1.47 1.42 2.0 1.2 1.14 2.08 1.32 1.21 2.29 2.06 1.88 3.75 1.06 1.16 1.48 0.42 0.55 0.79 2.0 1.86 2.79 1.6 1.49 3.47 1.91 1.65 3.36 2.72 2.37 4.73 1.27 1.41 2.09 0.55 0.66 1.16 5.5 6.04 8.87 4.05 4.54 8.27 4.75 4.98 9.24 6.57 8.04 13.11 2.14 3.15 6.09 1.1 2.03 4.34 2.84 2.68 5.71 2.02 2.28 6.48 2.67 2.4 5.5 5.65 4.05 16.29 1.58 2.11 3.81 0.68 1.19 2.14 4.1 4.04 5.87 1.82 1.76 4.2 3.0 2.69 5.59 10.92 9.04 20.3 1.17 1.37 3.05 0.46 0.62 1.64 1.81 1.81 2.31 1.48 1.47 2.6 1.71 1.65 3.17 2.06 2.1 3.43 1.29 1.48 2.14 0.64 0.86 1.25 1.04 1.06 1.87 0.8 0.79 1.71 1.01 0.96 1.8 1.25 1.37 3.65 0.82 0.91 1.18 0.32 0.39 0.6 2.37 2.46 3.62 1.7 1.81 3.34 1.97 2.04 3.59 2.94 3.48 6.34 1.24 1.56 2.73 0.49 0.74 1.63 2.56 2.41 3.46 2.1 2.01 3.98 2.48 2.16 4.03 3.15 3.27 6.03 1.46 1.66 2.67 0.62 0.84 1.65 1.21 1.18 1.32 0.82 0.82 1.04 0.88 0.83 1.15 0.83 1.06 1.97 0.74 0.81 1.09 0.3 0.39 0.45 2.6 2.17 3.5 1.93 1.66 4.39 2.43 1.74 4.64 4.57 3.23 6.39 1.43 1.59 2.53 0.55 0.69 1.42 1.29 0.87 1.21 0.94 0.62 1.37 0.84 0.68 1.44 2.11 1.53 3.89 0.72 0.66 0.96 0.2 0.33 0.56 lamp 3.93 4.23 6.87 3.1 3.45 7.02 3.03 3.39 8.15 6.82 7.61 14.22 1.68 2.43 5.17 0.64 1.4 3.58 laptop 1.02 1.04 1.96 0.75 0.79 1.59 0.8 0.85 1.66 1.04 1.21 2.46 0.83 0.87 1.28 0.32 0.34 0.6 loudspeaker 3.21 3.15 4.55 2.5 2.45 5.08 3.1 2.76 5.32 4.32 4.19 7.6 1.75 2.08 3.45 0.78 1.16 2.17 mailbox 2.44 2.61 4.98 1.66 1.74 5.18 2.16 2.1 5.1 3.82 4.2 10.51 1.15 1.59 3.42 0.39 0.78 2.56 microphone 4.42 5.06 7.04 3.44 3.9 8.52 2.83 3.49 6.87 6.58 7.56 16.74 2.09 2.76 5.7 0.7 1.66 4.48 microwaves 2.67 2.48 4.43 2.2 2.01 4.65 2.65 2.15 5.07 4.63 3.94 6.52 1.51 1.72 2.76 0.67 0.83 1.82 motorbike 2.63 2.55 3.52 2.03 2.01 3.13 2.29 2.25 3.54 2.17 2.48 5.09 1.38 1.52 2.26 0.75 1.1 1.92 mug 3.66 3.67 5.7 2.45 2.48 5.17 2.89 2.56 5.43 4.76 4.3 8.37 1.75 2.16 3.79 0.91 1.17 2.35 piano 3.86 4.04 6.04 2.64 2.74 4.83 2.99 2.89 5.64 4.57 5.26 9.26 1.53 1.82 3.21 0.76 1.06 2.23 pillow 2.33 2.38 3.87 1.85 1.81 3.68 2.31 2.26 4.19 4.21 3.82 7.89 1.42 1.67 3.04 0.61 0.82 1.56 pistol 1.92 1.62 2.52 1.25 1.17 2.65 1.5 1.3 2.62 2.27 2.09 7.2 1.11 1.06 1.76 0.43 0.66 1.3 flowerpot 4.53 4.68 6.46 3.32 3.39 6.04 3.61 3.45 6.28 4.83 5.51 10.68 2.02 2.48 4.19 1.01 1.51 2.77 printer 3.66 4.01 5.34 2.9 3.19 5.84 3.04 3.19 5.84 5.56 6.06 9.29 1.56 2.38 4.24 0.73 1.21 2.47 remote 1.14 1.2 1.98 0.99 0.97 2.04 1.14 1.17 2.16 1.74 2.37 4.61 0.89 1.05 1.29 0.36 0.53 0.71 rifle 1.27 1.02 1.37 0.98 0.8 1.31 0.98 0.86 1.46 1.72 1.45 3.02 0.83 0.77 1.16 0.3 0.45 0.79 rocket 1.37 1.18 1.88 1.05 1.04 1.87 1.04 1.0 1.93 1.65 1.61 3.82 0.78 0.92 1.44 0.23 0.48 0.99 skateboard 1.58 1.58 2.07 1.04 0.94 1.68 1.08 1.05 1.84 1.43 1.6 3.09 0.82 0.87 1.24 0.28 0.38 0.62 sofa 2.22 2telephone 1.07 1.06 1.75 0.8 0.8 1.67 1.02 0.95 1.78 1.3 1.47 3.37 0.81 0.89 1.18 0.31 0.38 0.59 tower 2.46 2.45 3.91 1.91 1.97 4.47 2.15 2.05 4.51 3.13 3.54 9.87 1.26 1.69 3.06 0.55 0.9 1.95 train 1.86 1.68 2.32 1.5 1.41 2.37 1.59 1.44 2.51 2.01 2.03 4.1 1.09 1.14 1.61 0.5 0.7 1.12 watercraft 4.65 9.88 1.89 2.34 5.16 1.22 1.79 3.45 bowl 2.61 2.3 4.55 2.66 2.35 3.97 2.46 2.16 4.84 4.35 5.0 14.59 1.77 1.97 3.9 1.05 1.32 2.4 camera 4.4 4.78 7.85 4.84 5.3 8.03 4.24 4.43 8.11 6.78 8.04 13.91 2.31 3.38 7.2 1.63 2.67 4.97 can 1.95 1.73 5.86 1.95 1.89 5.21 2.02 1.7 5.82 2.95 3.47 23.02 1.53 1.8 3.08 0.8 1.17 2.85 cap 6.07 5keyboard 0.98 0.96 1.35 1.07 1.0 1.23 0.79 0.77 1.55 1.13 1.16 2.58 0.73 0.77 1.11 0.43 0.45 0.63 dishwasher 2.09 1.8 4.55 2.45 2.09 3.53 2.51 1.77 4.72 3.44 3.78 9.31 1.79 1.7 3.27 0.93 1.05 2.04 earphone 6.86 686 3.3 6.96 mailbox 2.2 2.29 4.49 2.74 2.68 4.31 2.35 2.2 4.91 5.2 5.33 10.94 1.52 1.9 4.33 1.03 1.47 3.34 microphone 2.92 3.27 8.54 4.36 4.65 8.46 3.03 3.2 7.15 6.39 7.99 19.41 2.29 3.23 8.41 1.25 2.27 5.47 microwaves 2.29 2.12 5.17 2.59 2.35 4.47 2.67 2.12 5.41 3.89 4.08 9.01 1.74 1.81 3.82 1.01 1.18 2.14 pillow 2.07 2.11 3.73 2.09 2.16 3.54 2.08 2.05 4.01 4.15 4.29 12.01 1.43 1.69 3.43 0.92 1.24 2.39 printer 3.02 3.23 5.53 3.28 3.6 5.56 2.9 2.96 6.07 5.38 5.94 10.29 1.82 2.41 5.09 1.18 1.76 3.1 remote 0.89 0.92 1.85 0.95 1.08 1.58 0.89 0.89 2.28 1.51 1.75 6.0 0.82 1.02 1.29 0.44 0.58 0.78 rocket 1.28 1.09 2.0 1.39 1.22 2.01 1.14 0.96 2.03 1.84 1.51 4.01 0.97 0.79 1.6 0.39 0.72 1.39 skateboard 1.53 1.42 1.99 1.97 1.78 2.45 1.23 1.2 2.01 2.43 2.53 4.25 0.93 1.07 1.83 0.52 0.8 1.31 tower 2.25 2.25 4.74 2.37 2.4 4.35 2.2 2.17 5.47 3.38 4.15 13.11 1.35 1.8 3.85 0.82 1.35 2.48 washer 2.58 2.34 5.5 2.77 2.52 4.64 2.63 2.14 6.57 4.53 4.27 9.23 1.83 1.97 5.28 1.04 1.39 2.73 1.85 13.9 mean 2.79 2.77 5.49 3.22 3.13 5.43 2.65 2.46 5.52 5.37 5.95 13.55 1.84 2.23 4.95 1.05 1.67 3.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Complexity analysis. We report the the number of parameter (Params) and theoretical computation cost (FLOPs) of our method and five existing methods. We also provide the average Chamfer distances of all categories in ShapeNet-55 (CD 55 ) and unseen categories in ShapeNet34 (CD 34 ) as references.</figDesc><table><row><cell>Models</cell><cell cols="2">Params FLOPs CD55 CD34</cell></row><row><cell cols="2">FoldingNet [50] 2.30 M 27.58 G</cell><cell>3.12 3.62</cell></row><row><cell>PCN [51]</cell><cell>5.04 M 15.25 G</cell><cell>2.66 3.85</cell></row><row><cell>TopNet [37]</cell><cell>5.76 M 6.72 G</cell><cell>2.91 3.50</cell></row><row><cell>PFNet [16]</cell><cell>73.05 M 4.96 G</cell><cell>5.22 8.16</cell></row><row><cell>GRNet [48]</cell><cell>73.15 M 40.44 G</cell><cell>1.97 2.99</cell></row><row><cell>PoinTr</cell><cell>30.9 M 10.41 G</cell><cell>1.07 2.05</cell></row><row><cell cols="3">E. Visualization of the Predicted Centers</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, and Grant U1713214, in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.</p><p>We visualize the local center prediction results on ShapeNet-55. We adopt a coarse-to-fine strategy to recover the point cloud. Our method starts with the prediction of local centers, then we can obtain the final results by adding the points around the centers. As shown in <ref type="figure">Figure 8</ref>, Line (a) shows the input partial point cloud and the predicted point centers. Line (b) is the predicted point clouds. We see the predicted point proxies can successfully represent the overall structure of the point cloud and the details then are added in the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>In <ref type="figure">Figure 9</ref>, we provide more qualitative results on ShapeNet-55. We see our results are much better than baseline methods visually.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>CD-2 (? 1000) FoldingNet [50] PCN [51</idno>
		<title level="m">Table 6: Detailed results on ShapeNet-55. S., M. and H. stand for the simple, moderate and hard settings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grnet</surname></persName>
			<affiliation>
				<orgName type="collaboration">Ours-PoinTr S. M. H. S. M. H. S. M. H. S. M. H. S. M. H. S. M. H</orgName>
			</affiliation>
		</author>
		<imprint>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Table 7: Detailed results for the novel objects on ShapeNet-34. S., M. and H. stand for the simple, moderate and hard settings</title>
		<imprint/>
	</monogr>
	<note>? 1000</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6545" to="6554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Vision meets robotics: The kitti dataset. international journal of robotics research (ijrr)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Atlasnet: A papierm?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A papier-m?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pf-net: Point fractal network for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00280</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11596" to="11603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin Fan, Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointvoxel CNN for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="963" to="973" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for librispeech: Hybrid vs attention -w/o data augmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>L?scher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dense 3d point cloud reconstruction using a deep pyramid network. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A field model for repairing 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Duc Thanh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5676" to="5684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<idno>abs/1802.05751</idno>
	</analytic>
	<monogr>
		<title level="j">Image transformer. CoRR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI blog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02034</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sarmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Hyunjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5898" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Vconv-dae: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno>abs/1604.03755</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning 3d shape completion from laser scan data with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1955" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<title level="m">Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">What do single-view 3d reconstruction networks learn? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1905.03678</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaqu?n</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2442" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">O-CNN: octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shape inpainting using 3d generative adversarial network and recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2298" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="790" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TOG</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pmp-net: Point cloud completion by learning multi-step point moving paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Pei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d object reconstruction from a single depth view with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Foldingnet: Interpretable unsupervised learning on 3d point clouds. CoRR, abs/1712.07262</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PCN: point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Detail preserved point cloud completion via separated feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">More qualitative results on ShapeNet-55</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
