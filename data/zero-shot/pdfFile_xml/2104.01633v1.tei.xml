<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection &quot;MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection Bib: @inproceedings{feng2021mist, title={MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection} MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">year={2021}</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>author={Feng</roleName><forename type="first">Jia-Chang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi}</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Pazhou Lab</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection &quot;MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection Bib: @inproceedings{feng2021mist, title={MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection} MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition. 2021. booktitle={Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition}</title>
						<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition. 2021. booktitle={ the IEEE International Conference on Computer Vision and Pattern Recognition}						</meeting>
						<imprint>
							<date type="published" when="2021">year={2021}</date>
						</imprint>
					</monogr>
					<note>For reference of this work, please cite:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised video anomaly detection (WS-VAD) is to distinguish anomalies from normal events based on discriminative representations. Most existing works are limited in insufficient video representations. In this work, we develop a multiple instance self-training framework (MIST) to efficiently refine task-specific discriminative representations with only video-level annotations. In particular, MIST is composed of 1) a multiple instance pseudo label generator, which adapts a sparse continuous sampling strategy to produce more reliable clip-level pseudo labels, and 2) a self-guided attention boosted feature encoder that aims to automatically focus on anomalous regions in frames while extracting task-specific representations. Moreover, we adopt a self-training scheme to optimize both components and finally obtain a task-specific feature encoder. Extensive experiments on two public datasets demonstrate the efficacy of our method, and our method performs comparably to or even better than existing supervised and weakly supervised methods, specifically obtaining a frame-level AUC 94.83% on ShanghaiTech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video anomaly detection (VAD) aims to temporally or spatially localize anomalous events in videos <ref type="bibr" target="#b36">[37]</ref>. As increasingly more surveillance cameras are deployed, VAD is playing an increasingly important role in intelligent surveillance systems to reduce the manual work of live monitoring.</p><p>Although VAD has been researched for years, developing a model to detect anomalies in videos remains challenging, as it requires the model to understand the inherent differences between normal and abnormal events, especially anomalous events that are rare and vary substantially. Previous works treat VAD as an unsupervised learning task * Corresponding author  <ref type="figure">Figure 1</ref>: Our proposed MIST first assign clip-level pseudo labels? a = {? a i } to anomaly videos with the help of a pseudo label generator G. Then, MIST leverages information from all videos to refine a self-guided attention boosted feature encoder E SGA . <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref> , which encodes the usual pattern with only normal training samples, and then detects the distinctive encoded patterns as anomalies. Here, we aim to address the weakly supervised video anomaly detection (WS-VAD) problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref> because obtaining video-level labels is more realistic and can produce more reliable results than unsupervised methods. More specifically, existing methods in WS-VAD can be categorized into two classes, i.e. encoder-agnostic and encoder-based methods.</p><p>The encoder-agnostic methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref> utilize taskagnostic features of videos extracted from a vanilla feature encoder denoted as E (e.g. C3D <ref type="bibr" target="#b23">[24]</ref> or I3D <ref type="bibr" target="#b1">[2]</ref>) to estimate anomaly scores. The encoder-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35]</ref> train both the feature encoder and classifier simultaneously. The state-of-the-art encoder-based method is Zhong et al. <ref type="bibr" target="#b34">[35]</ref>, which formulates WS-VAD as a label noise learning problem and learns from the noisy labels filtered by a label noise cleaner network. However, label noise results from assigning video-level labels to each clip. Even though the cleaner network corrects some of the noisy labels in the time-consuming iterative optimization, the refinement of representations progresses slowly as these models are mistaught by seriously noisy pseudo labels at the beginning.</p><p>We find that the existing methods have not considered training a task-specific feature encoder efficiently, which offers discriminative representations for events under surveillance cameras. To overcome this problem for WS-VAD, we develop a two-stage self-training procedure <ref type="figure">(Figure 1</ref>) that aims to train a task-specific feature encoder with only video-level weak labels. In particular, we propose a Multiple Instance Self-Training framework (MIST) that consists of a multiple instance pseudo label generator and a selfguided attention boosted feature encoder E SGA . 1) MILpseudo label generator. The MIL framework is well verified in weakly supervised learning. MIL-based methods can generate pseudo labels more accurately than those simply assigning video-level labels to each clip <ref type="bibr" target="#b34">[35]</ref>. Moreover, we adopt a sparse continuous sampling strategy that can force the network to pay more attention to context around the most anomalous part. 2) Self-guided attention boosted feature encoder. Anomalous events in surveillance videos may occur in any place and with any size <ref type="bibr" target="#b12">[13]</ref>, while in commonly used action recognition videos, the action usually appears with large motion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Therefore, we utilize the proposed self-guided attention module in our proposed feature encoder to emphasize the anomalous regions without any external annotation <ref type="bibr" target="#b12">[13]</ref> but clip-level annotations of normal videos and clip-level pseudo labels of anomalous videos. For our WS-VAD modelling, we introduce a deep MIL ranking loss to effectively train the multiple instance pseudo label generator. In particular, for deep MIL ranking loss, we adopt a sparse-continuous sampling strategy to focus more on the context around the anomalous instance.</p><p>To obtain a task-specific feature encoder with smaller domain-gap, we introduce an efficient two-stage selftraining scheme to optimize the proposed framework. We use the features extracted from the original feature encoder to produce its corresponding clip-level pseudo labels for anomalous videos by the generator G. Then, we adopt these pseudo labels and their corresponding abnormal videos as well as normal videos to refine our improved feature encoder E SGA (as demonstrated in <ref type="figure">Figure 1</ref>). Therefore, we can acquire a task-specific feature encoder that provides discriminative representations for surveillance videos.</p><p>The extensive experiments based on two different feature encoders, i.e. C3D <ref type="bibr" target="#b23">[24]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref> show that our frame-work MIST is able to produce a task-specific feature encoder.</p><p>We also compare the proposed framework with other encoder-agnostic methods on two large datasets i.e. , UCF-Crime <ref type="bibr" target="#b22">[23]</ref> and ShanghaiTech <ref type="bibr" target="#b16">[17]</ref>. In addition, we run ablation studies to evaluate our proposed sparse continuous sampling strategy and self-guided attention module. We also illustrate some visualized results to provide a more intuitive understanding of our approach. Our experiments demonstrate the effectiveness and efficiency of MIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Weakly supervised video anomaly detection. VAD aims to detect anomaly events in a given video and has been researched for years <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>. Unsupervised learning methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5]</ref> encode the usual pattern with only normal training samples and then detect the distinctive encoded patterns as anomalies. Weakly supervised learning methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref> with video-level labels are more applicable to distinguish abnormal events and normal events. Existing weakly supervised VAD methods can be categorized into two classes, i.e. , encoder-agnostic methods and encoderbased methods. 1) Encoder-agnostic methods train only the classifier. Sultani et al. <ref type="bibr" target="#b22">[23]</ref> proposed a deep MIL ranking framework to detect anomalies; Zhang et al. <ref type="bibr" target="#b31">[32]</ref> further introduced inner-bag score gap regularization; Wan et al. <ref type="bibr" target="#b26">[27]</ref> introduced dynamic MIL loss and center-guided regularization. 2) Encoder-based methods train both a feature encoder and a classifier. Zhu et al. <ref type="bibr" target="#b37">[38]</ref> proposed an attention based MIL model combined with a optical flow based autoencoder to encode motion-aware features. Zhong et al. <ref type="bibr" target="#b34">[35]</ref> took weakly supervised VAD as a label noise learning task and proposed GCNs to filter label noise for iterative model training, but the iterative optimization was inefficient and progressed slowly. Some works focus on detecting anomalies in an offline manner <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> or a coarse-grained manner <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>, which do not meet the real-time monitoring requirements for real-world applications.</p><p>Here, our work is also an encoder-based method and work in an online fine-grained manner, but we use the learned pseudo labels to optimize our feature encoder E SGA rather than using video-level labels as pseudo labels directly. Moreover, we design a two-stage self-training scheme to efficiently optimize our feature encoder and pseudo label generator instead of iterative optimization <ref type="bibr" target="#b34">[35]</ref>.</p><p>Multiple Instance Learning. MIL is a popular method for weakly supervised learning. In video-related tasks, MIL takes a video as a bag and clips in the video as instances <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8]</ref>. With a specific feature/score aggregation function, video-level labels can be used to indirectly supervise instance-level learning. The aggregation functions vary, e.g. max pooling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> and attention pooling <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8]</ref>. In MIST includes a multiple instance pseudo label generator G and self-guided attention boosted feature encoder E SGA followed by a weighted-classification head H c . We first train a G and then generate pseudo labels for E SGA fine-tuning. this paper, we adopt a sparse continuous sampling strategy in our multiple instance pseudo label generator to force the network to pay more attention to context around the most anomalous part.</p><p>Self-training. Self-training has been widely investigated in semi-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. Self-training methods increase labeled data via pseudo label generation on unlabeled data to leverage the information on both labeled and unlabeled data. Recent deep self-training involves representation learning of the feature encoder and classifier refinement, mostly adopted in semi-supervised learning <ref type="bibr" target="#b11">[12]</ref> and domain adaptation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref>. In unsupervised VAD, Pang et al. <ref type="bibr" target="#b19">[20]</ref> introduced a self-training framework deployed on the testing video directly, assuming the existence of an anomaly in the given video.</p><p>Here, we propose a multiple instance self-training framework that assigns clip-level pseudo labels to all clips in abnormal videos via a multiple instance pseudo label generator. Then, we leverage information from all videos to finetune a self-guided attention boosted feature encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>VAD depends on discriminative representations that clearly represent the events in a scene, while action recognition datasets pretrained feature encoders are not perfect for surveillance videos because of the existence of a domain gap <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. To address this problem, we introduce a self-training strategy to refine the proposed improved feature encoder E SGA . An illustration of our method shown in <ref type="figure" target="#fig_1">Figure 2</ref> is detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given a video V = {v i } N i=1 with N clips, the annotated video-level label Y ? {1, 0} indicates whether an anomalous event exists in this video. We take a video V as a bag and clips v i in the video as instances. Specifically, a negative bag (i.e. Y = 0) marked as B n = {v n i } N i=1 has no anomalous instance, while a positive bag (i.e. Y = 1) denoted as B a = {v a i } N i=1 has at least one. In this work, given a pair of bags (i.e. a positive bag B a and a negative bag B n ), we first pre-extract the features</p><formula xml:id="formula_0">(i.e. {f a i } N i=1 and {f n i } N i=1</formula><p>for B a and B n , respectively) for each clip in the video V = {v i } N i=1 using a pretrained vanilla feature encoder, C3D or I3D, forming bags of features B a and B n . We then feed the pseudo label generator the extracted features to estimate the anomaly scores of the clips (i.e.</p><formula xml:id="formula_1">{s a i } N i=1 , {s n i } N i=1</formula><p>). Then, we produce pseudo labels? a = {? a i } N i=1 for anomalous video by performing smoothing and normalization on estimated scores to supervise the learning of the proposed self-guided attention boosted feature encoder, forming as two-stage self-training scheme <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our proposed feature encoder E SGA , adapted from vanilla feature encoder E (e.g. , I3D or C3D) by adding our proposed self-guided attention module, can be optimized with the estimated pseudo labels to eliminate the domain gap and produce task-specific representations. Actually, our proposed approach can be viewed as a two-stage method (see Algorithm 1): 1) we first generate clip-level pseudo labels for anomalous videos that have only video-level labels via the pseudo label generator, while the parameters of the pseudo label generator are updated by means of the deep MIL ranking loss. 2) After obtaining the clip-level pseudo labels of anomalous videos, our feature encoder E SGA can be trained on both normal and anomalous video data. Thus, we form a self-training scheme to optimize both the feature encoder E SGA and pseudo label generator G. The illustration shown in <ref type="figure" target="#fig_1">Figure 2</ref> provides an overview of our proposed method.</p><p>To better distinguish anomalous clips from normal ones, we introduce a self-guided attention module in the feature encoder, i.e. , E SGA , to capture the anomalous regions in videos to help the feature encoder produce more discriminative representations (see Section 3.3). Moreover, we introduce a sparse continuous sampling strategy in the pseudo label generator to enforce the network to pay more attention to the context around the most anomalous part (see Section 3.2). Finally, we introduce the deep MIL ranking loss to optimize the learning of the pseudo label generator, and we use cross entropy loss to train our proposed feature encoder E SGA supervised by pseudo labels of anomalous videos and clip-level annotations of normal videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pseudo Label Generation via Multiple Instance Learning</head><p>In contrast to <ref type="bibr" target="#b34">[35]</ref>, which simply assigns video-level labels to each clip and then trains the vanilla feature encoder at the very beginning, we introduce a MLP-based structure as the pseudo label generator trained under the MIL paradigm to generate pseudo labels, which are utilized in the refinement process of our feature encoder E SGA .</p><p>Even though recent MIL-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref> have made considerable progress, the process of slicing a video into fixed segments in an coarse-grained manner regardless of its duration is prone to bury abnormal patterns as normal frames that usually constitute the majority, even in abnormal videos <ref type="bibr" target="#b26">[27]</ref>. However, by sampling with a smaller temporal scale in a fine-grained manner, the network may overemphasize on the most intense part of an anomaly but ignore the context around it. In reality, anomalous events often last for a while. With the assumption of minimum duration of anomalies, the MIL network is forced to pay more attention to the context around the most anomalous part.</p><p>Moreover, to adapt to the variation in duration of untrimmed videos and class imbalance in amount, we introduce a sparse continuous sampling strategy: given the features for each clip extracted by a vanilla feature encoder E from a video {f i } N i=1 , we uniformly sample L subsets from these video clips, and each subset contains T consecutive clips, forming L sub-bags B = {f l,t } L,T l=1,t=1 , as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Remarkably, T , a hyperparameter to be tuned, also plays as the assumption of minimum duration of anomalies, as discussed in the previous paragraph. Here, we combine the MIL model with our continuous sampling strategy, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We feed extracted features into our pseudo label generator to produce corresponding anomalous scores {s l,t } L,T l=1,t=1 . Next, we perform average pooling of the predicted instance-level scores s l,t of each sub-bag score as S l below, which can be utilized in Eq. 7.</p><formula xml:id="formula_2">S l = 1 T T t=1 s l,t .<label>(1)</label></formula><p>After training, the trained multiple instance pseudo label generator predicts clip-level scores for all abnormal videos marked as S a = {s a i } N i=1 . By performing temporal smoothing with a moving average filter to relieve the jitter of anomaly scores with kernel size of k,</p><formula xml:id="formula_3">s a i = 1 2k i+k j=i?k s a j ,<label>(2)</label></formula><p>and min-max normalization, </p><formula xml:id="formula_4">y a i = s a i ? minS a /(maxS a ? minS a )), i ? [1, N ],<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-Guided Attention in Feature Encoder</head><p>In contrast to vanilla feature encoder E, which provides only task-agnostic representations for the down-stream task, we propose a self-guided attention boosted feature encoder E SGA adapted from E, which optimizes attention map generation via pseudo labels supervision to enhance the learning of task-specific representations. As <ref type="figure" target="#fig_3">Figure 4</ref> shows, the self-guided attention module (SGA) takes feature maps M b?4 and M b?5 as input, which are produced by the 4 th and 5 th blocks of vanilla feature encoder E, respectively. SGA includes three encoding units, namely F 1 , F 2 and F 3 , which are all constructed by convolutional layers. M b?4 is encoded as M * b?4 and then applied to attention map A generation, denoted as</p><formula xml:id="formula_5">A = F 1 (F 2 (M b?4 )).<label>(4)</label></formula><p>Finally, we obtain M A via the attention mechanism below:</p><formula xml:id="formula_6">M A = M b?5 + A ? M b?5 ,<label>(5)</label></formula><p>where ? is element-wise multiplication, and M A is applied for final anomaly scores prediction via weightedclassification head H c , a fully connected layer.</p><p>To assist the learning of the attention map, we introduce a guided-classification head H g that uses the pseudo labels as supervision. In H g , F 3 transforms M * b?4 into M. Specifically, M * b?4 and M hav 2K channels as K multiple detectors for each class, i.e. , normal and abnormal, to enhance the guided supervision <ref type="bibr" target="#b29">[30]</ref>. Then, we deploy spatiotemporal average pooling, K channel-wise average pooling on M and Softmax activation to obtain the guided anomaly scores for each class.</p><p>Remarkably, there are two classification heads in E SGA , i.e. , weighted-classification head H c and guided classification head H g , which are both supervised by pseudo labels via L 1 and L 2 , respectively. That is, we optimize E SGA with the pseudo labels (see Section 3.2). Therefore, the feature encoder E SGA can update its parameters on video anomaly datasets and eliminate the domain gap from the pretrained parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization Process</head><p>-Deep MIL Ranking Loss: Considering that the positive bag contains at least one anomalous clip, we assume that the clip from a positive bag with the highest anomalous score is the most likely to be an anomaly <ref type="bibr" target="#b7">[8]</ref>.To adapt our sparse continuous sampling in 3.2, we treat a sub-bag as an instance and acquire a reliable relative comparison between the mostly likely anomalous sub-bag and the most likely normal sub-bag:</p><formula xml:id="formula_7">max 1?l?L S n l &lt; max 1?l?L S a l<label>(6)</label></formula><p>Specifically, to avoid too many false positive instances in positive bags, we introduce a sparse constraint on positive bags, which instantiates Eq. 6 as a deep MIL ranking loss with sparse regularization:</p><formula xml:id="formula_8">L M IL = ? max 1?l?L S a l + max 1?l?L S n l + + ? L L l=1 S a l .<label>(7)</label></formula><p>where (?) + means max(0, ?), and the first term in Eq. 7 ensures that max 1?l?L S a l is larger than max 1?l?L S n l with a margin of . is a hyperparameter that is equal to 1 in this work. The last term in Eq. 7 is the sparse regularization indicating that only a few sub-bags may contain the anomaly, while ? is another hyperparameter used to balance the ranking loss with sparsity regularization. -Classification Loss: After obtaining the pseudo labels for an abnormal video in Eq. 3, we obtain the training pair {V a ,? a } that is further combined with {V n , Y n } to train our feature encoder E SGA . For this purpose, we apply the cross entropy loss function to the two classification heads (H c and H g ) in E SGA , i.e. L 1 and L 2 in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Finally, we train a task-specific feature encoder E SGA with the combination of L 1 and L 2 . In the inference stage, we use E SGA to predict clip-level scores for videos via weighted-classification head H c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>We conduct experiments on two large datasets, i.e. , UCF-Crime <ref type="bibr" target="#b22">[23]</ref> and ShanghaiTech <ref type="bibr" target="#b16">[17]</ref>, with two feature encoders, i.e. C3D <ref type="bibr" target="#b23">[24]</ref>   <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, we compute the area under the curve (AUC) of the frame-level receiver operating characteristics (ROC) as the main metric, where a larger AUC implies higher distinguishing ability. We also follow <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref> to evaluate robutness by the false alarm rate (FAR) of anomaly videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The multiple instance pseudo label generator, is a 3-layer MLP, where the number of units is 512, 32 and 1, respectively, regularized by dropout with probability of 0.6 between each layer. ReLU and Sigmoid functions are deployed after the first and last layer, respectively. Here, We adopt hyperparameters L = 32, T = 3, and ? = 0.01 and train the generator with the Adagrad optimizer with a learning rate of 0.01. While fine-tuning, we adopt the Adam optimizer with a learning rate of 1e ? 4 and a weight decay of 0.0005 and train 300 epochs. More details about implementation are reported in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with Related Methods</head><p>In <ref type="table" target="#tab_4">Table 1</ref>, we present the AUC, FAR to compare our MIST with related state-of-the-art online methods in terms of accuracy and robustness. We can find that MIST outperforms or performs similarly to all other methods in terms of all evaluation metrics from <ref type="table" target="#tab_4">Table 1</ref>, which confirms the efficacy of MIST. Specifically, the results of Zhong et al. <ref type="bibr" target="#b34">[35]</ref>, marked with * , are re-tested from the official released models 1 without deploying 10-crop 2 for fair comparison, 1 https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection. <ref type="bibr" target="#b1">2</ref> 10-crop is a test-time augmentation of cropping images into the center, four corners and their mirrored counterparts.    <ref type="bibr" target="#b12">[13]</ref>, which trains C3D RGB with external temporal annotations and N LN RGB with external spatiotemporal annotations. These results verify that our proposed MIST is more effective than previous works. For the ShanghaiTech dataset results in <ref type="table" target="#tab_5">Table 2</ref>, our MIST far outperforms other RGB-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27]</ref>, which validates the capacity of MIST. Remarkably, MIST also surpasses the multi-model method of AR-Net <ref type="bibr" target="#b26">[27]</ref> (I3D RGB+F low ) on AUC by more than 4% to 94.83% and gains a much lower FAR of 0.05%.</p><p>We detail the comparison with the state-of-the-art encoder-based method <ref type="bibr" target="#b34">[35]</ref> on ShanghaiTech in <ref type="figure">Figure  5</ref>. The multiple instance pseudo label generator performs much better than Zhong et al. <ref type="bibr" target="#b34">[35]</ref>, which indicates the drawback of utilizing video-level labels as clip-level labels. Even though Zhong et al. <ref type="bibr" target="#b34">[35]</ref> optimizes for three iterations, it falls far behind our MIST with 16.69% AUC on C3D, which solidly verifies the efficiency and efficacy of MIST. Moreover, our MIST is much faster in the inference stage, as Zhong et al. <ref type="bibr" target="#b34">[35]</ref> applies 10-crop augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder-Agnostic Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC (%) UCF-Crime</head><p>ShanghaiTech pretrained fine-tuned pretrained fine-tuned Sultani et al. <ref type="bibr" target="#b22">[23]</ref> 78   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Task-Specific Feature Encoder</head><p>To verify that our feature encoder can produce taskspecific representations that facilitate the other encoderagnostic methods, we also conduct related experiments with I3D as presented in <ref type="table" target="#tab_7">Table 3</ref>. It is noticeable that all results of encoder-agnostic methods are boosted after using our MIST fine-tuned features, showing a reduction in the domain gap. For example, AR-Net <ref type="bibr" target="#b26">[27]</ref> increases from 85.38% to 92.27% on the UCF-Crime dataset and achieves an improvement of 6.89% on the ShanghaiTech dataset. Therefore, our MIST can produce a more powerful taskspecific feature encoder that can be utilized in other approaches. We visualize the feature space of the pretrained I3D vanilla feature encoder and the MIST-fine-tuned encoder via t-SNE <ref type="bibr" target="#b17">[18]</ref> in <ref type="figure" target="#fig_4">Figure 6</ref>, which also indicates the refinement of feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>At first, we introduce another evaluation metric, i.e. score gap, which is the gap between the average scores of abnormal clips and normal clips. Larger score gap  indicates the network is more capable of distinguishing anomalies from normal events <ref type="bibr" target="#b14">[15]</ref>. We conduct ablation studies on UCF-Crime to analyze the impact of generated pseudo labels (PLs), the self-guided attention module (SGA), and classifier head H g in SGA of proposed feature encoder E SGA in <ref type="table" target="#tab_11">Table 5</ref>. Compared with the baseline and MIST w/o PLs , our MIST achievees a significant improvement when the generated pseudo labels are utilized. In particular, we observe 8.17% improvement in AUC and an approximately 17% score gap, which shows the efficacy of our multiple instance pseudo label generator with the sparse continuous sampling strategy. Pseudo labels also plays an important role. Compared with MIST, the performance of MIST w/o PLs drops seriously, even worse than the baseline for the low-quality supervision that influences the attention map A generation from SGA. Moreover, SGA enhances the feature encoder on emphasizing the informative regions and distinguishing abnormal events from normal ones. Compared with M IST w/oSGA , MIST increases by 2% in AUC and 5% in the score gap. Specifically, the guided-classification branch in SGA plays an important role in guiding the attention map generation, and there is a drop of more than 2% if such a branch is removed.</p><p>Ablation studies are also conducted on a sparse continuous sampling strategy on UCF-Crime and ShanghaiTech with C3D RGB and I3D RGB features. As shown in Table 4, when sampling the same number of clips for a bag and selecting the same number of top clips to represent the bag, our sparse continuous sampling strategy pays more attention to the context and does better than uniform sampling. Especially in ShanghaiTech, sparse continuous sampling gains 2.93% and 6.05% on two kinds of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visual Results</head><p>To further evaluate the performance of our model, we visualize the temporal predictions of the models. As presented in <ref type="figure" target="#fig_5">Figure 7</ref>, our model exactly localizes the anomalous events and predicts anomaly scores very close to zero on normal videos, showing the effectiveness and robustness of our model. We collect some failed samples in the right row of <ref type="figure" target="#fig_5">Figure 7</ref>. In addition, our model predicts the highest score at the end of Arrest001, where a man walks across the scene with his arm pointing forward as if brandishing a    gun. As the videos in UCF-Crime are low-resolution, it is difficult to judge such a confusing action without any other context information. Furthermore, the bottom-right part of <ref type="figure" target="#fig_5">Figure 7</ref> shows another failed case; i.e. , our model successfully localizes the major part of the anomalous burglary event and raises an alarm when the thieves are rushing out of the house, which should be treated as an anomaly but is wrongly labeled as a normal event in the ground truth. We also visualize the spatial activation map via Grad-CAM on M A <ref type="bibr" target="#b21">[22]</ref> for spatial explanation. As <ref type="figure" target="#fig_6">Figure 8</ref> shows, our model is able to sensitively focus on informative regions that help decide whether the scene is anomalous . This verifies that our self-guided attention module can boost the feature encoder to focus on anomalous regions. Addition-ally, compared with the activation maps generated from the MIST without guided-classification head H g and the MIST without the SGA module, the results of MIST are concentrated on the anomalous regions, which shows the rationality and effectiveness of our self-guided attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Discussions</head><p>The key of our MIST is to design a two stage selftraining strategy to train a task-specific feature encoder for video anomaly detection. Each component of our framework can be replaced by any other advanced module, e.g. , replacing C3D with I3D, or a stronger pseudo label generator to take the place of the multiple instance pseudo label generator. Additionally, the scheme of our framework can be adapted to other tasks, such as weakly supervised video action localization and video highlight detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a multiple instance self-training framework (MIST) to fine-tune a task-specific feature encoder efficiently. We adopt a sparse continuous sampling strategy in the multiple instance pseudo label generator to produce more reliable pseudo labels. With the estimated pseudo labels, our proposed feature encoder learns to focus on the most probable anomalous regions in frames facilitated by the proposed self-guided attention module. Finally, after a two-stage self-training process, we train a taskfeature encoder with discriminative representations that can also boost other existing methods. Remarkably, our MIST makes significant improvements on two public datasets. a) Samples of Kinetices-400 b) Samples of UCF-Crime <ref type="figure">Figure 9</ref>: Samples of action recognition dataset Kinetics-400 <ref type="bibr" target="#b10">[11]</ref> and video anomaly dataset UCF-Crime <ref type="bibr" target="#b22">[23]</ref>. The red boxes are the anomalous regions in frames and their corresponding enlarged images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparisons of Action Recognition Datasets and Anomaly Detection Datasets</head><p>As <ref type="figure">Figure 9</ref> shown, the samples from Kinetics-400 <ref type="bibr" target="#b10">[11]</ref> are actor-centered while the samples from UCF-Crime <ref type="bibr" target="#b22">[23]</ref> are not <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Additionally, the anomalies in frames are usually small and low-resolution. These situations indicate the domain gap between the two kinds of datasets. In this work, we propose MIST to minimize the domain gap by training both feature encoder and classifier in a two stage self-training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Pseudo Label Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Feature Extraction and Sampling</head><p>We deploy a vanilla feature encoder, i.e. C3D <ref type="bibr" target="#b23">[24]</ref> pretrained on Sport-1M <ref type="bibr" target="#b9">[10]</ref> or I3D pretrained on Kinetics-400 <ref type="bibr" target="#b10">[11]</ref> to extract features for generator training. We densely sample 16 frames per clip most of the times but 12 frames per clip for I3D on UCF-Crime. After extracting the features, sparse continuous sampling is applied to sample the L ? T clips to form bags of features B. Then, L M IL is deployed to optimize the generator. Specifically, we follow <ref type="bibr" target="#b22">[23]</ref> to select L = 32. As for T , we choose T = 3 for UCF-Crime and T = 7 for ShanghaiTech. We have shown the selection of K on ShanghaiTech with I3D RGB features in <ref type="figure">Figure 10</ref>. Additionally, ? is set as 0.01. <ref type="bibr" target="#b39">40</ref>  AUC(%) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Pseudo Label Refinement</head><p>The trained generator predicts clip-level scores S a = {s a i } N i=1 for all abnormal videos in the training set. Temporal moving average filter with kernel size k = 5 and min-max normalization are deployed to refine the anomaly</p><formula xml:id="formula_9">scores into? = {? a i } N i=1 .</formula><p>C. Details of Feature Encoder Finetuning C.1. Implementation of Self-Guided Attention Module</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref> of the submission, our proposed self-guided attention module includes 3 encoding units, namely F 1 , F 2 , F 3 . All of these encoding units are constructed by convolutional layers. Let C represents the num-ber of channel of M b?4 . F 1 consists of a 3 ? 3 ? 3 ? C 3DConv layer with the stride of 2 and a 1 ? 1 ? 1 ? 2K 3DConv layer, which are both activated by ReLU function; F 2 is a 1 ? 1 ? 1 ? 1 3DConv layer activated by Sigmoid function; F 3 is a 1 ? 1 ? 1 ? 2K 3DConv layer. Then, the attention map A is calculated as follows:</p><formula xml:id="formula_10">A = F 2 (F 1 (M b?4 )),<label>(8)</label></formula><p>while the guided classification predictionp is an aggregation results from M, which is calculated below:</p><formula xml:id="formula_11">M = F 3 (F 1 (M b?4 )).<label>(9)</label></formula><p>Specifically,p is transformed from M via spatiaotemporal average pooling ? and class-specific channel-wise average pooling ?:p = ?(?(M)),</p><p>which is further optimized by L 2 to guide the optimization of class-wise discriminative feature map M * b?4 and then strengthen the attention map generation indirectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Implementation of E SGA Finetuning</head><p>For UCF-Crime, we sample 16 abnormal videos and 16 normal videos per batch, and uniformly sample 3 clips from each video. For ShanghaiTech, we sample 10 abnormal videos and 10 normal videos per batch. The training process finishes in 300 epochs. Specifically, at the begining of finetuning, we conduct warm-up for 5 epochs. Since only a few clips of the abnormal video are anomalous, there exists a class-imbalance problem, especially for I3D. We introduce class-reweighting to cross-entropy loss as classweighted cross-entropy loss L w :</p><formula xml:id="formula_13">L w = ?w 0 ylogp ? w 1 (1 ? y)log(1 ? p),<label>(11)</label></formula><p>where w 0 and w 1 are class weights for abnormal and normal class, respectively. Specifically, L 1 and L 2 are adopted the same kind of loss function L w We adopt w 0 = 1.2 and w 1 = 0.8 for UCF-Crime, while w 0 = 0.8 and w 1 = 0.65 for ShanghaiTech. In the left of <ref type="figure">Figure 11</ref>, we report the AUC of STSA with different K. The performance goes up as the K get larger and reaches the top with K of 8 or 16. When the value getting even larger, it seems to be overfitting and get worse. Considering a trade-off between the efficiency with effectiveness, we set K = 8 in our framework for all other experiments.</p><p>After finetuning, we acquire a task-specific feature encoder E SGA . E SGA outperforms state-of-the-art encoderbased method Zhong et al. <ref type="bibr" target="#b34">[35]</ref>, which is shown in <ref type="figure" target="#fig_1">Figure  12</ref> in detail. Moreover, E SGA can focus on the anomalous regions in frames, which is shown in <ref type="figure" target="#fig_2">Figure 13</ref>. As the left 5 columns of the figure shown, self-guided attention module help the feature encoder in focusing the anomalous regions. We have also listed the failure on the right 2 columns of the figure, the results from too small size of anomaly regions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. More Quantitative Comparisons</head><p>We show more quantitative comparisons with Zhong et al. <ref type="bibr" target="#b34">[35]</ref> on UCF-Crime and ShanghaiTech on <ref type="figure" target="#fig_1">Figure 12</ref>. We observe a huge improvement in ShanghaiTech. As for UCF-Crime our method still do much better when compared fairly without using 10-Crop. Moreover, our method does much better on iter 1 as MIST does not need iterative optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. More Spatial Visualization</head><p>We also present more spatial visualization in <ref type="figure" target="#fig_2">Figure 13</ref>. We observe that MIST performs better than those without SGA or H g . The left two columns are the failure case where the front ground is extremely small and vague to be detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussions of the Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Label Noise Learning vs MIST</head><p>Zhong et al. <ref type="bibr" target="#b34">[35]</ref> treats weakly supervised video anomaly detection as a label noise learning task. However, the extreme label noise results from assigning video-level labels to each clip. In contrast, MIST offers pseudo labels with lower noise via multiple instance generator, which is more efficient. Additionally, MIST can further co-operate with label noise learning methods to refine pseudo labels iteratively and train a more powerful feature encoder.</p><p>In contrast to Zhong et al. that reduces the noise via a specific module, i.e. GCN-based label noise cleaner, we resist label noise via post procession likes min-max norm and Shoplifting001 Shooting015 <ref type="figure" target="#fig_2">Figure 13</ref>: More spatial anomaly activation maps visualization on UCF-Crime. The left 5 columns of the graphs are the successful results while the right 2 columns are the failures. The red boxes are the ground-truth spatial annotations <ref type="bibr" target="#b12">[13]</ref>.</p><p>temporal smoothing. As shown in <ref type="table">Table 7</ref>, we conduct these two types of refinement are do a great help in removing label noise. Moreover, we also use large a batch size with the help of gradient accumulation to reduce the label noise <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. 2D Feature Encoder vs 3D Feature Encoder</head><p>We also conduct experiment on 2D feature encoder the RGB branch of TSN <ref type="bibr" target="#b27">[28]</ref> but fail. Similar result is also reported in <ref type="bibr" target="#b12">[13]</ref>. Since the RGB branch of TSN operates only on a single frame, it fails in catching the motion to represent temporal information. Instead, we deploy two popular 3D Model Before (%) After (%) Gain(%) MIST-C3D 58.66 67.14 +8.48 MIST-I3D 63.63 73.37 +9.74 <ref type="table">Table 7</ref>: Performance comparisons of before and after refinement on ShanghaiTech in term of AUC scores of anomaly videos.</p><p>spatiotemporal feature encoders, i.e. C3D and I3D, whose results well-verified the capacity of MIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Fine-Grained vs Coarse-Grained and Online vs Offline</head><p>Our method focuses on online fine-grained anomaly detection. Previous works follow Sultani et al. <ref type="bibr" target="#b22">[23]</ref> to perform anomaly detection in a coarse-grained manner. However, in the real world, we expect anomaly detection can be applied for streaming surveillance videos to detect anomalies precisely and quickly, while the methods in coarse-grained do not meet the requirement. Some work like Ullah et al. <ref type="bibr" target="#b25">[26]</ref> performs anomaly detection in an offline manner based on an external assumption as complete observation of the testing videos. As discussed above, it also violates the expectation for detection on streaming video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our proposed MIST framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The workflow of our multiple instance pseudo label generator. Each bag contains L sub-bags, and each sub-bag is composed of T continuous clips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The structure of self-guided attention boosted feature encoder E SGA . GAP means the global average pooling operation, while Avg means K channel-wise average pooling in producing guided anomaly scores in guided classification head H g . A is the attention map. F 1 , F 2 , F 3 are three encoding units constructed by convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Feature space visualization of pretrained vanilla feature encoder I3D and the MIST fine-tuned encoder via t-SNE [18] on UCF-Crime testing videos. The red dots denote anomalous regions while the blue ones are normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the testing results on UCF-Crime (better viewed in color). The red blocks in the graphs are temporal ground truths of anomalous events. The orange circle shows the wrongly labeled ground truth, the blue circle indicates the wrongly predicted clip, and the red cricle indicates the correctly predicted clip.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Visualization results of anomaly activation maps (better viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>The effect of T for a fixed number of sub-bag 32 on ShanghaiTech dataset with I3D RGB features. Variations of AUC for different values of multiple detector K with C3D on UCF-Crime dataset. The dotted line is the result of MIST training without self-guided attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Quantitative Comparisons with state-of-the-art encoder-based method Zhong et al. [35] on UCF-Crime and Shang-haiTech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Combine E with self-guided attention module as ESGA, then fine-tune ESGA with supervision of Y n ?? a .</figDesc><table><row><cell>Algorithm 1 Multiple instance self-training framework</cell></row><row><cell>Input: Clip-level labeled normal videos V n = {v n i } N i=1 and cor-</cell></row><row><cell>responding clip-level labels Y n , video-level labeled abnormal</cell></row><row><cell>videos V a = {v a i } N i=1 , pretrained vanilla feature encoder E.</cell></row><row><cell>Output: Self-guided attention boosted feature encoder ESGA,</cell></row><row><cell>multiple instance pseudo label generator G, clip-level pseudo</cell></row><row><cell>labels? a for V a</cell></row><row><cell>Stage I. Pseudo Labels Generation.</cell></row><row><cell>1: Extract features of V a and V n from E as {f a i } N i=1 and</cell></row><row><cell>{f n i } N i=1 .</cell></row><row><cell>2: Training G with {f a i } N i=1 and {f n i } N i=1 and their corresponding</cell></row><row><cell>video-level labels according to Eq. 7.</cell></row><row><cell>3: Predict clip-level pseudo labels for each clip of V a via trained</cell></row><row><cell>G as? a .</cell></row><row><cell>Stage II. Feature Encoder Fine-tuning.</cell></row><row><cell>4:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>we refine the anomaly scores into? = {? a i } N i=1 . Specifically,? a i is in [0, 1] and acts as a soft pseudo label. Then, the pseudo labeled data {V a ,? a } are combined with clip-level labeled data {V n , Y n } as {V, Y } to fine-tune the proposed feature encoder E SGA .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>or I3D [2]. UCF-Crime is a large-scale dataset of real-world surveillance videos, including 13 types of anomalous events with 1900 long untrimmed videos, where 1610 videos are training videos and the others are test videos. Liu et al. [13] Comparisons with the state-of-the-art encoderbased method Zhong et al. [35] on ShanghaiTech. manually annotated bounding boxes of anomalous regions in one image per 16 frames for each abnormal video, and we use their annotation of test videos only to evaluate our model's capacity to identify anomalous regions.</figDesc><table><row><cell></cell><cell>95</cell><cell>MIL iter 1 iter 2 iter 3</cell><cell>93.13</cell><cell>94.83</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell>89.15</cell></row><row><cell>AUC (%)</cell><cell>80 85</cell><cell></cell><cell>86.61</cell></row><row><cell></cell><cell></cell><cell>76.16 76.44</cell><cell></cell></row><row><cell></cell><cell>75</cell><cell>73.79</cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Zhong et al. (C3D-RGB)</cell><cell>Ours (C3D-RGB)</cell><cell>Ours (I3D-RGB)</cell></row><row><cell cols="3">Figure 5:</cell><cell></cell></row></table><note>ShanghaiTech is a dataset of 437 campus surveillance videos. It has 130 abnormal events in 13 scenes, but all ab- normal videos are in the test set, as the dataset is proposed for unsupervised learning. To adapt to the weakly super- vised setting, Zhong et al. [35] re-organized the videos into 238 training videos and 199 testing videos. Evaluation Metrics. Following previous works</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons with existing online methods on UCF-Crime under different levels of supervision and fineness of prediction. The results in (?) are tested with 10-crop, while those marked by * are tested without.</figDesc><table><row><cell>Method</cell><cell cols="4">Feature Encoder Grained AUC (%) FAR (%)</cell></row><row><cell>Sultani et al. [23]</cell><cell>C3D RGB</cell><cell>Coarse</cell><cell>86.30</cell><cell>0.15</cell></row><row><cell>Zhang et al. [32]</cell><cell>C3D RGB</cell><cell>Coarse</cell><cell>82.50</cell><cell>0.10</cell></row><row><cell>Zhong et al. [35]</cell><cell>C3D RGB</cell><cell>Fine</cell><cell>76.44</cell><cell>-</cell></row><row><cell>AR-Net [27]</cell><cell>C3D RGB</cell><cell>Fine</cell><cell>85.01  *</cell><cell>0.57  *</cell></row><row><cell>AR-Net [27]</cell><cell>I3D RGB</cell><cell>Fine</cell><cell>85.38</cell><cell>0.27</cell></row><row><cell>AR-Net [27]</cell><cell>I3D RGB+F low</cell><cell>Fine</cell><cell>91.24</cell><cell>0.10</cell></row><row><cell>MIST</cell><cell>C3D RGB</cell><cell>Fine</cell><cell>93.13</cell><cell>1.71</cell></row><row><cell>MIST</cell><cell>I3D RGB</cell><cell>Fine</cell><cell>94.83</cell><cell>0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Quantitative comparisons with existing methods on ShanghaiTech. The results with* are re-implemented.while the results in brackets are reported on [35] using 10- crop augmentation. However, 10-crop augmentation may improve the performance but requires 10 times the compu- tation. Notably, the result of our MIST still slightly over- takes that of Zhong et al. [35] using 10-crop augmenta- tion (81.08% vs. 81.40% in terms of AUC and 2.2% vs. 2.19% for FAR). Moreover, our method outperforms the su- pervised method of Liu et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparisons between the features from the pretrained vanilla feature encoder and those from MIST on UCF-Crime and ShanghaiTech datasets by adopting encoder-agnostic methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons of sparse continuous sampling and uniform sampling for MIL generator training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Ablation Studies on UCF-Crime with I3D RGB . Baseline is the original I3D trained with video-level la- bels [35]. MIST is our whole model. MIST w/o PLs is trained without pseudo labels but with video-level labels. MIST w/o Hg is MIST trained without H g . MIST w/o SGA is trained without the self-guided attention module).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>D. More Experimental ResultsD.1. Speed and Computational Complexity</figDesc><table><row><cell>Model</cell><cell cols="3">#Params Speed (FPS) FLOPs (MAC)</cell></row><row><cell>MIST-I3D</cell><cell>31M</cell><cell>324.46</cell><cell>45.68G</cell></row><row><cell>MIST-C3D</cell><cell>85M</cell><cell>197.10</cell><cell>39.26G</cell></row><row><cell>Zhong-C3D[35]</cell><cell>78M</cell><cell>130.04</cell><cell>386.2G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Speed and computational complexity comparisons with Zhong et al.<ref type="bibr" target="#b34">[35]</ref>.There are four 1080Ti GPUs for stage 2 but one 1080Ti GPU for stage 1 and validation. In C3D (I3D) based model, #Params are 85 M (31 M), the FLOPs are 39.26 G (45.68 G) and the speed is 197.10 FPS (324.46 FPS). Compared to Zhong et al. that adopt 10-crop testing time augmentation, our method is much faster but costs much lower computational complexity as shown inTable 6.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Massih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Why can&apos;t i dance in the mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised domain adaptation for action recognition from drones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09833</idno>
		<title level="m">Mini-net: Multiple instance ranking network for video highlight detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A markov clustering topic model for mining behaviour in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring background-bias for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Margin learning embedded prediction for video anomaly detection with a few anomalies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-trained deep ordinal regression for end-to-end video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12173" to="12182" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Xiao Bai</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10694</idno>
		<title level="m">Deep learning is robust to massive label noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study. Knowledge and Information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="245" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ijaz Ul Haq, Khan Muhammad, Muhammad Sajjad, and Sung Wook Baik. Cnn features with bi-directional lstm for real-time anomaly detection in surveillance networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waseem</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised video anomaly detection via centerguided discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Boyang Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Not only look, but also listen: Learning multimodal violence detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="322" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly supervised coupled networks for visual sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Paul L Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyun</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Anomalynet: An anomaly detection network for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick Siow Mong</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2537" to="2550" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Video anomaly detection for smart surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00222</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motion-aware feature for improved video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. Mach. Vis. Conf</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
