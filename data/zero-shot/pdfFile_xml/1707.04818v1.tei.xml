<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RED: Reinforced Encoder-Decoder Networks for Action Anticipation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
							<email>jiyangga@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
							<email>zhenheny@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<email>nevatia@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RED: Reinforced Encoder-Decoder Networks for Action Anticipation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>: RED: REINFORCED ENCODER-DECODER NETWORKS FOR ACTION ANTICIPATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action anticipation aims to detect an action before it happens. Many real world applications in robotics and surveillance are related to this predictive capability. Current methods address this problem by first anticipating visual representations of future frames and then categorizing the anticipated representations to actions. However, anticipation is based on a single past frame's representation, which ignores the history trend. Besides, it can only anticipate a fixed future time. We propose a Reinforced Encoder-Decoder (RED) network for action anticipation. RED takes multiple history representations as input and learns to anticipate a sequence of future representations. One salient aspect of RED is that a reinforcement module is adopted to provide sequence-level supervision; the reward function is designed to encourage the system to make correct predictions as early as possible. We test RED on TVSeries, THUMOS-14 and TV-Human-Interaction datasets for action anticipation and achieve state-of-the-art performance on all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action anticipation refers to detection (i.e. anticipation) of an action before it happens. Many real world applications are related to this predictive capability, for example, a surveillance system can raise alarm before an accident happens, and allow for intervention; robots can use anticipation of human actions to make better plans and interactions <ref type="bibr" target="#b8">[9]</ref>. Note that, online action detection <ref type="bibr" target="#b0">[1]</ref> can be viewed as a special case for action anticipation, where the anticipation time is 0.</p><p>Action anticipation is challenging for many reasons. First, it needs to overcome all the difficulties of action detection which require strong discriminative representations of video clips and ability to separate action instances from the large and wide variety of irrelevant background data. Then, for anticipation, the representation needs to capture sufficient historical and contextual information to make future predictions that are seconds ahead.</p><p>State-of-the-art methods on online action detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> learn LSTM networks to encode history information and predict actions based on the hidden state of the LSTM. For action anticipation, early work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> was based on traditional hand-crafted features. c 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. Anticipate Future Actions (Answer Phone) <ref type="figure">Figure 1</ref>: Anticipating future actions by inferring from history information: the normal images represent past frames and the transparent images represent future frames.</p><p>Recently, Vondrick et al. <ref type="bibr" target="#b18">[19]</ref> proposed to use deep neural networks to first anticipate visual representations of future frames and then categorize the anticipated representations to actions. However, the future representation is anticipated based on a single past frame's representation, while actions are better modeled in a clip, i.e. multiple frames. Besides, their model only anticipates for single fixed time, it is desirable to be able to anticipate a sequence of continuous future representations. To address the anticipation challenges, we propose a Reinforced Encoder-Decoder (RED) network. The encoder-decoder network takes continuous steps of history visual representations as input and outputs a sequence of anticipated future representations. These anticipated representations are processed by a classification network for action classification. Squared loss is used for the representation anticipation and cross-entropy loss is used for action category anticipation (classification) during training. One drawback of the traditional crossentropy loss is that it only optimizes the encoder-decoder networks greedily at each time step, and lacks sequence level optimization <ref type="bibr" target="#b14">[15]</ref>. We propose to use reinforcement learning to train the encoder-decoder networks on sequence level. The reward function is designed to encourage the model to make the correct anticipations as early as possible. We test RED on TVSeries <ref type="bibr" target="#b0">[1]</ref>, THUMOS-14 and TV-Human-Interaction <ref type="bibr" target="#b12">[13]</ref> for action anticipation and online action detection. State-of-the-art performance has been achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we introduce work on related topics, including online action detection, offline action detection, action anticipation and reinforcement learning in vision.</p><p>Early and Online Action Detection Hoai et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> first proposed the problem of early event detection. They designed a max-margin framework which is based on structured output SVMs. Ma et al. <ref type="bibr" target="#b10">[11]</ref> address the problem of early action detection. They propose to train an LSTM network with ranking loss and merge the detection spans based on the frame-wise prediction scores generated by the LSTM. Recently, Geest et al. <ref type="bibr" target="#b0">[1]</ref> published a new dataset for online action detection, which consists of 16 hours (27 episodes) of TV series with temporal annotation for 30 action categories.</p><p>Offline Action Detection In the setting of offline action detection, the whole video is given and the task is to detect whether given actions occurs in this video and when does it occurs. S-CNN <ref type="bibr" target="#b15">[16]</ref> presented a two-stage action localization framework: first using a proposal network to generate temporal proposals and then scoring the proposals with a localization network. TURN <ref type="bibr" target="#b3">[4]</ref> proposed to use temporal coordinate regression to refine action boundaries for temporal proposal generation, which is proved to be effective and could be generalized to different action domains. TALL <ref type="bibr" target="#b2">[3]</ref> used natural language as query to localize actions in long videos and designed a cross-modal regression model to solve it.</p><p>Action Anticipation There have been some promising works on anticipating future action categories. Lan et al. <ref type="bibr" target="#b9">[10]</ref> designed a hierarchical representation, which describes human movements at multiple levels of granularities, to predict future actions in the wild. Pei et al. <ref type="bibr" target="#b13">[14]</ref> proposed an event parsing algorithm by using Stochastic Context Sensitive Grammar (SCSG) for inferring the goal of agents, and predicting the intended actions. Xie et al. <ref type="bibr" target="#b20">[21]</ref> proposed to infer people's intention of performing actions, which is a good clue for predicting future actions. Vondrick et al. <ref type="bibr" target="#b18">[19]</ref> proposed to anticipate visual representation by training CNN on large-scale unlabelled video data.</p><p>Reinforcement Learning in Vision We get inspiration from recent approaches that used REINFORCE <ref type="bibr" target="#b19">[20]</ref> to learn task-specific policies. Yeung et al. <ref type="bibr" target="#b23">[24]</ref> proposed to learn policies to predict next observation location for action detection task by using LSTM networks. Mnih et al. <ref type="bibr" target="#b11">[12]</ref> proposed to adaptively select a sequence of regions in images and only processing the selected regions at high resolution for the image classification task. Ranzato et al. <ref type="bibr" target="#b14">[15]</ref> proposed a sequence-level training algorithm for image captioning that directly optimizes the metric used at test time by policy gradient methods.</p><p>3 Reinforced Encoder-Decoder Network RED contains three modules: a video representation extractor; an encoder-decoder network to encode history information and anticipate future video representations; a classification network to anticipate action categories and a reinforcement module to calculate rewards, which is incorporated in training phase using a policy gradient algorithm <ref type="bibr" target="#b19">[20]</ref>. The architecture is shown in <ref type="figure" target="#fig_2">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Processing</head><p>A video is segmented into small chunks, each chunk contains f = 6 consecutive frames. The video chunks are processed by a feature extractor E v . It takes the video chunks u i as input and outputs chunk representation V i = E v (u i ). More details on video pre-processing and feature extractors could be found in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder-Decoder Network</head><p>The encoder-decoder network uses a LSTM network as basic cell. The input to this network is a vector sequence S in = {V i }, i ? [t ? T enc ,t), vector V i is a chunk visual representation, T enc is the length of the input sequence, t is the time point in the video. After the last input vector has been read, the decoder LSTM takes over the last hidden state of encoder LSTM and outputs a prediction for the target sequence S out = {V j } j ? [t,t + T dec ), where T dec is the length of the output sequence, i.e., the anticipation steps. The target sequence are representations of the video chunks that come after the input sequence.</p><p>The goal of decoder LSTM is to regress future visual representations, based on the last hidden state of the encoder networks. The loss function for training the encoder-decoder networks is the squared loss,</p><formula xml:id="formula_0">L reg = 1 N N ? k=1 T dec ? j=1 ||V k j ?V k j || (1)</formula><p>where N is the batch size,V k j is the anticipated representation and V k j is the ground truth representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification Network</head><p>The output vector sequence, S out , of the encoder-decoder networks is processed by the classification network, which has two fully connected layers, to output a classification distribution on action categories. The loss function of classification is the cross-entropy loss:</p><formula xml:id="formula_1">L cls = 1 N ? N k=1 ? T dec t=1 log(p(y k t |y k 1:t?1 )), where p(y j k )</formula><p>is the probability score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reinforcement Module</head><p>A natural expectation of action anticipation is to make the correct anticipation as early as possible. For example, we consider two anticipation sequences "000111" and "001110", assuming the ground truth sequence is "011111", where "1" represents that an action category is happening and "0" represents that no action is happening (i.e background). "001110" gives the correct anticipation earlier then "000111", so we consider it is a better anticipation at sequence level. However, cross-entropy loss would not capture such sequence-level distinctions, as it is calculated at each step to output higher confident scores on the ground truth category, and no sequence-level information is involved.</p><p>To consider sequence-level reward, we incorporate reinforcement learning into our system. The anticipation module (the encoder-decoder networks) and the classification module (the FC layers) together can be viewed as an agent, which interacts with the external environment (the feature vector taken as input at every time step). The parameters of this agent define a policy, whose execution results in the agent making an prediction. In the action detection and anticipation setting, a prediction refers to predicting the action category in the sequence at each time step. After making a prediction, the agent updates its internal state (the hidden state of LSTM), and observes a reward.</p><p>We design a reward function to encourage the agent to make the correct anticipation as early as possible. Assuming the agent outputs an anticipation sequence {? i }, and the corresponding ground truth labels are {y i }. In the ground truth label sequence, we denote the time position t f that the label start to change from background to some action class as transferring time, for example, in "001111", t f = 2. At each step t of the anticipation, the reward r t is calculated as</p><formula xml:id="formula_2">r t = ? t + 1 ? t f , if t ? t f and? t = y t ; r t = 0, otherwise<label>(2)</label></formula><p>where ? is a constant parameter. If the agent makes the correct prediction at the transferring time t s , it would receive the largest reward. The reward for making correct anticipation decays with time. The cumulative reward of the sequence is calculated as R = ? T dec t=1 r t . The goal is to maximize the reward expectation R when interacting with the environment, that is, to encourage the agent to output correct anticipations as early as possible. More formally, the policy of the agent induces a distribution over possible anticipation sequences p y(1:T ) , and we want to maximize the reward under this distribution:</p><formula xml:id="formula_3">J(? ) = E p(y(1:t));? ) [ T dec ? t=1 r t ] = E p(y(1:t));? ) [R]<label>(3)</label></formula><p>where y(1 : t) is the action category predicted by our model. To maximize J(? ), as shown in REINFORCE <ref type="bibr" target="#b19">[20]</ref>, an approximation to the gradient is given by</p><formula xml:id="formula_4">? ? J ? 1 N N ? k=1 T dec ? t=1 ? ? log?(a k t |h k 1:t , a k 1:t?1 )R k t<label>(4)</label></formula><p>where ? is the agent's policy. In our case, the policy ? is the probability distribution over action categories at each time step. Thus, the gradient can be written as</p><formula xml:id="formula_5">? ? J ? 1 N N ? k=1 T dec ? t=1 ? ? logp(y k t |y k 1:t?1 )(R k t ? b k t )<label>(5)</label></formula><p>where b k t is a baseline reward, which is estimated by a separate network. The network consists of two fully connected layer and takes the last hidden state of the encoder network as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Procedure</head><p>The goal of encoder-decoder networks is to anticipate future representations, so unlabelled video segments (no matter whether there contain actions) could be used as training samples. As the positive segments (i.e. some action is happening) are very small part of videos in the whole datasets, RED is trained by a two-stage process. In the first stage, the encoder-decoder networks for representation anticipation are trained by the regression loss L reg on all training videos in the dataset as initialization. In the second stage, the encoder-decoder networks are optimized by the overall loss function L, which includes the regression loss L reg , a crossentropy loss L cls introduced by classification networks and J introduced from reinforcement module on the positive samples in the videos:</p><formula xml:id="formula_6">L = L reg + L cls ? J (6)</formula><p>The classification network is only trained in the second stage by L cls and J.</p><p>The training samples for the first stage do not require any annotations, so they could be collected at any position in the videos. Specifically, at a time point t, the sequence for encoder networks is [V t?T enc ,V t ), the output ground truth sequence for decoder networks is </p><formula xml:id="formula_7">[V t ,V t+T dec ), where V t is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate our approach on standard benchmarks for action anticipation, including TVSeries <ref type="bibr" target="#b0">[1]</ref>, THUMOS-14 and TV-Human-Interaction <ref type="bibr" target="#b12">[13]</ref>. As no previous action anticipation results are available on THUMOS-14 and TVSeries, we build several strong baselines (including one similar to <ref type="bibr" target="#b18">[19]</ref>) to compare with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We extract frames from all videos at 24 Frames Per Second (FPS). The video chunk size is set to 6, so each chunk is 0.25-second long in the original video. We investigate two video feature extractors: a two-stream CNN model <ref type="bibr" target="#b21">[22]</ref> and VGG-16 <ref type="bibr" target="#b17">[18]</ref> model. For twostream model, we use central frame to calculate the appearance CNN feature in each chunk. The outputs of "Flatten_673" layer in ResNet are extracted; we calculate the optical flows <ref type="bibr" target="#b1">[2]</ref> between the 6 consecutive frames in a chunk, and feed them into the motion stream. The output of "global pool" layer in BN-Inception <ref type="bibr" target="#b6">[7]</ref> is used. We concatenate the motion features and the appearance features into 4096-dimensional vectors, which is used as chunk representation. To provide fair comparison with <ref type="bibr" target="#b0">[1]</ref>, we also use VGG-16 features. The central frame in a chunk is sampled and processed by a VGG-16 model (pre-trained on ImageNet), the output of f c6 is used as the chunk representation.</p><p>We set T enc = 16 and T dec = 8, which are 4 seconds and 2 seconds respectively. The hidden state of LSTM is set to 4096-dimensional and number of LSTM layer is 1. We also tried 2048-dimensional, but shows a worse performance. ? in Equation <ref type="formula" target="#formula_2">(2)</ref> is set to 1. Adam <ref type="bibr" target="#b7">[8]</ref> is adopted to train RED, learning rate is set to 0.001, and batch size is 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC:</head><p>We implement a method similar to that in <ref type="bibr" target="#b18">[19]</ref>, in which fully connected layers are trained to anticipate the future representation, as shown in <ref type="figure" target="#fig_5">Figure 3</ref>, on the left. Specifically, the chunk representation at time t are input to two fully connected layers and regressed to  Encoder-FC (EFC): The above baseline method FC only considers a single representation for anticipation. This baseline method extends to fully connected layers to a LSTM encoder network, which encodes the representations from t ? T enc to t, and anticipates the representation at t + T ant , as shown in <ref type="figure" target="#fig_5">Figure 3</ref>, on the right. The classification layers take the anticipated representations to anticipate action categories. We set T enc = 16, the same as the one of RED. T ant is set to 4.</p><p>Encoder-Decoder (ED): This baseline is similar to RED, but without the reinforcement module. The EFC baseline method considers sequenced history representations and output a single anticipated video representation at time t + T ant . Instead of anticipating a single future representation, the decoder learns to anticipate multiple continuous representations from time t to t + T dec . We set T enc = 16 and T dec = 8, the same as the ones of RED.</p><p>The aforementioned baseline models are trained by a two-stage process, which is similar to RED. In the first stage, the representation anticipation networks, i.e. 2-layer FC anticipation, EFC anticipation or encoder-decoder anticipation, are trained by a regression loss on all the videos in the dataset (THUMOS-14 or TVSeries) as initialization. In the second stage, the anticipation networks are trained by the regression loss and a cross-entropy loss, which is introduced by the classification network on the positive samples in the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>Dataset. The TVSeries Dataset <ref type="bibr" target="#b0">[1]</ref> is a realistic, large-scale dataset for temporal action detection; it contains 16 hours of videos (27 episodes) from six recent popular TV series. TVSeries contains 30 daily life action categories, such as "answer phone", "drive car", "close door". The temporal detection part of THUMOS-14 contains over 20 hours of videos from 20 sport classes. The training set of THUMOS-14 contains only trimmed videos, which are not suitable for action anticipation and action detection. There are 200 and 213 untrimmed videos in the validation and test set respectively. We train RED on the validation set and test it on the test set. TV-Human-Interaction <ref type="bibr" target="#b12">[13]</ref> dataset contains 300 video clips extracted from 23 different TV shows. There are 4 interactions: hand shakes, high fives, hugs and kisses in the dataset. The video lengths range from 1 second to 10 seconds.</p><p>Experiment setup. Training representation anticipation (stage 1) needs tens of hours of video data. TVSeries and THUMOS-14 have enough videos, but TV-Human-Interaction only contains tens of minutes. So we train and test our model on both TVSeries and THUMOS-14, and compare the performance of RED for anticipation time T a = 0.25s ? 2.0s. For TV-Human-Interaction, we use the stage-1 model trained on THUMOS-14 or TVSeries, and train stage-2 on TV-Human-Interaction. On TVSeries, the metric we use is calibrated Average Precision (cAP) which is proposed by <ref type="bibr" target="#b0">[1]</ref>. Calibrated precision (cPrec) uses a parameter w, which the ratio between negative frames and positive frames in the calculation of precision, cPrec = T P T P+FP/w , so that the average precision is calculated as if there were an equal amount of positive and negative frames. On THUMOS-14, we report per-frame mean Average Prevision (mAP) performance, which is used in <ref type="bibr" target="#b22">[23]</ref>. For TV-Human-Interaction, we report classification accuracy (ACC). Comparison of action anticipation methods. We first compare the performance at anticipation time T a = 1s. The results on TVSeries and THUMOS-14 are shown in <ref type="table" target="#tab_0">Table 1</ref>. Overall, RED outperforms FC on both datasets. Comparing with FC and EFC, we can see that encoding multiple history steps of representation improves the anticipation performance consistently. The difference between ED and EFC is the presence of decoder networks, EFC uses fully connected layers to anticipate only one future representation, but ED uses decoder networks to anticipate a sequence of continuous representations. Comparing ED and EFC, we can see that anticipating the future representation step by step makes the anticipations more accurate. Comparing ED and RED, it can be seen that the proposed reinforer benefits action anticipation. We compare RED with Vondrick et al. <ref type="bibr" target="#b18">[19]</ref> on TV-Human-Interaction at anticipation time T a = 1s, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Stage-1 of RED is trained on TVSeries (with VGG feature) or THUMOS-14 (with two stream feature), stage-2 is trained on TV-Human-Interaction. Note that, Vondrick et al. <ref type="bibr" target="#b18">[19]</ref> trained the representation anticipation model on THUMOS dataset and trained SVM classifiers for action anticipation. We can see that both RED-VGG and RED-TS outperform <ref type="bibr" target="#b18">[19]</ref>. Note that <ref type="bibr" target="#b18">[19]</ref> uses Alexnet features so possibly some of our gain comes just from use of stronger features. On the other hand, we use a much smaller subset of THUMOS (20 hours vs 400 hours) to train. Also, <ref type="table" target="#tab_0">Table 1</ref>, which includes a comparison with our implementation of <ref type="bibr" target="#b18">[19]</ref>, and using the same TS features as RED, does indicate that our network design is responsible for significant part of the improvements.</p><p>Varying anticipation time. Encoder-decoder network allows for sequence anticipation, unlike FC or EFC which can only anticipate at a fixed future time. A detailed comparison between "ED" and "RED" is shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref>. We test RED with two types of video representations on TVSeries: two-stream model and VGG-16. On THUMOS-14,  we report two-stream performance. The anticipation time ranges from 0.25s to 2s, which correspond to 1 video chunk and 8 video chunks (each chunk contains 6 frames, and frame extraction rate is 24). "TS" stands for two-stream. As shown in <ref type="table" target="#tab_2">Table 3</ref>, it can be seen that reinforcement module consistently improves the encoder-decoder networks for action anticipation at each step on both two stream features and VGG features. We think it is because the sequence-level supervision introduced by the designed reward function makes the optimization more effective. The reward function together with the cross-entropy loss not only trains the system to make the correct action anticipation at each single step, but also encourages it to produce the correct sequence prediction as early as possible. The results on THUMOS-14 are consitent with those on TVSeries, the sequence-level optimization from reinforcement learning consistently benefits the action anticipation. Comparison of online action detection As discussed before, online action detection can be treated as a special case for action anticipation, where the anticipation time T a = 0. We use the results at minimum anticipation time (T a = 0.25s) for online action detection. As shown in <ref type="table" target="#tab_4">Table 5</ref>, the previous state-of-the-art methods based on CNN, LSTM and Fisher Vector (FV) in <ref type="bibr" target="#b0">[1]</ref> achieved 60.8, 64.1 and 74.3 respectively. <ref type="bibr" target="#b0">[1]</ref> uses VGG features for "CNN" and "LSTM" methods, which are same as our RED-VGG. RED-VGG achieves 71.8, outperforms 64.1 by a large margin. RED-TS achieves the highest performance 79.2. Similar to TVSeries, we use the results of T a = 0.25s as results of online action detection on THUMOS-14. The results are shown in <ref type="table" target="#tab_5">Table 6</ref> and we can see that RED outperforms state-of-the-art methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose Reinforced Encoder-Decoder (RED) networks for action anticipation. RED takes multiple history representations as input and learns to anticipate a sequence of future representations, which are fed into classification networks to categorize actions. The salient aspect of RED is that a reinforcement module is adopted to provide sequence-level supervision. The reward function is designed to encourage the system to make correct prediction as early as possible. RED is jointly optimized by the cross-entropy loss, squared loss and the reward function via a two-stage training process. Experimental results on action anticipation show the effectiveness of proposed reinforcement module and the encoder-decoder network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Reinforced Encoder-Decoder (RED) networks architecture for action anticipation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the visual representation at t. For the second stage, the training samples are collected around positive action intervals. Specifically, given a positive action interval [t s ,t e ], the central time point t could be selected from t &gt; t s ? T enc to t &lt; t e . After picking the central time point t, [V t?T enc ,V t ) are used as input samples of the encoder, [V t ,V t+T dec ) are used as output ground truth visual representations for the anticipated sequence. [y t , y t+T dec ) are ground truth action labels for the anticipated sequence, which are used in classification cross-entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Baseline methods for action anticipation. The left one is FC and the right one is EFC. the future chunk representations at time t + T ant . The anticipating time is T ant . The anticipated representations are processed by another classification network, which consists of two fully connected layers. The outputs of the classification network are the action category anticipations. T ant is set to 4, which is 1 second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Action anticipation comparison on TVSeries (cAP %) test set and THUMOS-14</cell></row><row><cell cols="2">(per-frame mAP %) test set at 1s (4 chunks) with two-stream features.</cell></row><row><cell></cell><cell>FC EFC ED RED</cell></row><row><cell>TVSeries (cAP@T a =1s %)</cell><cell>72.4 73.3 74.6 75.5</cell></row><row><cell cols="2">THUMOS-14 (mAP@T a =1s %) 31.7 33.9 36.8 37.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Action anticipation comparison (ACC %) on TV-Human-Interaction at T a = 1s (4 chunks).</figDesc><table><row><cell></cell><cell cols="3">Vondrick et al.[19] (THUMOS) RED-VGG (TVSeries) RED-TS (THUMOS)</cell></row><row><cell>ACC@T a =1s (%)</cell><cell>43.6</cell><cell>47.5</cell><cell>50.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Detailed action anticipation (cAP %) comparison for ED and RED on TVSeries test set from T a = 0.25s to T a = 2.0s with two-stream representations and VGG features. time 0.25s 0.5s 0.75s 1.0s 1.25s 1.5s 1.75s 2.0s</figDesc><table><row><cell>ED-VGG</cell><cell>71.0</cell><cell>70.6 69.9</cell><cell>68.8 68.0</cell><cell>67.4 67.0</cell><cell>66.7</cell></row><row><cell cols="2">RED-VGG 71.2</cell><cell>71.0 70.6</cell><cell>70.2 69.2</cell><cell>68.5 67.5</cell><cell>66.8</cell></row><row><cell>ED-TS</cell><cell>78.5</cell><cell>78.0 76.3</cell><cell>74.6 73.7</cell><cell>72.7 71.7</cell><cell>71.0</cell></row><row><cell>RED-TS</cell><cell>79.2</cell><cell>78.7 77.1</cell><cell>75.5 74.2</cell><cell>73.0 72.0</cell><cell>71.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Detailed action anticipation (per-frame mAP %) comparison for ED and RED on THUMOS-14 test set from T a = 0.25s to T a = 2s with two-stream representations.</figDesc><table><row><cell>time</cell><cell cols="5">0.25s 0.5s 0.75s 1.0s 1.25s 1.5s 1.75s 2.0s</cell></row><row><cell>ED-TS</cell><cell>43.8</cell><cell>40.9 38.7</cell><cell>36.8 34.6</cell><cell>33.9 32.5</cell><cell>31.6</cell></row><row><cell cols="2">RED-TS 45.3</cell><cell>42.1 39.6</cell><cell>37.5 35.8</cell><cell>34.4 33.2</cell><cell>32.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison on online action detection in TVSeries test set.</figDesc><table><row><cell></cell><cell cols="5">CNN[1] LSTM[1] FV [1] RED-VGG RED-TS</cell></row><row><cell>cAP (%)</cell><cell>60.8</cell><cell>64.1</cell><cell>74.3</cell><cell>71.2</cell><cell>79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Online action detection comparison on THUMOS-14 test set (per-frame mAP %) with two stream features.</figDesc><table><row><cell></cell><cell cols="4">two-stream[17] LSTM[23] MultiLSTM[23] RED</cell></row><row><cell>mAP(%)</cell><cell>36.2</cell><cell>39.3</cell><cell>41.3</cell><cell>45.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cees Snoek, and Tinne Tuytelaars. Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Roeland De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02101</idno>
		<title level="m">Temporal activity localization via language query</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06189</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anticipating human activities for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Hema Swetha Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2071" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High five: Recognising human interactions in tv shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonso</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno>RED: REINFORCED ENCODER-DECODER NETWORKS FOR ACTION ANTICIPATION 11</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing video events with goal inference and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring&quot; dark matter&quot; and&quot; dark energy&quot; from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
