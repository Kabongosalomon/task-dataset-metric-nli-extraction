<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ContextNet: A Click-Through Rate Prediction Framework Using Contextual information to Refine Feature Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>She</surname></persName>
							<email>qingyun_she@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ContextNet: A Click-Through Rate Prediction Framework Using Contextual information to Refine Feature Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format: Zhiqiang Wang, Qingyun She, PengTao Zhang, Junlin Zhang. 2021. Con-textNet: A Click-Through Rate Prediction Framework Using Contextual information to Refine Feature Embedding. In Proceedings of ACM Conference (Conference&apos;17). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) estimation is a fundamental task in personalized advertising and recommender systems and it's important for ranking models to effectively capture complex high-order features. Inspired by the success of ELMO and Bert in NLP field, which dynamically refine word embedding according to the context sentence information where the word appears, we think it's also important to dynamically refine each feature's embedding layer by layer according to the context information contained in input instance in CTR estimation tasks. We can effectively capture the useful feature interactions for each feature in this way. In this paper, We propose a novel CTR Framework named ContextNet that implicitly models high-order feature interactions by dynamically refining each feature's embedding according to the input context. Specifically, ContextNet consists of two key components: contextual embedding module and ContextNet block. Contextual embedding module aggregates contextual information for each feature from input instance and ContextNet block maintains each feature's embedding layer by layer and dynamically refines its representation by merging contextual high-order interaction information into feature embedding. To make the framework specific, we also propose two models(ContextNet-PFFN and ContextNet-SFFN) under this framework by introducing linear contextual embedding network and two non-linear mapping sub-network in ContextNet block. We conduct extensive experiments on four real-world datasets and the experiment results demonstrate that our proposed ContextNet-PFFN and ContextNet-SFFN model outperform state-of-the-art models such as DeepFM and xDeepFM significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-through rate (CTR) estimation has become one of the most essential tasks in many real-world applications. Many models have Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA ? 2021 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn been proposed to resolve this problem such as Logistic Regression (LR) <ref type="bibr" target="#b20">[20]</ref>, Polynomial-2 (Poly2) <ref type="bibr" target="#b27">[27]</ref>, tree-based models <ref type="bibr" target="#b13">[13]</ref>, tensor-based models <ref type="bibr" target="#b16">[16]</ref>, Bayesian models <ref type="bibr" target="#b9">[9]</ref>, and Field-aware Factorization Machines (FFMs) <ref type="bibr" target="#b15">[15]</ref>.</p><p>Deep learning techniques have shown promising results in many research fields such as computer vision <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17]</ref>, speech recognition <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b30">30]</ref> and natural language understanding <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b22">22]</ref>. As a result, employing DNNs for CTR estimation has also been a research trend in this field <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b35">35]</ref>. Some deep learning based models have been introduced and achieved success such as Factorisation-Machine Supported Neural Networks(FNN) <ref type="bibr" target="#b35">[35]</ref>, Attentional Factorization Machine (AFM) <ref type="bibr" target="#b5">[6]</ref>, wide&amp;deep <ref type="bibr" target="#b33">[33]</ref>, DeepFM <ref type="bibr" target="#b11">[11]</ref>, xDeepFM <ref type="bibr" target="#b19">[19]</ref>, DIN <ref type="bibr" target="#b36">[36]</ref> etc.</p><p>Feature interaction is critical for CTR tasks and it's important for these ranking models to effectively capture complex features. Most DNN ranking models such as FNN and DeepFM use the shallow MLP layers to model high-order interactions in implicit way which has been proved to be ineffective <ref type="bibr" target="#b2">[3]</ref>. Some CTR model like xDeepFM <ref type="bibr" target="#b19">[19]</ref> explicitly introduces high-order feature interactions by adding sub-network into the network structure. However, that will significantly increases the computation time and it's hard to deploy it in real-world application.</p><p>Inspired by the success of ELMO <ref type="bibr" target="#b24">[24]</ref> and Bert <ref type="bibr" target="#b8">[8]</ref> in NLP field, which dynamically refine word embedding according to the sentence context where the word appears, we think it's also important to dynamically change the feature embedding according to other contextual features in the same instance it appears in CTR tasks. We can effectively capture the useful feature interactions for each feature by introducing the context aware feature embedding into CTR models.</p><p>Though AutoInt <ref type="bibr" target="#b29">[29]</ref> and Fi-GNN <ref type="bibr" target="#b18">[18]</ref> can also dynamically change feature embedding as our proposed model does, feature representation of these models is kind of weighted summation of pair-wise interaction . These models follow the rule of feature aggregation in summation way after pair-wise interaction, while our proposed model follow the rule of feature interaction in multiplicative way after feature aggregation by a specific network. Alex Beutel et.al <ref type="bibr" target="#b1">[2]</ref> have proved that addictive feature interaction is inefficient in capturing common feature crosses. They proposed a simple but effective approach named "latent cross" which is a kind of multiplicative interactions between the context embedding and the neural network hidden states in RNN model. Our work is inspired by both the Bert and "latent cross".</p><p>In this work, We propose a new CTR framework named Con-textNet which can dynamically refine feature's embedding according to the context it appears and effectively model high-order feature interactions for each feature. Specifically, ContextNet consists of two key components: contextual embedding module and Con-textNet block. Contextual embedding module aggregates contextual information for each feature from input instance and ContextNet block maintains each feature's embedding layer by layer and dynamically refines its representation by merging contextual high-order interaction information into feature embedding. So the ContextNet provides a flexible mechanism for each feature to dynamically and efficiently filter out the most useful high-order cross information for its own purpose in current context it appears. Another advantage of ContextNet over most DNN models is that it has good model interpretability. Notice that ContextNet is a new CTR framework instead of a specific model, which means we can design various detailed models based on this framework. We also propose two Con-textNet based models in this paper in order to make the ContextNet framework specific and experimental results prove its effectiveness.</p><p>The contributions of our work are summarized as follows:</p><p>(1) We propose a novel CTR Framework named ContextNet that implicitly models high-order feature interactions by dynamically refining the feature embedding according to the context information contained in input instance. (2) To make the ContextNet framework specific, we propose a contextual embedding network and two non-linear mapping sub-network in ContextNet block. So we design two specific ContextNet-based models in our work under the proposed framework, which is named ContextNet-PFFN and ContextNet-SFFN, respectively. (3) We conduct extensive experiments on four real-world datasets and the experiment results demonstrate that our proposed ContextNet-PFFN and ContextNet-SFFN outperform stateof-the-art models significantly.</p><p>The rest of this paper is organized as follows. Section 2 introduces some related works which are relevant with our proposed model. We introduce our proposed ContextNet framework in detail in Section 3. The experimental results on four real world datasets are presented and discussed in Section 4. Section 5 concludes our work in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Context Aware Word Embedding in NLP</head><p>Word embedding is a very important concept in NLP and it attempts to map words from a discrete space into a semantic space. In early stage, Word2vec <ref type="bibr" target="#b21">[21]</ref> and GloVe <ref type="bibr" target="#b23">[23]</ref> learn a constant embedding vector for a word and the embedding is same for a word in different sentences. However, it's obvious that polysemous word should have different embedding in various sentence contexts. To deal with this issue, context information of the sentence is used to predict a dynamic word embedding. For example, ELMO <ref type="bibr" target="#b24">[24]</ref> uses the bidirectional RNN to model the context information. The GPT <ref type="bibr" target="#b26">[26]</ref> and Bert <ref type="bibr" target="#b8">[8]</ref> model leverages the Transformer[31] model to jointly consider both the left and right context information in the sentence. Our work is inspired by these context aware word embedding approaches and we introduce context aware feature embedding into CTR tasks in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning based CTR Models</head><p>Many deep learning based CTR models have been proposed in recent years and how to effectively model the feature interactions is the key factor for most of these neural network based models.</p><p>Factorization-Machine Supported Neural Networks (FNN) <ref type="bibr" target="#b35">[35]</ref> is a feed-forward neural network using FM to pre-train the embedding layer. Wide &amp; Deep Learning <ref type="bibr" target="#b33">[33]</ref> jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems. However, expertise feature engineering is still needed on the input to the wide part of Wide &amp; Deep model. To alleviate manual efforts in feature engineering, DeepFM <ref type="bibr" target="#b11">[11]</ref> replaces the wide part of Wide &amp; Deep model with FM and shares the feature embedding between the FM and deep component. Most DNN CTR models rely on two or three MLP layers to model the high-order interactions in an implicit way and some research <ref type="bibr" target="#b2">[3]</ref> has proved MLP is an ineffective way to capture the high-order interactions.</p><p>Some works explicitly introduce high-order feature interactions by sub-network. Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b32">[32]</ref> efficiently captures feature interactions of bounded degrees in an explicit fashion. Similarly, eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b19">[19]</ref> also models the low-order and high-order feature interactions in an explicit way by proposing a novel Compressed Interaction Network (CIN) part. FiBiNET <ref type="bibr" target="#b14">[14]</ref> can dynamically learn feature importance via the Squeeze-Excitation network (SENET) mechanism and feature interactions via bilinear function. AutoInt <ref type="bibr" target="#b29">[29]</ref> proposes a multi-head self-attentive neural network with residual connections to explicitly model the feature interactions in the lowdimensional space. Fi-GNN <ref type="bibr" target="#b18">[18]</ref> represents the multi-field features in a graph structure, where each node corresponds to a feature field and different fields can interact through edges. Though AutoInt <ref type="bibr" target="#b25">[25]</ref> and Fi-GNN <ref type="bibr" target="#b18">[18]</ref> can also dynamically change feature embedding by proposing a multi-head self-attentive neural network or graph neural network, feature representation of these models is kind of weighted summation of pair-wise interaction. Many research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28]</ref> have proved that addictive feature interaction is inefficient in capturing common feature crosses. Our proposed models collect global contextual information by an independent contextual embedding network to change the feature representation in the multiplicative way. Our experimental results show this advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED MODEL</head><p>In this section, we will firstly introduce ContextNet framework and then describe the key components in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ContextNet Framework</head><p>As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, we propose a novel CTR Framework named ContextNet that implicitly models high-order feature interactions by dynamically refining the feature embedding according to the context information contained in input instance. ContextNet consists of two variable components: contextual embedding module and Con-textNet block. Contextual embedding module aggregates contextual information in the same instance for each feature and projects the collected contextual information to the same low-dimensional space as feature embedding lies in. Notice that the input of contextual embedding module is always from the feature embedding layer. ContextNet block implicitly model high-order interactions by merging contextual information into each feature's feature embedding firstly, and then conduct the non-linear transformation on the merged embedding in order to better capture high-order interactions. We can stack ContextNet block by block to form deeper network and the refined feature's embedding output of the previous block is the input of the next one. The different ContextNet block has the corresponding contextual embedding module to refine each feature's embedding. The final ContextNet block's output is feed into the prediction layer to give the instance's prediction value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Embedding</head><p>The input data of CTR tasks usually consists of sparse and dense features. Such features are encoded as one-hot vectors which often lead to excessively high-dimensional feature spaces for large vocabularies. The common solution to this problem is to introduce the embedding layer. Generally, the sparse input can be formulated as:</p><formula xml:id="formula_0">= [ 1 , 2 , ..., ]<label>(1)</label></formula><p>where denotes the number of fields, and ? R denotes a onehot vector for a categorical field with features and ? R is vector with only one value for a numerical field. We can obtain feature embedding for one-hot vector via:</p><formula xml:id="formula_1">= (2)</formula><p>where ? R ? is the embedding matrix of features and is the dimension of field embedding. The numerical feature can also be converted into the same low-dimensional space by:</p><formula xml:id="formula_2">= (3)</formula><p>where ? R is the corresponding field embedding with size .</p><p>Through the aforementioned method, an embedding layer is applied upon the raw feature input to compress it to a low dimensional, dense real-value vector. The result of embedding layer is a wide concatenated vector:</p><formula xml:id="formula_3">= ( 1 , 2 , ..., , ..., )<label>(4)</label></formula><p>where denotes the number of fields, and ? R denotes the embedding of one field. Although the feature lengths of instances can be various, their embeddings are of the same length ? , where is the dimension of field embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contextual Embedding</head><p>As discussed in Section 3.1, the contextual embedding module in ContextNet has two objectives: Firstly, ContextNet use this module to aggregate contextual information for each feature from input instance, that is to say, feature embedding layer. Secondly, the collected contextual information for one feature is projected to the same low-dimensional space as feature embedding lies in. We can formulate this process as follows:</p><formula xml:id="formula_4">= F (F ( , ; ? ); ? )<label>(5)</label></formula><p>where ? R denotes the contextual embedding of the -th feature and is the dimension of field embedding, F ( , ; ? ) is the contextual information aggregation function for the -th feature field which uses the embedding layer and feature embedding as input and ? denotes the parameters of aggregation model. F ( ; ? ) is the mapping function to project the contextual information into the same low-dimensional space with feature embedding lies in. ? denotes the parameters of projection model.</p><p>To make this module more specific, We propose a two-layer contextual embedding network (TCE) for this module in our paper. That is to say, we adapt the feed forward network as the aggregation function F ( , ; ? ) and projection function F ( ; ? ). Notice here that TCE is just a specific solution for this module and there are other options that deserve further exploration. However, the input of the contextual embedding module should be from embedding layer which contains original and global contextual information.</p><p>Next we will describe how contextual embedding network works. Suppose we have a feature which belongs to feature field . As depicted in <ref type="figure">Figure 2</ref>, two fully connected (FC) layers are used in TCE module. The first FC layer is called "aggregation layer" which is a relatively wider layer to collect the contextual information from embedding layer with parameters . The second FC layer is "projection layer" which projects the contextual information into the same low-dimension space with feature embedding and reduces dimensionality to the same size as feature embedding has. The projection layer has parameter . Formally,</p><formula xml:id="formula_5">= F (F ( , ; ? ); ? ) = ( ( )) (6)</formula><p>where ? R = ? refers to the embedding layer of input instance, ? R ? and ? R ? are parameters for aggregation and projection layer in TCE for field , respectively. and respectively denotes the neural number of aggregation and projection layer. Notice here that the aggregation layer is usually wider than the projection layer because the size of the projection layer is required to be equal to the feature embedding size . The wider aggregation layer will make the model be more expressive.</p><p>We can see from formula <ref type="formula">(6)</ref> that each feature field maintains its own parameters and for aggregation and projection layer, respectively. Suppose we have different feature fields in embedding layer, the parameter number of TCE will be * ( + ). We can reduce parameter number by sharing in aggregation layer among all fields. The parameter number of TCE will be reduce to + * . In order to reduce the model parameters, we adopt the following strategy: we share the parameters of aggregation layer among all feature fields while keep the parameters of projection layer private for each feature field. This strategy effectively balances the model complexity and the model's expressive ability because the private projection layer will make each feature extract useful contextual information for its purpose independently. This makes the TCE module look like the "share-bottom" structure in multi-task learning where the bottom hidden layers are shared across tasks as works <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b3">[4]</ref> did.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ContextNet Block</head><p>As discussed in Section 3.1, ContextNet block is used to dynamically refine each feature's embedding by merging the contextual embedding produced for that feature to implicitly capture the highorder feature interactions. To achieve this goal, there are two consequential procedures in ContenxtNet block as shown in <ref type="figure" target="#fig_0">Figure 1</ref>: embedding merging and a following non-linear transformation. We can stack ContextNet block by block to form deep network and the output of the previous block is the input of the next block.</p><p>Next we will describe how ContextNet block works. We use to denote the output feature embedding of the -th block, that is to say, is the input embedding of the -th feature for the ( + 1)-th block. +1 denotes the corresponding contextual embedding computed by TCE for the -th feature field in the ( + 1)-th block.</p><p>We can describe this process for the -th feature as follows:</p><formula xml:id="formula_6">+1 = F ? (F ( , +1 ; ? ); ? )<label>(7)</label></formula><p>where +1 ? R denotes the fine-tuned feature embedding outputted by the ( + 1)-th ContextNet block for the -th feature and is the dimension of field embedding, F ( , +1 ; ? ) is the merging function for the -th feature which uses the previous block's output feature embedding and contextual embedding +1 in current block as input. ? denotes the parameters pf merging function. F ? ( ; ? ) is the mapping function to conduct the non-linear transformation on the merged embedding in order to further capture high-order interactions for the -th feature. ? denotes the parameters of non-linear transformation function.</p><p>As for the merging function ( , +1 ; ? ), Hadamard product is used in this work to merge the feature embedding and the corresponding contextual embedding +1 as follows:</p><formula xml:id="formula_7">? +1 = 1 ? +1 1 , ..., ? +1 , ..., ? +1<label>(8)</label></formula><p>where is the size of embedding vector and contextual embedding +1 for the -th feature. Hadamard product is a kind of element-wise production operation without parameters.</p><p>As for the non-linear function ? ( ; ? ), we propose two neural networks which are shown in <ref type="figure">Figure 3</ref> in this paper: point-wise feed-forward network and single-layer feed-forward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-Wise Feed-Forward Network:</head><p>Though the ContextNet uses the contextual embedding module to aggregate all other feature's embedding and then project them to a fixed embedding size, ultimately it is still a linear model. For endowing the model with nonlinearity in order to capture highorder interactions better, we can apply a point-wise two-layer feedforward network to all identically (sharing parameters among all feature field):</p><formula xml:id="formula_8">F = ( ; ? ) = ( ( 1 ) 2 + )<label>(9)</label></formula><p>where 1 , 2 are ? matrices and is the dimension of field embedding. We also adapt the residual connection and layer normalization ( ) <ref type="bibr" target="#b0">[1]</ref>. The extra parameter number introduced by FFN is 1 + 2 which is not large because of the parameter sharing in FFN. ContextNet with this version FFN is called "ContextNet-PFFN" in the following part of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Layer Feed-Forward Network:</head><p>We propose another much simpler one-layer feed-forward network by reducing the residual connection and ReLU nonlinearity. We can apply this transformation to all identically with sharing parameters as follows:</p><formula xml:id="formula_9">F = ( ; ? ) = ( 1 )<label>(10)</label></formula><p>where 1 is ? matrix and the bias is also discarded. The layer normalization will bring the nonlinearity to high-order feature interactions though the ReLU is removed from the mapping function. The extra parameter number introduced by FFN is 1 which is small because of the parameter sharing in FFN. We call ContextNet with this version FFN "ContextNet-SFFN" in the following part of this paper. Though much simpler in the mapping form compared with ContextNet-PFFN, ContextNet-SFFN has comparable or even better performance in many datasets and we will discuss this in detail in Section 4.2.</p><p>The different ContextNet block has the corresponding contextual embedding module to refine each feature's embedding when we stack multi-block to form deeper network. We can further reduce the model parameter by sharing the parameters of aggregation layer or projection layer among TCE modules for each ContextNet block. The experiments are conducted about these three different parameter sharing strategies and we will discuss this in detail in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction Layer</head><p>To summarize, we give the overall formulation of our proposed model's output as:?=</p><formula xml:id="formula_10">( 0 + ?? * =1 )<label>(11)</label></formula><p>where?? (0, 1) is the predicted value of CTR, is the sigmoid function, is the feature filed number, is the feature embedding size, is the bit value of all feature's embedding vectors outputted by the last ContextNet block and is the learned weight for each bit value.</p><p>For binary classifications, the loss function is the log loss:</p><formula xml:id="formula_11">L = ? 1 ?? =1 log(?) + (1 ? ) log(1 ??)<label>(12)</label></formula><p>where is the total number of training instances, is the ground truth of -th instance and?is the predicted CTR. The optimization process is to minimize the following objective function:</p><formula xml:id="formula_12">= L + ???<label>(13)</label></formula><p>where denotes the regularization term and ? denotes the set of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Interpretability of the ContextNet</head><p>Compared with simple models such as LR <ref type="bibr" target="#b20">[20]</ref>, DNN models are notorious for lack of interpretability because of the non-linearity introduced by widely used MLP layers. One advantage of ContextNet over most DNN models is that it has good model interpretability.</p><p>The last ContextBlock's outputs maintain each feature's embedding which have merged useful high-order information and the prediction layer of ContextNet is actually a LR model. So we can easily compute each feature's weight and contribution to the final prediction score given an input instance as follows:</p><formula xml:id="formula_13">= ?? =1<label>(14)</label></formula><p>where ? R is the weight score of the -th feature in an instance. ? R is the embedding of the -th feature outputted by last ContextBlock and is the learned weight in prediction layer for bit . is the size of feature embedding. Positive score leads to positive label and negative score leads to negative label.</p><p>For a specific instance, we can detect important features which can explain why the instance has the final prediction label according to formula <ref type="bibr" target="#b14">(14)</ref>. We can also compute the feature importance on the whole training set level by accumulating or averaging scores as follows:</p><formula xml:id="formula_14">= ?? =1 | | (15) = ( ?? =1 | |)/( + )<label>(16)</label></formula><p>where ? R is the weight score of the -th feature in the whole set.</p><p>is the score for the -th feature in instance and the size of the training set is . The absolute value is adopted here because the score is either positive or negative. is the size of instance set where a feature appears and is a norm number to reduce the influence of low frequent features. We can find out the important features according to formula <ref type="bibr" target="#b16">(16)</ref> or discarding unimportant features with small scores to compress model according to formula <ref type="bibr" target="#b15">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULT</head><p>We evaluate the proposed approaches on four real-world datasets and answer the following research questions:</p><p>? RQ1 Does the proposed method performs better than existing state-of-the-art deep learning based CTR models? ? RQ2 What is the training efficiency of ContextNet? ? RQ3 What is the influence of various components in the ContextNet architecture? ? RQ4 How does the hyper-parameters of networks influence the performance of ContextNet? ? RQ5 Can ContextNet really gradually refine the feature embedding to capture feature interactions? How about the feature importance computation?</p><p>In the following, we will first describe the experimental settings, followed by answering the above research questions. (1) Criteo 1 Dataset: As a very famous public real world display ad dataset with each ad display information and corresponding user click feedback, Criteo data set is widely used in We randomly split instances by 8:1:1 for training , validation and test while <ref type="table" target="#tab_0">Table 1</ref> lists the statistics of the evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics. AUC (Area Under ROC) is used in our experiments as the evaluation metric. AUC's upper bound is 1 and larger value indicates a better performance.</head><p>RelaImp is also adopted as work <ref type="bibr" target="#b34">[34]</ref> does to measure the relative AUC improvements over the corresponding baseline model as another evaluation metric. Since AUC is 0.5 from a random strategy, we can remove the constant part of the AUC score and formalize the RelaImp as:</p><formula xml:id="formula_15">= ( ) ? 0.5 ( ) ? 0.5 ? 1<label>(17)</label></formula><p>Log loss is another widely used metric in binary classification, measuring the distance between two distributions. The log loss results of our experiments show similar trends with AUC, so we didn't present performances in this metric because of the limited space of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Models for Comparisons.</head><p>We compare the performance of the following models with our proposed approaches: FM, DNN, DeepFM, Deep&amp;Cross Network(DCN), xDeepFM, Transformer and AutoInt Model, all of which are discussed in Section 2. For DCN, xDeepFM, Transformer and AutoInt, 3-order feature interaction network structure is adopted as default setting as the original papers use. FM is considered as the base model in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement all the models with Tensorflow in our experiments. For optimization method, we use the Adam with a mini-batch size of 1024 and a learning rate is set to 0.0001. Focusing on neural networks structures in our paper, we make the dimension of field embedding for all models to be a fixed value of 10. For models with DNN part, the depth of hidden layers is set to 3, the number of neurons per layer is 400, all activation function are ReLU. Except for special mention, we have the default settings as follows for ContextNet: feature embedding is 10, default model has 3 ContextBlocks and hidden layer size of TCE module is 20. We conduct our experiments with 2 Tesla 40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison (RQ1)</head><p>The overall performance for CTR prediction of different models on four evaluation datasets is shown in <ref type="table" target="#tab_1">Table 2</ref>. We have the following key observations:</p><p>(1) ContextNet achieves the best performance on all four datasets and obtains significant improvements over the state-of-theart methods. It can boost the accuracy over the baseline FM by 2.80% to 11.17%, baseline DeepFM by 1.06% to 5.02%, as well as the best of DNN baselines by 0.78% to 4.24%.</p><p>We also conduct a significance test to verify that our proposed models outperforms baselines with the significance level = 0.01. It explicitly proves the proposed ContextNet indeed yields strong learning capacity by modeling highorder interactions implicitly using the feature contextual embedding. (2) As for the comparison of ContextNet-PFFN and ContextNet-SFFN, we can see from <ref type="table" target="#tab_1">Table 2</ref> that ContextNet-SFFN consistently outperforms ContextNet-PFFN model on all four datasets with the same settings, though it's much simpler in model structure and has less parameters. This means ContextNet-SFFN is a more applicable model in real-world applications. (3) For models which explicitly introduce high-order feature interactions by sub-network such as DCN, xDeepFM , Transformer and AutoInt, xDeepFM outperforms other two models on three datasets while DCN's performance is best on ML-1m dataset. Compared with DCN and xDeepFM, Both AutoInt and Transformer model show no advantage on any dataset. That means feature interaction in multiplicative way after feature aggregation by a specific network indeed has an advantage over feature aggregation in summation way after pair-wise interaction as AutoInt and Transformer did.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Efficiency (RQ2)</head><p>As mentioned in Section 3.4, In order to reduce the parameters of the linear contextual embedding module, we can share the parameters in TCE module for different ContextNet block. We have the following three strategies: 1) share the parameters in aggregation layer among corresponding TCE modules for each block(Share-A); 2) share the parameters in both aggregation layer and projection layer(Share-A&amp;P); 3) we don't share parameters(Share-Nothing), which is a default setting for all other experiments if not specially mentioned.  We conduct some experiments to explore the influence of the different parameter-sharing strategies on the model performance and <ref type="table" target="#tab_2">Table 3</ref> shows the results. We can see from the results that the performances of Share-Nothing and Share-A strategies are comparable on two datasets. However, the performance will degrade greatly if we share the parameters both in aggregation layer and projection layer. This indicates that it's very critical for the model's good performance for TCE module to extract different high-order interaction information for each refined feature of different Con-textNet block. Compared with Share-Nothing strategy, Share-A is maybe a better choice because it has less parameter and can maintain good model performance.</p><p>To compare the model efficiency of different models, we use the runtime per epoch as evaluation metric. DNN and DeepFM are regarded as efficiency baseline because they are relatively simple in network structure and are widely used in many real life applications. The comparison is conducted on Criteo dataset and the results are shown in <ref type="figure">Figure 4</ref>. xDeepFM is much more time-consuming compared with all the other models and this implies xDeepFM is hard to be applied in many real life scenarios. As for the training efficiency of our proposed ContextNet models, we can see that both the ContextNet-PFFN and ContextNet-SFFN have faster training speed compared with AutoInt and xDeepFM. If we share the parameters in aggregation layer of two proposed ContextNet models, the training speed can be further increased. The ContextNet-SFFN model with Share-A strategy can run just slightly slower than baseline model, which means that ContextNet is sufficiently efficient for real world applications.  We can see the usefulness of the different components of Con-textNet from the above-mentioned ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-Parameter Study(RQ4)</head><p>In this section, we study the impact of hyper-parameters on Con-textNet, including (1) the number of ContextNet blocks; (2) the number of feature embedding size. The experiments are conducted on Criteo and Malware datasets via changing one hyper-parameter while holding the other settings. Other two datasets show the similar trends and we didn't present them because of the limited space.</p><p>Number of ContextNet Blocks. To explore the influence of the number of ContextNet's blocks on model's performance, we conduct some experiments on Criteo dataset to stack blocks of ContextNet-PFFN model from 1 block to 11 blocks. <ref type="figure" target="#fig_3">Figure 5</ref> shows the experimental results. It can be observe that the performance increases with more blocks at the beginning and the performance can be maintained until the number is set greater than 10. Considering there are 4 hidden layers in one block of Context-PFFN model, we can see that ContextNet-PFFN has the depth of nearly 40 hidden layers in the model while keeping good performance. This may indicate that contextual embedding module helps the trainability of very deep network in CTR tasks.</p><p>Number of Feature Embedding Size. The results in <ref type="table" target="#tab_4">Table 5</ref> show the impact of the number of feature embedding size on model performance. We can observe that the performance of ContextNet-PFFN increases with the increase of embedding size at the beginning. However, model performance degrades when the embedding size is set greater than 50. Over-fitting of deep network is maybe the reason for this. Compared with the performance of the model shown in <ref type="table" target="#tab_1">Table 2</ref> which has a embedding size of 10, the bigger embedding size further increases model's performance on two datasets.   To verify that ContextNet indeed dynamically changes each feature's embedding block by block to capture feature interactions, we input a randomly sampled instance ( <ref type="figure">Figure 6</ref>, the instance has positive label with estimated CTR score 0.975) from ML-1m dataset into trained ContextNet-PFFN. Then we compute the dot product for each feature pair using each ContextBlock's outputted feature embedding, including the feature embedding layer. The high positive dot product value means the two feature have similar embedding content and are highly correlated interactions. The <ref type="figure">Figure 7</ref> shows the result. The deeper green color means bigger positive value while deeper red color denotes bigger negative dot product value. The elements on the diagonal of the matrix can be ignored because that's each feature's self dot product. We can see from the <ref type="figure">Figure 7</ref> that: For features in feature embedding layer, most dot product values are small numbers near zero, which indicates the embedding values are very small and there is no correlation between different fields of input features. <ref type="figure">( Fig. a of Figure 7</ref>). However, each feature's embedding is dynamically changed by ContextBlock to gradually find the most useful feature interactions for its own purpose: more bigger dot product values begin to appear which means correlations between features. Take the field "gender" as an example, if we adopt 5.0 as the threshold, the highly correlated features change from "occupation" <ref type="figure">( Fig. b of Figure 7</ref>) to "age" and "movie genres" <ref type="figure">( Fig.  c of Figure 7</ref>). The final useful feature interactions focus on the features from field "age", "movie year" and "movie genres" <ref type="figure">( Fig. d  of Figure 7)</ref>.</p><p>As for the feature importance analysis, we provide examples shown in <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 8</ref>. The feature contributes most to the final prediction score is "age"=35 in this instance( <ref type="figure">Figure 6</ref>). <ref type="figure">Figure  8</ref> shows the most important features in ML-1m dataset according to formula <ref type="bibr" target="#b16">(16)</ref> in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, We firstly propose a novel CTR Framework named ContextNet that implicitly models high-order feature interactions by dynamically refining the feature embedding. We also propose two specific models(ContextNet-PFFN and ContextNet-SFFN) under this framework. We conduct extensive experiments on four real-world datasets and the experiment results demonstrate that our proposed ContextNet-PFFN and ContextNet-SFFN model outperform state-of-the-art models such as DeepFM and xDeepFM significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Neural Structure of ContextNet Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Two Layer Contextual Embedding Structure of Non-Linear Transformation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 1 . 1</head><label>11</label><figDesc>Datasets. The following four data sets are used in our experiments:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Effect of Different Blocks on Model Performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>0.8109 0.8111 0.8113 0.8097 Malware 0.7399 0.7416 0.7417 0.7414 0.7405</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :Figure 8 :</head><label>678</label><figDesc>Example Instance from ML-1m Analysis of Dynamic Feature Embedding Top 10 Features of ML-1m Dataset 4.6 Analysis of Dynamic Feature Embedding (RQ5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets Avazu 4 Dataset: The Avazu dataset consists of several days of ad click-through data which is ordered chronologically.</figDesc><table><row><cell>Datasets</cell><cell cols="3">#Instances #fields #features</cell></row><row><cell>Criteo</cell><cell>45M</cell><cell>39</cell><cell>30M</cell></row><row><cell>Movielens</cell><cell>1M</cell><cell>7</cell><cell>7478</cell></row><row><cell>Malware</cell><cell>8.92M</cell><cell>82</cell><cell>0.97M</cell></row><row><cell>Avazu</cell><cell>40.43M</cell><cell>24</cell><cell>9.5M</cell></row><row><cell cols="4">many CTR model evaluation. There are 26 anonymous cate-</cell></row><row><cell cols="4">gorical fields and 13 continuous feature fields in Criteo data</cell></row><row><cell>set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(2) MovieLens 2 Dataset: MovieLens is a popular benchmark</cell></row><row><cell cols="4">dataset for evaluating recommendation algorithms. We adopt</cell></row><row><cell cols="4">the well-established MovieLens 1m(ML-1m) as our evalua-</cell></row><row><cell cols="4">tion dataset in this work, which contains 1 million ratings</cell></row><row><cell cols="3">from 6000 users on 4000 movies.</cell><cell></cell></row><row><cell cols="4">(3) Malware 3 Dataset: Malware is a dataset to predict a Win-</cell></row><row><cell cols="4">dows machine's probability of getting infected. The malware</cell></row><row><cell cols="4">prediction task can be formulated as a binary classification</cell></row><row><cell cols="4">problem like a typical CTR estimation task does.</cell></row><row><cell>(4)</cell><cell></cell><cell></cell><cell></cell></row></table><note>For each click data, there are 24 fields which indicate ele- ments of a single ad impression.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall performance (AUC) of different models on four datasets(CNet-PFFN means ContextNet-PFFN while CNet-SFFN means ContextNet-SFFN)</figDesc><table><row><cell></cell><cell cols="2">Criteo</cell><cell cols="2">ML-1m</cell><cell cols="2">Malware</cell><cell cols="2">Avazu</cell></row><row><cell></cell><cell>AUC</cell><cell>RelaImp</cell><cell>AUC</cell><cell>RelaImp</cell><cell>AUC</cell><cell>RelaImp</cell><cell>AUC</cell><cell>RelaImp</cell></row><row><cell>FM</cell><cell>0.7895</cell><cell>+0.00%</cell><cell>0.8446</cell><cell>+0.00%</cell><cell>0.7166</cell><cell>+0.00%</cell><cell>0.7785</cell><cell>+0.00%</cell></row><row><cell>DNN</cell><cell>0.8054</cell><cell>+5.49%</cell><cell>0.8527</cell><cell>+2.35%</cell><cell>0.7246</cell><cell>+3.70%</cell><cell>0.7820</cell><cell>+1.26%</cell></row><row><cell>DeepFM</cell><cell>0.8057</cell><cell>+5.60%</cell><cell>0.8537</cell><cell>+2.64%</cell><cell>0.7293</cell><cell>+5.86%</cell><cell>0.7833</cell><cell>+1.72%</cell></row><row><cell>DCN</cell><cell>0.8058</cell><cell>+5.63%</cell><cell>0.8595</cell><cell>+4.32%</cell><cell>0.7300</cell><cell>+6.19%</cell><cell>0.7830</cell><cell>+1.62%</cell></row><row><cell>xDeepFM</cell><cell>0.8064</cell><cell>+5.84%</cell><cell>0.8561</cell><cell>+3.34%</cell><cell>0.7310</cell><cell>+6.65%</cell><cell>0.7841</cell><cell>+2.01%</cell></row><row><cell cols="2">Transformer 0.8037</cell><cell>+4.90%</cell><cell>0.8578</cell><cell>+3.83%</cell><cell>0.7267</cell><cell>+4.66%</cell><cell cols="2">0.7819 +1.125%</cell></row><row><cell>AutoInt</cell><cell>0.8051</cell><cell>+5.39%</cell><cell>0.8569</cell><cell>+3.57%</cell><cell>0.7282</cell><cell>+5.36%</cell><cell>0.7824</cell><cell>+1.40%</cell></row><row><cell>CNet-PFFN</cell><cell>0.8104</cell><cell>+7.22%</cell><cell>0.8641</cell><cell>+5.66%</cell><cell cols="3">0.7399 +10.76% 0.7862</cell><cell>+2.76%</cell></row><row><cell>CNet-SFFN</cell><cell cols="8">0.8107 +7.32% 0.8681 +6.82% 0.7408 +11.17% 0.7863 +2.80%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Overall performance (AUC) of different param-</cell></row><row><cell cols="3">eter sharing strategies of Linear Contextual Module in</cell></row><row><cell>ContextNet-PFFN)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Criteo Malware</cell></row><row><cell cols="2">Share-Nothing 0.8104</cell><cell>0.7399</cell></row><row><cell>Share-A</cell><cell>0.8094</cell><cell>0.7400</cell></row><row><cell>Share-A&amp;P</cell><cell>0.7926</cell><cell>0.7117</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Remove LN or RC. From the results inTable 4, we can see that removing either LN or RC also decreases model performance, though the performance degradation is not as much as that of removing ICE and FFN.</figDesc><table><row><cell cols="2">Overall perfor-</cell><cell></cell></row><row><cell cols="2">mance (AUC) of models</cell><cell></cell></row><row><cell cols="2">removing different com-</cell><cell></cell></row><row><cell cols="2">ponents of ContextNet-</cell><cell></cell></row><row><cell>PFFN)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Criteo Malware</cell></row><row><cell cols="2">ContextNet-PFFN 0.8104</cell><cell>0.7399</cell></row><row><cell>-w/o TCE -w/o FFN -w/o LN -w/o RC</cell><cell>0.7923 0.8043 0.8069 0.8098</cell><cell>0.7125 Figure 4: Efficiency com-0.7354 parison of different mod-0.7373 els in terms of run time per 0.7380 epoch on Criteo dataset</cell></row><row><cell cols="3">4.4 Ablation Study (RQ3)</cell></row><row><cell cols="3">In this section, we perform ablation experiments over key compo-</cell></row><row><cell cols="3">nents of ContextNet in order to better understand their impacts on</cell></row><row><cell cols="3">Criteo and Malware datasets(the other two datasets show similar</cell></row><row><cell cols="3">trend), including linear contextual embedding (TCE), feed-forward</cell></row><row><cell cols="3">network (FFN), layer normalization (LN), and residual connection</cell></row><row><cell cols="3">(RC) . Table 4 shows the results of our default version (Block = 3),</cell></row><row><cell cols="2">and its variants on two datasets.</cell><cell></cell></row><row><cell cols="3">(1) Remove TCE: The performance of ContextNet-PFFN dra-</cell></row><row><cell cols="3">matically degrades on both datasets without TCE module.</cell></row><row><cell cols="3">It tells us that the contextual information gathered by TCE</cell></row><row><cell cols="3">module is critical for the ContextNet and we deem TCE mod-</cell></row><row><cell cols="3">ule extracts different high-order interactions information for</cell></row><row><cell>various features.</cell><cell></cell><cell></cell></row><row><cell cols="3">(2) Remove FFN: Without FFN, the model performance also</cell></row><row><cell cols="3">degrades obviously and that may indicate the non-linear</cell></row><row><cell cols="3">transformation on the result of element-wise product of fea-</cell></row><row><cell cols="3">ture embedding and contextual information is also important</cell></row><row><cell>for ContextNet.</cell><cell></cell><cell></cell></row><row><cell>(3)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Overall performance (AUC) of different feature embedding size of ContextNet-PFFN</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Conference'17, July 2017, Washington, DC, USA Zhiqiang Wang, Qingyun She, PengTao Zhang, Junlin Zhang</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Criteo http://labs.criteo.com/downloads/download-terabyte-click-logs/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">MovieLens 1m. https://grouplens.org/datasets/movielens/1m/ 3 Malware https://www.kaggle.com/c/microsoft-malware-prediction 4 Avazu http://www.kaggle.com/c/avazu-ctr-prediction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Cross: Making Use of Context in Recurrent Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Eleventh ACM International Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask Learning: A Knowledge-Based Source of Inductive Bias ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google Scholar Google Scholar Digital Library Digital Library</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><forename type="middle">Quinonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Omnipress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: A View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2488200</idno>
		<ptr target="https://doi.org/10.1145/2487575.2488200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;13)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Data Mining. IEEE</title>
		<imprint>
			<biblScope unit="page" from="995" to="1000" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering vs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Factorization Revisited</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end neural segmental models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1254" to="1264" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep Feedback Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/345</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/345" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2491" to="2497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
