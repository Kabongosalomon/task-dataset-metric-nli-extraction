<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Controllable and Guided Face Synthesis for Unconstrained Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<email>liufeng6@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Jain</surname></persName>
							<email>jain@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Controllable and Guided Face Synthesis for Unconstrained Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face Synthesis</term>
					<term>Model Training</term>
					<term>Target Dataset Distribu- tion</term>
					<term>Unconstrained Face Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although significant advances have been made in face recognition (FR), FR in unconstrained environments remains challenging due to the domain gap between the semi-constrained training datasets and unconstrained testing scenarios. To address this problem, we propose a controllable face synthesis model (CFSM) that can mimic the distribution of target datasets in a style latent space. CFSM learns a linear subspace with orthogonal bases in the style latent space with precise control over the diversity and degree of synthesis. Furthermore, the pre-trained synthesis model can be guided by the FR model, making the resulting images more beneficial for FR model training. Besides, target dataset distributions are characterized by the learned orthogonal bases, which can be utilized to measure the distributional similarity among face datasets. Our approach yields significant performance gains on unconstrained benchmarks, such as IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face recognition (FR) is now one of the most well-studied problems in the area of computer vision and pattern recognition. The rapid progress in face recognition accuracy can be attributed to developments in deep neural network models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b73">74]</ref>, sophisticated design of loss functions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b94">95]</ref>, and large-scale training datasets, e.g., MS-Celeb-1M <ref type="bibr" target="#b23">[24]</ref> and Web-Face260M <ref type="bibr" target="#b96">[97]</ref>.</p><p>Despite this progress, state-of-the-art (SoTA) FR models do not work well on real-world surveillance imagery (unconstrained) due to the domain shift issue, that is, the large-scale training datasets (semi-constrained) obtained via webcrawled celebrity faces lack in-the-wild variations, such as inherent sensor noise, low resolution, motion blur, turbulence effect, etc. For instance, 1:1 verification accuracy reported by one of the SoTA models <ref type="bibr" target="#b67">[68]</ref> on unconstrained IJB-S <ref type="bibr" target="#b33">[34]</ref> dataset is about 30% lower than on semi-constrained LFW <ref type="bibr" target="#b29">[30]</ref>. A potential remedy to such a performance gap is to assemble a large-scale unconstrained face dataset. However, constructing such a training dataset with tens of thousands of subjects is prohibitively difficult with high manual labeling cost. An alternative solution is to develop facial image generation models that can synthesize face images with desired properties. Face translation or synthesis using GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b91">92]</ref> or 3D face reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b95">96]</ref> has been well studied in photo-realistic image generation. However, most of these methods mainly focus on face image restoration or editing, and hence do not lead to better face recognition accuracies. A recent line of research <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b78">79]</ref> adopts disentangled face synthesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b77">78]</ref>, which can provide control over explicit facial properties (pose, expression and illumination) for generating additional synthetic data for varied training data distributions. However, the handcrafted categorization of facial properties and lack of design for cross-domain translation limits their generalizability to challenging testing scenarios. <ref type="bibr">Shi et al. [69]</ref> propose to use an unlabeled dataset to boost unconstrained face recognition. However, all of the previous methods can be considered as performing blind data augmentation, i.e., without the feedback of the FR model, which is required to provide critical information for improving the FR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesis Module</head><p>In <ref type="figure" target="#fig_0">Fig. 1</ref>, we show the difference between a blind and feedback-based face synthesis paradigm. For blind face synthesis, the FR model does not take part in the synthesis process, so there is no guidance from the FR model to avoid trivial synthesis. With feedback from the FR model, as in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, synthesized images can be more relevant to increasing the FR performance. Therefore, it is the goal of our paper to allow the FR model to guide the face synthesis towards creating synthetic datasets that can improve the FR performance.</p><p>It is not trivial to incorporate the signal from the FR model, as the direct manipulation of an input image towards decreasing the FR loss results in adversarial images that are not analogous to the real image distribution <ref type="bibr" target="#b22">[23]</ref>. We thus propose to learn manipulation in the subspace of the style space of the target properties, so that the control can be accomplished 1) in low-dimensions, 2) semantically meaningful along with various quality factors.</p><p>In light of this, this paper aims to answer these three questions: 1. Can we learn a face synthesis model that can discover the styles in the target unconstrained data, which enables us to precisely control and increase the diversity of the labeled training samples?</p><p>2. Can we incorporate the feedback provided by the FR model in generating synthetic training data, towards facilitating FR model training?</p><p>3. Additionally, as a by-product of our proposed style based synthesis, can we model the distribution of a target dataset, so that it allows us to quantify the distributional similarity among face datasets? Towards this end, we propose a face synthesis model that is 1) controllable in the synthesis process and 2) guided in the sense that the sample generation is aided by the signal from the FR model. Specifically, given a labeled training sample set, our controllable face synthesis model (CFSM) is trained to discover different attributes of the unconstrained target dataset in a style latent space. To learn the explicit degree and direction that control the styles in an unsupervised manner, we embed one linear subspace model with orthogonal bases into the style latent space. Within generative adversarial training, the face synthesis model seeks to capture the principal variations of the data distribution, and the style feature magnitude controls the degree of manipulation in the synthesis process.</p><p>More importantly, to extract the feedback of the FR model, we apply adversarial perturbations (FGSM) in the learned style latent space to guide the sample generation. This feedback is rendered meaningful and efficient because the manipulation is in the low dimensional style space as opposed to in the high dimensional image space. With the feedback from the FR model, the synthesized images are more beneficial to the FR performance, leading to significantly improved generalization capabilities of the FR models trained with them. It is worth noting that our pre-trained synthesis models could be a plug-in to any SoTA FR model. Unlike the conventional face synthesis models that focus on high quality realistic facial images, our face synthesis module is a conditional mapping from one image to a set of style shifted images that match the distribution of the target unconstrained dataset towards boosting its FR performance.</p><p>Additionally, the learned orthogonal bases characterize the target dataset distribution that could be utilized to quantify distribution similarity between datasets. The quantification of datasets has broad impact on various aspects. For example, knowing the dataset distribution similarity could be utilized to gauge the expected performance of FR systems in new datasets. Likewise, given a choice of various datasets to train an FR model, one can find one closest to the testing scenario of interest. Finally, when a new face dataset is captured in the future, we may also access its similarity to existing datasets in terms of styles, in addition to typical metrics such as number of subjects, demographics, etc.</p><p>In summary, the contributions of this work include: ? We show that a controllable face synthesis model with linear subspace style representation can generate facial images of the target dataset style, with precise control in the magnitude and type of style.</p><p>? We show that FR model performance can be greatly increased by synthesized images when the feedback of the FR model is used to optimize the latent style coefficient during image synthesis.</p><p>? Our learned linear subspace model can characterize the target dataset distribution for quantifying the distribution similarity between face datasets.</p><p>? Our approach yields significant performance gains on unconstrained face recognition benchmarks, such as IJB-B, IJB-C, IJB-S and TinyFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Controllable Face Synthesis With the remarkable ability of GANs <ref type="bibr" target="#b21">[22]</ref>, face synthesis has seen rapid developments, such as StyleGAN <ref type="bibr" target="#b39">[40]</ref> and its variations <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> which can generate high-fidelity face images from random noises. Lately, GANs have seen widespread use in face image translation or manipulation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b89">90]</ref>. These methods typically adopt an encoderdecoder/generator-discriminator paradigm where the encoder embeds images into disentangled latent representations characterizing different face properties. Another line of works incorporates 3D prior (i.e., 3DMM <ref type="bibr" target="#b4">[5]</ref>) into GAN for 3D-controllable face synthesis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b66">67]</ref>. Also, EigenGAN <ref type="bibr" target="#b25">[26]</ref> introduces the linear subspace model into each generator layer, which enables to discover layer-wise interpretable variations. Unfortunately, these methods mainly focus on high-quality face generation or editing on pose, illumination and age, which has a well-defined semantic meaning. However, style or domain differences are hard to be factorized at the semantic level. Therefore, we utilize learned bases to cover unconstrained dataset attributes, such as resolution, noise, etc. Face Synthesis for Recognition Early attempts exploit disentangled face synthesis to generate additional synthetic training data for either reducing the negative effects of dataset bias in FR <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b78">79]</ref> or more efficient training of pose-invariant FR models <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94]</ref>, resulting in increased FR accuracy. However, these models only control limited face properties, such as pose, illumination and expression, which are not adequate for bridging the domain gap between the semi-constrained and unconstrained face data. The most pertinent study to our work is <ref type="bibr" target="#b68">[69]</ref>, which proposes to generalize face representations with auxiliary unlabeled data. Our framework differs in two aspects: i) our synthesis model is precisely-controllable in the style latent space, in both magnitude and direction, and ii) our synthesis model incorporates guidance from the FR model, which significantly improves the generalizability to unconstrained FR. Domain Generalization and Adaptation Domain Generalization (DG) aims to make DNN perform well on unseen domains <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Conventionally, for DG, few labeled samples are provided for the target domain to generalize. Popular DG methods utilize auxiliary losses such as Maximum Mean Discrepancy or domain adversarial loss to learn a shared feature space across multiple source domains <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b55">56]</ref>. In contrast, our method falls into the category of Unsupervised Domain Adaptation where adaptation is achieved by adversarial loss, contrastive loss or image translation, and learning a shared feature  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50]</ref>. Alpha-distance and discrepancy distance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b49">50]</ref> measures a dissimilarity that depends on a loss function and the predictor. To avoid the dependency on the label space, <ref type="bibr" target="#b0">[1]</ref> proposes OT distance, an optimal transport distance in the feature space. However, it still depends on the ability of the predictor to create a separable feature space across domains. Moreover, a feature extractor trained on one domain may not predict the resulting features as separable in a new domain. In contrast, we propose to utilize the learned linear bases for latent style codes, which are optimized for synthesizing images in target domains, to measure the dataset distance. The style-based distance has the benefit of not being dependent on the feature space or the label space.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Controllable Face Synthesis Model</head><p>Generally, for face recognition model training, we are given a labeled semiconstrained dataset that consists of n face images X = {X} n i=1 and the corresponding identity labels. Meanwhile, similar to the work in <ref type="bibr" target="#b68">[69]</ref>, we assume the availability of an unlabeled target face dataset with m images Y = {Y} m i=1 , which contains a large variety of unconstrained factors. Our goal is to learn a style latent space where the face synthesis model, given an input image from the semi-constrained dataset, can generate new face images of the same subject, whose style is similar to the target dataset. Due to the lack of corresponding images, we seek an unsupervised algorithm that can learn to translate between domains without paired input-output examples. In addition, we hope this face synthesis model has explicit dimensions to control the unconstrained attributes.</p><p>Our face synthesis model is not designed to translate the intrinsic properties between faces, i.e., pose, identity or expression. It is designed to focus on capturing the unconstrained imaging environment factors in unconstrained face images, such as noise, low resolution, motion blur, turbulence effects, etc. These variations are not present in large-scale labeled training data for face recognition. Multimodal Image Translation Network. We adopt a multimodal imageto-image translation network <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref> to discover the underlying style distribution in the target domain. Specifically, as shown in <ref type="figure">Fig. 2</ref>, our face synthesis generator consists of an encoder E and a decoder G. Given an input image X ? R W ?H?3 , the encoder first extracts its content features C = E(X). Then, the decoder generates the output imageX ? R W ?H?3 , conditioned on both the content features and a random style latent code z ? Z d :X = G(C, z). Here, the style code z is utilized to control the style of the output image.</p><p>Inspired by recent works that use affine transformation parameters in normalization layers to represent image styles <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>, we equip the residual blocks in the decoder D with Adaptive Instance Normalization (AdaIN) layers <ref type="bibr" target="#b30">[31]</ref>, whose parameters are dynamically generated by a multilayer perceptron (MLP) from the style code z. Formally, the decoder process can be presented a?</p><formula xml:id="formula_0">X = G(C, MLP(z)).<label>(1)</label></formula><p>It is worth noting that such G can model continuous distributions which enables us to generate multimodal outputs from a given input. We employ one adversarial discriminator D to match the distribution of the synthesized images to the target data distribution; images generated by the model should be indistinguishable from real images in the target domain. The discriminator loss can be described as:</p><formula xml:id="formula_1">L D = ?E Y?Y [log(D(Y))] ? E X?X ,z?Z [log(1 ? D(X))].<label>(2)</label></formula><p>The adversarial loss for the generator (including E and G) is then defined as:</p><formula xml:id="formula_2">L adv = ?E X?X ,z?Z [log(D(X))].<label>(3)</label></formula><p>Domain-Aware Linear Subspace Model. To enable precise control of the targeted face properties, achieving flexible image generation, we propose to embed a linear subspace model with orthogonal bases into the style latent space. As illustrated in <ref type="figure">Fig. 2</ref>, a random style coefficient o ? N q (0, I) can be used to linearly combine the bases and form a new style code z, as in</p><formula xml:id="formula_3">z = Uo + ?,<label>(4)</label></formula><p>where U = [u 1 , ? ? ? , u q ] ? R d?q is the orthonormal basis of the subspace. ? ? R d denotes the mean style. This equation relates a q-dimensional coefficient o to a corresponding d-dimensional style vector (q &lt;&lt; d) by an affine transformation and translation. During training, both U and ? are learnable parameters. The entire bases U are optimized with the orthogonality constraint <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_4">L ort = |U T U ? I| 1 , where I is an identity matrix.</formula><p>The isotropic prior distribution of o does not indicate which directions are useful. However, with the help of the subspace model, each basis vector in U identifies a latent direction that allows control over target image attributes that vary from straightforward high-level face properties. This mechanism is algorithmically simple, yet leads to effective control without requiring ad-hoc supervision. Accordingly, Eqn. 1 can be updated asX = G(C, MLP(Uo + ?)). Magnitude of the Style Coefficient and Identity Preservation. Although the adversarial learning (Eqn. 2 and 3) could encourage the face synthesis module to characterize the attributes in the target data, it cannot ensure the identity information is maintained in the output face image. Hence, the cosine similarity S C between the face feature vectors f (X) and f (X) is used to enforce identity preservation:</p><formula xml:id="formula_5">L id =1?S C (f (X), f (X)), where f (?)</formula><p>represents a pre-trained feature extractor, i.e., ArcFace <ref type="bibr" target="#b11">[12]</ref> in our implementation.</p><p>Besides identifying the meaningful latent direction, we continue to explore the property of the magnitude a = ||o|| of the style coefficient. We expect the magnitude can measure the degree of identity-preservation in the synthesized imageX. In other words, S C (f (X), f (X)) monotonically increases when the magnitude a is decreased. To realize this goal, we re-formulate the identity loss:</p><formula xml:id="formula_6">L id = 1 ? S C (f (X), f (X)) ? g(a) 2 2 ,<label>(5)</label></formula><p>where g(a) is a function with respect to a. We assume the magnitude a is bounded in [l a , u a ]. In our implementation, we define g(a) as a linear function on [l a , u a ] with g(l a ) = l m , g(u a ) = u m : g(a) = (a ? l a ) um?lm ua?la + l m . By simultaneously learning the direction and magnitude of the style latent coefficients, our model becomes precisely controllable in capturing the variability of faces in the target domain. To our knowledge, this is the first method which is able to explore the complete set of two properties associated with the style, namely direction and magnitude, in unsupervised multimodal face translation. Model learning. The total loss for the generator (including encoder E, decoder G and domain-aware linear subspace model), with weights ? i , is</p><formula xml:id="formula_7">L G = ? adv L adv + ? ort L ort + ? id L id .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Guided Face Synthesis for Face Recognition</head><p>In this section, we introduce how to incorporate the pre-trained face synthesis module into deep face representation learning, enhancing the generalizability to unconstrained FR. It is effectively addressing, which synthetic images, when added as an augmentation to the data, will increase the performance of the learned FR model in the unconstrained scenarios?</p><p>Formally, the FR model is trained to learn a mapping F, such that F(X) is discriminative for different subjects. If F is only trained on the domain defined by semi-constrained X , it does not generalize well to unconstrained scenarios. However, X with identity label l in a training batch may be augmented with a random style coefficient o to produce a synthesized imageX with CFSM.</p><p>However, such data synthesis with random style coefficients may generate either extremely easy or hard samples, which may be redundant or detrimental to the FR training. To address this issue, we introduce an adversarial regularization strategy to guide the data augmentation process, so that the face synthesis module is able to generate meaningful samples for the FR model. Specifically, for a given pre-trained CFSM, we apply adversarial perturbations in the learned style latent space, in the direction of maximizing the FR model loss. Mathematically, given the perturbation budget ?, the adversary tries to find a style latent perturbation ? ? R d to maximize the classification loss function L cla :</p><formula xml:id="formula_8">? * = arg max ||?||?&lt;? L cla (F(X * ), l) , where X * = G(E(X), MLP(U(o + ?) + ?)). (7)</formula><p>Here, X * denotes the perturbed synthesized image. L cla could be any classification-based loss, e.g., popular angular margin-based loss, ArcFace <ref type="bibr" target="#b11">[12]</ref> in our implementation. In this work, for efficiency, we adopt the one-step Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b22">[23]</ref> to obtain ? * and subsequently update o:</p><formula xml:id="formula_9">o * = o + ? * , ? * = ? ? sgn (? z L cla (F(X * ), l)) ,<label>(8)</label></formula><formula xml:id="formula_10">(a) (h) (b) (c) (d) (e) (f) (g) Original Synthesized "</formula><p>Guided * where ?L cla (?, ?) denotes the gradient of L cla (?, ?) w.r.t. o, and sgn(?) is the sign function. Finally, based on the adversarialbased augmented face images, we further optimize the face embedding model F via the objective:</p><formula xml:id="formula_11">min ? L cla ([X * , X], l),<label>(9)</label></formula><p>where ? indicates the parameters of FR model F and [?] refers to concatenation in the batch dimension. In other words, it encourages to search for the best perturbations in the learned style latent space in the direction of maximal difficulty for the FR model. Examples within a mini-batch are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Dataset Distribution Measure As mentioned above, as a by-product of our learned face synthesis model, we obtain a target-specific linear subspace model, which can characterize the variations in the target dataset. Such learned linear subspace models allow us to quantify the distribution similarity among different datasets. For example, given two unlabeled datasets A and B, we can learn the corresponding linear subspace models {U A , ? A } and {U B , ? B }. We define the distribution similarity between them as</p><formula xml:id="formula_12">S(A, B) = 1 q q i S C (u i A + ? A , u i B + ? B ) ,<label>(10)</label></formula><p>where S C (?, ?) denotes the Cosine Similarity between the corresponding basis vectors in U A and U B respectively, and q is the number of the basis vectors.</p><p>Measuring the distance or similarity between datasets is a fundamental concept underlying research areas such as domain adaptation and transfer learning. However, the solution to this problem typically involves measuring feature distance with respect to a learned model, which may be susceptible to modal failure that the model may encounter in unseen domains. In this work, we provide an alternative solution via learned style bases vectors, that are directly optimized to capture the characteristics of the target dataset. We hope our method could provide new understandings and creative insights in measuring the dataset similarity. For visualizations of S among different datasets, please refer to Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>All face images are aligned and resized into 112 ? 112 pixels. The network architecture of the face synthesis model is given in the supplementary (Supp). In the main experiments, we set q=10, d=128, l a =0, u a =6, l m =0.05, u m =0.65, ? adv =1, ? ort =1, ? id =8, ?=0.314. For more details, refer to Sec. 4 or Supp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with SoTA FR methods</head><p>Datasets Following the experimental setting of <ref type="bibr" target="#b68">[69]</ref>, we use MS-Celeb-1M <ref type="bibr" target="#b23">[24]</ref> as our labeled training dataset. MS-Celeb-1M is a large-scale public face dataset with web-crawled celebrity photos. For a fair comparison, we use the cleaned MS1M-V2 (3.9M images of 85.7K classes) from <ref type="bibr" target="#b68">[69]</ref>. For our target data, WiderFace [91] is used. WiderFace is a dataset collected for face detection in challenging scenarios, with a diverse set of unconstrained variations. It is a suitable target dataset for training CFSM, as we aim to bridge the gap between the semi-constrained training faces and the faces in challenging testing scenarios. We follow <ref type="bibr" target="#b68">[69]</ref> and use 70K face images from WiderFace. For evaluation, we test on four unconstrained face recognition benchmarks: IJB-B, IJB-C, IJB-S and TinyFace. These 4 datasets represent real-world testing scenarios where faces are significantly different from the semi-constrained training dataset.</p><p>? IJB-B <ref type="bibr" target="#b88">[89]</ref> contains both high-quality celebrity photos collected in the wild and low-quality photos or video frames with large variations. It consists of 1, 845 subjects with 21.8K still images and 55K frames from 7, 011 videos.</p><p>? IJB-C <ref type="bibr" target="#b50">[51]</ref> is an extension of IJB-B, which includes about 3, 500 subjects with a total of 31, 334 images and 117, 542 unconstrained video frames.  ? IJB-S [34] is an extremely challenging benchmark where the images were collected in real-world surveillance environments. The dataset contains 202 subjects with an average of 12 videos per subject. Each subject also has 7 highquality enrollment photos under different poses. We test on three protocols, Surveillance-to-Still (V2S), Surveillance-to-Booking (V2B) and Surveillanceto-Surveillance (V2V). The first/second notation in the protocol refers to the probe/gallery image source. 'Surveillance' (V) refers to the surveillance video, 'still' (S) refers to the frontal high-quality enrollment image and 'Booking' (B) refers to the 7 high-quality enrollment images.</p><p>? TinyFace [9] consists of 5, 139 labelled facial identities given by 169, 403 native low resolution face images, which is created to facilitate the investigation of unconstrained low-resolution face recognition. Experiment Setting We first train CFSM with ? 10% of MS-Celeb-1M training data (n = 0.4M) as the source domain, and WiderFace as the target domain (m = 70K). The model is trained for 125, 000 steps with a batch size of 32. Adam optimizer is used with ? 1 = 0.5 and ? 2 = 0.99 at a learning rate of 1e ? 4.</p><p>For the FR model training, we adopt ResNet-50 as modified in <ref type="bibr" target="#b11">[12]</ref> as the backbone and use ArcFace loss function <ref type="bibr" target="#b11">[12]</ref> for training. We also train a model without using CFSM (i.e., replication of ArcFace) for comparison, denoted as ArcFace. The efficacy of our method (ArcFace+Ours) is validated by training an FR model with the guided face synthesis as the auxiliary data augmentation during training according to Eq. 9. Results. Tables 1 and 2 respectively show the face verification and identification results on IJB-B and IJB-C datasets. Our approach achieves SoTA performance on most of the protocols. For IJB-B, performance increase from us- <ref type="table">Table 3</ref>. Comparison with state-of-the-art methods on three protocols of the IJB-S and TinyFace benchmark. The performance is reported in terms of rank retrieval (closed-set) and TAR@FAR (open-set). It is worth noting that MARN <ref type="bibr" target="#b20">[21]</ref> is a multimode aggregation method and is fine-tuned on UMDFaceVideo <ref type="bibr" target="#b2">[3]</ref>, a video dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Labeled Train Data Backbone IJB-S V2S IJB-S V2B IJB-S V2V TinyFace Rank1 Rank5 1% 10% Rank1 Rank5 1% 10% Rank1 Rank5 1% 10% Rank1 Rank5 C-FAN <ref type="bibr" target="#b19">[20]</ref> MS1M- * ResNet- <ref type="bibr" target="#b63">64</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IJB-S V2V (TAR@FAR=1e-1)</head><p>ArcFace ArcFace+Ours <ref type="figure">Fig. 4</ref>. Comparison on three IJB-S protocols with varied number of training data on the x-axis (maximum is 3.9M). The plots shows that using guided CFSM as an augmentation (ArcFace+Ours) can lead to higher performance in all settings. Note that CFSM trained on 70K unlabeled data is more useful than 3M original data as shown by the higher V2V performance of Ours with 0.5M than Baseline 3.9M.</p><p>ing CFSM (ArcFace+Ours) is 3.69% for TAR@FAR=1e ? 5, and 2.10% for TAR@FAR=1e ? 6 on IJB-C. Since both IJB-B and IJB-C are a mixture of high quality images and low quality videos, the performance gains with the augmented data indicate that our model can generalize to both high and low quality scenarios. In Tab. 3, we show the comparisons on IJB-S and TinyFace. With our CFSM (ArcFace+Ours), ArcFace model outperforms all the baselines in both face identification and verification tasks, and achieves a new SoTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation and Analysis</head><p>In this experiment, we compare the face verification and identification performance on the most challenging IJB-S and TinyFace datasets. Large-scale Training Data vs. Augmentation. To further validate the applicability of our CFSM as an augmentation in different training settings, we adopt IResNet-100 <ref type="bibr" target="#b11">[12]</ref> as the FR model backbone and utilize SoTA AdaFace loss function <ref type="bibr" target="#b42">[43]</ref> and large-scale WebFace12M <ref type="bibr" target="#b96">[97]</ref> dataset for training. As compared in Tab. 3, our model still improves unconstrained face recognition accuracies by a promising margin (Rank1: +2.43% on the IJB-S V2V protocol and +1.58% on TinyFace) on a large-scale training dataset (WebFace12M). Effect of Guidance in CFSM. To validate the effectiveness of the proposed controllable and guided face synthesis model in face recognition, we train a FR model with CFSM as augmentation but with random style coefficients (Ours*), and compare with guided CFSM (Ours). Tab. 3 shows that the synthesis with random coefficients does not bring significant benefit to unconstrained IJB-S dataset performance. However, when samples are generated with guided CFSM, the trained FR model performs much better. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the effect of guidance in the training images. For low quality images, too much degradation leads to images with altered identity. <ref type="figure" target="#fig_1">Fig. 3</ref> shows that guided CFSM avoids synthesizing bad quality images that are unidentifiable. Effect of the Number of Labeled Training Data. To validate the effect of the number of labeled training data, we train a series of models by adjusting the number of labeled training samples from 0.5M to 3.9M and report results both on ArcFace and ArcFace+Ours settings. <ref type="figure">Fig. 4</ref> shows the performance on various IJB-S protocols. For the full data usage setting, our model trained with guided CFSM as an augmentation outperforms the baseline by a large margin. Also note that the proposed method trained with 1/8th (0.5M) labeled data still achieves comparable performance, or even better than the baseline with 3.9M labeled data on V2V protocol. This is due to CFSM generating target data-specific augmentations, thus demonstrating the value of our controllable and guided face synthesis, which can significantly boost unconstrained FR performance. <ref type="figure">Fig. 6</ref>. Interpretable magnitude of the style coefficients. Given an input image, we randomly sample two sets of style coefficients o1 (left) and o2 (right) for all 3 models (respectively trained with the IJB-S, WiderFace and AgeDB datasets as the target data). We dynamically adjust the magnitude of these two coefficients by 0.5a, a, 1.5a, 2a, 3a, 4a, where a = o ||o|| . As can be seen, our model indeed realizes the goal of changing the degree of style synthesis with the coefficient magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis and Visualizations of the Face Synthesis model</head><p>In this experiment, we quantitatively evaluate the distributional similarity between datasets based on the learned linear subspace model for face synthesis. To this end, we choose 6 face datasets that are publicly available and popular for face recognition testing. These datasets are LFW <ref type="bibr" target="#b29">[30]</ref>, AgeDB-30 <ref type="bibr" target="#b53">[54]</ref>, CFP-FP <ref type="bibr" target="#b65">[66]</ref>, IJB-B <ref type="bibr" target="#b88">[89]</ref>, WiderFace (WF) <ref type="bibr" target="#b90">[91]</ref> and IJB-S <ref type="bibr" target="#b33">[34]</ref>. <ref type="figure" target="#fig_3">Figure 5(a)</ref> shows examples from these 6 datasets. Each dataset has its own style. For example, CFP-FP includes profile faces, WiderFace has mostly low resolution faces, and IJB-S contains extreme unconstrained attributes. During training, for each dataset, we randomly select 12K images as our target data to train the synthesis model. For the source data, we use the same subset of MS-Celeb-1M as in Sec. 4.1. Distribution Similarity Based on the learned dataset-specific linear subspace model, we calculate the pairwise distribution similarity score via Eqn. 10. As shown in <ref type="figure" target="#fig_3">Fig. 5(b)</ref>, the score reflects the style correlation between datasets. For instance, strong correlations among IJB-B, IJB-S and WiderFace (WF) are observed. We further visualize the learned basis vectors [u 1 , ..., u q ] and the mean style ? in <ref type="figure" target="#fig_3">Fig. 5(c)</ref>. The basis vectors are well clustered and the discriminative grouping indicates the correlation between dataset-specific models. <ref type="figure">Fig. 6</ref> shows face images generated by the learned CFSM. It can be seen that when the magnitude increases, the corresponding synthesized faces reveal more dataset-specific style variations. This implies the magnitude of the style code is a good indicator of image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizations of Style Latent Spaces</head><p>We also visualize the learned U of 6 models in <ref type="figure" target="#fig_4">Fig. 7</ref>. As we move along a basis vector of the learned subspace, the synthesized images change their style in dataset-specific ways. For instance, with target as WiderFace or IJB-S, synthesized images show various low quality styles such as blurring or turbulence effect. CFP dataset contains cropped images, and the "crop" style manifests in certain directions. Also, we can observe the learned U are different among datasets, which further verifies that our learned linear subspace model in CFSM is able to capture the variations in target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We answer the fundamental question of "How can image synthesis benefit the end goal of improving the recognition task? " Our controllable face synthesis model (CFSM) with adversarial feedback of FR model shows the merit of task-oriented image manipulation, evidenced by significant performance increases in unconstrained face datasets (IJB-B, IJB-C, IJB-S and TinyFace). In a broader context, it shows that adversarial manipulation could go beyond being an attacker, and serve to increase recognition accuracies in vision tasks. Meanwhile, we define a dataset similarity metric based on the learned style bases, which capture the style differences in a label or predictor agnostic way. We believe that our research has presented the power of a controllable and guided face synthesis model for unconstrained FR and provides an understanding of dataset differences. Acknowledgments. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via 2022-21102100004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head><p>In this supplementary material, we provide: ? Additional implementation details including the network structure of the face synthesis model and the training process.</p><p>? Additional ablation studies, including the effects of different target datasets, the dimensionality of the style coefficient, the perturbation budget, and the ratio of real and synthetic images in each mini-batch.</p><p>? Additional visualizations of the adversarial training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Implementation Details</head><p>Network Structure. The network architecture of the generator (including the encoder E and decoder G) used in our face synthesis module is illustrated in Tab. 4. We apply Instance Normalization <ref type="bibr" target="#b80">[81]</ref> to the encoder and Adaptive Instance Normalization <ref type="bibr" target="#b30">[31]</ref> to RESBLOKs (the residual basic block) of the decoder. The encoder takes an image X with the resolution of 112 ? 112 as input, and outputs its content feature C ? R 256?28?28 . The input and output to the decoder are C and the synthesized imageX, respectively. Additionally, as shown in <ref type="figure" target="#fig_5">Fig. 8</ref>, the parameters of the Adaptive Instance Normalization (AdaIN) layer in residual blocks are dynamically generated by a multiplayer perceptron (MLP) from the linear subspace model. Following <ref type="bibr" target="#b84">[85]</ref>, we employ multi-scale discriminators with 3 scales as our discriminator D. Training Process. We summarize the training process in Tab. 5. In Stage 1, we train our controllable face synthesis module with the identity consistency loss and the adversarial objective. In Stage 2, based on the pre-trained and fixed face synthesis model, we introduce an adversarial regularization strategy to guide the data augmentation process and train the face feature extractor F. Specifically, in the adversarial FR model training,</p><formula xml:id="formula_13">given B face images {X} B i=1</formula><p>in a mini-batch, our synthesis model (CFSM) is utilized to produces their synthesized versionX with initial random style coefficients {o} B i=1 . Based on the Eqn. 7 and 8 (main paper), we obtain the updated style coefficients {o * } B i=1 with perturbations. We then generate the perturbed images {X * } B i=1 with CFSM. Finally, we randomly select half of {X} B i=1 and half of {X * } B i=1 to form a new training batch for the FR model training. Note that, every epoch of the FR model training we will randomly initialize different style coefficients, even for the same training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Ablation Studies</head><p>Effect of Different Target Datasets. To study how the choice of target dataset in face synthesis model training would affect the face recognition performance, we choose two other datasets, LFW <ref type="bibr" target="#b29">[30]</ref> and IJB-S <ref type="bibr" target="#b33">[34]</ref> to train the face synthesis models and apply them for the FR model training. During training, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Encoder (E) Decoder (G) 1 CONV-(N64,K7,S1), ReLU RESBLK-(N256,K3,S1) 2 CONV-(N128,K4,S2), ReLU RESBLK-(N256,K3,S1) 3 CONV-(N256,K4,S2), ReLU RESBLK-(N256,K3,S1) 4 RESBLK-(N256,K3,S1) RESBLK-(N256,K3,S1) 5 RESBLK-(N256,K3,S1) CONV-(N128,K5,S1), ReLU 6 RESBLK-(N256,K3,S1) CONV-(N64,K5,S1), ReLU 7 RESBLK-(N256,K3,S1) CONV-(N3,K7,S1), TanH for each dataset, we randomly select unlabeled 12K face images as the target data to train the face synthesis model. For efficiency, we train the FR models with 0.5M labeled training samples from the MS-Celeb-1M dataset. The diversity of the three face datasets can be ranked as IJB-S &gt; WiderFace &gt; LFW. We show the comparisons on IJB-S protocols in <ref type="figure" target="#fig_6">Fig. 9</ref>, which shows that the more diverse the unlabeled target dataset is, the more performance gain is obtained. In particular, although LFW is similar to MS-Celeb-1M, it can introduce additional diversity in the dataset when augmented with our controllable and guided face synthesis model. Using unlabeled IJB-S images as the target data further improves the performance on the IJB-S dataset, which indicates that our model can be applied for boosting face recognition with limited unlabeled samples available. Effect of the Dimensionality (q) of the Style Coefficient. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the recognition performances on IJB-S over the dimensionality of the style coefficient. <ref type="figure" target="#fig_0">Fig. 10</ref> shows that the dimensionality of the style coefficient does have significant effects on the recognition performance. The model with q = 10 performs slightly better in face verification setting, such as V2S and V2B (TAR@FAR=1e-2). The results also indicate that learning manipulation in the low-dimensional subspace is effective and robust for face recognition.  Effect of the Perturbation Budget (?). We conduct experiments to demonstrate the effect of the perturbation budget ?. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, we can clearly find that a large perturbation budget (? = 0.628) leads to a better performance in the protocol of Surveillance-to-Surveillance (V2V) while performs slightly worse in the protocols of Surveillance-to-Still (V2S) and Surveillanceto-Booking (V2B). These observations are not surprising because the large style coefficient perturbation would generate faces with low qualities, which is beneficial for improving generalization to the unconstrained testing scenarios. Effect of the Ratio of Real and Synthetic Images in Each Mini-batch.</p><formula xml:id="formula_14">Output C ? R B?256?28?28X ? R B?3?W ?H</formula><p>As illustrated in Sec. 3.2 (main paper), we combine the original real images and their corresponding synthesized version as a mini-batch for the FR model training. In this experiment, we further study the ratio of real (R) and synthetic (S) images in each mini-batch. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, with more synthetic images in each mini-batch (R:S = 25% : 75%), the model achieves the best performance in the most challenging Surveillance-to-Surveillance (V2V) protocol (Rank1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Visualizations</head><p>The Perturbations in Direction or Magnitude. In adversarial FR model training, our synthesis model is able to offer two meaningful possibilities to perform style coefficient perturbation: magnitude and direction. To study the perturbation properties (direction or magnitude), we collect the initial style coefficient o and style perturbation ? * of 10K samples during the FR model training. We first measure the Cosine Similarity S C <ref type="figure" target="#fig_0">(Fig. 13 (a)</ref>) between the initial style coefficient o and the updated one o * = o + ? * . Then we present the histogram of the differences ( <ref type="figure" target="#fig_0">Fig. 13 (b)</ref>) between the magnitude of o and o * : a * ? a, where a * = ||o||, a = ||o * ||. Finally, in <ref type="figure" target="#fig_0">Fig. 13 (c)</ref>, we show the Sc over (a * ? a). As observed in <ref type="figure" target="#fig_0">Fig. 13</ref>, the style coefficient perturbation guided by FR model training indeed leads to the changes of both magnitude and direction of the initial style coefficient, which supports the motivation of our controllable face synthesis model design. More interestingly, the synthesis model attempts to achieve a balance between magnitude and direction in the adversarial-based augmentation process (see <ref type="figure" target="#fig_0">Fig. 13 (c)</ref>). For example, when the magnitude is decreasing ((a ? a * ) &lt; 0), the model is inclined to generate faces in lower quality but more target styles (lower Sc). In contrast, when the magnitude is increasing ((a ? a * ) &gt; 0), the model prefers to generate faces with higher quality but less target style (larger Sc). Additional Visualizations of X,X and X * . In <ref type="figure" target="#fig_0">Fig. 14</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Given an input face image, our controllable face synthesis model (CFSM) enables precise control of the direction and magnitude of the targeted styles in the generated images. The latent style has both the direction and the magnitude, where the direction linearly combines the learned bases to control the type of style, while the magnitude controls the degree of style. (b) CFSM can incorporate the feedback provided by the FR model to generate synthetic training data that can benefit the FR model training and improve generalization to the unconstrained testing scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Plot of mini-batch samples augmented with CFSM during training of the FR model. Top: Original images. Middle: Synthesized results before the feedback of the FR model. Bottom: Synthesized results after the feedback. The guide from the FR model can vary the images' style for increased difficulty (a-d), and preventing the images from identity lost (e-h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Few examples from each dataset. Note differences in style. For example, AgeDB contains old grayscale photos, WiderFace has mostly low resolution faces, and IJB-S includes extreme unconstrained attributes (i.e., noise, motion blur or turbulence effect). (b) shows the pairwise distribution similarity scores among datasets that are calculated using the learned subspace via Eq. 10. Note both IJB-B and WiderFace have high similarity scores with IJB-S. (c) The t-SNE plot of the learned [u1, ..., u10] and mean style ?. The dots represent ui + ? and the stars denote ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Given a single input image, we visualize the synthesized images by traversing along with the learned orthonormal basis U in the 6 dataset-specific models. For each dataset, Rows 1?3 illustrate the first 3 basis vectors traversed. Columns 1?5 show the directions which are scaled to emphasize their effect, i.e., only one element of style coefficient o varies from ?3? to 3? while other q?1 elements remain 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Additional illustration of the decoder network structure. The parameters of Adaptive Instance Normalization (AdaIN) in residual blocks are dynamically generated by a multiplayer perceptron (MLP) from the linear subspace model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Evaluation results on IJB-S with different target datasets. Baseline refers to the performance of the FR model trained on 0.5 million labeled samples (a subset of MS-Celeb-1M) without using the proposed face synthesis model. In this experiment, other 3 FR models are trained on the 0.5 million labeled samples with the proposed face synthesis models, which are trained with additional 12K unlabeled samples (from LFW, WiderFace or IJB-S, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Evaluation results on IJB-S with the different dimensionalities of the style coefficient (q = 5, 10, 20). Baseline refers to the performance of the FR model trained on 0.5 million labeled samples (a subset of MS-Celeb-1M) without using the proposed face synthesis model. In this experiment, other 3 models are trained on the 0.5 million labeled samples with the proposed face synthesis model, which is trained with additional 70K unlabeled samples from WiderFace. Evaluation results on IJB-S with different perturbation budget values (? = 0.157, 0.314 or 0.628). Baseline refers to the performance of the FR model trained on 0.5 million labeled samples (a subset of MS-Celeb-1M) without using the proposed face synthesis model. In this experiment, other 3 models are trained on the 0.5 million labeled samples with the proposed face synthesis model, which is trained with additional 70K unlabeled samples from WiderFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Evaluation results on IJB-S with different ratios of real (R) and synthetic (S) images in each mini-batch (R:S = 75% : 25%, 50% : 50% or 25% : 75%). Baseline refers to the performance of the FR model trained on 0.5 million labeled samples (a subset of MS-Celeb-1M) without using the proposed face synthesis model. In this experiment, other 3 models are trained on the 0.5 million labeled samples with the proposed face synthesis model, which is trained with additional 70K unlabeled samples from WiderFace. (a) Histogram of the Cosine Similarity between the initial style coefficient o and its updated one o * with perturbation. (b) Histogram of differences between the magnitude of o and o * : a ? a * , where a * = ||o||, a = ||o * ||. (c) Scatter plot showing the correlation between Sc and a ? a * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>, we show the original examples X, synthesized examples with initial style coefficientsX and synthesized examples with style perturbations X * in a mini-batch during the FR model training. Additionally, we visualize the pairwise error maps among these 3 types of data. As shown, the guide from the FR model encourages the face synthesis model to generate images with either increased or decreased target face style. Training examples in a mini-batch with our face synthesis model during the FR model training. For each image set, we show the original images X, synthesized results with initial style coefficients, and synthesized results with style perturbations X * . We additionally show their corresponding error maps: |X ? X|, |X * ? X| and |X ? X * |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Training ObjectiveFR Model Guided Synthesis for Face Recognition Controllable Face Synthesis Module (CFSM)</head><label></label><figDesc></figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 1</cell></row><row><cell></cell><cell>E</cell><cell></cell><cell>G</cell><cell>Outputs</cell><cell>Discriminator</cell><cell>Target Domain</cell></row><row><cell></cell><cell>+</cell><cell>MLP</cell><cell cols="2">AdaIN Para.</cell></row><row><cell>Style Coef.</cell><cell cols="2">Linear Subspace</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>*</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 2</cell></row><row><cell cols="2">FR Training Data</cell><cell cols="2">Adversarial Augmentation</cell><cell>FR model training with synthesized images</cell></row><row><cell cols="6">Fig. 2. Overview of the proposed method. Top (Stage 1): Pipeline for training the</cell></row><row><cell cols="6">controllable face synthesis module that mimics the distribution of the target domain.</cell></row><row><cell cols="6">L adv ensures target domain similarity, L id enforces the magnitude of o to control</cell></row><row><cell cols="6">the degree of synthesis, and Lort factorizes the target domain style with linear bases.</cell></row><row><cell cols="6">Bottom (Stage 2): Pipeline for using the pre-trained face synthesis module for the</cell></row><row><cell cols="6">purpose of training an FR model. The synthesis module works as an augmentation to</cell></row><row><cell cols="6">the training data. We adversarially update o to maximize L cls of a given FR model.</cell></row><row><cell cols="6">space that works for both the original and target domains [15, 36, 57, 58, 65, 80].</cell></row><row><cell cols="6">Our method augments data resembling the target domain with unlabeled images.</cell></row><row><cell cols="6">Dataset Distances It is important to characterize and contrast datasets in</cell></row><row><cell cols="6">computer vision research. In recent years, various notions of dataset similarity</cell></row><row><cell cols="3">have been proposed</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art methods on the IJB-B benchmark. '*' denotes a subset of data selected by the authors.</figDesc><table><row><cell>Method</cell><cell>Train Data, #labeled(+#unlabeled)</cell><cell>Backbone</cell><cell cols="3">Verification 1e ? 5 1e ? 4 1e ? 3 Rank1 Rank5 Identification</cell></row><row><cell>VGGFace2 [7]</cell><cell>VGGFace2, 3.3M</cell><cell cols="4">SE-ResNet-50 70.50 83.10 90.80 90.20 94.6</cell></row><row><cell>AFRN [35]</cell><cell>VGGFace2-*, 3.1M</cell><cell cols="4">ResNet-101 77.10 88.50 94.90 97.30 97.60</cell></row><row><cell>ArcFace [12]</cell><cell>MS1MV2, 5.8M</cell><cell>ResNet-50</cell><cell cols="3">84.28 91.66 94.81 92.95 95.60</cell></row><row><cell>MagFace [53]</cell><cell>MS1MV2, 5.8M</cell><cell>ResNet-50</cell><cell>83.87 91.47 94.67</cell><cell>?</cell><cell>?</cell></row><row><cell>Shi et al. [69]</cell><cell>cleaned MS1MV2, 3.9M(+70K)</cell><cell>ResNet-50</cell><cell cols="3">88.19 92.78 95.86 95.86 96.72</cell></row><row><cell>ArcFace</cell><cell>cleaned MS1MV2, 3.9M</cell><cell>ResNet-50</cell><cell cols="3">87.26 94.01 95.95 94.61 96.52</cell></row><row><cell>ArcFace+Ours</cell><cell>cleaned MS1MV2, 3.9M(+70K)</cell><cell cols="4">ResNet-50 90.95 94.61 96.21 94.96 96.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art methods on the IJB-C benchmark.</figDesc><table><row><cell>Method</cell><cell cols="2">Train Data, #labeled(+#unlabeled) Backbone</cell><cell cols="4">Verification 1e ? 6 1e ? 5 1e ? 4 Rank1 Rank5 Identification</cell></row><row><cell>VGGFace2 [7]</cell><cell>VGGFace2, 3.3M</cell><cell>SE-ResNet-50</cell><cell>-</cell><cell cols="3">76.80 86.20 91.40 95.10</cell></row><row><cell>AFRN [35]</cell><cell>VGGFace2-*, 3.1M</cell><cell>ResNet-101</cell><cell>-</cell><cell cols="3">88.30 93.00 95.70 97.60</cell></row><row><cell>PFE [68]</cell><cell>MS1M- * , 4.4M</cell><cell>ResNet-64</cell><cell>-</cell><cell cols="3">89.64 93.25 95.49 97.17</cell></row><row><cell>DUL [8]</cell><cell>MS1M- * , 3.6M</cell><cell>ResNet-64</cell><cell>-</cell><cell cols="3">90.23 94.20 95.70 97.60</cell></row><row><cell>ArcFace [12]</cell><cell>MS1MV2, 5.8M</cell><cell>ResNet-50</cell><cell cols="4">80.52 88.36 92.52 93.26 95.33</cell></row><row><cell>MagFace [53]</cell><cell>MS1MV2, 5.8M</cell><cell>ResNet-50</cell><cell cols="2">81.69 88.95 93.34</cell><cell>?</cell><cell>?</cell></row><row><cell>Shi et al. [69]</cell><cell>cleaned MS1MV2, 3.9M(+70K)</cell><cell>ResNet-50</cell><cell cols="4">87.92 91.86 94.66 95.61 97.13</cell></row><row><cell>ArcFace</cell><cell>cleaned MS1MV2, 3.9M</cell><cell>ResNet-50</cell><cell cols="4">87.24 93.32 95.61 95.89 97.08</cell></row><row><cell>ArcFace+ours</cell><cell>cleaned MS1MV2, 3.9M(+70K)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ResNet-50 89.34 94.06 95.90 96.31 97.48</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>50.82 61.16 16.44 24.19 53.04 62.67 27.40 29.70 10.05 17.55 0.11 0.68 ResNet-50 59.29 66.91 39.92 50.49 60.58 67.70 32.39 44.32 17.35 28.34 1.16 5.37 ResNet-50 58.78 66.40 40.99 50.45 60.66 67.43 43.12 51.38 14.81 26.72 2.51 5.72 62.21 66.85 ArcFace+Ours* MS1MV2-* ResNet-50 61.69 68.33 43.99 53.34 62.20 69.50 44.38 53.49 18.14 31.34 2.09 4.51 62.39 67.36 ArcFace+Ours MS1MV2-* ResNet-50 63.86 69.95 47.86 56.44 65.95 71.16 47.28 57.24 21.38 35.11 2.96 7.41 63.01 68.21 AdaFace [43] WebFace12M IResNet-100 71.35 76.24 59.40 66.34 71.93 76.56 59.37 66.68 36.71 50.03 4.62 11.84 72.29 74.97 AdaFace+Ours WebFace12M IResNet-100 72.54 77.59 60.94 66.02 72.65 78.18 60.26 65.88 39.14 50.91 5.05 13.17 73.87 76.77</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>MARN [21]</cell><cell>MS1M- *</cell><cell>ResNet-64 58.14 64.11 21.47 ?</cell><cell>59.26 65.93 32.07 ?</cell><cell cols="4">22.25 34.16 0.19 ?</cell><cell>?</cell><cell>?</cell></row><row><cell>PFE [68]</cell><cell>MS1M- *</cell><cell cols="6">ResNet-64 50.16 58.33 31.88 35.33 53.60 61.75 35.99 39.82 9.20 20.82 0.84 2.83</cell><cell>?</cell><cell>?</cell></row><row><cell>ArcFace [12]</cell><cell>MS1MV2</cell><cell cols="2">ResNet-50 50.39 60.42 32.39 42.99 52.25 61.19 34.87 43.50</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Shi et al. [69]</cell><cell cols="8">MS1MV2-* ?</cell><cell>?</cell></row><row><cell>ArcFace [12]</cell><cell>MS1MV2-*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Network architectures of the generator of face synthesis module. RESBLK denotes the residual basic block. [Keys: N=Neurons, K=Kernel size, S=Stride, B=Batch size].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Stages of the training process. MLP, U, ? Lort, L adv , LD, L id</figDesc><table><row><cell cols="2">Network or parameters</cell><cell>Loss</cell></row><row><cell>Stage 1 E, G, D, Stage 2 F, ?  *</cell><cell></cell><cell>L cla</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric dataset distances via optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Riggable 3d face reconstruction via in-network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The do&apos;s and don&apos;ts for CNNbased face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<editor>SIG-GRAPH</editor>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">VGGface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>FG</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Low-resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">UV-GAN: Adversarial facial uv map completion for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019)</title>
		<meeting><address><addrLine>1, 7, 8, 10</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangled and controllable face image generation via 3D imitative-contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised adversarial learning to generate photorealistic face images of new identities from 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">3D guided fine-grained face manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Video face recognition: Component-wise feature aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICB</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Low quality video face recognition: Multi-mode aggregation recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EigenGAN: Layer-wise eigen-learning for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face relighting with geometrically consistent shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarkis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation by mixing them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szab?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 07-49</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Curric-ularFace: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">IJB-S: Iarpa janus surveillance video benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BTAS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attentional feature-pair relation networks for accurate face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep video portraits</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">AdaFace: Quality adaptive margin for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Analyzing and reducing the damage of dataset bias to face recognition with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Diverse image-toimage translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Conditional image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">AdaptiveFace: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">SphereFace: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>COLT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<title level="m">Iarpa janus benchmark-C: Face dataset and protocol</title>
		<imprint>
			<publisher>ICB</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">MOST-GAN: 3d morphable stylegan for disentangled face image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Medin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MagFace: A universal representation for face recognition and quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">AgeDB: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Reducing domain gap by reducing style bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">HoloGAN: Unsupervised learning of 3d representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Semi-supervised monocular 3D face reconstruction with end-to-end shape-preserved domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Ganimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SynFace: Face recognition with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Face-forensics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Mor-phGAN: One-shot face synthesis gan for detecting recognition bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Apostoloff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05225</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01288</idno>
		<title level="m">Facefeat-GAN: a two-stage approach for identity-preserving face synthesis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2019) 1</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Boosting unconstrained face recognition with auxiliary unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPRW (2021) 2, 4, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards universal representation learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<title level="m">Single image portrait relighting. TOG</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">StyleRig: Rigging stylegan for 3D control over portrait images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Disentangled representation learning GAN for poseinvariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Generating photo-realistic training data to improve face recognition accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Trigueros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hartnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">NormFace: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACMMM</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">CosFace: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Highresolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Mis-Classified vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Towards real-world blind face restoration with generative facial prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<title level="m">Iarpa janus benchmark-B face dataset</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Elegant: Exchanging latent encodings with GAN for transferring multiple face attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">GAN prior embedded network for blind face restoration in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Towards large-pose face frontalization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">3D-aided dual-agent GANs for unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">WebFace260M: A benchmark unveiling the power of million-scale deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
