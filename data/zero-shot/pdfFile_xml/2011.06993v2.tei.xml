<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLERT: Document-Level Features for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
							<email>schweter.mlstefan@schweter.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universit?t zu Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
							<email>alan.akbik@hu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universit?t zu Berlin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FLERT: Document-Level Features for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art approaches for named entity recognition (NER) typically consider text at the sentence-level and thus do not model information that crosses sentence boundaries. However, the use of transformerbased models for NER offers natural options for capturing document-level features. In this paper, we perform a comparative evaluation of document-level features in the two standard NER architectures commonly considered in the literature, namely "fine-tuning" and "feature-based LSTM-CRF". We evaluate different hyperparameters for document-level features such as context window size and enforcing document-locality. We present experiments from which we derive recommendations for how to model document context and present new state-of-the-art scores on several CoNLL-03 benchmark datasets. Our approach is integrated into the FLAIR framework to facilitate reproduction of our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is the well-studied NLP task of predicting shallow semantic labels for sequences of words, used for instance for identifying the names of persons, locations and organizations in text. Current approaches for NER often leverage pre-trained transformer architectures such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> or XLM <ref type="bibr" target="#b8">(Lample and Conneau, 2019)</ref>. Document-level features. While NER is traditionally modeled at the sentence-level, transformerbased models offer a natural option for capturing document-level features by passing a sentence with its surrounding context. As <ref type="figure">Figure 1</ref> shows, this context can then influence the word representations of a sentence: The example sentence "I love Paris" is passed through the transformer together with the next sentence that begins with "The city is", potentially helping to resolve the ambiguity of the word "Paris". A number of prior works have employed such document-level features <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b18">Virtanen et al., 2019;</ref><ref type="bibr" target="#b22">Yu et al., 2020)</ref> but only in combination with other contributions and thus have not evaluated the impact of using document-level features in isolation. Contributions. With this paper, we close this experimental gap and present an evaluation of document-level features for NER. As there are two conceptually very different approaches for transformer-based NER that are currently used across the literature, we evaluate document-level features in both:</p><p>1. In the first, we fine-tune the transformer itself on the NER task and only add a linear layer for word-level predictions <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>.</p><p>2. In the second, we use the transformer only to provide features to a standard LSTM-CRF sequence labeling architecture <ref type="bibr" target="#b7">(Huang et al., 2015)</ref> and thus perform no fine-tuning.</p><p>We discuss the differences between both approaches and explore best hyperparameters for each. In their best determined setup, we then perform a comparative evaluation. We find that (1) document-level features significantly improve NER quality and that (2) fine-tuning generally outperforms feature-based approaches. We also determine best settings for document-level context and report several new state-of-the-art scores on the classic CoNLL benchmark datasets. Our approach is integrated as the "FLERT"-extension into the FLAIR framework <ref type="bibr" target="#b0">(Akbik et al., 2019a)</ref> to facilitate further experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document-Level Features</head><p>In a transformer-based architecture, documentlevel features can easily be realized by passing a sentence with its surrounding context to obtain word embeddings, as illustrated in <ref type="figure">Figure 1</ref>.</p><formula xml:id="formula_0">E' 1 E' 2 E' 3 E' n I love Paris . T 1 T 2 T 3 T n ... ... E [CLS] E 62 E 63 E 64 ...</formula><p>[CLS] the vote .</p><formula xml:id="formula_1">C [CLS] C 62 C 63 C 64 ... E'' 1 E'' 2 E'' 3 E [SEP]</formula><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The city is [SEP]</head><p>C' 1 C' 2 C' 3 C <ref type="bibr">[SEP]</ref> ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left context</head><p>Right context <ref type="figure">Figure 1</ref>: To obtain document-level features for a sentence that we wish to tag ("I love Paris", shaded green), we add 64 tokens of left and right tokens each (shaded blue). As self-attention is calculated over all input tokens, the representations for the sentence's tokens are influenced by the left and right context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-LOC O O O</head><p>Prior approaches. This approach was first employed by <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> with what they described as a "maximal document context", though technical details were not listed. Subsequent work used variants of this approach. For instance, Virtanen et al. (2019) experiment with adding the following (but not preceding) sentence as context to each sentence. <ref type="bibr" target="#b22">Yu et al. (2020)</ref> instead use a 64 surrounding token window for each token in a sentence, thus calculating a large context on a per-token basis. By contrast, Luoma and Pyysalo (2020) adopt a multisentence view in which they combine predictions from different windows and sentence positions. Our approach. In this paper, we instead use a conceptually simple variant in which we create context on a per-sentence basis: For each sentence we wish to classify, we add 64 subtokens of left and right context, as shown in <ref type="figure">Figure 1</ref>. This has computational and implementation advantages in that each sentence and its context need only be passed through the transformer once and that added context is limited to a relatively small window. Furthermore, we can still follow standard procedure in shuffling sentences at each epoch during training, since context is encoded on a per-sentence level. We use this approach throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Parameter Experiments</head><p>As mentioned in the introduction, there are two common architectures for transformer-based NER, namely fine-tuning and feature-based approaches.</p><p>In this section, we briefly introduce the differences between both approaches and conduct a study to identify best hyperparameters for each. The best respective setups are then used in the final comparative evaluation in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Data set. We use the development datasets of the CoNLL shared tasks <ref type="bibr" target="#b17">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b16">Tjong Kim Sang, 2002)</ref> for NER on four languages (English, German, Dutch and Spanish). Following <ref type="bibr" target="#b22">Yu et al. (2020)</ref> we report results for both the original and revised dataset for German (denoted as DE 06 ).</p><p>Transformer model. In all experiments in this section, we employ the multilingual XLM-RoBERTa (XLM-R) transformer model proposed by . We use the xlm-roberta-large model in our experiments, trained on 2.5TB of data from a cleaned Common Crawl corpus <ref type="bibr" target="#b19">(Wenzek et al., 2020)</ref> for 100 different languages Embeddings (+WE). For each setup we experiment with concatenating classic word embeddings to the word-level representations obtained from the transformer model. Following <ref type="bibr" target="#b2">Akbik et al. (2018)</ref>, we use GLOVE embeddings <ref type="bibr" target="#b11">(Pennington et al., 2014)</ref> for English and FASTTEXT embeddings (Bojanowski et al., 2017) for other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">First Approach: Fine-Tuning</head><p>Fine-tuning approaches typically only add a single linear layer to a transformer and fine-tune the entire architecture on the NER task. To bridge the difference between subtoken modeling and tokenlevel predictions, they apply subword pooling to create token-level representations which are then passed to the final linear layer. Conceptually, this approach has the advantage that everything is modeled in a single architecture that is fine-tuned as a whole. More details on parameters and architecture are provided in the Appendix.  <ref type="table">Table 1</ref>: Evaluation of different variants using the fine-tuning approach. The evaluation is performed against the development set of all 4 languages of the CoNLL-03 shared task for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluated variants. We compare two variants:</head><p>Transformer-Linear In the first, we use the standard approach of adding a simple linear classifier on top of the transformer to directly predict tags.</p><p>Transformer-CRF In the second, we evaluate if it is helpful to add a conditional random fields (CRF) decoder between the transformer and the linear classifier <ref type="bibr" target="#b14">(Souza et al., 2019)</ref>.</p><p>Results are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Second Approach: Feature-Based</head><p>Feature-based approaches instead use the transformer only to generate embeddings for each word in a sentence and use these as input into a standard sequence labeling architecture, most commonly a LSTM-CRF <ref type="bibr" target="#b7">(Huang et al., 2015)</ref>. The transformer weights are frozen so that training is limited to the LSTM-CRF. Conceptually, this approach benefits from a well-understood model training procedure that includes a real stopping criterion. See Appendix B for more details on training parameters. Evaluated variants. We compare two variants:</p><p>All-layer-mean In the first, we obtain embeddings for each token using mean pooling across all transformer layers, including the word embedding layer. This representation has the same length as the hidden size for each transformer layer. This approach is inspired by the ELMOstyle <ref type="bibr" target="#b12">(Peters et al., 2018</ref>) "scalar mix".</p><p>Last-four-layers In the second, we follow <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> to only use the last four transformer layers for each token and concatenate their representations into a final representation for each token. It thus has four times the length of the transformer layer hidden size.</p><p>The results for English 1 are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results: Best Configurations</head><p>We evaluate both approaches in each variant in all possible combinations of adding standard word embeddings "(+WE)" and document-level features "(+Document features)". Each setup is run three times to report average F1 and standard deviation.</p><p>Results. For fine-tuning, we find that additional word embeddings and using a CRF decoder improves results only for some languages, and often only minimally so (see <ref type="table">Table 1</ref>). We thus choose a minimal Transformer-Linear architecture. For the feature-based approach, we find that an all-layermean strategy and adding word embeddings very clearly yields the best results (see <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparative Evaluation</head><p>With the best configurations identified in Section 3 on the development data, we conduct a final comparative evaluation on the test splits of the CoNLL-03 datasets, with and without document features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>The evaluation results are listed in <ref type="table" target="#tab_4">Table 3</ref>. We make the following observations: Fine-tuning document-level features best. As <ref type="table" target="#tab_4">Table 3</ref> shows, we find that fine-tuning outperforms the feature-based approach across all experiments (??2 pp on average). Similarly, we find that document-level features clearly outperform sentence-level features (?1.15 pp on average). We     <ref type="table" target="#tab_4">Table 3</ref>) as is possible for finetuning as no stopping criterion is used. For German, we outperform <ref type="bibr" target="#b22">(Yu et al., 2020)</ref> by ?1.81 pp and ??2 pp on the original and revised German datasets respectively. For Dutch, we see an increase of by ?1.5 pp over the next best approach. While we do not set new state-of-the-art scores for English and Spanish, our results are very competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Impact of context window size <ref type="table" target="#tab_5">(Table 4)</ref>. We evaluate the impact of the number of surrounding tokens used in document-level contexts on performance using the best configuration for finetuning approach. The context window is searched in <ref type="bibr">[48,</ref><ref type="bibr">64,</ref><ref type="bibr">96,</ref><ref type="bibr">128]</ref>. As <ref type="table" target="#tab_5">Table 4</ref> shows, impact is marginal, with 64 the best across languages. Entity type analysis <ref type="table" target="#tab_6">(Table 5)</ref>. We perform a pertype analysis to compare average results across entity types with and without document-level features. We find that while the difference in F1-score depend on the type and the language, in particular the ORG (organization) and PER (person) entity types improve the most when including document-level features, indicating that cross-sentence contexts are most important here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We evaluated document-level features in two commonly used NER architectures, for which we determined best setups. Our experiments show that document-level features significantly improve overall F1-score and that fine-tuning outperforms the feature-based LSTM-CRF. We also surprisingly find that enforcing document boundaries improves results, potentially adding to recent evidence that transformers have difficulties in learning positional signals <ref type="bibr" target="#b6">(Huang et al., 2020)</ref>. We integrate our approach as the "FLERT"-extension 2 into the FLAIR framework, to enable the research community to leverage our best determined setups for training and applying state-of-the-art NER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training: Fine-tuning Approach</head><p>Fine-tuning only adds a single linear layer to a transformer and fine-tunes the entire architecture on the NER task. To bridge the difference between subtoken modeling and token-level predictions, they apply subword pooling to create tokenlevel representations which are then passed to the final linear layer. A common subword pooling strategy is "first" <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> which uses the representation of the first subtoken for the entire token. See <ref type="figure">Figure 2</ref> for an illustration.  Training procedure. To train this architecture, prior works typically use the AdamW <ref type="bibr" target="#b9">(Loshchilov and Hutter, 2019)</ref> optimizer, a very small learning rate and a small, fixed number of epochs as a hardcoded stopping criterion . We adopt a one-cycle training strategy <ref type="bibr" target="#b13">(Smith, 2018)</ref>, inspired from the HuggingFace transformers <ref type="bibr" target="#b20">(Wolf et al., 2019)</ref> implementation, in which the learning rate linearly decreases until it reaches 0 by the end of the training. <ref type="table" target="#tab_8">Table 6</ref> lists the architecture parameters we use across all our experiments.</p><p>A.2 Training: Feature-based Approach <ref type="figure" target="#fig_0">Figure 3</ref> gives an overview of the feature-based approach: Word representations are extracted from the transformer by either averaging over all layers (all-layer-mean) or by concatenating the representations of the last four layers (last-four-layers). These are then input into a standard LSTM-CRF architecture <ref type="bibr" target="#b7">(Huang et al., 2015)</ref> as features. We again use the subword pooling strategy illustrated in <ref type="figure">Figure 2</ref>. Training procedure. We adopt the standard training procedure used in earlier works. We with SGD with a larger learning rate that is annealed against the development data. Training terminates when the learning rate becomes too small. The param- First subword pooling <ref type="figure">Figure 2</ref>: Illustration of first subword pooling. The input "The Eiffel Tower" is subword-tokenized, splitting "Eiffel" into three subwords (shaded green). Only the first ("E") is used as representation for "Eiffel".</p><p>eters used for training a feature-based model are shown in <ref type="table" target="#tab_11">Table 7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Reproducibility Checklist</head><p>Dataset statistics.  Average training runtime. We conduct experiments on a NVIDIA V-100 (16GB) for fine-tuning and a NVIDIA RTX 3090 TI (24GB) for the feature-based approach. We report average training times for our best configurations in <ref type="table" target="#tab_14">Table 9</ref>.</p><p>Approach EN DE / DE 06 NL ES Fine-Tuning 10h 10h 10h 5h Feature-based 7h 5.5h 5.75h 5.5h ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The city is [SEP]</head><p>C' 1 C' 2 C' 3 C <ref type="bibr">[SEP]</ref> ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left context</head><p>Right context  Number of model parameters. The reported number of model parameters from  for XLM-R is 550M. Our fine-tuned model has 560M parameters (?1.8%), whereas the feature-based model comes with 564M parameters (?2.5%). Evaluation metrics. We evaluate our models using the CoNLL-2003 evaluation script 3 and report averaged F1-score over three runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Overview of feature-based approach. Self-attention is calculated over all input tokens (incl. left and right context). The final representation for each token in the sentence ("I love Paris", shaded green) can be calculated as a) mean over all layers of transformer-based model or b) concatenating the last four layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Linear 96.64 ? 0.14 89.06 ? 0.18 91.86 ? 0.41 93.41 ? 0.19 88.95 ? 0.19 + Document features 96.82 ? 0.07 89.79 ? 0.13 93.09 ? 0.06 94.19 ? 0.14 90.34 ? 0.27 + WE 96.82 ? 0.13 88.96 ? 0.10 92.12 ? 0.10 93.51 ? 0.09 89.09 ? 0.36 + WE + Document features 97.02 ? 0.09 89.74 ? 0.46 92.83 ? 0.12 94.01 ? 0.27 90.17 ? 0.25 Transformer-CRF 96.79 ? 0.11 88.52 ? 0.10 92.21 ? 0.07 93.61 ? 0.15 88.77 ? 0.20 + Document features 96.90 ? 0.06 89.67 ? 0.24 92.87 ? 0.21 94.16 ? 0.07 90.56 ? 0.09 + WE 96.79 ? 0.15 88.84 ? 0.15 91.97 ? 0.09 93.36 ? 0.04 88.63 ? 0.47 + WE + Document features 96.87 ? 0.00 89.69 ? 0.22 92.88 ? 0.26 94.34 ? 0.13 90.37 ? 0.14</figDesc><table><row><cell>Fine-tuning Approach</cell><cell>EN</cell><cell>DE</cell><cell>DE 06</cell><cell>NL</cell><cell>ES</cell></row><row><cell>Transformer-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Feature-based Approach</cell><cell>EN</cell></row><row><cell cols="2">LSTM-CRF (last-four-layers) 91.17 ? 0.29</cell></row><row><cell>+ Document features</cell><cell>94.23 ? 0.19</cell></row><row><cell>+ WE</cell><cell>92.19 ? 0.46</cell></row><row><cell>+ WE + Document features</cell><cell>94.61 ? 0.10</cell></row><row><cell cols="2">LSTM-CRF (all-layer-mean) 94.37 ? 0.06</cell></row><row><cell>+ Document features</cell><cell>96.09 ? 0.07</cell></row><row><cell>+ WE</cell><cell>95.63 ? 0.04</cell></row><row><cell>+ WE + Document features</cell><cell>96.53 ? 0.10</cell></row></table><note>1 Other languages show similar results (omitted for space).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Evaluation of feature-based approach on CoNLL-03 development set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CRF (all layer mean) no 91.83 ? 0.06 82.88 ? 0.28 87.35 ? 0.17 89.87 ? 0.45 88.78 ? 0.08 LSTM-CRF (all layer mean) yes 93.12 ? 0.14 84.86 ? 0.11 89.88 ? 0.26 91.73 ? 0.21 88.98 ? 0.11 Fine-tuning Transformer-Linear no 92.79 ? 0.10 86.60 ? 0.43 90.04 ? 0.37 93.50 ? 0.15 89.94 ? 0.24 Transformer-Linear yes 93.64 ? 0.05 86.99 ? 0.24 91.55 ? 0.07 94.87 ? 0.20 90.14 ? 0.14 Fine-tuning (Ablations) Transformer-Linear yes (+enforce) 93.75 ? 0.16 87.35 ? 0.15 91.33 ? 0.18 95.21 ? 0.08 -Transformer-Linear (+DEV) yes (+enforce) 94.09 ? 0.07 88.34 ? 0.36 92.23 ? 0.21 95.19 ? 0.32 -</figDesc><table><row><cell>Approach</cell><cell cols="2">Doc. features? EN</cell><cell>DE</cell><cell>DE 06</cell><cell>NL</cell><cell>ES</cell></row><row><cell>Feature-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM-Best published</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Akbik et al. (2019b)</cell><cell>pooling</cell><cell cols="2">93.18 ? 0.09 -</cell><cell cols="3">88.27 ? 0.30 90.44 ? 0.20 -</cell></row><row><cell>Yu et al. (2020)</cell><cell>yes</cell><cell>93.5</cell><cell>86.4</cell><cell>90.3</cell><cell>93.7</cell><cell>90.3</cell></row><row><cell>Strakov? et al. (2019)</cell><cell>yes</cell><cell>93.38</cell><cell>85.10</cell><cell>-</cell><cell>92.69</cell><cell>88.81</cell></row><row><cell>Yamada et al. (2020)</cell><cell>yes</cell><cell>94.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparative evaluation of best configurations of fine-tuning and feature-based approaches on test data. 89.64 92.87 94.19 90.34 92.77 96 96.90 89.67 92.58 94.03 90.31 92.70 128 96.90 88.97 92.56 94.22 90.15 92.56</figDesc><table><row><cell>CW</cell><cell>EN</cell><cell>DE</cell><cell>DE 06</cell><cell>NL</cell><cell>ES</cell><cell>Avg.</cell></row><row><cell cols="7">48 96.86 89.47 92.63 94.09 90.31 92.67</cell></row><row><cell cols="2">64 96.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparative evaluation of context window sizes of fine-tuning approach on development set.</figDesc><table><row><cell>Entity</cell><cell>EN</cell><cell>DE</cell><cell>DE 06</cell><cell>NL</cell><cell>ES</cell></row><row><cell>LOC</cell><cell cols="5">+0.44 +0.23 +1.97 -0.74 +0.17</cell></row><row><cell cols="6">MISC +0.22 -0.90 +1.66 +1.16 +0.72</cell></row><row><cell>ORG</cell><cell cols="5">+1.21 +0.56 +0.74 +1.66 +0.11</cell></row><row><cell>PER</cell><cell cols="5">+1.19 +1.15 +1.50 -0.34 +0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Relative change in F1 for different entity types and languages when adding document-level features.</figDesc><table><row><cell>thus find fine-tuning with document-level features</cell></row><row><cell>to work best across all languages.</cell></row><row><cell>Enforcing document boundaries. For fine-</cell></row><row><cell>tuning, we also test an ablation in which we trun-</cell></row><row><cell>cate document-features at document boundaries,</cell></row><row><cell>meaning that context can only come from the same</cell></row><row><cell>document. As the columns "yes (+enforce)" in</cell></row><row><cell>Table 3 show, this increases F1-score across nearly</cell></row><row><cell>all experiments. Our initial expectation that trans-</cell></row><row><cell>formers would learn automatically to respect docu-</cell></row><row><cell>ment boundaries (marked up in all datasets except</cell></row><row><cell>Spanish) did not materialize, thus we recommend</cell></row><row><cell>enforcing document boundaries if possible.</cell></row><row><cell>New state-of-the-art results. Combining fine-</cell></row><row><cell>tuning, and strict document-level features yields</cell></row><row><cell>new state-of-the-art scores for several datasets. Es-</cell></row><row><cell>pecially when including dev data in training (indi-</cell></row><row><cell>cated as +DEV in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Parameters used for fine-tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Parameters for feature-based approach.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">shows the number of</cell></row><row><cell cols="3">sentences for for each dataset.</cell><cell></cell><cell></cell></row><row><cell cols="2">Split EN</cell><cell cols="2">DE / DE 06 NL</cell><cell>ES</cell></row><row><cell cols="3">Train 14,987 12,705</cell><cell cols="2">16,093 8,323</cell></row><row><cell>Dev</cell><cell>3,466</cell><cell>3,068</cell><cell>2,969</cell><cell>1,915</cell></row><row><cell>Test</cell><cell>3,684</cell><cell>3,160</cell><cell>5,314</cell><cell>1,517</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Number of sentences for each CoNLL dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Average training runtimes for our approaches.</figDesc><table><row><cell>C [CLS]</cell><cell>C 62</cell><cell>C 63</cell><cell>C 64</cell><cell>T 1</cell><cell>T 2</cell><cell>T 3</cell><cell>T n</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E [CLS]</cell><cell>E 62</cell><cell>E 63</cell><cell>E 64</cell><cell>E' 1</cell><cell>E' 2</cell><cell>E' 3</cell><cell>E' n</cell><cell>E'' 1</cell><cell>E'' 2</cell><cell>E'' 3</cell><cell>E [SEP]</cell></row><row><cell>[CLS]</cell><cell>the</cell><cell>vote</cell><cell>.</cell><cell>I</cell><cell>love</cell><cell>Paris</cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To be released with FLAIR version 0.8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.clips.uantwerpen.be/ conll2003/ner/bin/conlleval</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FLAIR: An easy-to-use framework for state-of-theart NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="724" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improve transformer models with better relative position embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.298</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3327" to="3335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring cross-sentence contexts for named entity recognition with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="904" to="914" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F?bio</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Lotufo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10649</idno>
		<title level="m">Portuguese named entity recognition using bert-crf</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07076</idno>
		<title level="m">Multilingual is not enough: Bert for finnish</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">1910</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
