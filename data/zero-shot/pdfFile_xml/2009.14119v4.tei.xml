<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asymmetric Loss For Multi-Label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-29">29 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
							<email>emanuel.benbaruch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
							<email>tal.ridnik@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
							<email>nadav.zamir@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<email>asaf.noy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
							<email>itamar.friedman@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
							<email>matan.protter@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Asymmetric Loss For Multi-Label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-29">29 Jul 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Typical natural images contain multiple objects and concepts <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>, highlighting the importance of multi-label classification for real-world tasks. Recently, remarkable advances have been made in multi-label benchmarks such as MS-COCO <ref type="bibr" target="#b19">[20]</ref>, NUS-WIDE <ref type="bibr" target="#b6">[7]</ref>, Pascal-VOC <ref type="bibr" target="#b10">[11]</ref> and Open Images <ref type="bibr" target="#b16">[17]</ref>. Notable success was reported by exploiting label correlation via graph neural networks which represent the label relationships <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> or word embeddings based on knowledge priors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>. Other approaches are based on modeling image parts and attentional regions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>, as well as using recurrent neural * Equal contribution (a) (b) <ref type="figure">Figure 1</ref>: (a) Real world challenges in multi-label classification. A typical image contains few positive samples, and many negative ones, leading to high negative-positive imbalance. Also, missing labels in ground-truth are common in multi-label datasets. (b) Proposed solution with ASL.</p><p>The loss properties will be detailed in Section 2.5 networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>. Despite their effectiveness, recent approaches are characterized by extensive architecture modifications and relying on additional external information, such as word embeddings and NLP models. In this work, we question whether such intricate solutions are truly necessary for achieving high performance in multi-label classification tasks. In particular, we demonstrate that a careful design of the loss function can greatly benefit classification accuracy, while still maintaining a simple and efficient solution, based on standard architectures and training schemes.</p><p>A key characteristic of multi-label classification is the inherent positive-negative imbalance created when the overall number of labels is large. Most images contain only a small fraction of the possible labels, implying that the number of positive samples per category will be, on average, much lower than the number of negative samples. To address this, <ref type="bibr" target="#b32">[33]</ref> suggested a loss function for statically handling the imbalance in multi-label problems. However, it was aimed specifically at long-tail distribution scenarios. High imbalance is also encountered in dense object detection, where it stems from the ratio of foreground vs. background regions. Some solutions based on resampling methods were proposed, by selecting only a subset of the possible background examples <ref type="bibr" target="#b22">[23]</ref>. However, resampling methods are not suitable for handling multi-label classification imbalancing, since each image contains many labels, and resampling cannot change the distribution of only a specific label.</p><p>Another common solution in object detection is to adopt the focal loss <ref type="bibr" target="#b18">[19]</ref>, which decays the loss as the label's confidence increases. This puts focus on hard samples, while down-weighting easy samples, which are mostly related to easy background locations. Surprisingly, focal loss is seldom used for multi-label classification, and cross-entropy is often the default choice (see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12]</ref>, for example). Since high negative-positive imbalance is also encountered in multi-label classification, focal loss might provide better results, as it encourages focusing on relevant hard-negative samples, which are mostly related to images that do not contain the positive class, but do contain some other confusing categories. Nevertheless, for the case of multi-label classification, treating the positive and negative samples equally, as proposed by focal loss, is sub-optimal, as it results in the accumulation of more loss gradients from negative samples, and down-weighting of important contributions from the rare positive samples. In other words, the network might focus on learning features from negative samples while underemphasizing learning features from positive samples.</p><p>In this paper, we introduce an asymmetric loss (ASL) for multi-label classification, which explicitly addresses the negative-positive imbalance. ASL is based on two key properties: first, to focus on hard negatives while maintaining the contribution of positive samples, we decouple the modulations of the positive and negative samples and assign them different exponential decay factors. Second, we propose to shift the probabilities of negative samples to completely discard very easy negatives (hard thresholding). By formulating the loss derivatives, we demonstrate that probability shifting also enables to discard very hard negative samples, suspected as mislabeled, which are common in multi-label problems <ref type="bibr" target="#b9">[10]</ref>.</p><p>We compare ASL to the common symmetrical loss functions, cross-entropy and focal loss, and show significant mAP improvement using our asymmetrical formulation. By analyzing the model's probabilities, we demonstrate the effectiveness of ASL in balancing between negative and positive samples. We also introduce a method that dynamically adjusts the asymmetry level throughout the training process, by demanding a fixed gap between positive and negative average probabilities, allowing simplification of the hyperparameter selection process.</p><p>The paper's contributions can be summarized as follow:</p><p>? We design a novel loss function, ASL, which explicitly copes with two main challenges in multi-label classification: high negative-positive imbalance, and ground-truth mislabeling. ? We thoroughly study the loss properties via detailed gradient analysis. An adaptive procedure for controlling the asymmetry level of the loss is introduced, to simplify the process of hyper-parameter selection. ? Using ASL, we obtain state-of-the-art results on four popular multi-label benchmarks. For example, we reach 86.6% mAP on MS-COCO dataset, surpassing the previous top result by 2.8%. ? Our solution is effective and easy to use. It is based on standard architectures, does not increase training and inference time, and does not need any external information, in contrast to recent approaches. To make ASL accessible, we share our trained models and a fully reproducible training code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Asymmetric Loss</head><p>In this section, we will first review cross-entropy and focal loss. Then we will introduce the components of the proposed asymmetric loss (ASL), designed to address the inherent imbalance nature of multi-label datasets. We will also analyze ASL gradients, provide probability analysis, and present a method to set the loss' asymmetry levels during training dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Binary Cross-Entropy and Focal Loss</head><p>As commonly done in multi-label classification, we reduce the problem to a series of binary classification tasks. Given K labels, the base network outputs one logit per label, z k . Each logit is independently activated by a sigmoid function ?(z k ). Let's denote y k as the ground-truth for class k. The total classification loss, L tot , is obtained by aggregating a binary loss from K labels:</p><formula xml:id="formula_0">L tot = K k=1 L (?(z k ), y k ) .<label>(1)</label></formula><p>A general form of a binary loss per label, L, is given by:</p><formula xml:id="formula_1">L = ?yL + ? (1 ? y)L ?<label>(2)</label></formula><p>Where y is the ground-truth label (for brevity we omitted the class index k), and L + and L ? are the positive and negative loss parts, respectively. Following <ref type="bibr" target="#b18">[19]</ref>, focal loss is obtained by setting L + and L ? as:</p><formula xml:id="formula_2">? ? ? ? ? L + = (1 ? p) ? log(p) L ? = p ? log(1 ? p)<label>(3)</label></formula><p>where p = ?(z) is the network's output probability and ? is the focusing parameter. ? = 0 yields binary cross-entropy. By setting ? &gt; 0 in Eq. 3, the contribution of easy negatives (having low probability, p ? 0.5) can be downweighted in the loss function, enabling to focus more on harder samples during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Asymmetric Focusing</head><p>When using focal loss for multi-label training, there is an inner trade-off: setting high ?, to sufficiently down-weight the contribution from easy negatives, may eliminate the gradients from the rare positive samples. We propose to decouple the focusing levels of the positive and negative samples. Let ? + and ? ? be the positive and negative focusing parameters, respectively. We obtain asymmetric focusing by redefining the loss:</p><formula xml:id="formula_3">? ? ? ? ? L + = (1 ? p) ? + log(p) L ? = p ? ? log(1 ? p)<label>(4)</label></formula><p>Since we are interested in emphasizing the contribution of positive samples, we usually set ? ? &gt; ? + . Asymmetric focusing decouples the decay rates of positive and negative samples. Through this, we achieve better control over the contribution of positive and negative samples to the loss function, and help the network learn meaningful features from positive samples, despite their rarity.</p><p>It should be noted that methods which address class imbalance via static weighting factors were proposed in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref>. However, <ref type="bibr" target="#b18">[19]</ref> found that those weighting factors interact with the focusing parameter, making it necessary to select the two together. In practice, <ref type="bibr" target="#b18">[19]</ref> even suggested a weighting factor which favors background samples (? = 0.25). In section 3 we will show that simple linear weighting is insufficient to tackle the negative-positive imbalance issue in multi-label classification properly. For those reasons, we chose to avoid adding static weighting factors to our focusing formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Asymmetric Probability Shifting</head><p>Asymmetric focusing reduces the contribution of negative samples to the loss when their probability is low (soft thresholding). Since the level of imbalancing in multi-label classification can be very high, this attenuation is not always sufficient. Hence, we propose an additional asymmetric mechanism, probability shifting, that performs hard thresholding of very easy negative samples, i.e., it fully discards negative samples when their probability is very low. Let's define the shifted probability, p m , as:</p><formula xml:id="formula_4">p m = max(p ? m, 0)<label>(5)</label></formula><p>Where the probability margin m ? 0 is a tunable hyperparameter. Integrating p m into L ? of Eq.(3), we get an asymmetric probability-shifted focal loss:</p><formula xml:id="formula_5">L ? = (p m ) ? log(1 ? p m )<label>(6)</label></formula><p>In <ref type="figure" target="#fig_1">Figure 2</ref> we draw the probability-shifted focal loss, for negative samples, and compare it to regular focal loss and cross-entropy. From a geometrical point-of-view, we can  see that probability shifting is equivalent to moving the loss function to the right, by a factor m, thus getting L ? = 0 when p &lt; m. We will later show, via gradient analysis, another important property of the probability shifting mechanism -it can also reject mislabeled negative samples. Notice that the concept of probability shifting is not limited to cross-entropy or focal loss, and can be used on many loss functions. Linear hinge loss <ref type="bibr" target="#b0">[1]</ref>, for example, can also be seen as (symmetric) probability shifting of linear loss. Also notice that logits shifting, as suggested in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b32">[33]</ref>, is different from probability shifting due to the nonlinear sigmoid operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">ASL Definition</head><p>To define the Asymmetric Loss (ASL), we integrate the two mechanisms of asymmetric focusing and probability shifting into a unified formula:</p><formula xml:id="formula_6">ASL = ? ? ? ? ? L + = (1 ? p) ? + log(p) L ? = (p m ) ? ? log(1 ? p m )<label>(7)</label></formula><p>Where p m is defined in Eq. <ref type="bibr" target="#b4">(5)</ref>. ASL allows us to apply two types of asymmetry for reducing the contribution of easy negative samples to the loss function -soft thresholding via the focusing parameters ? ? &gt; ? + , and hard thresholding via the probability margin m.</p><p>It can be convenient to set ? + = 0, so that positive samples will incur simple cross-entropy loss, and control the level of asymmetric focusing via a single hyper-parameter, ? ? . For experimentation and generalizability, we still keep the ? + degree of freedom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Gradient Analysis</head><p>To better understand the properties and behavior of ASL, we next provide an analysis of the loss gradients, in comparison to the gradients of cross entropy and focal loss. Looking at the gradients is useful since, in practice, the network weights are updated according to the gradient of the loss, with respect to the input logit z. The loss gradients for negative samples in ASL are:</p><formula xml:id="formula_7">dL ? dz = ?L ? ?p ?p ?z = (p m ) ? ? 1 1 ? p m ? ? ? log 1 ? p m p m p(1 ? p) (8) Where p = 1</formula><p>1+e ?z , and p m is defined in Eq.(5). In <ref type="figure" target="#fig_2">Figure 3</ref> we present the normalized gradients of ASL, and compare it to other losses. Following <ref type="figure" target="#fig_2">Figure 3</ref>, we can roughly split the negative samples in ASL into three loss-regimes: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Hard-threshold -very easy negatives, with p &lt; m, that</head><p>should be ignored, in order to focus on harder samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Soft-threshold -negative samples, with p &gt; m, that</head><p>should be attenuated when their probability is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mislabeled -very hard negative samples, with p &gt; p * ,</head><p>where p * is defined as the point where d dp dL dz = 0, which are suspected as mislabeled -when the network computes a very large probability for a negative sample, it is possible that the sample was mislabeled, and its correct label should be positive. It has been shown by <ref type="bibr" target="#b9">[10]</ref> that multi-label datasets are prone to mislabeling of negative samples, probably because the manual labeling task is difficult. When dealing with highly imbalanced datasets, even a small mislabeling rate of negative samples largely impact the training. Hence, rejection of mislabelled samples can be beneficial. The rejection must be done carefully, to allow the network to propagate gradients from actual misclassified negative examples.</p><p>In <ref type="table" target="#tab_0">Table 1</ref> we compare the properties and abilities of ASL to other losses, according to the gradient analysis. We can  see that only when we combine the two asymmetry mechanisms, focusing and probability margin, we enjoy all the abilities and advantages which are beneficial for imbalanced datasets: hard thresholding of very easy samples, non-linear attenuation of easy samples, rejection of mislabeled samples and continuous loss gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Probability Analysis</head><p>In this section, we wish to provide further support to our claim that in multi-label datasets, using a symmetric loss such as cross entropy or focal loss is sub-optimal for learning positive samples' features. We do that by monitoring the average probabilities outputted by the network during the training. This allows us to evaluate the network's level of confidence for positive and negative samples. Low confidence suggests that features were not learned properly. We begin by defining p t as:</p><formula xml:id="formula_8">p t = p if y = 1 1 ?p otherwise<label>(9)</label></formula><p>wherep denotes the average probability of the samples in a batch at each iteration. Denote by p + t and p ? t the average probabilities of the positive and negative samples, respectively, and by ?p the probability gap:</p><formula xml:id="formula_9">?p = p + t ? p ? t .<label>(10)</label></formula><p>A balanced training should demonstrate similar level of mean confidence for positive and negative samples, i.e., ?p should be small throughout and at the end of the training.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref> we present the average probabilities p + t and p ? t along the training, for three different loss functions: cross-entropy, focal loss and ASL. <ref type="figure" target="#fig_4">Figure 4</ref> demonstrates the limitation of using symmetric losses for imbalanced datasets. When training with either cross-entropy loss or focal loss, we observe that p ? t ? p + t (at the end of the training, ?p = ?0.23 and ?p = ?0.1, respectively). This implies that the optimization process gave too much weight to negative samples. Conversely, when training with ASL we can eliminate the gap, implying that the network has the ability to properly emphasize positive samples.</p><p>Notice that by lowering the decision threshold p th at inference time (a sample will be declared as positive if p &gt; p th ), we can control the precision vs. recall tradeoff, and favor high true-positive rate over low false-negative rate. However, a large negative probability gap, as obtained by the symmetric losses, suggests that the network underemphasized gradients from positive samples and converged to a local minima, with sub-optimal performances. We will validate this claim in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Adaptive Asymmetry</head><p>Hyper-parameters of a loss function are usually adjusted via a manual tuning process. This process is often cumbersom,e and requires a level of human expertise. Based on our probability analysis, we wish to offer a simple intuitive way of dynamically adjusting ASL's asymmetry levels, with a single interpretable control parameter.</p><p>In the last section we demonstrated that ASL enables to balance a network, and prevent a situation where negative samples have significantly larger p t than positive samples (?p &lt; 0). We now wish to go the other way around, and adjust ? ? dynamically throughout the training, to match a desired probability gap, denoted by ?p target . We can achieve this by a simple adaptation of ? ? after each batch, as described in Eq. 11.</p><formula xml:id="formula_10">? ? ? ? ? + ?(?p ? ?p target )<label>(11)</label></formula><p>where ? is a dedicated step size. As we increase ?p target , Eq. 11 enables us to dynamically increase the asymmetry level throughout the training, forcing the optimization process to focus more on the positive samples' gradients. Notice that using similar logic to Eq. 11, we can also dynamically adjust the probability margin, or simultaneously adjust both asymmetry mechanisms. For simplicity, we chose to explore the case of adjusting only ? ? throughout the training, with ? + = 0 and a small fixed probability margin. <ref type="figure" target="#fig_10">Figure 9</ref> in appendix A presents the values of ? ? and ?p throughout the training, for ?p target = 0.1. After 10% of the training, the network converges successfully to the target probability gap, and to a stable value of ? ? . In the next section we will analyze the mAP score and possible use-cases for this dynamic scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Study</head><p>In this section, we will provide thorough experimentations to better understand the different losses, and demonstrate the improvement we gain from ASL, compared to other losses. We will also test our adaptive asymmetry mechanism, and compare it to a fixed scheme. For testing, we will use the well-known MS-COCO <ref type="bibr" target="#b19">[20]</ref> dataset (see Section 4.1.1 for full dataset and training details).</p><p>Focal Loss Vs Cross-Entropy: In <ref type="figure" target="#fig_6">Figure 5</ref> we present the mAP scores obtained for different values of focal loss ? (? = 0 is cross-entropy). We can see from <ref type="figure" target="#fig_6">Figure 5</ref>   with cross-entropy loss, the mAP score is lower than the one obtained with focal loss (84.0% vs 85.1%). Top scores with focal loss are obtained for 2 ? ? ? 4. With ? below that range, the loss does not provide enough down-weighting for easy negative samples. With ? above that range, there is too much down-weighting of the rare positive samples.</p><p>Asymmetric Focusing: In <ref type="figure" target="#fig_7">Figure 6</ref> we test the asym-metric focusing mechanism: for two fixed values of ? ? , 2 and 4, we present the mAP score along the ? + axis. 6 demonstrates the effectiveness of asymmetrical focusing -as we decrease ? + (hence increasing the level of asymmetry), the mAP score significantly improves.</p><p>Interestingly, simply setting ? + = 0 leads to the best results in our experiments. That may further support the importance of keeping the gradient magnitudes high for positive samples. Indeed, allowing ? + &gt; 0 may be useful for cases where there is also an abundance of easy positive samples. Note that we also tried training with ? + &lt; 0, to extend the asymmetry further. However, these trials did not converge, therefore they are not presented in <ref type="figure" target="#fig_7">Figure 6</ref>.</p><p>Asymmetric Probability Margin: In <ref type="figure" target="#fig_8">Figure 7</ref> we apply our second asymmetry mechanism, asymmetric probability margin, on top of cross-entropy loss (? = 0) and two levels of (symmetric) focal loss, ? = 2 and ? = 4. We can see from <ref type="figure" target="#fig_8">Figure 7</ref> that both for cross-entropy and focal loss, introducing asymmetric probability margin improves the mAP score. For cross-entropy, the optimal probability margin is low, m = 0.05, in agreement with our gradient analysis -cross-entropy with probability margin produces a non-smooth loss gradient, with less attenuation of easy samples. Hence, a small probability margin, which still enables hard threshold for very easy samples and rejection of mislabeled samples, is better. For focal loss, the optimal probability margin is significantly higher, 0.3 ? m ? 0.4. This again can be explained by analyzing the loss gradients: since focal loss already has non-linear attenuation of easy samples, we need a larger probability margin to introduce meaningful asymmetry. We can also see that when introducing asymmetric probability margin, better scores are obtained for ? = 2 compared to ? = 4, meaning that asymmetric probability margin works better on top of a modest amount of focal loss.</p><p>Comparing Different Asymmetries: Until now we tested each ASL asymmetry separately. In <ref type="table">Table 2</ref> we present the mAP scores achieved when combining the asymmetries, and compare them to the scores obtained when applying each asymmetry alone. Also, we compare ASL results to another asymmetric mechanism -focal loss combined with linear weighting, as proposed in <ref type="bibr" target="#b22">[23]</ref>, that statically favors positive samples. The optimal static weight was searched over a range of values between 0.5 to 0.95, with skips of 0.05.  <ref type="table">Table 2</ref>: MS-COCO mAP scores for different asymmetric methods. Focusing mAP obtained for ? + = 0, ? ? = 3. Margin mAP obtained for ? = 2, m = 0.3. Combined mAP obtained for ? + = 0, ? ? = 4, m = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We can see from <ref type="table">Table 2</ref> that the best results are obtained when combining the two components of asymmetry. This correlates with our analysis of the loss gradients in <ref type="figure" target="#fig_2">Figure  3</ref>, where we demonstrate how combining the two asymmetries enables discarding of very easy samples, nonlinear attenuation of easy samples and rejection of possibly mislabeled very hard negative samples, a result which is not possible when applying only one type of asymmetry. <ref type="table">Table 2</ref> also shows that using static weighing is insufficient to properly handle the high negative-positive imbalance in multilabel classification, and ASL, which operates dynamically on easy and hard samples, performs better.</p><p>Adaptive Asymmetry: We now examine the effectiveness of adjusting the ASL asymmetry levels dynamically, via the procedure proposed in Eq.11. In <ref type="table" target="#tab_2">Table 3</ref> we present the mAP score, and the final value of ? ? , obtained for various values of ?p target .</p><p>We can see from  <ref type="table" target="#tab_2">Table 3</ref>: Adaptive Asymmetry. mAP scores and ? ? obtained from adaptive asymmetry runs, for different ?p target .</p><p>demanding the unbiased case ?p target = 0, a significant improvement is achieved compared to focal loss (85.8% vs. 85.1%). Even better scores are obtained when using a higher probability gap, ?p target = 0.2. Interestingly, extra focus on the rare positive samples (?p target &gt; 0) is better than just demanding the unbiased case. Notice that the top mAP scores obtained from the dynamic scheme are still lower by 0.2% compared to the best ASL score with a fixed ? ? . One possible reason for this (small) degradation is that the training process is highly impacted by the first epochs <ref type="bibr" target="#b12">[13]</ref>. Tuning hyper-parameter dynamically may be sub-optimal at the beginning of the training, which decreases the overall performance. To compensate for the initial recovery iterations, dynamically-tuned ? ? tends to converge to higher values, but the overall score is still somewhat hindered. Due to this decline, we chose to use a fixed asymmetry scheme in section 4.</p><p>Still, the dynamic scheme can be appealing to a nonexpert user, as it allows control of the asymmetry level via one simple interpretable hyper-parameter. In addition, we will explore in the future ways to expand this scheme for other applications, such as tuning ? ? adaptively per class, which can be impractical with a regular exhaustive search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset Results</head><p>In this section, we will evaluate ASL on four popular multi-label classification datasets, and compare its results to known state-of-the-art techniques, and to other commonly used loss functions. We will also test ASL's applicability to other computer vision tasks, such as single-label classification and object detection. <ref type="bibr" target="#b19">[20]</ref> is a widely used dataset to evaluate computer vision tasks such as object detection, semantic segmentation and image captioning, and has been adopted recently to evaluate multi-label image classification. For multi-label classification, it contains 122, 218 images with 80 different categories, where every image contains on average 2.9 labels, thus giving an average positive-negative ratio of: 2.9 80?2.9 = 0.0376. The dataset is divided to a train-ing set of 82, 081 images and a validation set of 40, 137 images. Following conventional settings for MS-COCO <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21]</ref>, we report the following statistics: mean average precision (mAP), average per-class precision (CP), recall (CR), F1 (CF1) and the average overall precision (OP), recall (OR) and F1 (OF1), for the overall statistics and top-3 highest scores. Among these metrics, mAP, OF1, and CF1 are the main metrics, since they take into account both falsenegative and false-positive rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-Label Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">MS-COCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO</head><p>In <ref type="table" target="#tab_3">Table 4</ref> we compare ASL results to known state-ofthe-art methods from the literature, for the main metrics (Full training details and loss hyper-parameters are provided in appendix B). In <ref type="table" target="#tab_9">Table 8</ref> in appendix C we bring results for all the metrices. We can see from  ing ASL we significantly outperform previous state-of-theart methods on ResNet101, the commonly used architecture in multi-label classification, and improve the top mAP score by more than 1%. Other metrics also show improvement.</p><p>Notice that our ASL-based solution does not require architecture modifications, and does not increase inference and training times. This is in contrast to previous top solutions, which include intricate architecture modifications (attentional regions <ref type="bibr" target="#b11">[12]</ref>, GCNs <ref type="bibr" target="#b37">[38]</ref>), injecting external data like label embeddings <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5]</ref>, and using teacher models <ref type="bibr" target="#b20">[21]</ref>. However, ASL is fully complementary to those methods, and employing them as well could lead to further score improvement, at the cost of increasing training complexity and reducing throughput. In addition, we see from <ref type="table" target="#tab_3">Table  4</ref> that using a newer architecture like TResNet-L, that was designed to match the GPU throughput of ResNet101 <ref type="bibr" target="#b24">[25]</ref>, we can further improve the mAP score, while still keeping the same training and inference time. This is another contribution of our proposed solution -identifying that modern fast architectures can give a big boost to multi-label classification, and the common usage of ResNet101 can be suboptimal.</p><p>In <ref type="figure" target="#fig_9">Figure 8</ref> we test the applicability of ASL for different backbones, by comparing the different loss functions on three commonly used architectures: OFA-595 <ref type="bibr" target="#b1">[2]</ref>, ResNet101 and TResNet-L. We can see from <ref type="figure" target="#fig_9">Figure 8</ref> that on all backbones, ASL outperforms focal loss and crossentropy, demonstrating its robustness to backbone selection, and its superiority over previous loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy</head><p>Focal Loss ASL Impact of pretraining and input resolutions: In <ref type="table" target="#tab_6">Table  5</ref> we compare the mAP results obtained with a standard ImageNet-1K pretraining, and the newer ImageNet-21K pretraining <ref type="bibr" target="#b23">[24]</ref>. We can see that using better pretraining has dramatic impact on the results, increasing the mAP score by almost 2%. We also show in <ref type="table" target="#tab_6">Table 5</ref> that increasing input resolution from 448 to 640 can further improve results.   <ref type="bibr" target="#b31">[32]</ref> 91.9 -FeV+LV <ref type="bibr" target="#b33">[34]</ref> 92.0 -SSGRL <ref type="bibr" target="#b3">[4]</ref> 93.4 95.0 ML-GCN <ref type="bibr" target="#b5">[6]</ref> 94.0 -BMML <ref type="bibr" target="#b17">[18]</ref> -95.0 ASL (ResNet101) 94.4 95.3 ASL (TResNet-L) 94.6 95.8 <ref type="table">Table 6</ref>: Comparison of ASL to known state-of-the-art models on Pascal-VOC dataset. Metrics are in %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We can see from <ref type="table">Table 6</ref> that ASL achieves new state-ofthe-art results on Pascal-VOC, with and without additional pre-training. In <ref type="table" target="#tab_10">Table 9</ref> in the appendix we compare different loss functions on Pascal-VOC, showing that ASL outperforms cross-entropy and focal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">NUS-WIDE</head><p>In appendix E we bring results on another common multilabel dataset, NUS-WIDE <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_0">Table 10</ref> shows that ASL again outperforms top previous approaches by a large margin, and reach new state-of-the-art result on NUS-WIDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Open Images</head><p>Open Images (v6) <ref type="bibr" target="#b16">[17]</ref> is a large scale dataset, which consists of 9 million training images and 125, 436 test images. It is partially annotated with human labels and machinegenerated labels. The scale of Open Images is much larger than previous multi-label datasets such as NUS-WIDE, Pascal-VOC and MS-COCO. Also, it contains a considerable amount of unannotated labels. That allows us to test ASL on extreme classification <ref type="bibr" target="#b38">[39]</ref>, and high mislabeleing scenarios. Full dataset and training details appear in the appendix F. To the best of our knowledge, no results for other methods were published yet for v6 variant of Open Images. Hence, we compare ASL only to the other common loss functions in multi-label classification. Yet we hope that our result can serve as a benchmark for future comparisons.</p><p>Open Images Results appear in <ref type="table">Table 7</ref>. We can see from <ref type="table">Table 7</ref> that ASL significantly outperforms focal loss and cross-entropy on Open Images, demonstrating that ASL is suitable for large datasets and extreme classification cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Additional Computer Vision Tasks</head><p>In addition to multi-label classification, we wanted to test ASL on other relevant computer vision tasks. Since finegrain single-label classification and object detection tasks usually contain a large portion of background or long-tail  <ref type="table">Table 7</ref>: Comparison of ASL to focal loss and crossentropy on Open Images V6 dataset.</p><p>cases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, and are known to benefit from using focal loss, we chose to test ASL on these tasks. In sections G and H in the appendix we show that ASL outperform focal loss on relevant datasets for these additional tasks, demonstrating that ASL is not limited to multi-label classification only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an asymmetric loss (ASL) for multi-label classification. ASL contains two complementary asymmetric mechanisms, which operate differently on positive and negative samples. By examining ASL derivatives, we gained a deeper understanding of the loss properties. Through network probability analysis, we demonstrate the effectiveness of ASL in balancing between negative and positive samples, and proposed an adaptive scheme that can dynamically adjusts the asymmetry levels throughout the training. Extensive experimental analysis shows that ASL outperforms common loss functions and previous state-of-the-art methods on popular multi-label classification benchmarks, including MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Label General Training Details</head><p>Unless stated explicitly otherwise, we used the following training procedure: We trained the model for 60 epochs using Adam optimizer and 1-cycle policy <ref type="bibr" target="#b25">[26]</ref>, with maximal learning rate of 2e-4. For regularization, we used standard augmentation techniques <ref type="bibr" target="#b7">[8]</ref>. We found that the common ImageNet statistics normalization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref> does not improve results, and instead used a simpler normalization -scaling all the RGB channels to be between 0 and 1.</p><p>Following the experiments in section 3, for ASL we used ? ? = 4, ? + = 0 and m = 0.05, and for focal loss we used ? = 2. Our default and recommended backbone for multi-label training is TResNet-L. However, for fair comparison to previous works we also added ResNet101 backbone results on some datasets (TResNet-L and ResNet101 are equivalent in runtime).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing MS-COCO On All Common Metrics</head><p>In <ref type="table" target="#tab_9">Table 8</ref> we compare ASL results, to known stateof-the-art methods, on all common metrics for MS-COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparing Loss Function on Pascal-VOC Dataset</head><p>In <ref type="table" target="#tab_10">Table 9</ref> we compare ASL results to other loss functions on Pascal-VOC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. NUS-WIDE</head><p>NUS-WIDE <ref type="bibr" target="#b6">[7]</ref> dataset originally contained 269,648 images from Flicker, that have been manually annotated with 81 visual concepts. Since some urls have been deleted, we were able to download only 220,000 images, similar to <ref type="bibr" target="#b9">[10]</ref>. We can find in previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref> other variants of NUS-WIDE dataset, and its hard to do a one-to-one comparison. We recommend using our publicly available variant 1 for standardization and a completely fair comparison in future works. We used the standard 70-30 train-test split <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21]</ref>. Our training settings were identical to the ones used for MS-COCO. We can see from <ref type="table" target="#tab_0">Table 10</ref> that ASL improves the known state-of-the-art results on NUS-WIDE by a large margin. In <ref type="table" target="#tab_0">Table 11</ref> we compare ASL results to other loss functions on NUS-WIDE dataset, again showing that ASL outperform cross-entropy and focal-loss..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Open Images Training Details</head><p>Due to missing links on flicker, we were able to download only 114, 648 test images from Open Images dataset, which contain about 5, 400 unique tagged classes. For dealing with the partial labeling methodology of Open Images dataset, we set all untagged labels as negative, with reduced weights. Due to the large the number of images, we trained our network for 30 epochs on input resolution of 224, and finetuned it for 5 epochs on input resolution of 448. Since the level of positive-negative imbalancing is significantly higher than MS-COCO, we increased the level of loss asymmetry: For ASL, we trained with ? ? = 7, ? + = 0. For Focal loss, we trained with ? = 4. Other training details are similar to the ones used for MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Fine-Grain Single-Label Classification Results</head><p>For testing ASL on fine-grain single-label classification, we chose to work on the competitive Herbarium 2020 FGVC7 Challenge <ref type="bibr" target="#b15">[16]</ref>. The goal of Herbarium 2020 is to identify vascular plant species from a large, long-tailed collection Herbarium specimens provided by the New York Botanical Garden (NYBG). The dataset contains over 1M images representing over 32,000 plant species. This is a dataset with a long tail; there are a minimum of 3 specimens per species, however, some species are represented by more   Method mAP CF1 OF1 S-CLs <ref type="bibr" target="#b20">[21]</ref> 60.1 58.7 73.7 MS-CMA <ref type="bibr" target="#b35">[36]</ref> 61.4 60.5 73.8 SRN <ref type="bibr" target="#b39">[40]</ref> 62.0 58.5 73.4 ICME <ref type="bibr" target="#b4">[5]</ref> 62.8 60.7 74.1 ASL (ResNet101) 63.9 62.7 74.6 ASL (TResNet-L) 65.2 63.6 75.0  than a hundred specimens. The metric chosen for the competition is macro F1 score. For Focal loss, we trained with ? = 2. For ASL, we trained with ? ? = 4, ? + = 0. The metric chosen for the competition is macro F1 score. In <ref type="table" target="#tab_0">Table 12</ref> we bring results of ASL on Herbarium dataset, and compare it to regular focal loss. We can see from <ref type="table" target="#tab_0">Table 12</ref> that ASL outperforms focal loss on this fine-grain singlelabel classification dataset by a large margin. Note that Herbarium 2020 was a CVPR-Kaggle classification competition. Our ASL test-set score would achieve the 3rd place in the competition, among 153 teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method macro F1 [%] Focal Loss</head><p>76.1 ASL 77.6 <ref type="table" target="#tab_0">Table 12</ref>: Comparison of ASL to focal loss on Herbarium dataset. Macro-F1 is the competition official metrics. All results are on an unseen private-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Object Detection Results</head><p>For testing ASL on object detection, we used the MS-COCO [20] dataset (object detection task), which contains a training set of 118k images, and an evaluation set of 5k images. For training, we used the popular mm-detection <ref type="bibr" target="#b2">[3]</ref> package, with the enhancements discussed in ATSS <ref type="bibr" target="#b37">[38]</ref> and FCOS <ref type="bibr" target="#b27">[28]</ref> as the object detection method. We trained a TResNet-M [25] model with SGD optimizer for 70 epochs, with momentum of 0.9 , weight decay of 0.0001 and batch size of 48. We used learning rate warm up, initial learning rate of 0.01 and 10x reduction at epochs 40, 60. For ASL we used ? + = 1, ? ? = 2. For focal loss we used the common value, ? = 2 <ref type="bibr" target="#b18">[19]</ref>. Note that unlike multi-label and fine-grain single-label classification datasets, for object detection ? + = 0 was not the optimal solution. The reason for this might be the need to balance the contribution from the 3 losses used in object detection (classification, bounding box and centerness). We should further investigate this issue in the future.</p><p>Our object detection method, FCOS <ref type="bibr" target="#b27">[28]</ref>, uses 3 different types of losses: classification (focal loss), bounding box (IoU loss) and centerness (plain cross-entropy). The only component which is effected by the large presence of background samples is the classification loss. Hence, for testing we replaced only the classification focal loss with ASL.</p><p>In <ref type="table" target="#tab_0">Table 13</ref> we compare the mAP score obtained from ASL training to the score obtained with standard focal loss. We can see from <ref type="table" target="#tab_0">Table 13</ref> that ASL outscores regular focal loss, yielding an 0.4% improvement to the mAP score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP [%]</head><p>Focal Loss 44.0 ASL 44.4 <ref type="table" target="#tab_0">Table 13</ref>: Comparison of ASL to focal loss on MS-COCO detection dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Loss Comparisons. Comparing probabilityshifted focal loss to regular focal loss and cross-entropy, for negative samples. We used ? ? = 2 and m = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Gradient Analysis. Comparing the loss gradients vs. probability for different loss regimes. CE = Cross-Entropy (m = ? ? = 0), CE+PS = Cross-Entropy with Probability Shifting (m &gt; 0, ? ? = 0), AF = Asymmetric Focusing (m = 0, ? ? &gt; 0), ASL (m &gt; 0, ? ? &gt; 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Probability analysis. The mean probability of positive and negative samples along the training with cross-entropy, focal loss and ASL, on MS-COCO. For focal loss we used ? = 2. For ASL we used ? + = 0, ? ? = 2, m = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>mAP Vs. Focal Loss ?. Comparing MS-COCO mAP score for different values of focal loss ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>mAP Vs. Asymmetric Focusing ? + . Comparing MS-COCO mAP score for different value of asymmetric focusing ? + , for ? ? = 2 and ? ? = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>mAP Vs. Asymmetric Probability Margin. Comparing MS-COCO mAP score for different values of asymmetric probability margin, on top of a symmetric focal loss, with ? = 0, 2, 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Testing different losses on various backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Adaptive Asymmetry Dynamics. Values of ? ? and ?p throughout the training, for ?p target = 0.1. ? + is set to 0, m is set to 0.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Properties of different loss -CE (Cross-Entropy), AF (Asymmetric Focusing), PS (Probability Shifting).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>that even without any tuning,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>that us-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of ASL to state-of-the-art methods on MS-COCO. All metrics are in %. Results are reported for input resolution 448.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of MS-COCO mAP scores for different ImageNet pretraining schemes, and input resolutions. All metrics are in %.</figDesc><table><row><cell>4.1.2 Pascal-VOC</cell></row><row><cell>Pascal Visual Object Classes Challenge (VOC 2007) [11]</cell></row><row><cell>is another popular dataset for multi-label recognition. It</cell></row><row><cell>contains images from 20 object categories, with an aver-</cell></row><row><cell>age of 2.5 categories per image. Pascal-VOC is divided to</cell></row><row><cell>a trainval set of 5,011 images and a test set of 4,952 im-</cell></row><row><cell>ages. Our training settings were identical to the ones used</cell></row><row><cell>for MS-COCO. Notice that most previous works on Pascal-</cell></row><row><cell>VOC used simple ImageNet pre-training, but some used ad-</cell></row><row><cell>ditional data, like pre-training on MS-COCO or using NLP</cell></row><row><cell>models like BERT. For a fair comparison, we present our</cell></row><row><cell>results once with ImageNet pre-training, and once with ad-</cell></row><row><cell>ditional pre-train data (MS-COCO) and compare them to</cell></row><row><cell>the relevant works. Results appear in Table 6.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>82.5 72.2 77.0 84.0 75.6 79.6 87.1 63.6 73.5 89.4 66.0 76.0 ML-GCN [6] 83.0 85.1 72.0 78.0 85.8 75.4 80.3 87.2 64.6 74.2 89.1 66.7 76.3 KSSNet [21] 83.7 84.6 73.2 77.2 87.8 76.73.9 80.3 88.1 65.5 75.1 91.0 66.3 76.7 ASL (TResNet-L) 86.6 87.2 76.4 81.4 88.2 79.2 81.8 91.8 63.4 75.1 92.9 66.4 77.4</figDesc><table><row><cell>Method</cell><cell>mAP CP</cell><cell cols="2">All CR CF1 OP OR OF1</cell><cell>CP</cell><cell cols="4">Top 3 CR CF1 OP OR OF1</cell></row><row><cell>CADM [5]</cell><cell cols="3">82.3 2 81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS-CMA [36]</cell><cell cols="8">83.8 82.9 74.4 78.4 84.4 77.9 81.0 86.7 64.9 74.3 90.9 67.2 77.2</cell></row><row><cell>MCAR [12]</cell><cell cols="2">83.8 85.0 72.1 78.0 88.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 Our</cell><cell cols="2">NUS-WIDE</cell><cell>variant</cell><cell>can</cell><cell>be</cell><cell>download</cell><cell>from:</cell></row><row><cell></cell><cell></cell><cell cols="7">https://drive.google.com/file/d/0B7IzDz-4yH_HMFdiSE44R1lselE/vie</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison of ASL to known state-of-the-art models on MS-COCO dataset. All metrics are in %. Results are reported for input resolution 448.</figDesc><table><row><cell></cell><cell>mAP</cell><cell>mAP</cell></row><row><cell>Method</cell><cell>(ImageNet</cell><cell>(Extra</cell></row><row><cell></cell><cell>Only Pretrain)</cell><cell>Pretrain Data)</cell></row><row><cell>CE</cell><cell>93.2</cell><cell>95.0</cell></row><row><cell>Focal Loss</cell><cell>93.8</cell><cell>95.4</cell></row><row><cell>ASL</cell><cell>94.6</cell><cell>95.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison of ASL to other loss functions on Pascal-VOC dataset. Metrics are in %.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of ASL to known state-of-the-art models on NUS-WIDE dataset. All metrics are in %.</figDesc><table><row><cell>Method</cell><cell>mAP CF1 OF1</cell></row><row><cell>CE (Ours)</cell><cell>63.1 61.7 74.6</cell></row><row><cell cols="2">Focal loss (Ours) 64.0 62.9 74.7</cell></row><row><cell>ASL (Ours)</cell><cell>65.2 63.6 75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison of ASL to known other loss functions on NUS-WIDE dataset. All metrics are in %.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification with a reject option using a hinge loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><forename type="middle">H</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with joint class-aware map disentangling and label correlation embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2007 (voc2007) results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-label image recognition with multi-class attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01755</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiat Chuan Tan</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/herbarium-2020-fgvc7" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">herbarium-2020-fgvc7</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bi-modal learning with channel-wise attention for multilabel image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="9965" to="9977" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximizing subset accuracy with recurrent neural networks in multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Menc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyunwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>F?rnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5413" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Baris Can Cam, Sinan Kalkan, and Emre Akbas. Imbalance problems in object detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13630</idno>
		<title level="m">Tresnet: High performance gpu-dedicated architecture</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A baseline for multi-label image classification using an ensemble of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-label classification with label graph superimposing. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention-driven dynamic graph convolutional network for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with imagelevel supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
