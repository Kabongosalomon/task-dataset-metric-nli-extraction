<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AXIAL ATTENTION IN MULTIDIMENSIONAL TRANSFORMERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weissenborn</forename><surname>Dirk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salimans</forename><surname>Tim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AXIAL ATTENTION IN MULTIDIMENSIONAL TRANSFORMERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Autoregressive models are a family of exact likelihood-based generative models that represent the joint distribution of data x = (x 1 , . . . , x N ) as a product of conditionals p ? (x) = N i=1 p ? (x i | x &lt;i ). Neural network models in this family have achieved state-of-the-art log likelihoods on highdimensional image and video datasets <ref type="bibr" target="#b14">(van den Oord et al., 2016a;</ref><ref type="bibr" target="#b1">Chen et al., 2018;</ref><ref type="bibr" target="#b7">Menick &amp; Kalchbrenner, 2018;</ref><ref type="bibr" target="#b9">Parmar et al., 2018;</ref><ref type="bibr" target="#b2">Child et al., 2019;</ref><ref type="bibr" target="#b17">Weissenborn et al., 2019;</ref><ref type="bibr" target="#b11">Salimans et al., 2017;</ref><ref type="bibr" target="#b13">Uria et al., 2016;</ref><ref type="bibr" target="#b8">Parikh et al., 2016;</ref><ref type="bibr" target="#b12">Theis &amp; Bethge, 2015;</ref><ref type="bibr" target="#b15">van den Oord et al., 2016b)</ref> due to architectural innovations that enable the following capabilities:</p><p>1. Large, high information bandwidth receptive fields for each pixel x i , capable of expressing long-range dependencies over previous pixels x &lt;i , and 2. Computationally efficient, vectorizable computation of the log likelihood and its gradient.</p><p>Autoregressive model architectures that can read long-range dependencies over large receptive fields are able to express all joint distributions over the data. Meanwhile, architectures that admit fast log likelihood gradient computation are suitable for training using a stochastic gradient method on a maximum likelihood objective-a straightforward, stable training procedure for generative models.</p><p>These desiderata make self-attention a compelling building block for autoregressive model architectures. Self-attention is a neural network operation that is able to transform a sequence y 1 , . . . , y N into a sequence y 1 , . . . , y N , where each y i depends on all y i by way of a single vectorizable computation <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. Self-attention is remarkably effective at learning long-range dependencies between data dimensions and neural networks that incorporate self-attention in their designs are state-of-the-art on many tasks from language modelling and machine translation to image and video modelling <ref type="bibr" target="#b9">(Parmar et al., 2018;</ref><ref type="bibr" target="#b2">Child et al., 2019)</ref>.  <ref type="figure">Figure 1</ref>: The Axial Transformer model for 2-dimensional tensors. Before sampling a channel we encode all previous channels and frames with 8 blocks of unmasked row and unmasked column attention (left). Then, for each row, we apply 4 blocks of unmasked row and masked column attention to integrate the previously sampled rows for the active channels into our encoded representation (middle). Finally, we shift the encoded representation up to make sure the conditioning information satisfies causality, and we run the inner decoder consisting of 4 blocks of masked row attention to sample a new row in the image (right).</p><p>But the power of self-attention comes at the price of computational complexity. The memory and computation it consumes grow quadratically with the sequence length N making it prohibitively expensive to directly apply self-attention to long sequences. In the case of autoregressive models of multidimensional tensors such as images or videos, the aim to capture large receptive fields in multiple dimensions further exacerbates the problem as even a modest number of receptive field steps in each dimension can encompass a large total number of locations. Various approaches have been proposed to alleviate this difficulty at the cost of either limiting the receptive field or requiring operations that may not be broadly available on GPUs or TPUs.</p><p>We propose the Axial Transformer, a simple yet effective self-attention-based autoregressive model for data organized as multidimensional tensors. Rather than applying attention to a flattened string of tensor elements, our model instead applies attention along a single axis of the tensor without flattening-we refer to this as "axial attention." Since the length of any single axis (that is, the height or width of an image) is typically much smaller than the total number of elements, an axial attention operation enjoys a significant saving in computation and memory over standard self-attention: for a d-dimensional tensor with shape N = N 1/d ? ? ? ? ? N 1/d , axial attention saves a O(N (d?1)/d ) factor of resources over standard self-attention.</p><p>Our Axial Transformer architecture allows for the majority of the context x &lt;i to be embedded with a high degree of parallelism without introducing conditional independence assumptions among any of the locations, but has an interesting property that it is amenable to a simple-to-implement fast sampling procedure. To sample one row of an image, the Axial Transformer only runs an autoregressive Transformer over that one row only, without re-embedding pixels from previous rows. We structure the Axial Transformer, however, so that it always defines a fully expressive joint distribution. No dependencies on previous pixels are ever lost.</p><p>We evaluate Axial Transformers on image and video modelling benchmarks. We show that Axial Transformer achieves state-of-the-art results on ImageNet-32 and on ImageNet-64. We also show that, simply by stacking a video along the channel dimension, the Axial Transformer can be directly applied to the channel-stacked video without nearly any modification. On the BAIR Robot Pushing benchmark, the Axial Transformer significantly outperforms previous results without using an architecture specially designed for videos. The generated samples on these datasets are of the expected high quality.</p><p>Axial Transformers do not require subroutines for GPUs or TPUs that may exhibit unfavorable memory bandwidth and computation trade-offs. Axial Transformers are simple to implement using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>To set the stage for our discussion, we first review self-attention and its computational resource requirements in the context of autoregressive modeling. A self-attention layer takes as input a length N sequence of D-dimensional embeddings X (a N ? D matrix) and produces an output sequence Y (also a N ? D matrix) via: This quadratic complexity makes it impractical to apply self-attention to images and videos directly as flattened vectors: a small 32?32?3 image has 3072 dimensions. Sequences such as these are too long for self-attention, so attempts to scale self-attention to these modalities generally involve restricting these sequence lengths in a modality-aware manner while attempting to preserve modeling performance.</p><formula xml:id="formula_0">Q = XW Q , K = XW K , V = XW V A = softmax QK / ? D , Y = AV W Q , W K ,</formula><p>One strategy is to restrict the conditioning context x &lt;i to a carefully designed small subset of the data dimensions. While this reduces the cost of attention, which is only performed over these small subsets instead of the full data, the model can no longer express all joint distributions over the data. <ref type="bibr" target="#b9">Parmar et al. (2018)</ref> propose image models with conditioning context x &lt;i restricted to a small window of the full image, but the implementation requires redundant data copies to extract and process these windows. <ref type="bibr" target="#b17">Weissenborn et al. (2019)</ref> similarly scale video autoregressive models by restricting the context, again preventing their model from expressing all joint distributions over pixels. Our models do not restrict context and hence we obtain better log likelihoods, as we will see in section 4.</p><p>A different strategy is to stack multiple sparse attention layers, each with restricted context for computational efficiency, but in a manner that overlapping these layers yields a full-context model. <ref type="bibr" target="#b2">Child et al. (2019)</ref> propose two sparse attention patterns with this property. However, the architecture they propose that works best for images (the Strided Sparse Transformer) requires custom sparse attention GPU kernels to implement a specific block-sparse variant of matrix-matrix-multiply. The model cannot be easily implemented on other hardware such as TPUs.</p><p>See table 1 for a summary of these architecture design tradeoffs. Our goal in this paper is to design attention-based autoregressive models that attain the best of all worlds. Our Axial Transformer, described in subsequent sections, has a full conditioning context, so its ability to express joint distributions is never limited. The Axial Transformer also does not require any redundant data copies or custom kernels to implement in an efficient way. Indeed, we designed, and will make open source, an efficient implementation that uses only standard operations in deep learning libraries.  <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> yes no yes no Image Transformer <ref type="bibr" target="#b9">(Parmar et al., 2018)</ref> no yes yes no Block Transformer <ref type="bibr" target="#b17">(Weissenborn et al., 2019)</ref> no yes yes no Strided Sparse Transformer <ref type="bibr" target="#b2">(Child et al., 2019)</ref> yes yes no no Axial Transformer (ours) yes yes yes yes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AXIAL TRANSFORMERS</head><p>We now describe Axial Transformers, our self-attention-based autoregressive models for highdimensional data tensors. We describe its basic building block in section 3.1 and then we complete the description into a full autoregressive model in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AXIAL ATTENTION</head><p>We first introduce our basic building block for developing self-attention-based autoregressive models for high-dimensional data tensors.</p><p>The proposed approach does not change the original shape of the multidimensional data tensor and performs a masked or unmasked attention over a single axis of the tensor at a time. We call this operation axial attention, denoted by Attention k (x). It performs attention over axis k of the tensor x, mixing information along axis k while keeping information along other axes independent. It is straightforward to implement: axial attention over axis k can be implemented by transposing all axes except k to the batch axis, calling standard attention as a subroutine, then undoing the transpose (an alternative is to use the einsum operation available in most deep learning libraries).</p><p>When the data is an image, we call Attention 1 column attention, as it mixes information within columns while keeping separate columns independent. We call Attention 2 row attention for analogous reasons. Axial attention on a square image of size N = S ? S performs attention on S sequences of length S-this is a total of O(S ? S 2 ) = O(N ? N ) computation-an O( ? N ) savings in computation over standard self-attention. In general, for a d-dimensional tensor with N = S d , axial attention saves O(N (d?1)/d ) computation over standard attention. Of course, a single layer of axial attention along some axis k does not have the full receptive field since it covers a single axis, but we will see in section 3.2 that stacking two axial attention layers allows the model to obtain a global receptive field.</p><p>It will be important for us to also define MaskedAttention k to be the causally masked variant of Attention k : component i of the result of MaskedAttention k (x) along axis k depends on only components 1, . . . , i of x along axis k. The receptive fields of these attention patterns, both unmasked and masked, are illustrated in <ref type="figure" target="#fig_0">fig. 2</ref>. We will use these masked blocks to build our autoregressive model in section 3.2.</p><p>Axial attention can be used within standard Transformer layers in a straightforward manner to produce Axial Transformer layers. The basic building blocks are the same as those found in the standard Transformer architecture:</p><p>? LayerNorm(x): layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>, and</p><p>? Dense D (x): a dense layer operating over the last axis of the input x. The letter D denotes the dimension of the output activations. If the input has shape H ? W ? C, then this operation is identical to a 1 ? 1 convolution, and the output has shape H ? W ? D.</p><p>We use these to define ResNet axial attention blocks operating on tensors of D-dimensional embeddings <ref type="bibr" target="#b16">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b2">Child et al., 2019)</ref>:</p><formula xml:id="formula_1">? FeedforwardBlock(x) = x + Dense D (Nonlinearity(Dense D (LayerNorm(x)))) ? AttentionBlock k (x) = x + Dense D (Attention k (LayerNorm(x))) ? TransformerBlock k (x) = FeedforwardBlock(AttentionBlock k (x))</formula><p>D is chosen to be some constant factor larger than D, from 1 to 4 <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. We also define a MaskedTransformerBlock k using MaskedAttention k in place of Attention k .</p><p>Operations similar to unmasked axial attention have been proposed in other contexts in computer vision <ref type="bibr" target="#b4">(Huang et al., 2019)</ref>. Our focus in forthcoming sections is the use of masked axial attention and its utility in autoregressive image modeling, which is not explored in these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AXIAL TRANSFORMERS</head><p>We now describe Axial Transformers, our axial attention-based autoregressive models for images and videos. We will use the axial attention operations described in section 3.1 as building blocks in a multi-layer autoregressive model of the form p ? (x) = N i=1 p ? (x i | x &lt;i ) following the raster scan ordering of pixels. We will accomplish this by building an autoregressive model over rows (section 3.2.1), then conditioning each row on previous rows (section 3.2.1), then further conditioning on previous channels and frames (section 3.2.2). Decomposing the model in this manner also leads to a simple fast and partly parallel sampling procedure (section 3.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">A MODEL FOR SINGLE-CHANNEL IMAGES</head><p>We begin with an autoregressive model for a single-channel image x with shape H ? W , with each pixel taking an integer value in [0, 255] representing its intensity. As is standard practice with Transformers, pixel intensities are first embedded into a H ? W ? D tensor of D-dimensional embeddings, which we call h. The architecture's responsibility is to transform h into a H ?W ?256 tensor of logits suitable for classification or sampling. These logits must depend only on previous pixels in the input x along the raster scan ordering to ensure that the architecture defines a valid autoregressive model.</p><p>Inner Decoder: a row-wise model Our idea is to begin with masked row attention layers to create a "row-wise" model:</p><formula xml:id="formula_2">h ? Embed(x) h ? ShiftRight(h) + PositionEmbeddings h ? MaskedTransformerBlock 2 (h) ?L row</formula><p>Here, L row is the number of masked row attention blocks applied to h. PositionEmbeddings is a H ? W ? D tensor of position embeddings that inform the attention layers of the position. For parameter efficiency we use "additively factorized" position embeddings, meaning that we parameterize them as a broadcasted sum of H ? 1 ? D embeddings for rows and 1 ? W ? D embeddings for columns.</p><p>The operation ShiftRight shifts the input right by one pixel, which has the effect of shifting the receptive field left by one pixel. This ensures that the masked row attention layers exclude the current pixel from their receptive field, which is crucial for architecture to define a correct autoregressive model.</p><p>As this model employs row attention only, it enjoys the computational efficiency benefits described in section 3.1. However, it clearly does not define a full-context model because each location in the output does not depend on input pixels in previous rows. If we were to use the resulting h as logits for pixel intensity prediction, we would obtain a set of H independent autoregressive models p(x i,j |x i,1 , . . . , x i,j?1 ) for each row <ref type="figure">i ? [1, H]</ref>, not a single autoregressive model with full context. We address this issue next.</p><p>Outer Decoder: capturing the rows above Each pixel x i,j in the aforementioned model already depends on previous pixels in its own row x i,&lt;j . We just need to make it depend on all previous rows x &lt;i,: too. So, we insert unmasked row and masked column layers in the beginning of the model <ref type="figure">Figure 3</ref>: Arrangement of inputs to the encoding network of the Axial Transformer. Previously available or generated channels of an image or video are sequentially stacked in the input. A variable number of padding planes are used as placeholders for future generated channels. A final integer plane signals to the Axial Transformer the channel that is being generated at that step.</p><p>as follows (newly inserted operations are underlined):</p><formula xml:id="formula_3">h ? Embed(x) u ? h + PositionEmbeddings u ? MaskedTransformerBlock 1 (TransformerBlock 2 (u)) ?L upper /2 h ? ShiftDown(u) + ShiftRight(h) + PositionEmbeddings h ? MaskedTransformerBlock 2 (h) ?L row</formula><p>The tensor u represents context captured above the current pixel. It is computed by unmasked row and masked column attention layers, repeated to a total of L upper layers to increase model capacity, which make u cover the receptive field at all rows above and including the current pixel. The ShiftDown operation shifts u down one pixel, which shifts its receptive field up one pixel. Thus we have a context which captures all pixels above while excluding the current row, which we add to h as input to the masked row layers. We have thus converted the row-wise model into a fully expressive autoregressive model that captures not only pixels in the current row but also those above.</p><p>Following standard practice, we pass the final h through layer normalization and a final dense layer to produce logits with shape H ? W ? 256. The logits at each location depend on all previous pixel locations in the raster scan ordering.</p><p>Semi-Parallel Sampling Naive implementations of sampling from sequential models are notoriously slow because they require re-evaluating the entire network to sample each location. In the case of our model for a ? N ? ? N square image, each network evaluation takes O(N ? N (L upper + L row )) time, so sampling the whole image would take O(N 2 ? N (L upper + L row )), which is far too large.</p><p>Fortunately, our architecture is amenable to a particularly simple implementation of a faster sampling that is able to compute large sections of the model in parallel (see <ref type="figure">Figure 1</ref>). Pseudocode is as follows:</p><p>1. For each row i ? [1, H]:</p><p>(a) Compute the upper context u including information about all x &lt;i, * using the upper layers (b) For each column j ? [1, W ]:</p><p>i. Sample x i,j conditioned on u and prior elements of row i (x i,&lt;j ).</p><p>Because the L row row-wise layers are independent over rows (they depend on other rows only through the upper context, as explained in section 3.2.1), sampling one row can be accomplished by evaluating the row-wise layers for that one row only, completely ignoring other rows. Thus, in one row of ? N pixels, each pixel can be sampled in O(N L row ), so all pixels can be sampled in O(N 2 L row ). Before each of the ? N rows can be sampled, the upper context must be computed in O(N ? N L upper ), for a total of O(N 2 L upper ) over the course of all rows. Thus we arrive at O(N 2 (L upper +L row )) in total, which is ? N faster than the naive implementation. To our knowledge, sampling speedups of this type are not possible with contemporary work on scaling Transformers to images and videos <ref type="bibr" target="#b2">(Child et al., 2019;</ref><ref type="bibr" target="#b17">Weissenborn et al., 2019)</ref>. Multiscale PixelCNN <ref type="bibr" target="#b10">(Reed et al., 2017)</ref> 3.95 3.70 PixelCNN/RNN <ref type="bibr" target="#b14">(van den Oord et al., 2016a)</ref> 3.86 3.63 Gated PixelCNN <ref type="bibr" target="#b15">(van den Oord et al., 2016b)</ref> 3.83 3.57 PixelSNAIL <ref type="bibr" target="#b1">(Chen et al., 2018)</ref> 3.80 3.52 SPN <ref type="bibr" target="#b7">(Menick &amp; Kalchbrenner, 2018)</ref> 3.79 3.52 Image Transformer <ref type="bibr" target="#b9">(Parmar et al., 2018)</ref> 3.77 -Strided Sparse Transformer <ref type="bibr" target="#b2">(Child et al., 2019</ref>   <ref type="bibr" target="#b3">(Ebert et al., 2017)</ref>. We condition on a single video frame and model the next 15 frames, similar to <ref type="bibr" target="#b17">Weissenborn et al. (2019)</ref>. <ref type="bibr" target="#b6">Kumar et al. (2019)</ref> instead condition on the 3 prior frames of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model bits/dim next 15 frames</head><p>VideoFlow <ref type="bibr" target="#b6">(Kumar et al., 2019)</ref> 1.87 Video Transformer <ref type="bibr" target="#b17">(Weissenborn et al., 2019)</ref> 1.35 Axial Transformer (ours) 1.29</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">CHANNEL ENCODER FOR MULTI-CHANNEL IMAGES AND VIDEOS</head><p>We have just described an architecture for a single-channel image of shape H ? W . Here, we show how to extend the architecture to multi-channel images or videos of shape H ? W ? C (here C is either the number of channels in a multi-channel image, or the product of the number of channels and timesteps in a video). One way to model such data of shape H ? W ? C is to simply stack the channels on top of each other into a single-channel image of shape (H ?C)?W or H ?(W ?C). This is simple to implement, but does increase the sequence length for column attention or row attention, which can be undesirable for large C. We instead opt to model one channel at a time as a singlechannel image, but now conditioned on previous channels using an extra set of unmasked row and unmasked column attention layers. This means that we have a model of the form p(x :,:,c | x :,:,&lt;c ), where previous channels x :,:,&lt;c are processed into a H ? W ? D tensor of context information, which is then added into the first encoding blocks of the model in section 3.2.1 <ref type="figure">(Figure 3</ref>).</p><p>We do not share any parameters among any of these layers. At training time, we train on a random channel slice of each image: we process the previous slices using these unmasked attention layers to produce a context tensor, and maximize the likelihood of the randomly chosen slice conditioned on this context. This amounts to training on an unbiased estimate of log likelihood for the whole data tensor. See <ref type="figure">fig. 1</ref> for an illustration of this complete model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We benchmarked our models on standard datasets for generative image and video models:  <ref type="table" target="#tab_2">(table 2)</ref>, thereby also showing the effectiveness of the remaining parts of the Axial Transformer that capture the context of the rows above. We also find however that the full four layers of the inner decoder of the Axial Transformer provide an additional boost in performance as well as significantly faster training. The Axial Transformer + LSTM inner decoder has the advantage of requiring only a couple of matrixvector products to compute the layers at each autogressive step, comparing favourably with about the 12 matrix-vector products required by the Axial Transformer, but the slower training time would make the LSTM inner decoder quickly impractical for larger tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SAMPLES</head><p>In <ref type="figure">fig. 4 and fig. 5</ref>, we show samples from our 64 ? 64 and 32 ? 32 ImageNet models. The samples are globally coherent and show visibly recognizable scenes, meaning that our Axial Transformer architecture successfully captures long-range dependencies across thousands of data dimensions in these image datasets. The samples also don't show any architecture-correlated artefacts. In addition, in <ref type="figure">fig. 6</ref> we show samples from the BAIR Robotic Pushing dataset. The first frame is each row is given by the dataset and the rest are continuation. We note the high quality exactness of details and the very large diversity (at temperature 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed the Axial Transformer, an self-attention-based autoregressive model for data organized as high dimensional tensors. It is based on axial attention, a simple generalization of self-attention that scales better with the dimension of input data, achieving a O(N (d?1)/d ) savings in computation and memory for a d-dimensional input tensor with N elements. Axial attention is easy to implement and does not require custom kernels to run efficiently on modern accelerators. Axial Transformers use axial self-attention layers and a shift operation to naturally and efficiently build full receptive fields of multidimensional tensors. Our model matches or outperforms the state-of-theart on ImageNet-32 and ImageNet-64 image benchmarks and sets a significant new state-of-the-art on the BAIR Robot Pushing video benchmark. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Types of axial attention layers that are the building blocks of the Axial Transformer. The blue locations correspond to the receptive field of the output red location. efficient operations that are widely available in deep learning frameworks (primarily dense-dense MatMuls). An open source implementation of our models is available at anonymized URL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and W V are D ? D parameter matrices responsible for projecting the entries of the sequence X into keys, queries, and values, respectively. Each entry of the output sequence Y is a linear combination of values in V weighted by the attention matrix A, which itself is computed from similarities between all pairs of query and key vectors. Both the expressive power and the resource cost of self-attention come from computing A and Y : it takes O(N 2 ) time and space to compute the pairwise similarities between Q and K and to compute the linear combination of V vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>64 ? 64 ImageNet samples at temperature 1.0 32 ? 32 ImageNet samples at temperature 0.99 Figure 6: 15 ? 64 ? 64 BAIR Robot Pushing samples at temperature 1.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Trade-offs of recently proposed multidimensional Transformer architectures.</figDesc><table><row><cell>Model</cell><cell>Full receptive field</cell><cell>Attention faster than O(N 2 )</cell><cell>Needs no custom kernels</cell><cell>Semi-parallel context aggregation</cell></row><row><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Unconditional and class-conditional image modeling results (bits/dim)</figDesc><table><row><cell>Model</cell><cell>ImageNet 32x32 ImageNet 64x64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Video modeling results (bits/dim) on the BAIR Robotic Pushing dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>downsampled ImageNet (van den Oord et al., 2016a)  and BAIR Robot Pushing<ref type="bibr" target="#b3">(Ebert et al., 2017)</ref>. All Axial Transformers have 8 total layers in the encoder, 8 layers in the outer decoder and 4 layers in the inner decoder. We use a hidden size of 2048 neurons throughout and for all setups and 16 heads with 128 neurons each for the attention component. We train for approximately 200k steps on ImageNet32 and ImageNet64 and for 200k steps on BAIR Robot Pushing. Our models can overfit on ImageNet32, but on the other datasets the models keep on gradually improving with more steps. Seetable 2 and table 3for our results.4.1 ABLATION STUDYTo push the limits of the semi-parallel sampling by making the inner decoder as small as possible, we train an Axial Transformer with the inner decoder replaced by a single LSTM layer of 2048 units. This slows down training time by about 20% on ImageNet32 and about 80% on ImageNet64 when maintaining the number of steps and all else fixed. We find that the Axial Transformer + LSTM inner decoder performs rather well on the ImageNet32 and ImageNet64 benchmarks</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PixelSNAIL: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="863" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="344" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<title level="m">Videoflow: A flow-based generative model for video</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2912" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1927" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural autoregressive distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7184" to="7220" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional image generation with PixelCNN decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02634</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
