<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Behind the Curtain: Learning Occluded Shapes for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
							<email>qiangenx@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Zhong</surname></persName>
							<email>yiqizhon@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Behind the Curtain: Learning Occluded Shapes for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: In a LiDAR scan (a) and (b), locating an object is difficult when its shape is largely missing. We discover three causes of shape miss: external-occlusion (red regions in (c)), signal miss (blue regions in (c)), and self-occlusion (green regions in (d)). BtcDet learns the occupancy probability of complete object shapes (e) and achieves the state-of-the-art detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Advances in LiDAR sensors provide rich 3D data that supports 3D scene understanding. However, due to occlusion and signal miss, LiDAR point clouds are in practice 2.5D as they cover only partial underlying shapes, which poses a fundamental challenge to 3D perception. To tackle the challenge, we present a novel LiDAR-based 3D object detection model, dubbed Behind the Curtain Detector (BtcDet), which learns the object shape priors and estimates the complete object shapes that are partially occluded (curtained) in point clouds. BtcDet first identifies the regions that are affected by occlusion and signal miss. In these regions, our model predicts the probability of occupancy that indicates if a region contains object shapes. Integrated with this probability map, BtcDet can generate high-quality 3D proposals. Finally, the probability of occupancy is also integrated into a proposal refinement module to generate the final bounding boxes. Extensive experiments on the KITTI Dataset <ref type="bibr" target="#b7">(Geiger et al. 2013)</ref> and the Waymo Open Dataset (Sun et al. 2019) demonstrate the effectiveness of BtcDet. Particularly, for the 3D detection of both cars and cyclists on the KITTI benchmark, BtcDet surpasses all of the published state-of-the-art methods by remarkable margins. Code is released 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With high-fidelity, the point clouds acquired by LiDAR sensors significantly improved autonomous agents's ability to understand 3D scenes. LiDAR-based models achieved stateof-the-art performance on 3D object classification , visual odometry <ref type="bibr" target="#b26">(Pan et al. 2021)</ref>, and 3D object detection . Despite being widely used in these 3D applications, LiDAR frames are technically 2.5D. After hitting the first object, a laser beam will return and leave the shapes behind the occluder missing from the point cloud.</p><p>To locate a severely occluded object (e.g., the car in <ref type="figure">Figure 1(b)</ref>), a detector has to recognize the underlying object shapes even when most of its parts are missing. Since shape miss inevitably affects object perception, it is important to answer two questions:</p><p>? What are the causes of shape miss in point clouds?</p><p>? What is the impact of shape miss on 3D object detection?</p><p>(a) The points to recover different shape miss regions.</p><p>(b) The 3D Average Precisions with shape miss recovery. <ref type="figure">Figure 2</ref>: The impact of the three types of shape miss. (b) shows PV-RCNN's ) car 3D detection APs with different occlusion levels on the KITTI <ref type="bibr" target="#b7">(Geiger et al. 2013</ref>) val split. NR means no shape miss recovery. EO, SM, and SO indicate adding car points in the regions of external-occlusion, signal miss and self-occlusion, respectively, as visualized in (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Causes of Shape Miss</head><p>To answer the first question, we study the objects in KITTI <ref type="bibr" target="#b7">(Geiger et al. 2013</ref>) and discover three causes of shape miss. <ref type="figure">Figure 1(c)</ref>, occluders block the laser beams from reaching the red frustums behind them. In this situation, the external-occlusion is formed, which causes the shape miss located at the red voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>External-occlusion. As visualized in</head><p>Signal miss. As <ref type="figure">Figure 1</ref>(c) illustrates, certain materials and reflection angles prevent laser beams from returning to the sensor after hitting some regions of the car (blue voxels). After projected to range view, the affected blue frustums in <ref type="figure">Figure 1</ref>(c) appear as the empty pixels in <ref type="figure">Figure 1(a)</ref>.</p><p>Self-occlusion. LiDAR data is 2.5D by nature. As shown in <ref type="figure">Figure 1(d)</ref>, for a same object, its parts on the far side (the green voxels) are occluded by the parts on the near side. The shape miss resulting from self-occlusion inevitably happens to every object in LiDAR scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Impact of Shape Miss</head><p>To analyze the impact of shape miss on 3D object detection, we evaluate the car detection results of the scenarios where we recover certain types of shape miss on each object by borrowing points from similar objects (see the details of finding similar objects and filling points in Sec. 3.1).</p><p>In each scenario, after resolving certain shape miss in both the train and val split of KITTI <ref type="bibr" target="#b7">(Geiger et al. 2013)</ref>, we train and evaluate a popular detector PV-RCNN . The four scenarios are: ? NR: Using the original data without shape miss recovery.</p><p>? EO: Recovering the shape miss caused by externalocclusion (adding the red points in <ref type="figure">Figure 2(a)</ref>).</p><p>? EO+SM: Recovering the shape miss caused by externalocclusion and signal miss (adding the red and blue points in <ref type="figure">Figure 2(a)</ref>).</p><p>? EO+SM+SO: Recovering all the shape miss (adding the red, blue and green points in <ref type="figure">Figure 2(a)</ref>).</p><p>We report detection results on cars with three occlusion levels (level labels are provided by the dataset). As shown in <ref type="figure">Figure 2</ref>(b), without recovery (NR), it is more difficult to detect objects with higher occlusion levels. Recovering shapes miss will reduce the performance gaps between objects with different levels of occlusion . If all shape miss are resolved (EO+SM+SO), the performance gaps are eliminated and almost all objects can be effectively detected (APs &gt; 99%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">The Proposed Method</head><p>The above experiment manually resolves the shape miss by filling points into the labeled bounding boxes and significantly improve the detection results. However, during test time, how do we resolve shape miss without knowing bounding box labels?</p><p>In this paper, we propose Behind the Curtain Detector (BtcDet). To the best of our knowledge, BtcDet is the first 3D object detector that targets the object shapes affected by occlusion. With the knowledge of shape priors, BtcDet estimates the occupancy of complete object shapes in the regions affected by occlusion and signal miss. After being integrated into the detection pipeline, the occupancy estimation benefits both region proposal generation and proposal refinement. Eventually, BtcDet surpasses all of the state-ofthe-art methods published to date by remarkable margins. <ref type="figure">Figure 3</ref>: The detection pipeline. BtcDet first identifies the regions of occlusion and signal miss R OC ? R SM? n these regions, BtcDet estimates the shape occupancy probability P(O S ) (the orangex voxels have P(O S ) &gt; 0.3). When the backbone network ? extracts detection features from the point cloud, P(O S ) is concatenated with ?'s intermediate feature maps. Then, a RPN network takes the output and generates 3D proposals. For each proposal (e.g., the green box), BtcDet pools the local geometric features f geo to the nearby grids and finally generate the final bounding box prediction (the red box) and the confidence score.</p><p>Learning shapes for 3D object detection. Bounding box prediction requires models to understand object shapes. Some detectors learn the shape related statistics as an auxiliary task. PartA 2  learns object part locations. SA-SSD and AssociateDet <ref type="bibr" target="#b26">(He et al. 2020;</ref><ref type="bibr" target="#b4">Du et al. 2020)</ref> use auxiliary networks to preserve structural information. Studies <ref type="bibr" target="#b44">Yan et al. 2020;</ref><ref type="bibr" target="#b25">Najibi et al. 2020;</ref><ref type="bibr" target="#b43">Xu et al. 2021</ref>) such as SPG conduct point cloud completion to improve object detection. These models are shapeaware but overlook the impact of occlusion on object shapes.</p><p>Occlusion handling in computer vision. The negative impact of occlusion on various computer vision tasks, including tracking , image-based pedestrian detection <ref type="bibr" target="#b51">(Zhang et al. 2018)</ref>, image-based car detection <ref type="bibr" target="#b31">(Reddy et al. 2019</ref>) and semantic part detection <ref type="bibr" target="#b32">(Saleh et al. 2021)</ref>, is acknowledged. Efforts addressing occlusion include the amodal instance segmentation <ref type="bibr" target="#b5">(Follmann et al. 2019)</ref>, the Multi-Level Coding that predicts the presence of occlusion <ref type="bibr" target="#b30">(Qi et al. 2019b</ref>). These studies, although focus on 2D images, demonstrate the benefits of modeling occlusion to solving visual tasks. Point cloud visibility is addressed in  and is used in multi-frame detection and data augmentation. This method, however, does not learn and explore the visibility's influence on object shapes. Our proposed BtcDet is the first 3D object detector that learns occluded shapes in point cloud data. We compare )'s approach with ours in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Behind the Curtain Detector</head><p>Let ? denote the parameters of a detector, {p 1 , p 2 , ..., p N } denote the LiDAR point cloud, X , D, S ob , S oc denote the estimated box center, the box dimension, the observed objects shapes and the occluded object shapes, respectively. Most LiDAR-based 3D object detectors <ref type="bibr" target="#b49">(Yi et al. 2020;</ref><ref type="bibr" target="#b0">Chen et al. 2020;</ref><ref type="bibr" target="#b37">Shi and Rajkumar 2020)</ref> only supervise the bounding box prediction. These models have</p><formula xml:id="formula_0">? M LE = argmax ? P (X , D | {p 1 , p 2 , ..., p N }, ?), (1)</formula><p>while structure-aware models <ref type="bibr" target="#b26">He et al. 2020;</ref><ref type="bibr" target="#b4">Du et al. 2020)</ref> also supervise S ob 's statistics so that</p><formula xml:id="formula_1">? M LE = argmax ? P (X , D, S ob | {p 1 , p 2 , ..., p N }, ?). (2)</formula><p>None of the above studies explicitly model the complete object shapes S = S ob ? S oc , while the experiments in Sec. 1.2 show the improvements if S is obtained. BtcDet estimates S by predicting the shape occupancy O S for regions of interest. After that, BtcDet conducts object detection conditioned on the estimated probability of occupancy P(O S ). The optimization objectives can be described as follows:</p><formula xml:id="formula_2">argmax ? P (O S | {p 1 , p 2 , ..., p N }, R SM , R OC , ?), (3) argmax ? P (X , D | {p 1 , p 2 , ..., p N }, P(O S ), ?).<label>(4)</label></formula><p>Model overview. As illustrated in <ref type="figure">Figure 3</ref>, BtcDet first identifies the regions of occlusion R OC and signal miss R SM , and then, let a shape occupancy network ? estimate the probability of object shape occupancy P(O S ). The training process is described in Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Shapes in Occlusion</head><p>Approximate the complete object shapes for ground truth labels. Occlusion and signal miss preclude the knowledge of the complete object shapes S. However, we can assemble the approximated complete shapes S, based on two assumptions: ? Most foreground objects resemble a limited number of shape prototypes, e.g., pedestrians share a few body types. ? Foreground objects, especially vehicles and cyclists, are roughly symmetric. We use the labeled bounding boxes to query points belonging to the objects. For cars and cyclists, we mirror the object points against the middle section plane of the bounding box.</p><p>A heuristic H(A, B) is created to evaluate if a source object B covers most parts of a target object A and provides points that can fill A's shape miss. To approximate A's complete shape, we select the top 3 source objects B 1 , B 2 , B 3 with the best scores. The final approximation S consists of A's original points and the points of B 1 , B 2 , B 3 that fill A's shape miss. The target objects are the occluded object in the current training frame, while the source objects are other objects of the same class in the detection training set. Both can be extracted by the ground truth bounding boxes. Please find details of H(A, B) in Appendix B and more visualization of assembling S in Appendix G.</p><p>Identify R OC ? R SM in the spherical coordinate system. According to our analysis in Sec. 1.1, "shape miss" only exists in the occluded regions R OC and the regions with signal miss R SM (see <ref type="figure">Figure 1</ref>(c) and (d)). Therefore, we need to identify R OC ? R SM before learning to estimate shapes.</p><p>In real-world scenarios, there exists at most one point in the tetrahedron frustum of a range image pixel. When the laser is stopped at a point, the entire frustum behind the point is occluded. We propose to voxelize the point cloud using an evenly spaced spherical grid so that the occluded regions can be accurately formed by the spherical voxels behind any LiDAR point. As shown in <ref type="figure" target="#fig_0">Figure 4</ref>(a), each point (x, y, z) is transformed to the spherical coordinate system as (r, ?, ?):</p><formula xml:id="formula_3">r = (x 2 + y 2 + z 2 ), ? = arctan2(y, x),<label>(5)</label></formula><formula xml:id="formula_4">? = arctan2(z, x 2 + y 2 ).</formula><p>R OC includes nonempty spherical voxels and the empty voxels behind these voxels. In <ref type="figure">Figure 1</ref>(a), the dashed lines mark the potential areas of signal miss. In range view, we can find pixels on the borders between the areas having Li-DAR signals and the areas of no signal. R SM is formed by the spherical voxels that project to these pixels.</p><p>Create training targets. In R OC ? R SM , we predict the probability P(O S ) for voxels if they contain points of S. As illustrated in 4(b), S are placed at the locations of the corresponding objects. We set O S = 1 for the spherical voxels that contain S, and O S = 0 for the others. O S is used as the ground truth label to approximate the occupancy O S of the complete object shape. Estimating occupancy has two advantages over generating points:</p><p>? S is assembled by multiple objects. The shape details approximated by the borrowed points are inaccurate and the point density of different objects is inconsistent. The occupancy O S avoids these issues after rasterization. ? The plausibility issue of point generation can be avoided.</p><p>Estimate the shape occupancy. In R OC ?R SM , we encode each nonempty spherical voxel with the average properties of the points inside (x,y,z,feats), then, send them to a shape occupancy network ?. The network consists of two downsampling sparse-conv layers and two up-sampling inverseconvolution layers. Each layer also includes several submanifold sparse-convs (Graham and van der Maaten 2017) (see Appendix D). The spherical sparse 3D convolutions are similar to the ones in the Cartesian coordinate, except that the voxels are indexed along (r, ?, ?). The output P(O S ) is supervised by the sigmoid cross-entropy Focal Loss <ref type="bibr" target="#b22">(Lin et al. 2017</ref>):</p><formula xml:id="formula_5">L f ocal (p v ) = ?(1 ? p v ) ? log(p v ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">p v = P(O S ) if O S = 1 at voxel v 1 ? P(O S ) otherwise, L shape = v?R OC ?R SM w v ? L f ocal (p v ) |R OC ? R SM | ,<label>(7)</label></formula><p>where w v = ? if v ? regions of shape miss 1 otherwise.</p><p>Since S borrows points from other objects in the shape miss regions, we assign them a weighting factor ?, where ? &lt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shape Occupancy Probability Integration</head><p>Trained with the customized supervision, ? learns the shape priors of partially observed objects and generates P(O S ).</p><p>To benefit detection, P(O S ) is transformed from the spherical coordinate to the Cartesian coordinate and fused with ?, a sparse 3D convolutional network that extracts detection features in the Cartesian coordinate.. For example, a spherical voxel has a center (r, ?, ?) which is transformed as x = rcos?cos?, y = rcos?sin?, z = rsin?. Assume x, y, z is inside a Cartesian voxel v i,j,k . Since several spherical voxels can be mapped to v i,j,k , v i,j,k takes the max value of these voxels SV (v i,j,k ):</p><formula xml:id="formula_7">P(O S ) v ijk = max({P(O S ) sv : sv ? SV (v i,j,k )}). (8)</formula><p>The occupancy probability of these Cartesian voxels forms a sparse tensor map P(O S ) ? = {P(O S ) v }, which is, then, down-sampled by max-poolings into multiple scales and concatenated with ?'s intermediate feature maps:  where f in ?i , f out ?i?1 and maxpool i?1 ?2 (?) denote the input features of ?'s ith layer, the output features of ?'s i?1th layer, and applying stride-2 maxpooling i ? 1 times, respectively.</p><formula xml:id="formula_8">f in ?i = f out ?i?1 , maxpool i?1 ?2 (P(O S ) ? ) ,<label>(9)</label></formula><p>The Region Proposal Network (RPN) takes the output features of ? and generates 3D proposals. Each proposal includes (x p , y p , z p ), (l p , w p , h p ), ? p , p p , namely, center location, proposal box size, heading and proposal confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Occlusion-Aware Proposal Refinement</head><p>Local geometry features. BtcDet's refinement module further exploits the benefit of the shape occupancy. To obtain accurate final bounding boxes, BtcDet needs to look at the local geometries around the proposals. Therefore, we construct a local feature map f geo by fusing multiple levels of ?'s features. In addition, we also fuse P(O S ) ? into f geo to bring awareness to the shape miss in the local regions. P(O S ) ? provides two benefits for proposal refinement: ? P(O S ) ? only has values in R OC ? R SM so that it can help the box regression avoid the regions outside R OC ? R SM , e.g., the regions with cross marks in <ref type="figure">Figure 3</ref>. ? The estimated occupancy indicates the existence of unobserved object shapes, especially for empty regions with high P(O S ) , e.g., some orange regions in <ref type="figure">Figure 3</ref>. f geo is a sparse 3D tensor map with spatial resolution of 400 ? 352 ? 5. The process for producing f geo is described in Appendix D.</p><p>RoI pooling. On each proposal, we construct local grids which have the same heading of the proposal. To expand the receptive field, we set a size factor ? so that:</p><formula xml:id="formula_9">w grid = ? ? w p , l grid = ? ? l p , h grid = ? ? h p . (10)</formula><p>The grid has a dimension of 12 ? 4 ? 2. We pool the nearby features f geo onto the nearby grids through trilinearinterpolation (see <ref type="figure">Figure 3</ref>) and aggregates them by sparse 3D convolutions. After that, the refinement module predicts an IoU-related class confidence score and the residues between the 3D proposal boxes and the ground truth bounding boxes, following <ref type="bibr" target="#b45">(Yan et al. 2018;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Total Loss</head><p>The RPN loss L rpn and the proposal refinement loss L pr follow the most popular design among detectors <ref type="bibr" target="#b45">Yan et al. 2018)</ref>. The total loss is:</p><formula xml:id="formula_10">L total = 0.3L shape + L rpn + L pr .<label>(11)</label></formula><p>More details of the losses and the network architectures can be found in Appendix C and D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe the implementation details of BtcDet and compare BtcDet with state-of-the-art detectors on two datasets: the KITTI Dataset <ref type="bibr" target="#b7">(Geiger et al. 2013</ref>) and the Waymo Open Dataset . We also conduct ablation studies to demonstrate the effectiveness of the shape occupancy and the feature integration strategies. More detection results can be found in the Appendix F. The quantitative and qualitative evaluations of the occupancy estimation can be found in the Appendix E and H.</p><p>Datasets. The KITTI Dataset includes 7481 LiDAR frames for training and 7518 LiDAR frames for testing. We follow <ref type="bibr" target="#b1">(Chen et al. 2017)</ref> to divide the training data into a train split of 3712 frames and a val split of 3769 frames. In all of our experiments, we train our models with a batch size of 8 on 4 GTX 1080 Ti GPUs. On the KITTI Dataset, we train BtcDet for 40 epochs, while on the WOD, we train BtcDet for 30 epochs. The BtcDet is end-to-end optimized by the ADAM optimizer (Kingma and Ba 2014) from scratch. We applies the widely adopted data augmentations <ref type="bibr" target="#b3">Deng et al. 2020;</ref><ref type="bibr" target="#b18">Lang et al. 2019;</ref><ref type="bibr" target="#b46">Yang et al. 2020;</ref><ref type="bibr" target="#b48">Ye et al. 2020)</ref>, which includes flipping, scaling, rotation and the ground-truth augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on the KITTI Dataset</head><p>We evaluate BtcDet on the KITTI val split after training it on the train split. To evaluate the model on the KITTI test set, we train BtcDet on 80% of all train+val data and hold out the remaining 20% data for validation. Following the protocol in <ref type="bibr" target="#b7">(Geiger et al. 2013)</ref>, results are evaluated by the Average Precision (AP) with an IoU threshold of 0.7 for cars and 0.5 for pedestrians and cyclists.</p><p>KITTI validation set. As summarized in <ref type="table" target="#tab_2">Table 1</ref>, we compare BtcDet with the state-of-the-art LiDAR-based 3D object detectors on cars, pedestrians and cyclists using the AP under 40 recall thresholds (R40). We reference the R40 APs of SA-SSD, PV-RCNN and Voxel R-CNN to their papers, the R40 APs of SECOND to <ref type="bibr" target="#b26">(Pang et al. 2020)</ref> and the R40 APs of PointRCNN and PointPillars to the results of the officially released code. We also report the published 3D APs under 11 recall thresholds (R11) for the moderate car objects. On all object classes and difficulty levels, BtcDet outperforms models that only supervise bounding boxes (Eq.1) as well as structure-aware models (Eq.2). Specifically, BtcDet outperforms other models by 2.05% 3D R11 AP on the moderate car objects, which makes it the first detector that reaches above 86% on this primary metric.</p><p>KITTI test set. As shown in <ref type="table" target="#tab_3">Table 2</ref>, we compare BtcDet with the front runners on the KITTI test leader board. Besides the official metrics, we also report the mAPs that average over the APs of easy, moderate, and hard objects. As of May. 4th, 2021, compared with all the models associated with publications, BtcDet surpasses them on car and cyclist detection by big margins. Those methods include the models that take inputs of both LiDAR and RGB images and the ones taking LiDAR input only. We also list more comparisons and the results in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on the Waymo Open Dataset</head><p>We also compare BtcDet with other models on the Waymo Open Dataset (WOD). We report both 3D mean Average Precision (mAP) and 3D mAP weighted by Heading (mAPH) for vehicle detection. The official metrics also include separate mAPs for objects belonging to different distance ranges. Two difficulty levels are also introduced, where the LEVEL 1 mAP calculates for objects that have more than 5 points and the LEVEL 2 mAP calculates for objects that have more than 1 point.</p><p>As shown in <ref type="table" target="#tab_4">Table 3</ref>, BtcDet outperforms these state-ofthe-art detectors on all distance ranges and all difficulty levels by big margins. BtcDet outperforms other detectors on the LEVEL 1 3D mAP by 2.99% and the LEVEL 2 3D mAP by 3.51%. In general, BtcDet brings more improvement on the LEVEL 2 objects, since objects with fewer points usually suffer more from occlusion and signal miss. These strong results on WOD, one of the largest published LiDAR datasets, manifest BtcDet's ability to generalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We conduct ablation studies to demonstrate the effectiveness of the shape occupancy and the feature integration strategies. All model variants are trained on the KITTI train split and evaluated on the val split.</p><p>Shape Features. As shown in <ref type="table" target="#tab_6">Table 4</ref>, we conduct ablation studies by controlling the shape features learned by ? and the features used in the integration. All the model variants share the same architecture and integration strategies.</p><p>Similarly to , BtcDet 2 directly fuses the binary map of. R OC ? R SM into the detection pipeline. Although the binary map provides the information of occlusion, the improvement is limited since that the regions with code 1 are mostly background regions and less informative.</p><p>BtcDet 3 learns P(O S ) ? directly. The network ? predicts probability for Cartesian voxels. One Cartesian voxel will cover multiple spherical voxels when being close to the sensor, and will cover a small portion of a spherical voxel when being located at a remote distance. Therefore, the occlusion regions are misrepresented in the Cartesian coordinate.</p><p>BtcDet 4 convert the probability to hard occupancy, which cannot inform the downstream branch if a region is less likely or more likely to contain object shapes.</p><p>These experiments demonstrate the effectiveness of our choices for shape features, which help the main model improve 2.86 AP over the baseline BtcDet 1 .</p><p>Integration strategies. We conduct ablation studies by choosing different layers of ? to concatenate with P(O S ) ? and whether to use P(O S ) ? to form f geo . The former mostly affects the proposal generation, while the latter affects proposal refinement.</p><p>In <ref type="table" target="#tab_7">Table 5</ref>, the experiment on BtcDet 5 shows that we can improve the final prediction AP by 0.8 if we only integrate P(O S ) ? for proposal refinement. On the other hand, the experiment on BtcDet 6 shows the integration with ? alone can Method Car 3D APR40</p><p>Ped   These experiments demonstrate both the integration with ? and the integration to form f geo can bring improvement independently. When working together, two integrations finally help BtcDet surpass all the state-of-the-art models.</p><note type="other">. 3D APR40 Cyc. 3D APR40 3D APR11 Easy</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we analyze the impact of shape miss on 3D object detection, which is attributed to occlusion and signal miss in point cloud data. To solve this problem, we propose Behind the Curtain Detector (BtcDet), the first 3D object detector that targets this fundamental challenge. A training method is designed to learn the underlying shape   priors. BtcDet can faithfully estimate the complete object shape occupancy for regions affected by occlusion and signal miss. After the integration with the probability estimation, both the proposal generation and refinement are significantly improved. In the experiments on the KITTI Dataset and the Waymo Open Dataset, BtcDet surpasses all the published state-of-the-art methods by remarkable margins. Ablation studies further manifest the effectiveness of the shape features and the integration strategies. Although our work successfully demonstrates the benefits of learning occluded shapes, there is still room to improve the model efficiency.</p><p>Designing models that expedite occlusion identification and shape learning can be a promising future direction.</p><p>The datasets we use for experiments are the KITTI Dataset <ref type="bibr" target="#b7">(Geiger et al. 2013</ref>) and the Waymo Open Dataset . Both of them are well-known and licensed for academic research. We license our code under "Apache License 2.0". The code will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Heuristic for Source Object Selection</head><p>To approximate the complete object shapes for a target object A, a <ref type="figure">heuristic H(A, B)</ref> is created to evaluate if a source object B covers most of A and can provides points in the regions of A's shape miss. The lower the score, the better a object B is for A. The heuristic is:</p><formula xml:id="formula_11">H(A, B) = x?P A min y?P B ||x ? y|| ? ?IoU (D A , D B ) (12) + ?/ {x : x ? V ox(P B ), x / ? V ox(P A )} ,</formula><p>where P A and P B are the object point sets and D A and D B are their bounding boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? The first term x?P</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Target and Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Region Proposal Network (RPN)</head><p>We follow the most popular RPN design of anchor-based 3D detection models <ref type="bibr" target="#b18">(Lang et al. 2019;</ref><ref type="bibr" target="#b45">Yan et al. 2018;</ref><ref type="bibr" target="#b3">Deng et al. 2020)</ref>.</p><p>To generate region proposals, for each class, we first set anchor size as the size of the average 3D objects, and set anchor orientations at 0 ? and 90 ? . Then, we We adopt the box encoding for RPN, which is introduced in <ref type="bibr" target="#b18">(Lang et al. 2019;</ref><ref type="bibr" target="#b45">Yan et al. 2018</ref>):</p><formula xml:id="formula_12">x t = x g ? x a d a , y t = y g ? y a d a , z t = z g ? z a h a ,</formula><p>where d a = (l a 2 + w a 2 ) ; (13)</p><formula xml:id="formula_13">w t = log( w g w a ), l t = log( l g l a ), h t = log( h g h a )</formula><p>,</p><formula xml:id="formula_14">? t = ? g ? ? a ,<label>(14)</label></formula><p>where x, y, z are the box centers, w, l, h and ? are width, length, height and yaw angle of the boxes, respectively. The subscripts t, a, g denote encoded value, anchor and ground truth, respectively. Car (KITTI) or vehicle (WOD) anchors are assigned to ground-truth objects if their IoUs are above 0.6 (f g = 1) or treated as in background if their IoUs are less than 0.45 (f g = 0). The anchors with IoUs in between are ignored in training. For pedestrians and cyclists, the foreground object matching threshold is 0.5 and the background matching threshold is 0.35.</p><p>To deal with adversarial angle problem (the orientation at 0 or ?), we follow <ref type="bibr" target="#b45">(Yan et al. 2018</ref>) and set the regression loss for orientation as:</p><formula xml:id="formula_15">L ? rpn = SmoothL 1 (sin(? p ? ? t )),<label>(15)</label></formula><p>where "p" indicates the predicted value. Since the above loss treats opposite directions indistinguishably, a direction classifier is also used and supervised by a softmax loss L dir . We use Focal Loss <ref type="bibr" target="#b22">(Lin et al. 2017)</ref> as the classification loss:</p><formula xml:id="formula_16">L cls rpn = L f ocal (p t ) = ?? t (1 ? p t ) ? log(p t ),<label>(16)</label></formula><p>where p t = ? ? ? p p if the box are assigned to a foreground object f g = 1 1 ? p p otherwise, in which p p is the predicted foreground score. The parameters of the focal loss are ? = 0.25 and ? = 2. The total loss of RPN is:</p><formula xml:id="formula_17">L rpn = 1 N a Na i L cls rpn + 1(f g ? 1) (17) ? [2(L ? rpn + L reg rpn ) + 0.2L dir rpn ] ,</formula><p>where N a is the number of sampled anchors, 1(f g ? 1) means the regression losses are only applied on the foreground anchors, L reg rpn is the SmoothL 1 regression loss on the encoded x,y,z,w,l,h as described in Eq.13 and Eq.14 and L dir rpn is the direction classification loss for predicting the angle bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proposal Refinement</head><p>Following <ref type="bibr" target="#b19">Li et al. 2019;</ref><ref type="bibr" target="#b45">Yan et al. 2018;</ref><ref type="bibr" target="#b3">Deng et al. 2020)</ref>, there are two branches in the proposal refinement module, one for class confidence score and another for box regression. We follow <ref type="bibr" target="#b19">Li et al. 2019;</ref><ref type="bibr" target="#b35">Shi et al. 2019b</ref>) and adopt the 3D IoU weighted RoI confidence training target for each RoI:</p><formula xml:id="formula_18">y g = ? ? ? 1 if IoU &gt; 0.75, 2 ? IoU ? 0.5 if 0.25 &lt; IoU ? 0.75, 0 if IoU ? 0.25,<label>(18)</label></formula><p>To conduct regression for bounding box refinement, We adopt the state-of-the-art residual-based box encoding functions:</p><formula xml:id="formula_19">x r = x g ? x p d p , y r = y g ? y p d p , z r = z g ? z p h p , with d p = (l p 2 + w p 2 ) ;<label>(19)</label></formula><formula xml:id="formula_20">w r = log( w g w p ), l r = log( l g l p ), h r = log( h g h p ), ? r = ? g ? ? p ,<label>(20)</label></formula><p>where x, y, z are the box centers, w, l, h and ? are the width, length, height and yaw angle of the boxes, respectively. The subscripts r, p, g denote residue, 3D proposal and ground truth, respectively. The total proposal refinement loss are:</p><formula xml:id="formula_21">L pr = 1 N p Np i L cls pr + (21) 1(IoU ? 0.55) ? (L ? pr + L reg pr ) ,</formula><p>where N p is the number of sampled proposal, L cls pr is the binary cross entropy loss using the training targets Eq.21, 1(IoU ? 0.55) means we only apply regression loss on positive proposals, L ? pr and L reg pr are similar to the corresponding regression losses in the RPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Total Loss</head><p>The total loss is the combination of L shape introduced in Section 3.1 of the main paper, the RPN loss L rpn and the proposal refinement loss L pr :</p><formula xml:id="formula_22">L total = 0.3L shape + L rpn + L pr ,<label>(22)</label></formula><p>where we conduct grid search and find the weighting factor of 0.3 helps BtcDet achieve the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Network Architecture</head><p>In this section, we describe the network architecture of the shape occupancy network, the detection feature backbone network, the region proposal network, and the proposal refinement network of BtcDet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Shape Occupancy Network ?</head><p>As visualized in <ref type="figure" target="#fig_3">Figure 6</ref>, we use a lightweight spherical sparse 3D convolution network with five sparse-conv layers. Two of them are down-sampling and two of them are upsampling layers, each consists of a regular sparse-conv of stride 2 following by a sub-manifold sparse-conv <ref type="bibr" target="#b9">(Graham and van der Maaten 2017)</ref>. The dimensions of these layers' output features are <ref type="bibr">16, 32, 64, 32, and 32, respectively.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Detection Backbone Network ?</head><p>The backbone of the detection feature extraction network follows <ref type="bibr" target="#b3">Deng et al. 2020)</ref> but has thinner network layers. The point cloud is voxelized into Cartesian voxels where the features of each occupied voxel are the mean of the points' xyz and features (e.g., intensity). Besides, the sparse probability tensor of object occupancy in the spherical coordinate has been transformed to the Cartesian coordinate P(O S ) ? , so that two channels from P(O S ) ? can be concatenated with layers of ?. One channel holds the occupancy probability P(O S ) and the other holds the binary code if P(O S ) exists in a voxel. As visualized in <ref type="figure" target="#fig_4">Figure 7</ref>, three down-sampling layers down-sample the features to 8? smaller, which are fed into the region proposal network. The feature maps of the second, the third, and the final layer are further integrated with P(O S ) ? to form a local geometric feature f geo , which supports the proposal refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Region Proposal Network</head><p>We stack the input features to bird-eye view 2D features. Then, a thinner version of the 2D convolution networks in <ref type="bibr" target="#b18">(Lang et al. 2019;</ref><ref type="bibr" target="#b45">Yan et al. 2018)</ref> propagates the features and output residues of 2 anchors per class per grid on the output feature maps. Instead of dimensions of 256 as in <ref type="bibr" target="#b3">Deng et al. 2020)</ref>, the intermediate feature maps in our 2D convolution networks has feature dimension of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Proposal Refinement Network</head><p>A local grid of a region proposal has a grid size of (2, 4, 12) along the locally orientated axis of Z, Y, X. As shown in <ref type="figure">Figure 8</ref>, the aggregation network of the pooled local geometric features consist of three layers with the strides of (1,1,2), (1,2,2), (2,2,3). The first two layers have zero paddings, while the last layer does not. After that, we send them to several fully connected layers. We have 3 ? 3 ? 3 this kind of local grids for each proposal box. The center of a local grid (x grid , y grid , z grid ) have a shift away from the proposal center (x p , y p , z p ) by distances (? x ? {??, 0} ? w p , ? y = {??, 0} ? l p , ? z = {??, 0} ? h p ), where w p , l p , h p is the width, length and height of the proposal box. We find ? = 0.25 achieves the best results. The proposal refinement network aggregates results from all these shifted local grids and outputs the residues regression and the class confidence score, which lead to the final bounding box predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Occupancy Estimation of Complete Object Shapes</head><p>We show the evaluation of the occupancy estimation in <ref type="table" target="#tab_9">Table  6</ref>. The results are averaged among all voxels in the regions of</p><formula xml:id="formula_23">R OC ? R SM . A prediction is considered positive if P(O S ) &gt; threshold.</formula><p>The metrics we evaluate are precision, recall, F1 score, accuracy, and object coverage. The object coverage is the percentage of all bounding boxes that contain at least one positive voxel (P(O S ) &gt; threshold). We show the measures on three thresholds of 0.3, 0.5, and 0.7. The accuracy results under all thresholds are very high (?99%) since the classes are extremely imbalanced. However, no matter under which threshold, we can achieve relatively high object coverage, which means the estimation is faithful enough for RPN and other downstream networks to rely on.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Comparison Results on the KITTI Test Set</head><p>We show more results of comparisons between BtcDet and other state-of-the-art detectors in <ref type="table">Table 7</ref>. Because the aver- <ref type="figure">Figure 8</ref>: The architecture of the proposal refinement network in BtcDet's detection pipeline. The blue layers are the regular sparse 3D convolutions <ref type="bibr" target="#b8">(Graham 2015)</ref> and the yellow layers are fully-connected layers. The stride numbers correspond to Z, Y, X axis. In addition of the above network, We also aggregate the nearby points of the proposal.  age point number in pedestrians is smaller than other objects, the shape estimation is sensitive to a few observed points. Therefore, if the point number distribution of pedestrians in the test split is different, our model may not be able to provide an accurate shape occupancy estimation. As a result, BtcDet's pedestrian detection on KITTI's test split does not perform as well as on KITTI's val split. We consider improving the results with this situation in our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More Visualization for the Complete Object Shape Approximation</head><p>We show more results of the assembled complete object shapes of cyclists and pedestrians in this section. <ref type="figure">Figure 9</ref> visualizes the process for cyclists which includes mirroring both source and target objects. <ref type="figure">Figure 10</ref> and 11 visualizes the process for pedestrians which does not mirror the objects since pedestrians are less likely to be symmetric. The blue points are the points of the target object and the red points are the points of the source objects. The assembled object faithfully covers the originally partially observed parts of the target objects and provides reasonable recovery points in the shape miss regions of the target objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Visualization of the Occupancy Probability P(O S )</head><p>We show the qualitative results of the occupancy probability for vehicle objects on the Waymo Open Dataset . <ref type="figure">Figure 12</ref> contains zoomed in views of the occupancy probability while <ref type="figure">Figure 13</ref> contains full scene views. The higher probability one is estimated, the larger opacity we apply to the spherical voxel.  <ref type="table">Table 7</ref>: Comparison results for all three classes of objects on the KITTI test set, evaluated by the 3D Average Precision (AP) of 40 recall thresholds (R40) on the KITTI server. The mAPs are averaged over the APs of easy, moderate, and hard objects. <ref type="figure">Figure 9</ref>: The assembly process to approximate the complete object shapes for cyclists on KITTI <ref type="bibr" target="#b7">(Geiger et al. 2013)</ref>. The red points are from the source objects, the blue points are from target objects, the complete shape of the target objects are approximated by borrowing the points from the selected source objects. <ref type="figure">Figure 10</ref>: The assembly process to approximate the complete shapes for pedestrians on KITTI <ref type="bibr" target="#b7">(Geiger et al. 2013)</ref>. The red points are from the source objects, the blue points are from target objects, the complete shape of the target objects are approximated by borrowing the points from the selected source objects (red). <ref type="figure">Figure 11</ref>: The assembly process to approximate the complete shapes for pedestrians on KITTI <ref type="bibr" target="#b7">(Geiger et al. 2013)</ref>. The red points are from the source objects, the blue points are from target objects, the complete shape of the target objects are approximated by borrowing the points from the selected source objects (red). <ref type="figure">Figure 12</ref>: The zoomed in views of the predicted occupancy probability for vehicle objects on the Waymo Open Dataset . The higher probability it is predicted, the larger opacity we apply to the spherical voxel. <ref type="figure">Figure 13</ref>: The full scene views of the predicted occupancy probability for vehicle objects on the Waymo Open Dataset . The higher probability it is predicted, the larger opacity we apply to the spherical voxel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Learning Occluded Shapes. (a) The regions of occlusion or signal miss R OC ? R SM can be identified after the spherical voxelization for the point cloud. (b) To label the occupancy O S (1 or 0), We place the approximated complete object shapes S (red points) in the corresponding boxes. (c) A shape occupancy network ? predicts the shape occupancy probability P(O S ) for voxels in R OC ? R SM , supervised by O S . (d) Voxels are colored orange if it has a prediction P(O S ) &gt; 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Assemble the approximated complete shape S for an object (blue) by using points from top match objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A min y?P B ||x ? y|| measures if A's points are well covered by B's points (half Chamfer Distance). ? The second term ?IoU (D A , D B ) measures the similarity of their bounding box size. ? The third term ?/ {x : x ? V ox(P B ), x / ? V ox(P A )} measures the number of extra voxels that B can add to A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The network architecture of the shape occupancy network. The blue layers are regular sparse 3D convolutions<ref type="bibr" target="#b8">(Graham 2015)</ref>, the purple layers are sub-manifold sparse 3D convolutions (Graham and van der Maaten 2017), while the green layers are inverse sparse 3D convolutions (spatial up-sampling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The architecture of the detection feature backbone network. The blue layers are the regular sparse 3D convolutions<ref type="bibr" target="#b8">(Graham 2015)</ref> and the purple layers are sub-manifold sparse 3D convolutions (Graham and van der Maaten 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>geo are composed of P(O S ) and the multi-scale features from ?. For each region proposal, we construct local grids covering the proposal box. BtcDet pools the local geometric features f geo onto the local grids, aggregates the grid features, and generates the final bounding box predictions. (See Sec. 3.3.)</figDesc><table><row><cell>3.1.</cell></row><row><cell>Next, BtcDet extracts the point cloud 3D features by a</cell></row><row><cell>backbone network ?. The features are sent to a Region Pro-</cell></row><row><cell>posal Network (RPN) to generate 3D proposals. To leverage</cell></row><row><cell>the occupancy estimation, the sparse tensor P(O S ) is con-</cell></row><row><cell>catenated with the feature maps of ?. (See Sec. 3.2.)</cell></row><row><cell>Finally, BtcDet applies the proposal refinement. The lo-</cell></row><row><cell>cal geometric features f</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The Waymo Open Dataset (WOD) consists of 798 segments of 158361 LiDAR frames for training and 202 segments of 40077 LiDAR frames for validation. The KITTI Dataset only provides LiDAR point clouds in 3D, while the WOD also provides LiDAR range images. Implementation and training details. BtcDet transforms the point locations (x, y, z) to (r, ?, ?) for the KITTI Dataset, while directly extracting (r, ?, ?) from the range images for the WOD. On the KITTI Dataset, we use a spherical voxel size of (0.32m, 0.52 ? , 0.42 ? ) within the range [2.24m, 70.72m] for r, [?40.69 ? , 40.69 ? ] for ? and [?16.60 ? , 4.00 ? ] for ?. On the WOD, we use a spherical voxel size of (0.32m, 0.81 ? , 0.31 ? ) within the range [2.94m, 74.00m] for r, [?180 ? , 180 ? ] for ? and [?33.80 ? , 6.00 ? ] for ?. Determined by grid search, we set ? = 2 in Eq.6, ? = 0.2 in Eq.7 and ? = 1.05 in Eq.10.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Lang et al. 2019)  87.75 78.39 75.18 57.30 51.41 46.87 81.57 62.94 58.98   77.28 SECOND<ref type="bibr" target="#b45">(Yan et al. 2018)</ref> 90.97 79.94 77.09 58.01 51.88 47.05 78.50 56.74 52.83 Shi et al. 2020) 92.57 84.83 82.69 64.26 56.67 51.91 88.88 71.95 66.78 83.90 Voxel R-CNN (Deng et al. 2020) 92.38 85.29 82.Comparison on the KITTI val set, evaluated by the 3D Average Precision (AP) under 40 recall thresholds (R40). The 3D APs on under 11 recall thresholds are also reported for the moderate car objects.</figDesc><table><row><cell></cell><cell></cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Car Mod.</cell></row><row><cell cols="10">PointPillars (76.48</cell></row><row><cell>SA-SSD (He et al. 2020)</cell><cell cols="3">92.23 84.30 81.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.91</cell></row><row><cell cols="4">PV-RCNN (86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.52</cell></row><row><cell>BtcDet (Ours)</cell><cell cols="9">93.15 86.28 83.86 69.39 61.19 55.86 91.45 74.70 70.08</cell><cell>86.57</cell></row><row><cell>Method</cell><cell>Reference</cell><cell cols="2">Modality</cell><cell cols="3">Car 3D AP R40 Easy Mod. Hard</cell><cell>mAP</cell><cell cols="2">Cyc. 3D AP R40 Easy Mod. Hard</cell><cell>mAP</cell></row><row><cell>EPNet (Huang et al. 2020)</cell><cell cols="7">ECCV 2020 LiDAR+RGB 89.81 79.28 74.59 81.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3D-CVF (Yoo et al. 2020)</cell><cell cols="7">ECCV 2020 LiDAR+RGB 89.20 80.05 73.11 80.79</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointPillars (Lang et al. 2019)</cell><cell>CVPR 2019</cell><cell cols="2">LiDAR</cell><cell cols="6">82.58 74.31 68.99 75.29 77.10 58.65 51.92 62.56</cell></row><row><cell>STD (Yang et al. 2019)</cell><cell>ICCV 2019</cell><cell cols="2">LiDAR</cell><cell cols="6">87.95 79.71 75.09 80.92 78.69 61.59 55.30 65.19</cell></row><row><cell>HotSpotNet (Chen et al. 2020)</cell><cell>ECCV 2020</cell><cell cols="2">LiDAR</cell><cell cols="6">87.60 78.31 73.34 79.75 82.59 65.95 59.00 69.18</cell></row><row><cell>PartA 2 (Shi et al. 2020)</cell><cell>TPAMI 2020</cell><cell cols="2">LiDAR</cell><cell cols="6">87.81 78.49 73.51 79.94 79.17 63.52 56.93 66.54</cell></row><row><cell>3DSSD (Yang et al. 2020)</cell><cell>CVPR 2020</cell><cell cols="2">LiDAR</cell><cell cols="6">88.36 79.57 74.55 80.83 82.48 64.10 56.90 67.83</cell></row><row><cell>SA-SSD (He et al. 2020)</cell><cell>CVPR 2020</cell><cell cols="2">LiDAR</cell><cell cols="4">88.75 79.79 74.16 80.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Asso-3Ddet (Du et al. 2020)</cell><cell>CVPR 2020</cell><cell cols="2">LiDAR</cell><cell cols="4">85.99 77.40 70.53 77.97</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PV-RCNN (Shi et al. 2020)</cell><cell>CVPR 2020</cell><cell cols="2">LiDAR</cell><cell cols="6">90.25 81.43 76.82 82.83 78.60 63.71 57.65 66.65</cell></row><row><cell cols="2">Voxel R-CNN (Deng et al. 2020) AAAI 2021</cell><cell cols="2">LiDAR</cell><cell cols="4">90.90 81.62 77.06 83.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CIA-SSD (Zheng et al. 2021)</cell><cell>AAAI 2021</cell><cell cols="2">LiDAR</cell><cell cols="4">89.59 80.28 72.87 80.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TANet (Liu et al. 2020)</cell><cell>AAAI 2021</cell><cell cols="2">LiDAR</cell><cell cols="6">83.81 75.38 67.66 75.62 73.84 59.86 53.46 62.39</cell></row><row><cell>BtcDet (Ours)</cell><cell>AAAI 2022</cell><cell cols="2">LiDAR</cell><cell cols="6">90.64 82.86 78.09 83.86 82.81 68.68 61.81 71.10</cell></row><row><cell>Improvement</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="6">-0.26 +1.24 +0.94 +0.67 +0.33 +2.73 +2.81 +1.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on the KITTI test set, evaluated by the 3D Average Precision (AP) of 40 recall thresholds (R40) on the KITTI server. BtcDet surpasses all the leader board front runners that are associated with publications released before our submission. The mAPs are averaged over the APs of easy, moderate, and hard objects. Please find more results in Appendix F.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LEVEL 1 3D mAP</cell><cell></cell><cell>mAPH</cell><cell></cell><cell cols="2">LEVEL 2 3D mAP</cell><cell></cell><cell>mAPH</cell></row><row><cell>Method</cell><cell cols="10">Overall 0-30m 30-50m 50m-Inf Overall Overall 0-30m 30-50m 50m-Inf Overall</cell></row><row><cell>PointPillar (Lang et al. 2019)</cell><cell>56.62</cell><cell>81.01</cell><cell>51.75</cell><cell>27.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MVF (Zhou et al. 2020b)</cell><cell>62.93</cell><cell>86.30</cell><cell>60.02</cell><cell>36.02</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SECOND (Yan et al. 2018)</cell><cell>72.27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.69</cell><cell>63.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.33</cell></row><row><cell>Pillar-OD (Wang et al. 2020)</cell><cell>69.80</cell><cell>88.53</cell><cell>66.50</cell><cell>42.93</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AFDet (Ge et al. 2020)</cell><cell>63.69</cell><cell>87.38</cell><cell>62.19</cell><cell>29.27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PV-RCNN (Shi et al. 2020)</cell><cell>70.30</cell><cell>91.92</cell><cell>69.21</cell><cell>42.17</cell><cell>69.69</cell><cell>65.36</cell><cell>91.58</cell><cell>65.13</cell><cell>36.46</cell><cell>64.79</cell></row><row><cell cols="2">Voxel R-CNN (Deng et al. 2020) 75.59</cell><cell>92.49</cell><cell>74.09</cell><cell>53.15</cell><cell>-</cell><cell>66.59</cell><cell>91.74</cell><cell>67.89</cell><cell>40.80</cell><cell>-</cell></row><row><cell>BtcDet (ours)</cell><cell>78.58</cell><cell>96.11</cell><cell>77.64</cell><cell>54.45</cell><cell>78.06</cell><cell>70.10</cell><cell>95.99</cell><cell>70.56</cell><cell>43.87</cell><cell>69.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison for vehicle detection on the Waymo Open Dataset validation set. improve the AP by 1.2 for proposal box and final bounding box prediction AP by 2.0 over the baseline.The comparisons of BtcDet 7 , BtcDet 8 and BtcDet (main) demonstrates integrating P(O S ) ? with ?'s first two layers is the best choice. Since P(O S ) is a low level feature while the third layer of ? would contain high level features, we observe a regression when BtcDet 8 also concatenates P(O S ) ? with ?'s third layer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the learned features (Sec. 3.1) and the features fused into ? and f geo (Sec. 3.2). BtcDet 2 directly use a binary map that labels R OC ? R SM . and ? indicate the spherical and the Cartesian coordinate. The "1" operator converts float values to binary codes with a threshold of 0.5. All variants share the same architecture.</figDesc><table><row><cell>Model</cell><cell>Integrate</cell><cell>Integrate</cell><cell>Proposal bbox</cell><cell>Final bbox</cell></row><row><cell>Variant</cell><cell>Layers of ?</cell><cell>fgeo</cell><cell>3D APR11</cell><cell>3D APR11</cell></row><row><cell>BtcDet1(base)</cell><cell>?</cell><cell>?</cell><cell>77.75</cell><cell>83.71</cell></row><row><cell>BtcDet5</cell><cell>?</cell><cell></cell><cell>77.73</cell><cell>84.50</cell></row><row><cell>BtcDet6</cell><cell>1,2</cell><cell>?</cell><cell>78.97</cell><cell>85.72</cell></row><row><cell>BtcDet7</cell><cell>1</cell><cell></cell><cell>78.54</cell><cell>85.73</cell></row><row><cell>BtcDet8</cell><cell>1,2,3</cell><cell></cell><cell>78.76</cell><cell>86.11</cell></row><row><cell>BtcDet (main)</cell><cell>1,2</cell><cell></cell><cell>78.93</cell><cell>86.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies on which layers of ? are fused with P(O S ) ? (Eq. 9) and whether to fuse P(O S ) ? into f geo . We evaluate on the KITTI's moderate car objects and show the 3D AP R11 of the proposal and final bounding box.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The results of occupancy estimation. The model is trained on the KITTI's train split and then evaluated on the KITTI's val split. The precision, recall, F1 score, accuracy and object coverage are evaluated by setting P(O S ) &gt; the corresponding thresholds. Those metrics are evaluated considering all voxels in R OC ? R SM . The object coverage is the the percentage of all bounding boxes that contain at least one voxel that is predicted as positive.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Ped. 3D AP R40Car3DAP R40 Cyc. 3D AP R40 Easy Mod. Hard mAP Easy Mod. Hard mAP Easy Mod. Hard mAP F-PointNet (Qi et al. 2018) 50.53 42.15 38.08 43.59 82.19 69.79 60.59 70.86 72.27 56.12 49.01 59.13 AVOD-FPN (Ku et al. 2018) 50.46 42.27 39.04 43.92 83.07 71.76 65.73 73.52 63.76 50.55 44.93 53.08 F-ConvNet (Wang and Jia 2019) 52.16 43.38 38.8 44.78 87.36 76.39 66.69 76.81 81.98 65.07 56.54 67.86 Yan et al. 2018) 48.73 40.57 37.77 42.36 83.34 72.55 65.82 73.90 71.33 52.08 45.83 56.41 PointPillars (Lang et al. 2019) 51.45 41.92 38.89 44.09 82.58 74.31 68.99 75.29 77.10 58.65 51.92 62.56 PointRCNN (Shi et al. 2019a) 47.98 39.37 36.01 41.12 86.96 76.50 71.39 78.28 74.96 58.82 52.53 62.10 3D Iou Loss (Zhou et al. Yang et al. 2019) 53.29 42.47 38.35 44.70 87.95 79.71 75.09 80.92 78.69 61.59 55.30 65.19 SegVoxelNet (Yi et al. Chen et al. 2020) 53.10 45.37 41.47 46.65 87.60 78.31 73.34 79.75 82.59 65.95 59.00 69.18 PartA 2 (Shi et al. 2020) 53.10 43.35 40.06 45.50 87.81 78.49 73.51 79.94 79.17 63.52 56.93 66.54 Shi and Rajkumar 2020) 51.92 43.77 40.14 45.28 88.33 79.47 72.29 80.03 78.60 63.48 57.08 66.39 3DSSD (Yang et al. 2020) 50.64 43.09 39.65 44.46 88.36 79.57 74.55 80.83 82.48 64.10 56.90 67.83 SA-SSD (He et al. Shi et al. 2020) 52.17 43.29 40.29 45.25 90.25 81.43 76.82 82.83 78.60 63.71 57.65 66.65 Voxel R-CNN (Deng et al. Liu et al. 2020) 53.72 44.34 40.49 46.18 83.81 75.38 67.66 75.62 73.84 59.86 53.46 62.39 BtcDet (Ours) 47.80 41.63 39.30 42.91 90.64 82.86 78.09 83.86 82.81 68.68 61.81 71.10</figDesc><table><row><cell>Uber-MMF (Liang et al. 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.40 77.43 70.22 78.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EPNet (Huang et al. 2020)</cell><cell cols="5">52.79 44.38 41.29 46.15 89.81 79.28 74.59 81.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLOCsPVCas (Pang et al. 2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.94 80.67 77.15 82.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3D-CVF (Yoo et al. 2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.20 80.05 73.11 80.79</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SECOND (2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.16 75.64 70.70 77.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fast PointRCNN (Chen et al. 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.29 77.40 70.24 77.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STD (2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.04 76.13 70.76 77.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VoxelFPN (Kuang et al. 2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.63 76.70 69.44 77.26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HotSpotNet (SERCNN (Zhou et al. 2020a)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.74 78.96 74.14 80.28</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Point-GNN (2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.75 79.79 74.16 80.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Asso-3Ddet (Du et al. 2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.99 77.40 70.53 77.97</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PV-RCNN (2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.90 81.62 77.06 83.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CIA-SSD (Zheng et al. 2021)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.59 80.28 72.87 80.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TANet (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Data and Code License</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6526" to="6534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast point rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15712</idno>
		<title level="m">Voxel R-CNN: Towards High Performance Voxelbased 3D Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13329" to="13338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to see the invisible: End-to-end trainable amodal instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Follmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>K?nig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?rtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>B?ttger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12671</idno>
		<title level="m">Afdet: Anchor free one stage 3d object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02890</idno>
		<title level="m">Sparse 3D convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure Aware Single-stage 3D Object Detection from Point Cloud</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What You See is What You Get: Exploiting Visibility for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxel-FPN: Multi-scale voxel feature aggregation for 3D object detection from LIDAR point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">704</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15396</idno>
		<title level="m">SIENet: Spatial Information Enhancement Network for 3D Object Detection from Point Cloud</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-aware three-dimensional mean-shift with occlusion handling for robust object tracking in RGB-D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="664" to="677" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tanet: Robust 3d object detection from point clouds with triple attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11677" to="11684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dops: Learning to detect 3d objects and predict their 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11913" to="11922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03771</idno>
		<idno>arXiv:2009.00784</idno>
		<title level="m">CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Amodal instance segmentation with kins dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3014" to="3023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occlusion-net: 2d/3d occluded keypoint localization using graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7326" to="7335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Occlusion Handling in Generic Object Detection: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sz?n?si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>V?mossy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="477" to="000484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<title level="m">From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04838</idno>
		<title level="m">Scalability in Perception for Autonomous Driving: Waymo Open Dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pillar-based object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10323</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1742" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="5661" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spg: Unsupervised domain adaptation for 3d object detection via semantic point generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15446" to="15456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03762</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pointbased 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Segvoxelnet: Exploring semantic context and depth-aware features for 3d vehicle detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2274" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12636</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Occlusion-aware R-CNN: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint 3D Instance Segmentation and Object Detection for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-toend multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
