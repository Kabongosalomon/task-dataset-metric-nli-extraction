<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">B</forename><surname>Rist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Emmerichs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariu</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
						</author>
						<title level="a" type="main">Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-LiDAR</term>
					<term>semantic scene completion</term>
					<term>semantic segmentation</term>
					<term>geometry representation</term>
					<term>deep implicit functions !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic scene completion is the task of jointly estimating 3D geometry and semantics of objects and surfaces within a given extent. This is a particularly challenging task on real-world data that is sparse and occluded. We propose a scene segmentation network based on local Deep Implicit Functions as a novel learning-based method for scene completion. Unlike previous work on scene completion, our method produces a continuous scene representation that is not based on voxelization. We encode raw point clouds into a latent space locally and at multiple spatial resolutions. A global scene completion function is subsequently assembled from the localized function patches. We show that this continuous representation is suitable to encode geometric and semantic properties of extensive outdoor scenes without the need for spatial discretization (thus avoiding the trade-off between level of scene detail and the scene extent that can be covered). We train and evaluate our method on semantically annotated LiDAR scans from the Semantic KITTI dataset. Our experiments verify that our method generates a powerful representation that can be decoded into a dense 3D description of a given scene. The performance of our method surpasses the state of the art on the Semantic KITTI Scene Completion Benchmark in terms of geometric completion intersection-over-union (IoU).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A UTONOMOUS mobile robots have to base their actions almost exclusively on an internal representation of their current environment. Perception systems are built to create and update such a representation from real-time raw sensor data. We are interested in a model of the current environment that preferably condenses the information that is important for the task at hand or makes it easy to extract relevant information. For robot navigation it is required to estimate whether a certain area is occupied by an object and what semantic meaning different objects and surfaces hold. Even non-mobile settings, e.g. mapping applications, benefit from an effective geometric and semantic completion of lowresolution or incomplete sensor data. To fulfill this need 3D completion aims to map and infer the true geometry of objects from sensor input. Semantic scene completion extends this task to larger arrangements of multiple objects and requires to predict the corresponding semantic classes.</p><p>Sensor data can only reflect partial observations of the real world. First, this is because of the physical properties of the sensors themselves which impose limits on their ultimate resolution, frequency, and minimal amount of noise with which they capture data. Second, it is because every sensor is restricted to its current perspective. Thus, after the point-of-view sensor data is mapped into the 3D scene, the result will always be characterized by a distance-decreasing sampling density, occlusions and blind spots (see regions marked A, B, C in <ref type="figure">Fig. 1 respectively)</ref>. Multiple sensors mounted on a single vehicle do not alleviate that issue   <ref type="figure">1</ref>. Illustration of the semantic scene completion task and the output of our method. Sensors are limited in their resolution and restricted to a single perspective of their surroundings. A LiDAR scan (herein depicted as black points) is characterized by a varying degree of sparsity caused either by distance (A), occlusions from objects (B) or sensor blind spots (C). Our method is able to complete the sparse scan geometrically and semantically and can be applied to large spatial extents as typically found in outdoor environments. The underlying representation is not tied to a fixed output resolution and describes the scene using a continuous function (right side, color indicates semantic class). Therefore the geometry does not exhibit quantization artefacts resulting from a discretization into voxels (left side). significantly. They are usually positioned rather close together, so that their view of the surroundings still exhibits almost the same degree of occlusions and shadows. Hence, the completion task in 3D Euclidean space represents a key challenge for perception in real-time cognitive robotics: Making predictions about currently unobserved areas by the use of context and experience. This ability is only necessary arXiv:2011.09141v3 [cs.CV] 12 Apr 2021 for real-time perception systems. In a static world without time constraints it would be possible to just move the sensors towards areas of interest to gain evidence of their true appearance. But unlike static worlds, mobile robots need to reason about the nature of objects given only the current observations. The semantic scene completion task is based on a correlation between the semantic class of an object or surface and its physical 3D geometry. In the case of LiDAR, the sensor observes a part of the scene's geometry. The semantics that can be deduced from this geometry can be used to then again complete the missing geometry. Regardless of the dataset in use, hidden geometry can only be completed by means of what is probable but never with absolute certainty. This probability is in turn associated with the type of objects within the scene. Naturally, human perception exhibits the same inherent limitations as computer sensors when it comes to physical limitations and the laws of 3D geometry. However, humans make up for this by fitting a powerful model to infer even large missing pieces of geometric and semantic information about their surroundings.</p><p>Our approach is a deep learning method that we train on a large number of semantically annotated LiDAR measurements. The model leverages the training data as prior knowledge to reason about the geometry and semantics of the complete 3D scene from a single LiDAR scan as input. We propose to represent the scene completion output with localized Deep Implicit Functions (DIFs). A DIF is a continuous function over 3D space which classifies individual positions. The composed scene completion function f c LDIF ? R 3 ? [0, 1] N +1 is defined over all scene positions and outputs a classification vector over N semantic classes and free space. This continuous representation avoids a tradeoff between achievable spatial output resolution and the extent of the 3D scene that can be processed. <ref type="figure">Fig. 1</ref> presents a visualization of the resulting function and a comparison to a voxelized output.</p><p>When it comes to the representation of geometry, existing works on object or scene completion focus most commonly on voxelization <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>. However, this results in satisfactory output resolutions only for volumes of limited extent. Approaches using DIFs to represent shapes <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref> only encode single objects into a fixed size latent vector. Most previous work completes 3D geometry on the assumption that the scene in question is covered evenly with sensor measurements, such as indoor scenes recorded with RGB-D cameras. In comparison, the density of a LiDAR scan decreases steadily with distance so that gaps between measurements get larger. Distance to the sensor and occlusions lead to areas where the actual ground truth geometry cannot be inferred anymore from the measurements. This label noise and the varying sparsity is a challenge for current models <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our method requires accurate 3D measurements of a scene to be trained for geometric completion. These measurements can be obtained from one or multiple LiDAR sensors, or a LiDAR sensor that is moved through the scene, provided that all measurements can be transformed into a single reference coordinate system. If semantic annotations are not available our method can still be trained for pure completion of scene geometry. This paper builds upon our earlier work on LiDARbased scene segmentation <ref type="bibr" target="#b14">[14]</ref>. For this work, we created a training procedure for semantic scene completion based on accumulated LiDAR data and conducted an extensive experimental evaluation of our design choices and parameters. In summary, our contributions are:</p><p>? We produce a representation for both geometry and semantics of 3D scenes by Deep Implicit Functions with spatial support derived from a 2D multi-resolution grid. Our combination with continuous output coordinates make dense decoding of large spatial extents feasible. ? We generate point-like training targets from timeaccumulated real-world LiDAR data and the included free space information. Dynamic objects are considered separately to ensure consistency. ? In experiments on the Semantic KITTI Scene completion benchmark, we show that the proposed approaches outperform voxel-based methods on geometric completion accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>First, this section discusses ways to represent geometry and surfaces within the context of reconstruction algorithms. Second, related work about geometric completion is categorized into completion of single object shapes and completion of indoor scenes from synthetic or RGB-D data. Finally, we take a look at the state of the art in semantic segmentation and scene completion of outdoor scenes from real-world LiDAR data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Geometry and Surface Representation</head><p>Most commonly the output representation for 3D scene completion is a voxel occupancy grid <ref type="bibr" target="#b3">[4]</ref>, voxelized (truncated) signed distance functions (SDFs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[15]</ref>, or interpolation and CRFs <ref type="bibr" target="#b16">[16]</ref> for sub-voxel accuracy. A differentiable deep marching cubes algorithm replaces the SDF as an intermediate representation and enables to train the surface representation end-to-end <ref type="bibr" target="#b17">[17]</ref> but the resulting representation is still constrained to the underlying voxel resolution. The general trade-off between output resolution and computational resources is an issue for 3D representations <ref type="bibr" target="#b0">[1]</ref>. Octree-based convolutional neural networks (CNNs) have been proposed to represent space at different resolutions and to perform gradual shape refinements <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>. Recent works represent 3D shapes and surfaces implicitly as isosurfaces of an output function which classifies single points in Euclidean 3D space <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. Depending on the output function's complexity this approach has the capacity and expressiveness to represent fine geometric details. An encoder creates a parameter vector that makes the output function dependent on the actual input data for geometric reconstruction. Both the output function and encoder are represented as deep neural networks (DNNs) and trained by backpropagation. They either use oriented surfaces <ref type="bibr" target="#b12">[12]</ref> or watertight meshes <ref type="bibr" target="#b11">[11]</ref> from ShapeNet <ref type="bibr" target="#b22">[22]</ref> as synthetic full-supervision training targets. These methods have improved the state of the art significantly for shape reconstruction and completion. However, their scope is limited to the reconstruction of single objects. These approaches do not generalize or scale well because of the nature of a single fixed-size feature vector that represents a shape globally.</p><p>Recently, DIFs are combined with grid structures or other support positions that improve their spatial capabilities to describe larger scene extents <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref> or more complex geometric details of individual objects <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> instead of only simple shapes.</p><p>To represent more complex details in 3D shapes, a set of local analytic 3D functions with limited support can be combined with deep implicit functions to predict occupancy <ref type="bibr" target="#b26">[26]</ref>. The latent representations of individual small synthetic object parts can be used to assemble a large 3D scene <ref type="bibr" target="#b23">[23]</ref>. For this purpose, synthetic objects are first auto-encoded to generate the latent space. Then, a possible representation of a scene is found by iterative inference. This setup only requires a decoder from latent grid to the 3D scene. Concurrent to our work, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref> encode 3D points into a 2D grid or 3D feature volume and perform bilinear or trilinear interpolation on this feature space. Here <ref type="bibr" target="#b25">[25]</ref> explicitly considers features from multiple resolutions and the query position in only used for interpolation, not in the decoder. <ref type="bibr" target="#b24">[24]</ref> uses the query position for interpolation and again as concatenation to the latent feature in the decoder. The feature grid is singleresolution. For geometric reconstruction of indoor RGB-D data, the full volumetric grid performs best. With a focus on representation and reconstruction of geometry, the method is trained on synthetic watertight-meshes and uniformly sampled point clouds are used as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shape Completion</head><p>Poisson Surface Reconstruction is a state-of-the-art reconstruction algorithm for an object's surface from measured oriented points <ref type="bibr" target="#b27">[27]</ref>. As with other implicit representations the resulting geometry needs to be extracted by marching cubes or an iterative octree variant of marching cubes <ref type="bibr" target="#b11">[11]</ref>. Poisson Surface Reconstruction handles noise and imperfect data well and adapts to different local sampling densities. However, it is of limited use on real-time real world data as it is unable to leverage prior knowledge to complete unseen or sparse regions unlike methods based on learned shape representations.</p><p>Many data-driven, learning-based and symmetry-based approaches have been proposed for shape completion. We refer to Stutz et al. <ref type="bibr" target="#b28">[28]</ref> for an overview and focus on shape completion on LiDAR scans. 3D models can be used to train a DNN for the shape completion problem on synthetic data and perform inference on real LiDAR scans <ref type="bibr" target="#b29">[29]</ref>. Alternatively, a shape prior from synthetic data can be used for amortized maximum likelihood inference to avoid the domain gap between synthetic and real data <ref type="bibr" target="#b28">[28]</ref>. Recently, it has been shown that synthetic data can be avoided altogether by using a multi-view consistency constraint to train shape completion only from LiDAR scans without full supervision <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Scene Completion</head><p>For a recent comprehensive survey on semantic scene completion we refer to <ref type="bibr" target="#b31">[31]</ref>. The subject of scene completion has first gotten momentum from the wide availability of RGB-D cameras leading to the advent of indoor semantic segmentation datasets such as the NYUv2 Depth Dataset <ref type="bibr" target="#b32">[32]</ref> and ScanNet <ref type="bibr" target="#b33">[33]</ref>. <ref type="bibr" target="#b1">[2]</ref> is a pioneering work to infer full scene geometry from a single depth image in an output space of voxelized SDFs. Generalization to entirely new shapes is data-driven and implemented with voxel occupancy predicted by a structured random forest. A specially created table-top scene dataset with ground truth from a Kinect RGB-D camera is used as full-supervision training target.</p><p>A volumetric occupancy grid with semantic information can be predicted from voxelized SDFs as input in an end-toend manner <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>. They apply their methods to synthetic indoor data from the SUNCG dataset. While <ref type="bibr" target="#b3">[4]</ref> is appropriate only on single RGB-D images, <ref type="bibr" target="#b0">[1]</ref> extends to larger spatial extents. Multiple measures improve geometric precision and consistency: Using SDFs as output representation per voxel, an iterative increase of voxel resolution, and the division of space into interleaving voxel groups.</p><p>Voxelized SDFs and semantic segmentation can be inferred by explicit fusion of single depth images with RGB data <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr" target="#b23">[23]</ref> validates the geometric representation power of DIFs in combination with a structured latent space approach on indoor RGB-D data of the Matterport3D dataset <ref type="bibr" target="#b34">[34]</ref>. The details in the completion of RGB-D scans from Matterport3D can be improved by progressive spatial upsampling in the decoder and a deliberate loss formulation that does not penalize unseen areas <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Segmentation and Scene Completion on LiDAR Data</head><p>Numerous prior works focus on semantic segmentation of all observed data points resulting in a pixel-wise or pointwise classification of LiDAR data. These methods do not predict any labels for invisible parts of space from the sensor's perspective. However, datasets and benchmarks on real-world road scenes have defined a standard of semantic classes that is significant while simultaneously advancing the state of the art <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. CNN-architectures on RGB-Images for segmentation and detection <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref> have inspired sensor-view based approaches in the more recent LiDAR-based segmentation task <ref type="bibr" target="#b40">[39]</ref>, <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref>. Neural network architectures adjust to the three dimensional nature of a segmentation or detection problem through voxelization of input data <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b46">[45]</ref>, combination with sensorview range images <ref type="bibr" target="#b47">[46]</ref>, and use of surface geometry <ref type="bibr" target="#b40">[39]</ref>. Computation, memory efficiency and representation of details of voxel architectures can be improved by combining a coarser voxel structure with a point-feature branch for details <ref type="bibr" target="#b46">[45]</ref> and neural architecture search <ref type="bibr" target="#b48">[47]</ref>.</p><p>The scene completion problem on real-world data has only recently been advanced by the large-scale Semantic KITTI dataset <ref type="bibr" target="#b5">[6]</ref> featuring point-wise semantic annotations on LiDAR together with a private test set and a segmentation benchmark for semantic scene completion. Methods originally applied to scene completion from depth images <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> can be adapted for LiDAR scene completion: The Semantic KITTI authors <ref type="bibr" target="#b5">[6]</ref> adapt the Two-Stream (TS3D) approach <ref type="bibr" target="#b2">[3]</ref> which is originally applied to depth images of indoor scenes of the NYUv2 dataset. TS3D combines geometric information from a depth image and a predicted semantic segmentation from an RGB image in a volumetric voxel grid. For Semantic KITTI outdoor scenes, they use a state-of-the-art DeepNet53 segmentation network trained on Cityscapes <ref type="bibr" target="#b36">[36]</ref> and SatNet <ref type="bibr" target="#b49">[48]</ref> for voxel output.</p><p>The three recent methods LMSCNet <ref type="bibr" target="#b6">[7]</ref>, JS3CNet <ref type="bibr" target="#b8">[8]</ref>, and S3CNet <ref type="bibr" target="#b9">[9]</ref> only use LiDAR data as input. The usage of U-net architectures for down-, upsampling, and spatial context is a common architectural pattern. LMSCNet <ref type="bibr" target="#b6">[7]</ref> operates on the voxelized LiDAR input and uses a 2D-CNN backbone for feature extraction. The voxelized output is inferred with a monolithic hybrid-network that predicts the completion end-to-end. LMSCNet can output a lowerresolution coarse version of a scene at an intermediate stage.</p><p>However, their experiments show that the single-output version trained only on the highest resolution performs slightly better than the multi-scale version trained with multiple-resolution losses.</p><p>S3CNet <ref type="bibr" target="#b9">[9]</ref> and JS3CNet <ref type="bibr" target="#b8">[8]</ref> both use the raw LiDAR scan as input. Both also propose to use a lower resolution scene representation internally which is subsequently upsampled into the full output voxel resolution. JS3CNet proposes a two-stage approach: First, a semantic segmentation of the input LiDAR scan is inferred. Second, a neural network fuses the voxelized semantic segmentation and point-wise feature vectors into the voxelized representation of the completed scene. S3CNet augments the input LiDAR scan with a calculation of normal surface vectors from the depth-completed range image and TSDF values. These are stored in a sparse tensor. A semantic 2D BEV map and a 3D semantic sparse tensor are predicted in parallel. These are then subsequently fused into a full 3D tensor. The final scene completion is obtained after a second semantically-based post-processing. The authors conduct ablations and attribute a large share of the final results to the post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our method takes as input a LiDAR scan and outputs the corresponding scene completion function f c LDIF ? R 3 ? [0, 1] N +1 . This function maps every 3D position p within the scene to a probability vector that we define to represent the semantic class of the position p. The dependence of the completion function f c LDIF on the input data is expressed by the superscript vector c. Positions belonging to objects in the scene are categorized into N semantic classes. The additional class free space represents positions that are not occupied by any object (instead they are occupied by air). The resulting total of N + 1 classes is able to describe every position within the scene. Hence the f c LDIF function uniformly represents the geometric and semantic segmentation of space instead of only the physical boundaries of objects. The global f c LDIF function is built from many local functions f L . Every local function has two distinctive inputs: The coordinate of interest ?p and a parameterization vector c V . In the context of DIFs, producing an output function f c L means generating a parameterization (conditioning) vector c V . When the parameterization vector c V is fixed we obtain the conditioned function f c L which is only dependent on the remaining input coordinate ?p. Our approach to the composition of the f c LDIF function is designed to encode large outdoor scenes. While related works on single object shape representation encode geometry information in a fixed size conditioning vector, we add spatial structure to the latent space through the use of a 2D feature grid. Each grid entry is a conditioning vector for a local function. The grid is chosen to be two-dimensional, uniform and represents the xy-coordinates of a flattened scene that omits the vertical dimension. We use three grids, each with its own feature resolution. An illustration is given in <ref type="figure" target="#fig_3">Fig. 2</ref>. As a consequence of the grid approach, the amount of conditioning information is tied to the spatial extent of the scene. The intuition is that each individual conditioning vector now describes only a small part of the complete scene in the vicinity of its own position. Each grid entry always encodes a volume of the same size, regardless of the overall scene extent.</p><p>We propose a convolutional encoder to generate the feature maps that make up the conditioning grid. Outdoor scenes are mainly composed from objects at different locations on the ground plane (xy). Therefore the configuration of outdoor scenes is assumed to be translation-invariant in x and y direction. Intuitively, the encoding of the front of a car or a part of a tree can be the same regardless of the absolute position of the object within the scene. For this reason we consider the implementation of the encoder as a convolutional neural network as appropriate. <ref type="figure" target="#fig_3">Fig. 2</ref> gives a schematic overview over the point cloud encoding stage, feature selection, and decoding a position into a coordinate classification.</p><p>The next section describes the details of the composition of the global completion function f c LDIF from multiple conditioning vectors and grid resolutions. A sampling-based supervised training method from real-world LiDAR data is proposed and details on the used network architecture and inference procedure follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Structure of Latent Feature Grid</head><p>Composition of f c LDIF : Centerpiece of our method is the formulation of latent conditioning vectors that are spatially arranged in a grid and generated by a convolutional encoder network on LiDAR point clouds. Each individual conditioning vector c V parameterizes a local segmentation function f c L (?p V ) to classify a position of interest p. Even though the domain of individual local functions is R 3 and therefore infinite, the classification will only be meaningful for positions that are close to the conditioning vector's position within the scene.</p><p>It is necessary to define how a conditioning vector is selected for a given query coordinate p. It is straightforward to use the single vector of the grid cell that contains the coordinate p when projected onto the ground plane. But with this approach the resulting global function would exhibit discontinuities between grid cells. Instead, we select the four grid cells with the closest center coordinates for the query coordinate p. Thus we obtain four individual classifications for p and perform bilinear interpolation according to p's position within the square of the surrounding grid cell center points. We denote the set of the four closest   conditioning cells the support region V p of the coordinate p and the corresponding coefficients for bilinear interpolation w. This yields the global classification function</p><formula xml:id="formula_0">f c LDIF (p) = V ?V p w(?p V )f L (c V , ?p V ) (1) with ?p V = p ? o V<label>(2)</label></formula><p>for a coordinate p. o V is the center position of a cell V and c V is the conditioning vector at cell V . The coefficients for bilinear weighing w(?p V ) sum to 1. Intuitively, the spatial extent of a scene can be thought of as covered by overlapping function patches f L . Each function f L has its own coordinate origin o V at the center of its grid cell V . Eq. (2) conveys the translation of scene coordinates p into the coordinate system of the conditioning vector's grid cell that shall describe p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-resolution scene representation:</head><p>An important aspect of the composition of f c LDIF is the use of three individual conditioning vectors from three different resolutions levels. The intuition behind this is that the geometric structure of a scene is composed of different levels of detail. There is the coarse positioning of the ground level and large structures as well as more fine-grained details like curbstones, small objects and poles. We reproduce this range in the network structure to facilitate learning of a smooth representation with more details and more consistency over cell boundaries. The conditioning information for a single local function f L is composed from three resolution-specific feature vecors. We opt for features c V = (c 1 , c 2 , c 3 ) from the resolution ratios 1?16, 1?4, and 1?1 that originate from a U-netstructured <ref type="bibr" target="#b50">[49]</ref> convolutional feature encoder, as illustrated in <ref type="figure" target="#fig_3">Fig. 2</ref>. The resolution ratios correspond to grid cells with 5.12 m, 1.28 m, and 0.32 m edge length respectively.</p><p>For a scene position p, we select the four closest feature vectors at the highest resolution feature map as support region. The single cell where the coordinate resides in is selected in each case of the two lower resolutions. This 2 ? 2 square of support positions at the highest resolution is used for bi-linear interpolation. Hence there are four local segmentation functions that are able to describe the single position p in the scene. All four need to be evaluated to obtain the final interpolated classification result.</p><p>Each conditioning vector c i , i ? {1, 2, 3} belongs to a grid cell V i at resolution i defining a coordinate system relative to its own position through its origin o V i . Due to the hierarchical set of vectors (c 1 , c 2 , c 3 ) at different resolutions, we also obtain a corresponding 3-tuple of relative coordinates</p><formula xml:id="formula_1">?p V = (p 1 , p 2 , p 3 ) with p i = p ? o V i as input for f L .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training on LiDAR Point Clouds</head><p>Sampling targets for supervised training: The decoder neural network and feature encoder are trained end-to-end using individual coordinates within the scene and their associated training labels. This set of coordinate-label tuples is generated from different data sources. The large number of time-accumulated LiDAR measurements is used as primary training target. Each LiDAR point has a position in the reference coordinate frame and an associated semantic label.</p><p>Together, these positions make up all training targets for the occupied classes. The top row of <ref type="figure" target="#fig_4">Fig. 3</ref> shows the single input point cloud and the accumulated training targets with semantic annotations.</p><p>Next, we need to obtain positions that are of the free space class, so not occupied by any object. The preprocessing that accumulates LiDAR points keeps track of all voxels that are observed at least once, but empty. In every such empty voxel we sample a free space position target uniformly at random. This ensures that the scene extent is evenly covered with free space information.</p><p>We use the input point cloud as a second source of free space positions. The straight line between a LiDAR measurement and the sensor's position at time of measurement is empty, meaning not occupied by any object. We exploit this reality for self-supervised training of object geometry. The goal of our scene completion function is to resemble physical boundaries. Wherever surfaces are scanned by the LiDAR sensor we would like to have a sharp transition of the completion function from the prediction of an occupied class to a free space prediction. Therefore we sample free space positions on the straight lines between LiDAR measurement and sensor position. We use an exponential decaying probability distribution to sample the free space positions close to the surfaces of objects. The approach of close surface sampling of free space targets and the combination of surface sampled and global training positions is similar to <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function:</head><p>Training the classifier involves three separate loss terms: semantic L S , geometric L G , and consistency L C loss. Semantics and geometry could also be covered by a single cross-entropy classification problem. However, the formulation with individual losses allows to include positions that are known to be occupied by an object without information about an object class, e.g. unlabeled LiDAR points. Moreover, geometric and semantic loss terms can be weighted more easily against each other. The overall loss</p><formula xml:id="formula_2">L = ? S P L S + ? G P L G + ? C P L C<label>(3)</label></formula><p>is the weighted sum of the individual losses that are each in turn summed over all training targets. We write the predicted probability vector at position p as</p><formula xml:id="formula_3">[f 1 , . . . , f N , f N +1 ] ? = f c LDIF (p)</formula><p>. The scalar f N +1 is the predicted probability of the free space class.</p><p>The semantic loss L S is a cross-entropy loss between the classification output vector [f 1 , . . . , f N ] ? and semantic ground truth. The ground truth free space probability for LiDAR targets is always zero as LiDAR measurements L are assumed to be located on objects. This loss is not evaluated for free space targets.</p><p>The geometric reconstruction loss </p><formula xml:id="formula_4">L G = H ? ? l occupied , l free ? , N i=1 f i , f N +1 ? ? ?<label>(4)</label></formula><formula xml:id="formula_5">] ? = [1, 0] ? .</formula><p>The consistency loss</p><formula xml:id="formula_6">L C = JSD (f L,0 (p), . . . , f L,m (p)) (5) = H ? ? 1 m V ?V p f L (c V , ?p V ) ? ? ? 1 m V ?V p H f c L (?p V )<label>(6)</label></formula><p>for a given coordinate p is the Jensen-Shannon divergence (JSD) between m = V p probability distributions predicted by the local segmentation functions f L on the support region V p of a consistency point p. H(P) denotes the entropy of distribution P. The JSD is symmetric and always bounded.</p><p>Multiple local functions f L make a prediction for the same position in the scene. The unweighted output of these local functions f L exhibit grid artefacts between neighboring cells. The consistency loss acts as a regularizer by penalizing divergence between the grid cells without the need to specify any particular semantic or free space target label. Thereby, this loss term is available at any position within the scene, not only at regions where training targets from LiDAR points or sampled free space targets are occurring. We provide our numerically stable formulations of the geometric and consistency loss terms in the appendix (Section B.2, Section B.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>All DNN network details and the hyperparameters are listed in the appendix in <ref type="table">Table 4</ref> and <ref type="table" target="#tab_10">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiDAR point cloud encoding:</head><p>At the base layer we use a voxel-wise point cloud feature encoder from recent literature <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b45">[44]</ref>. The encoder transforms the raw input point set into a fixed-size bird's-eye view feature representation <ref type="figure" target="#fig_3">(Fig. 2</ref>, top-left) that corresponds to the spatial extent of the scene and is a suitable input for a convolutional feature extractor. Note that the encoder input feature space is in principle unrelated to the R 3 domain of the generated completion function. This means that the point cloud encoder can make use of additional information of the sensor. We supply the reflectivity value of every LiDAR point as an extra feature. The positions of LiDAR points are encoded as separate coordinates relative to the mean position of the points within the voxel and the voxel center.</p><p>Decoder for batch-norm conditioned classification: Spatial encoding is implicitly modeled with a local output function f L that needs to be conditioned on the latent vector c V of the feature extractor. This single-position classification function is implemented as a Multi-layer perceptron (MLP) that uses conditioned batch normalization (CBN) layers to express its dependency on the latent vectors <ref type="bibr" target="#b51">[50]</ref>. Hereby, the resulting mean and variance of feature maps is generated by an affine transformation of the respective conditioning vectors. Our method divides the latent coding c V into resolution-specific</p><formula xml:id="formula_7">latent vectors c V = (c V 1 , c V 2 , c V 3 )</formula><p>and their associated relative positions ?p V = (?p 1 , ?p 2 , ?p 3 ). This information then conditions the output function from coarse to fine: Thus beginning with the lowest resolution latent vector and adding more fine-grained information in the later layers  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference and Visualization</head><p>We use latent conditioning vectors to define a function f c LDIF over R 3 to represent geometry and semantics in a single classification vector. Depending on the task at hand this implicit representation necessitates different procedures to obtain explicit results. In any case, the completion function is evaluated for an arbitrary number of query coordinates at test time. For point-wise semantic segmentation the po-sitions of the LiDAR points themselves are used as query points at test time to obtain semantic predictions for the point cloud itself. In this mode, it is previously known that none of the query positions can accurately be classified as free space. Therefore, the predicted class value is just the argmax over all non-free-space semantic classes. A regular voxel grid with per-voxel semantic information is a common dense output structure for semantic scene completion. We query the completion function for all corner points of all voxels on the voxel grid. Every corner point is shared by eight voxels. A voxel is marked as occupied when at least a single corner of the voxel is assigned any occupied class. The semantic label is averaged from all corners which are predicted as occupied. A threshold ? empty voxel ? (0, 1) declares the free space probability under which a coordinate is considered occupied. This hyper-parameter controls the position on the precision-recall curve for the occupied class and is tuned on the training set to reach the maximum IoU of the occupied class.</p><p>To visualize the f c LDIF function we generate 3D meshes to represent the isosurface of the scalar free space function as close as possible (see <ref type="figure" target="#fig_5">Fig. 4</ref>, left column). From the N +1 semantic classes of the vector-valued f c LDIF function we extract the free space probability isosurface at a threshold ? free space ? (0, 1). This isosurface {p ? R 3 (f c LDIF (p)) N +1 = ? free space } resembles the estimated boundaries of all objects in the scene and therefore gives an idea of the learned scene representation. To extract the mesh, we use multiresolution IsoSurface Extraction (MISE) <ref type="bibr" target="#b11">[11]</ref>. MISE evaluates points in an equally spaced grid from coarse to fine. By only evaluating the points of interest close to the isosurface the number of calculations is reduced considerably. Subsequently, the marching cubes algorithm is applied and the resulting mesh is refined by minimizing a loss term for each vertex using the proximity to the desired threshold value and the gradient information for faces of the mesh. This approach removes artefacts from the marching cubes algorithm and requires that gradients wrt. the position of input points are available. We query the f c LDIF function for all face-center positions of the resulting mesh and color the mesh based on these semantic predictions. <ref type="figure" target="#fig_4">Fig. 3</ref> compares the mesh visualization and voxelized output that is obtained from the completion function.</p><p>We create a ground segmentation image to inspect the completion function at positions which are hidden in the scene. First, semantic segmentation is applied to the input point cloud. The LiDAR points that are identified to belong to one of the ground classes are selected. Then, the positions of the selected ground points are used for a bi-variate spline interpolation of all ground positions. A dense regular topview grid of predicted ground positions is extracted. We query the completion function and display the predictions for the previously selected ground classes as image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first describe the details of our training dataset and how it differs from the published Semantic KITTI scene completion dataset. Next, we introduce other published methods for real-world outdoor semantic scene completion and compare the quantitative results on the Each row displays qualitative results and ground truth for a single scene on the Semantic KITTI validation set. The single LiDAR scan used as input for our method is depicted as an overlay of black points. The far-right section in each scene view demonstrates that our approach is able to operate on areas that include hardly any LiDAR measurements anymore. The method is data-based and takes advantage of experience from the training dataset to facilitate predictions based on the larger context of the scene. This is particularly visible from the completed courses of streets and sidewalks.</p><p>closed test set through the public benchmark. Finally, we perform an ablation study about the upsampling architecture, hyperparameter choices and semantic supervision signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and LiDAR Accumulation</head><p>The Semantic KITTI authors construct the semantic scene completion task from LiDAR scans of the KITTI Odometry dataset <ref type="bibr" target="#b52">[51]</ref> and their corresponding semantic annotations <ref type="bibr" target="#b5">[6]</ref>. The LiDAR sensor is a Velodyne HDL-64 that rotates with a frequency of 10 Hz. The continuously measured LiDAR points from a full revolution are bundled into a LiDAR scan. The cut between scans is the negative x-axis in sensor coordinates so that every scan begins and ends looking backwards. LiDAR points are annotated with their respective semantic class. The recordings are made up of 21 sequences in total. The data is split on a per-sequence basis: Ten sequences for training (19 130 point clouds), one sequence for validation (4071 point clouds) and eleven sequences for testing (20 351 point clouds). In the KITTI Odometry dataset the LiDAR scans are already ego-motion corrected. All points within a single 360?scan are transformed into the coordinate system located at the sensor's position in the moment the sensor was looking in the direction of the vehicle's front. In addi- tion, the Semantic KITTI authors provide a frame-by-frame point cloud registration. Sequences and registration are crucial as they allow to accumulate LiDAR measurements of a longer time span into a single fixed reference coordinate system. This process creates the annotations of the semantic scene completion task without requiring any additional manual annotations. The Semantic KITTI completion task combines the sequence of future LiDAR scans to generate the completion target of the scene at the time of the input LiDAR scan. This includes movements of dynamic objects and therefore requires to predict object motion to solve the task in full. Section 4.2 details how we deviate from this handling of dynamic objects and explains the static scene accumulation targets that we propose instead.</p><p>The Semantic KITTI scene completion task uses a voxelized scene as output representation. A voxelized input LiDAR scan is also provided next to the raw LiDAR scan from the KITTI Odometry dataset. However, we do not use the provided voxelized scene to train our method as it is designed to classify individual positions. Instead of creating a labeled voxel grid from accumulated LiDAR measurements we use all of the individual points as training targets. The accumulated point clouds are sub-sampled to include only a maximum of 10 points within each original Semantic KITTI voxel. This reduces the overall dataset size and eliminates a large part of the redundancy in regions that are scanned by the sensor in multiple frames. The second column of <ref type="figure" target="#fig_5">Fig. 4</ref> shows examples of the accumulation result. The input LiDAR point cloud is shown as on overlay over the prediction in the first column (left). We use the same extent for accumulation as the Semantic KITTI scene completion dataset: A square with 51.2 m edge length where the egovehicle is located in the middle of an edge facing the center of the square.</p><p>The difficulty of the scene completion task gets apparent when looking at the pronounced sparsity of the input point cloud in a distance of around 50 m from the sensor. In sparse regions most geometric details have to be inferred from scene context. It is apparent that there are geometric and semantic ambiguities within the 3D scenes which cannot be decided with high confidence from the single input LiDAR scan. <ref type="figure" target="#fig_6">Fig. 5</ref> shows a projection of the LiDAR point cloud into the camera view of the ego-vehicle. The Velodyne HDL-64 sensor features a vertical field of view that at the top only covers a few degrees over the level horizon. Thus, in the vicinity of the ego-vehicle the LiDAR only covers a height of about 2 m over ground. The scene completion target does however include geometry further up because it includes LiDAR points that were recorded from a greater distance of the ego-vehicle. This is another prominent ambiguity of the training data that requires a method to guess e.g. if there is a traffic sign attached to a pole without actual evidence from the sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Handling of Dynamic Objects</head><p>We use static training and evaluation data for the semantic scene completion task. We regard this variant as more suitable for a meaningful evaluation of performance compared to the handling of dynamic objects in the original scene completion annotations. The KITTI Odometry scenes contain dynamic objects such as moving cars and pedestrians. These objects are additionally annotated with a dynamic flag. The original Semantic KITTI scene completion data accumulates the occupied voxels from dynamic objects in the reference frame just as the voxels of any other static object. Effectively this creates spatio-temporal tubes of moving traffic participants along their respective path. Therefore, fully solving the Semantic KITTI scene completion task requires predicting the future trajectories of traffic participants. As we focus on geometric reconstruction of the scene in the instant of the input LiDAR scan we take a different approach for dynamic objects: When accumulating LiDAR measurements, we only keep the single current scan on dynamic objects. By omitting the following scans over dynamic objects no trajectory tube is created. Next, it is necessary to ensure that no free space points get sampled within the extent of a dynamic object. As the object potentially moves from its initial position, the following LiDAR scans will record the initial position as free space. So to prevent free space targets within the actual object we record the shadow cast by the object in the first frame and treat the occluded regions as unseen regions where no free space points are sampled (see black regions in <ref type="figure" target="#fig_7">Fig. 6a</ref>). These two measures make the replicated geometry consistent in the presence of dynamic objects. The resulting set of training targets reflects the true scene at the moment of the input LiDAR scan. Areas where we cannot obtain consistent targets from future frames are ignored in the training.</p><p>In <ref type="figure" target="#fig_7">Fig. 6</ref> we compare the two approaches for dynamic objects and show an example. We quantitatively measure the difference in performance when using the different dataset targets for evaluation. Note, that in this comparison, our method is trained on our static version of the data in both cases. This allows us to better judge the performance reported by the benchmark on the private test set. We see that there is almost no quantitative difference for the geometric completion evaluation (Occupied IoU) because static objects are prevalent over dynamic objects. However, for semantic scene completion we expect a significant difference. Object classes with a large proportion of dynamic voxels perform much worse if a method does not predict the object's movement. By not requiring our method to predict complicated object trajectories of even completely invisible objects we generate a consistent supervision signal. Qualitative results of other methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref> on Semantic KITTI show that they do not predict tubes as well, but instead also complete dynamic objects as if they were static. Having said spatio-temporal tubes) handle dynamic objects differently. We remove all free space targets within the shadows of dynamic objects (marked as black regions) to obtain a consistent static scene. We evaluate the same model on both variants to measure this difference quantitatively. The impact on overall reconstruction performance in terms of IoU for occupied and free space class is marginal because of the prevalence of voxels belonging to static objects. However, the impact on IoU of small object classes that are primarily dynamic (e.g. Person, Bicyclist) is significant and leads to an increase in mIoU over all classes of about 2.1 %. The comparison highlights that our method is in fact able to recognize smaller traffic participants. But an additional requirement to predict their motion will hide this ability.</p><p>this, the benchmark metric of course penalizes all methods equally for not predicting spatio-temporal objects tubes for dynamic objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scene Completion Evaluation</head><p>In accordance with the scene completion benchmark <ref type="bibr" target="#b5">[6]</ref> we use the mean intersection-over-union (mIoU) metric to assess both geometric completion performance and semantic segmentation accuracy. This metric is calculated on a pervoxel basis for the semantic scene completion task and on a per-point basis for single-scan LiDAR semantic segmentation. The semantic scene completion task is ranked by the mIoU value over all semantic classes including the free space class. The mere geometric completion performance is rated by the IoU value over all occupied classes combined, that is all classes except for free space. The threshold ? empty voxel ? (0, 1) is selected individually for each network variant based on the training set. This ensures that precision and recall values are balanced out, resulting in the respective maximum value for completion IoU and semantic mIoU. <ref type="figure" target="#fig_8">Fig. 7</ref> plots the precision-recallcurve for the occupied class on the validation set together with IoU values for our best performing network.</p><p>We apply test-time augmentation (TTA) to our best performing approach for better comparison to the concurrent work JS3CNet. The regular predictions and predictions with TTA are submitted separately to the benchmark. TTA is implemented by augmenting the input point cloud at test time and averaging over the lattice grid predictions before generating the final voxel grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Scene Completion Benchmark Results</head><p>We compare our approach against four recently published deep-learning-based methods on the challenging outdoor LiDAR semantic scene completion task. Quantitative results are reported by their respective authors on the benchmark and are compared in <ref type="table" target="#tab_3">Table 1</ref>.</p><p>The performance of our method surpasses all other methods in pure geometric completion performance (57.7 %). Here we exceed the second-best performing method LMSCNet-singlescale <ref type="bibr" target="#b6">[7]</ref> by a margin of 1.0 %. The authors of JS3CNet <ref type="bibr" target="#b8">[8]</ref> only report benchmark results with TTA, so we use TTA as well for comparison. JS3CNet achieves a marginally higher mIoU (+0.2 %) than our method with TTA, while being considerable inferior in geometric completion (-2.3 % IoU). JS3CNet is more accurate on small object classes and less accurate on the larger ground classes. S3CNet <ref type="bibr" target="#b9">[9]</ref> outperforms all other methods by a large margin on the semantics of small object classes, resulting in the best mIoU value. For the other object classes, it does however perform comparably or even worse to our method. Overall, when it comes to geometric accuracy, S3CNet underperforms significantly. This might be a result of the semantic post-processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We use the Semantic KITTI validation split and the static scene data variant for evaluation of the ablation study. All ablation results are listed in <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-resolution upsampling and decoder variants (Table 3, architecture):</head><p>The individual local functions are arranged in a grid where each cell has an edge length of 0.32 m. The encoder uses a number of pooling layers and generally produces feature maps at lower resolutions of up to 16 times the output grid size. Our baseline Local-DIFs variant achieves a high resolution output grid by two independent upsample approaches. The first is upsampling and concatenating the lower resolution feature maps progressively in the encoder. The second is to supply pairs of relative coordinates and conditioning vectors for different resolutions. The decoder then handles the fusion of multiple feature maps. The conditioned-batch normalization (CBN) works as an attention mechanism between latent vector and   <ref type="figure" target="#fig_3">9 52.2 31.3 31.2 6.7 16.1 41.5 45.0 45.9 35.8 16.0 39.5 34.0 21.2 31.0</ref>   query position. This variant is unique to decoder architecture based on DIFs. We drop one of the two upsample approaches at a time resulting in two model architecture variants: Local-DIFs-CBN does not have transposed convolutions for upsampling in the encoder. Local-DIFs-c3 uses only the highest resolution feature map in the decoder. Both completion and semantic scene completion performance is highest when using the baseline model that can rely on both upsample pathways. Building the decoder only on the high resolution feature map in Local-DIFs-c3 reduces performance to a lesser extent than removing the transposed convolutions in the encoder in Local-DIFs-CBN. In both cases the drop in semantic scene completion is more noticeable than the drop in pure geometric completion. Local-DIFs-CBN reduces the number of trainable parameters compared to the baseline to about 78 % ( <ref type="table">Table 2</ref>). The transposed convolutions account for a considerable share of the total parameters of the encoder. This experiment indicates that a decoder based on coarse grid cells together with coordinates as an attention mechanism can reduce the number of network weights required for upsampling.</p><p>Inspired by <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, we construct a continuous representation decoder without the use of CBN. This third architecture variant feature interpolation performs bilinear interpolation on each resolution of the 2D feature grid to obtain a latent feature vector corresponding directly to the query position. As this feature only contains information about the xy-position we also concatenate the z-position of the query position onto this positioned vector. The resulting decoder structure contains almost the same number of parameters. While the overall performance is comparable to the baseline, the accuracy in semantic mIoU declines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid cell size (Table 3, cell size):</head><p>We review the impact of the architecture's grid cell size by scaling the base cell size of 0.32 m to {75.0 %, 81.25 %, 150 %, 200 %} of its original value. The lower resolution feature maps as well as the input voxelization resolution are scaled accordingly. Larger grid cells tend to have only a negligible impact on the large ground object classes. However, semantic mIoU drops due to overall lower accuracy over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss weighting (Table 3, loss):</head><p>The individual loss weights ? {S,G,L} of the baseline network are ? S = 7.5, ? G = 2.0, ? C = 1.0. We vary this weighting towards a larger contribution of the semantic loss, a larger contribution of the geometric loss, and a disabled consistency loss. Reducing the semantic loss weight does help with geometric reconstruction accuracy. However, the semantic segmentation accuracy does not improve over the baseline level by a higher relative weighting. <ref type="table" target="#tab_5">Table 3</ref>, Data: )Previous work uses deep neural networks to perform geometric scene completion both with and without semantic understanding of objects or scenes. This choice primarily depends on the existence on semantic ground truth annotations. Previous experiments suggest a correlation between semantic classification of ob-jects on the SUNGC dataset and the accuracy on geometric completion of the scene <ref type="bibr" target="#b3">[4]</ref>. We investigate if the semantic supervision signal helps with understanding objects in the scene and therefore also with geometric reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of semantic supervision signal on geometric completion quality (</head><p>We compare our baseline model on the validation set with two models that are trained on variants of the training dataset. First, we map the 19 semantic classes of the semantic KITTI dataset to a simpler set of only 9 classes. For instance, similar object classes are pooled into categories for small and large traffic participants. Secondly, we omit semantic classes altogether and only differentiate between occupied and free while training. Quantitative results are listed in <ref type="table" target="#tab_5">Table 3</ref> grouped under Data. The performance on geometric completion is almost unaffected by semantic supervision: 57.6 % for the baseline vs. 57.9 % without semantic predictions. It is still noteworthy that the seemingly more difficult task of semantic scene completion is solved by a network of the same size almost without a loss in geometric completion performance. This suggests that the semantic and geometric completion task are indeed related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of scene completion training data:</head><p>We analyze the single frame segmentation performance measured by the mIoU over the segmentation of all input LiDAR points. For this purpose, we train our method on single LiDAR scans with free space sampling and compare it to the baseline trained for scene completion on accumulated data. The networks are identical and the accumulated scene completion targets are a super set of the semantic segmentation of a single LiDAR scan. The segmentation performance of the scene completion model with accumulated supervision is almost 6 % lower than that of the model only trained on single frame segmentation. Smaller object classes see the strongest declines. The quantitative results of this comparison and the differences in IoU scores are listed in the appendix in <ref type="table" target="#tab_13">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>It is noteworthy that we can compete on a benchmark based on a voxelized representation even though we do not use voxels as input or training targets. The voxelized scene that we generate from a post-processing step is more accurate than the end-to-end learned voxelization of other methods. We believe that our continuous representation benefits the learning of a spatially accurate scene representation. Voxelization causes quantization noise in the input signal and the supervision signal which we can avoid entirely.</p><p>We have analyzed how the generated scene completion function behaves when confronted with sparse measurements. The ground segmentation images illustrate that the representation generalizes to areas that are never directly observed with the LiDAR sensor. Our method learns to interpolate the course of the road, sidewalks or parking areas between measurements. <ref type="figure" target="#fig_10">Fig. 8</ref> highlights completion modes for areas that are highly predictable (top row) and areas where completion is based on a best guess from the prior data distribution obtained from the training dataset (bottom row). For the latter part we say that the DNN completes the scene from experience when presented with practically no evidence from measurements. We examine another aspect of the scene completion function and plot the results in the right column of <ref type="figure" target="#fig_10">Fig. 8</ref>. As before, we create a mesh that approximates the decision surface of the completion function at a certain threshold for the free space probability. In addition we determine the gradient of the free space value wrt. to the surface normal. The magnitude of this gradient is now transformed into a pseudo-color of the mesh. With a larger magnitude the transition from free space to an occupied class gets sharper. It is clearly visible that the ground level has a sharp transition even in high distances as it is easier to predict. Smaller objects show generally smaller gradients at their surfaces. But it is also noteworthy that the invisible rear side of objects as well as the predicted clouds of parking-car-probabilities have a small magnitude. Meaning that there is a softer transition in the completion function. It appears that the free space gradient correlates with the certainty of the spatial position of a surface. However, it can not be considered a well-calibrated measure of uncertainty in the output, but probably more as an indication of such.</p><p>We identify a failure mode of our method when it comes to the representation of fine geometric details and the drop of single frame segmentation performance as analyzed in the ablation about completion vs. segmentation training data (Section 4.5, final paragraph). This loss in segmentation performance is significant given that the segmentation is derived from the exact same input point cloud. We do not have a definitive explanation for the magnitude of this circumstance. A possibility is to attribute the drop to the domination of the learning process by the completion task that leads to poorer performance on the segmentation task. An effect that can similarly be observed in multitask learning setups. Another effect that contributes is that the completion task exhibits many ambiguities in the areas where the input point cloud is sparse. There predictions are dominated by the dataset prior where small object classes are underrepresented. The convolutional architecture shares all weights over the spatial scene extent so that this kind of label noise contributes to the blurring of smaller object classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>We presented a novel approach to predict a semantically completed 3D scene from a single LiDAR scan. Our method is able to infer 3D geometry and semantics in sparsely measured areas from context and prior experience. In doing so we address two essential challenges: The first is to use LiDAR data and the included free space information as supervision signal. The second is being able to process large spatial extents for outdoor use while maintaining a high spatial resolution of the predicted completion at the same time. The key aspect is to encode LiDAR point clouds in a structured latent representation that is then decoded using local deep implicit functions at multiple resolutions. The output representation can be post-processed to obtain a voxel representation or 3D meshes for visualization purposes. We believe that we have set an important LiDAR-only baseline in the emerging field of large-extent outdoor scene completion.</p><p>Our approach surpasses all other methods on the challenging voxel-based Semantic KITTI scene completion The ablation experiments demonstrated the advantage of the multi-resolution latent grid over a single resolution and verify the selected hyper-parameters. We showed that learning semantic classes along with geometry does not induce a performance penalty on the geometric completion performance. Uncertainty is inherent in the real-world scene completion task. As future work it will be rewarding to address this uncertainty by means of calibrating the network output or learning of a mapping to uncertainty from the input data. A well-calibrated uncertainty estimate will help to take full advantage of learning-based scene completion.  <ref type="table">Table 4</ref> lists all layers, inputs and operations of our DNN architecture. <ref type="table" target="#tab_10">Table 5</ref> lists hyperparameters for the latent grid size, network training, and inference. We use TensorFlow to implement online data processing, neural network weight optimization, and network inference. The Adam optimizer is used for optimization. We use linear learning rate warmup over the first 2 * (1?? 2 ) ?1 training iterations as proposed by <ref type="bibr" target="#b53">[52]</ref> for untuned warmup of Adam's learning rate.</p><formula xml:id="formula_8">0m ?1 1m ?1 2m ?1 3m ?1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A NEURAL NETWORK ARCHITECTURE, IMPLEMENTA-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TION AND HYPERPARAMETERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B STABLE FORMULATION OF THE GEOMETRIC AND CONSISTENCY LOSS TERMS</head><p>Our network head outputs a probability vector f c LDIF (p) = [f 1 , . . . , f N , f N +1 ] ? for a query position p. The first N entries are defined to represent semantic object classes while the last entry f N +1 is defined to represent the additional free space class. These probabilities are obtained by softmaxnormalization of the network output logits. The following sections explain how numerical stability issues prevent us from calculating the cross-entropy loss terms from the softmax output. However, the paper has so far derived the definitions of geometric and consistency loss only from the softmax probabilities. Therefore, the following sections deduce stable formulations for the geometric and consistency loss from the unscaled output logits as required to calculate the loss and gradients for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Stable Cross-Entropy</head><p>Probability-like vectors are computed from the unscaled network logits z i by applying exponential normalization ("softmax"). In order to calculate the cross-entropy loss from logits z i and true probabilities y i</p><formula xml:id="formula_9">L = i y i log f i = i y i logsoftmax(z i )<label>(7)</label></formula><p>the logsoftmax function is required. It is common and necessary for implementations of Eq. <ref type="bibr" target="#b6">(7)</ref> to take advantage of the identity softmax(z) = softmax(z + c) to allow for the equivalent stable formulation</p><formula xml:id="formula_10">logsoftmax(z i ) = log exp(z i ) ? j exp(z j )<label>(8)</label></formula><formula xml:id="formula_11">= z i ? b ? log j exp(z j ? b)<label>(9)</label></formula><p>with b = max (z)</p><p>to avoid both numerical under-or overflow issues that would otherwise arise in the formulation log(exp(.)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Geometric Loss</head><p>The geometric loss loss differentiates between the binary occupied label of a given point. The geometric loss is formulated separately from the semantic loss to also include LiDAR measurements without a semantic annotation. The geometric loss L G makes it necessary to conceptually sum up the output probabilities f 1 , . . . , f N of all the semantic object classes (all classes except for the free space class). It is straightforward to formulate the cross-entropy loss L G from the complete probability vector f ? R N +1 :</p><formula xml:id="formula_12">L G = y occupied log(f occupied ) + (1 ? y occupied ) log(f free ) (10) = y occupied log N i=1 f i + (1 ? y occupied ) log(f N +1 ) (11)</formula><p>However, the fused formulation of logsoftmax (Eq. (9)) prevents us from just adding up the softmax outputs ? N i f i and inserting them into Eq. <ref type="bibr" target="#b11">(11)</ref>. Instead, we use the same normalization trick as in Eq. (9) to define free space and occupied logits as</p><formula xml:id="formula_13">z occupied = b occ + log N i=1 exp(z i ? b occ )<label>(12)</label></formula><p>with b occ = max(z 1 , . . . , z N )</p><formula xml:id="formula_14">(13) z free = z N +1<label>(14)</label></formula><p>from the outputs z 1 , . . . , z N +1 so that the equality</p><formula xml:id="formula_15">softmax z occupied z free = ? N i=1 f i f N +1<label>(15)</label></formula><p>holds. Thus, the converted logits z free and z occupied can be used for the computation of the geometric loss as in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Consistency Loss</head><p>The same numerical stability issue needs to be considered when implementing the consistency loss. We can write the consistency loss L C as Jensen-Shannon-Divergence (JSD) between M probability vectors P (1) , . . . , P (M ) :</p><formula xml:id="formula_16">P (?) = [p (?) 1 , . . . , p (?) n ] ? R n<label>(16)</label></formula><p>L C = JSD P (1) , . . . , P (M )</p><formula xml:id="formula_17">= H P ? 1 M M ?=1 H P (?)<label>(17)</label></formula><p>with P = (p 1 , . . . ,</p><formula xml:id="formula_19">p n ) = 1 M M ?=1 P (?) = ? n i=1 (p i ) log (p i ) + 1 M M ?=1 n i=1 p (?) i log p (?) i<label>(19)</label></formula><p>The loss L C equals the difference between the entropy of the averaged probability distribution H P and the average of the individual entropies 1 M ? M ?=1 H P (?) . As in Eq. <ref type="formula" target="#formula_9">(7)</ref>, this term needs to be calculated from the unscaled output logits z (?) i . The probabilities</p><formula xml:id="formula_20">p (?) i = exp(z (i) i ) ? n j=1 exp(z (?) j )<label>(20)</label></formula><p>are again deduced from the logits z (?) i by softmax normalization. The second term of Eq. <ref type="bibr" target="#b19">(19)</ref> is calculated using the definition of the stable logsoftmax function in Eq. (9). Special consideration is necessary to compute the logarithm of averaged probabilities log p i in the first term in Eq. <ref type="bibr" target="#b19">(19)</ref>. <ref type="bibr">TABLE 4</ref> Network architecture: Detailed network architecture and input format definition. The ID of each row is used to reference the output of the row. ? indicates that the layer immediately above is an input. N denotes the number of LiDAR points falling within the xy-grid extent. M denotes the number of input cells in the x and y direction. We are assuming a square N = M xy-extent. O denotes the number of positions to classify. Latent feature selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Inputs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="28">-Query positions [O ? 3]</head><p>Positions to be classified by the decoder <ref type="bibr">29 28</ref> Cell assignment [O ? 2] Assign cells to each query position <ref type="bibr">30 28</ref> Cell assignment Assign cells to each query position <ref type="bibr">31 28</ref> Cell assignment Assign cells to each query position <ref type="bibr">32 18, 29</ref> Gather Query position in local cell-relative coordinate system <ref type="bibr">38 28, 30</ref> Calc. position p 2 Query position in local cell-relative coordinate system <ref type="bibr">39 28, 31</ref> Calc. position p 3 Query position in local cell-relative coordinate system Decoder with conditioned batch-normalization <ref type="bibr" target="#b41">40</ref>         <ref type="table" target="#tab_13">Table 7</ref> includes the comparison of the single frame segmentation performance as discussed in section 4.5 impact of scene completion training data in the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D IMPACT OF SCENE COMPLETION TRAINING DATA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E QUALITATIVE RESULTS</head><p>We visualize the scene completion function on example scenes of the Semantic KITTI test set in <ref type="figure">Fig. 9</ref>. The scene extent is not limited to the scene size of the Semantic KITTI benchmark, but instead quadrupled in terms of area to cover the full 360?LiDAR scan. The corresponding ground level segmentation is displayed in the rightmost column. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>C. Rist is with the Intelligent Vehicles Group, TU Delft (NL) and with Mercedes-Benz AG, Stuttgart (DE). ? D. Emmerichs is with Mercedes-Benz AG, Stuttgart (DE). ? M. Enzweiler is with Esslingen University of Applied Sciences (DE). ? D. Gavrila is with the Intelligent Vehicles Group, TU Delft (NL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.</head><label></label><figDesc>Fig. 1. Illustration of the semantic scene completion task and the output of our method. Sensors are limited in their resolution and restricted to a single perspective of their surroundings. A LiDAR scan (herein depicted as black points) is characterized by a varying degree of sparsity caused either by distance (A), occlusions from objects (B) or sensor blind spots (C). Our method is able to complete the sparse scan geometrically and semantically and can be applied to large spatial extents as typically found in outdoor environments. The underlying representation is not tied to a fixed output resolution and describes the scene using a continuous function (right side, color indicates semantic class). Therefore the geometry does not exhibit quantization artefacts resulting from a discretization into voxels (left side).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>@</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Network architecture: The feature extractor creates a top-view feature map of the input point cloud. The CNN-encoder outputs feature maps at three different resolutions that make up the latent representation of the 3D scene. The decoder classifies individual coordinates within the 3D scene extent. Latent feature vectors and relative-coordinates are processed by conditioned batch normalization in the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Left to right, top to bottom: Input points, ground truth accumulated points, mesh visualization of continuous output function, derived voxelization at 20 cm edge length. Geometric details can be represented more accurately by our continuous output function as compared to the voxelization resolution of the Semantic KITTI dataset. Our method does not cause artefacts on slanted surfaces (e.g. road plane) or edges between objects. of the MLP. The decoder diagram on the right of Fig. 2 illustrates this setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Columns from left to right: Completed scene, accumulated LiDAR as ground truth, ground segmentation, and corresponding ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>LiDAR scan (green) projected into reference RGB image. The vertical field of view of the KITTI LiDAR sensor only covers a range up to a few degrees over the horizon. Nevertheless, the resulting scene completion training targets cover objects at more than 2 m over the ground since they are accumulated from more distant positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Our dataset ((a) static scene) and the official benchmark ((b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Precision-recall curve for the occupied class. We plot the (m)IoU values for occupied, free and semantic classes of the baseline network variant. Markers are at the free space thresholds that are evaluated, interpolation in between.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>R o a d S i d e w a l k P a r k i n g o t h e r g r . B u i l d i n g F e n c e C a r T r u c k o t h e r v e h . B i c y c l e M o t o r c y c l e P e r s o n B i c y c l i s t M o t o r c y c l . V e g e t a t i o n T r u n k T e r r a i n P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Left: Top-view scene completion. Right: Magnitude of the gradient of the free space probability wrt. the surface normal. The top row demonstrates a highly predictable completion of road surface, the boundaries of the sidewalk, and a car in proximity to the ego-vehicle. Far away from the ego-vehicle, the bottom row shows how our method guesses the most likely classification of each individual scene coordinate in the absence of almost all evidence from actual measurements. The scene completion function is softer at these object boundaries (red surfaces). benchmark in terms of geometric completion IoU (+1.0 %).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>6</head><label>6</label><figDesc>Quantitative results of baseline and ablations on the validation set (higher is better) d S i d e w a l k P a r k i n g o t h e r g r . B u i l d i n g F e n c e C a r T r u c k o t h e r v e h . B i c y c l e M o t o r c y c l e P e r s o n B i c y c l i s t M o t o r c y c l . V e g e t a t i o n T r u n k T e r r a i n P o l e T r . S i g n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>R o a d S i d e w a l k P a r k i n g o t h</head><label></label><figDesc>e r g r . B u i l d i n g F e n c e C a r T r u c k o t h e r v e h . B i c y c l e M o t o r c y c l e P e r s o n B i c y c l i s t M o t o r c y c l . V e g e t a t i o n T r u n k T e r r a i n P o l e T r . S i g n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Training the architecture involves common spatial augmentations of the input LiDAR point clouds in sensor coordinates. We use random uniform rotation over full 360?, random uniform scaling between ?5 %, random uniform translations between ?5 cm. When training we use a top-view input grid with 256 ? 256 voxels which results in a square with edge length of 40.96 m within the scene. The grid is initially centered over the area where the accumulated training targets have been generated. The voxel grid is shifted off-center using normally-distributed offsets with standard deviation ? = 8 m. We sample a single free space point for each point in the input LiDAR point cloud and a single free space point within each empty voxel. Additionally, 2500 random scene locations are sampled and contribute to the consistency loss term, but do not have any other annotations. When training, only two out of the four nearest local functions f L are evaluated for each query point to be able to include almost twice as many query training targets in a single batch. The two selected weighting coefficients w are scaled up accordingly. Depending on available VRAM and desired batch size the total number of training targets is clipped to a maximum value. For a KITTI scan with around 120 000 points and GPUs with 16 GB VRAM we selected a batch size of two and 400 000 training targets per GPU. Training on four Tesla-V100-GPUs with an effective batch size of eight took around four days to complete.</figDesc><table /><note>Training details:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Quantitative scene completion results for our method and recently published approaches on the Semantic KITTI Scene Completion Benchmark (in Intersection-over-Union, higher is better). ?: Method uses test-time augmentation.</figDesc><table><row><cell cols="2">Geometric Completion Semantic Completion</cell></row><row><cell>Method / IoU [%]</cell><cell>Occ. mIoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Quantitative results of baseline and ablations on the validation set (higher is better)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Geometric Completion</cell><cell>Semantic Completion</cell></row><row><cell></cell><cell>Variation</cell><cell cols="2">IoU Occ. Precision Recall</cell><cell>mIoU</cell></row><row><cell></cell><cell>Local-DIFs (Baseline)</cell><cell>57.8</cell><cell>73.1 73.4</cell><cell>26.1</cell></row><row><cell></cell><cell>(Local-DIFs + TTA )</cell><cell>(58.5)</cell><cell>74.2 73.5</cell><cell>(26.9)</cell></row><row><cell>Arch.</cell><cell>Local-DIFs-CBN Local-DIFs-c3 Feature interpolation</cell><cell>55.4 57.1 57.4</cell><cell>71.9 70.8 72.7 72.6 73.0 73.0</cell><cell>23.8 24.2 25.5</cell></row><row><cell>Cell size</cell><cell>Cell size 75.0 % Cell size 87.5 % Cell size 150 % Cell size 200 %</cell><cell>54.1 57.1 56.7 56.7</cell><cell>70.6 69.8 73.8 71.7 71.6 73.1 72.5 72.3</cell><cell>23.8 25.6 24.1 23.3</cell></row><row><cell>Loss</cell><cell>? S = 15, ? G = 1 ? S = 3.75, ? G = 4 ? C = 0</cell><cell>55.6 58.2 56.9</cell><cell>71.1 71.7 74.5 72.7 72.0 73.0</cell><cell>24.0 24.7 25.0</cell></row><row><cell>Data</cell><cell>Simplified sem. Without sem.</cell><cell>57.8 57.9</cell><cell>74.1 72.3 73.6 73.1</cell><cell>(38.8) (57.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Full Professor for Autonomous Mobile Systems. His current research focuses on scene understanding for mobile robotics. In 2012, he received the IEEE Intelligent Transportation Systems Society Best PhD Dissertation Award and the Uni-DAS Research Award for his work on vision-based pedestrian recognition. He is part of the team that won the 2014 IEEE Intelligent Transportation Systems Outstanding Application Award. In 2014, he was honored with a Junior-Fellowship of the Gesellschaft f?r Informatik. The Netherlands), where he since heads the Intelligent Vehicles group as Full Professor. His research deals with sensor-based detection of humans and analysis of behavior, recently in the context of the self-driving cars in urban traffic. He received the Outstanding Application Award 2014 and the Outstanding Researcher Award 2019, both from the IEEE Intelligent Transportation Systems Society.</figDesc><table><row><cell>Christoph B. Rist received his Bachelor's de-</cell></row><row><cell>gree in 2014 and his Master's degree in 2017,</cell></row><row><cell>both in Electrical Engineering and Information</cell></row><row><cell>Technology from Karlsruhe Institute of Technol-</cell></row><row><cell>ogy, Germany. He is currently pursuing his Ph.D.</cell></row><row><cell>degree in the Intelligent Vehicles group at TU</cell></row><row><cell>Delft (The Netherlands) while working in the</cell></row><row><cell>Corporate Research of Mercedes-Benz AG in</cell></row><row><cell>Stuttgart (Germany). His current research fo-</cell></row><row><cell>cuses on LiDAR perception for autonomous driv-</cell></row><row><cell>ing.</cell></row><row><cell>David Emmerichs received his Bachelor's de-</cell></row><row><cell>gree in 2016 and his Master's degree in 2018 in</cell></row><row><cell>Physics from the RWTH Aachen University, Ger-</cell></row><row><cell>many. He is currently pursuing his Ph.D. degree</cell></row><row><cell>at IWR, Heidelberg University while working in</cell></row><row><cell>the Corporate Research of Mercedes-Benz AG,</cell></row><row><cell>Stuttgart (both Germany). His current research</cell></row><row><cell>focuses on LiDAR perception for autonomous</cell></row><row><cell>driving.</cell></row><row><cell>Markus Enzweiler received the Ph.D. degree</cell></row><row><cell>in computer science from the Univ. of Heidel-</cell></row><row><cell>berg, Germany (2011). From 2010, he was with</cell></row><row><cell>Mercedes-Benz AG R&amp;D in Stuttgart, Germany,</cell></row><row><cell>most recently as a Technical Manager for Li-</cell></row><row><cell>DAR and camera. He co-developed the Daimler</cell></row><row><cell>vision-based pedestrian detection system which</cell></row><row><cell>is available in Mercedes-Benz cars. In 2021,</cell></row><row><cell>he moved to Esslingen University of Applied</cell></row><row><cell>Sciences as a Dariu M. Gavrila received the Ph.D. degree in</cell></row><row><cell>computer science from the Univ. of Maryland</cell></row><row><cell>at College Park, USA, in 1996. From 1997, he</cell></row><row><cell>was with Daimler R&amp;D, Ulm, Germany, where</cell></row><row><cell>he became a Distinguished Scientist. He led</cell></row><row><cell>the vision-based pedestrian detection research,</cell></row><row><cell>which was commercialized 2013-2014 in vari-</cell></row><row><cell>ous Mercedes-Benz models. In 2016, he moved</cell></row><row><cell>to TU Delft (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Conditioning vector. Units: 2 * 32 as mean ?, var ? 41 35 Dense Conditioning vector. Units: 2 * 32 as mean ?, var ? 42 36 Dense Conditioning vector. Units: 2 * 32 as mean ?, var ? Output logits vector [z 1 , . . . , z n ] of single local function f L</figDesc><table><row><cell>32</cell><cell>Dense</cell><cell>[O ? 64]</cell><cell></cell></row><row><cell>43 37</cell><cell>Dense+BN</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>44 ?, 40</cell><cell>Conditioning</cell><cell>[O ? 32]</cell><cell>Apply feature-wise mean and var y = ? ? x + ?</cell></row><row><cell>45 ?</cell><cell>ReLU</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>46 ?, 38</cell><cell>Concat</cell><cell>[O ? 35]</cell><cell></cell></row><row><cell>47 ?</cell><cell>Dense+BN</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>48 ?, 41</cell><cell>Conditioning</cell><cell>[O ? 32]</cell><cell>Apply feature-wise mean and var y = ? ? x + ?</cell></row><row><cell>49 ?</cell><cell>ReLU</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>50 ?, 39</cell><cell>Concat</cell><cell>[O ? 35]</cell><cell></cell></row><row><cell>51 ?</cell><cell>Dense+BN</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>52 ?, 42</cell><cell>Conditioning</cell><cell>[O ? 32]</cell><cell>Apply feature-wise mean and var y = ? ? x + ?</cell></row><row><cell>53 ?</cell><cell>ReLU</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>54 ?</cell><cell>(Dense+ReLU) x2</cell><cell>[O ? 32]</cell><cell></cell></row><row><cell>55 ?</cell><cell>Dense</cell><cell>[O ? 20]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 Hyperparameters Parameter Value Description Grid setup and spatial augmentations</head><label>5</label><figDesc>Edge length of highest resolution hierarchy cells. Lower resolutions have edge lengths of 4? = 1.28 m and 16? = 5.12 m. ? 2 0.16 m Edge length of LiDAR input cells. The ratio 1 2 between LiDAR input cells and output cells is determined by the network architecture (Table 4). M training ? M training 256 ? 256 Number of input cells when training. Equals a spatial extent of 40.96 m. This extent is randomly translated to off-center positions. Standard deviation of normally-distributed grid offsets a transl , b transl ?0.05 m Boundaries of random uniform translation of input point cloud a scale , b scale ?5 % Boundaries of random uniform scaling of input point cloud Visualization meshes use the isosurface at free space probability ? free space . Empty voxel threshold for upsampling variant ? empty voxel Feature Interpolation 0.04 Empty voxel threshold for feature interpolation variant</figDesc><table><row><cell>?</cell><cell>0.32 m</cell><cell></cell></row><row><cell>?</cell><cell>8 m</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Loss and optimizer</cell></row><row><cell>? S</cell><cell>7.5</cell><cell>Constant weighting of semantic loss L S .</cell></row><row><cell>? G</cell><cell>2.0</cell><cell>Constant weighting of geometric loss L G .</cell></row><row><cell>? C</cell><cell>1.0</cell><cell>Constant weighting of consistency loss L C .</cell></row><row><cell>B</cell><cell>8 = (2 ? 4 GPUs)</cell><cell>Training batch size</cell></row><row><cell>l</cell><cell>1.0 ? 10 ?3</cell><cell>Initial learning rate</cell></row><row><cell>dstep, drate</cell><cell>40 k; 0.5</cell><cell>Staircase learning rate decay</cell></row><row><cell>wwarmup, wrate</cell><cell cols="2">2  *  (1 ? ? 2 ) ?1 = 2000 Learning rate warm-up steps</cell></row><row><cell>Adam ? 1 , ? 2</cell><cell>0.9; 0.999</cell><cell>Adam optimizer momenta</cell></row><row><cell></cell><cell></cell><cell>Inference</cell></row><row><cell>? free space</cell><cell>0.3</cell><cell></cell></row><row><cell>? empty voxel Local-DIFs</cell><cell>0.04</cell><cell>Empty voxel threshold</cell></row><row><cell>? empty voxel Local-DIFs-CBN</cell><cell>0.05</cell><cell>Empty voxel threshold for upsampling variant</cell></row><row><cell>? empty voxel Local-DIFs-c3</cell><cell>0.04</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>31.8 3.3 38.6 13.6 51.3 32.3 10.6 4.3 3.3 15.7 24.7 0.0 40.1 19.6 50.6 25.7 14.0 (Local-DIFs + TTA) (58.5) 74.2 73.5 (26.9) 72.2 45.4 36.0 2.8 39.2 14.7 52.8 33.2 9.3 4.1 4.4 16.4 24.9 0.0 41.8 19.7 52.1 26.3 15.0 23.8 63.5 37.1 30.6 0.1 37.1 13.5 48.5 22.6 7.5 7.6 2.6 15.1 24.9 0.0 39.4 18.5 45.0 24.9 13.3 Local-DIFs-c3 57.1 72.7 72.6 24.2 69.6 42.9 33.7 2.5 37.3 12.7 50.0 27.3 5.9 2.5 3.5 13.1 11.9 0.0 40.6 18.6 49.5 23.8 14.7 Feature interpolation 57.4 73.0 73.0 25.5 68.5 43.4 30.0 2.4 39.4 12.9 51.4 23.3 10.5 2.7 2.6 14.7 34.9 0.0 40.6 19.1 47.9 26.3 14.3 23.8 62.0 39.3 27.3 1.7 37.3 14.4 50.2 30.4 13.0 5.1 3.4 13.3 16.1 0.0 39.3 16.6 43.0 24.8 14.7 Cell size 87.5 % 57.1 73.8 71.7 25.6 68.3 42.0 33.2 1.6 38.2 13.9 51.4 29.6 9.8 3.3 5.3 14.0 25.2 0.0 41.1 18.6 49.0 26.7 14.9 Cell size 150 % 56.7 71.6 73.1 24.1 69.0 41.1 30.1 1.3 37.8 11.6 49.6 31.7 5.6 1.9 2.2 10.4 22.2 0.0 40.2 17.3 48.8 24.0 13.2 Cell size 200 % 56.7 72.5 72.3 23.3 71.0 42.4 25.8 1.0 37.5 8.5 48.4 22.0 12.8 2.3 2.8 2.7 20.9 0.0 39.6 16.6 51.3 23.2 13.4 Loss ? S = 15, ? G = 1 55.6 71.1 71.7 24.0 68.4 42.3 29.8 2.3 37.9 12.9 49.6 29.1 7.2 1.4 2.0 14.3 15.6 0.0 39.0 18.7 46.7 25.3 13.7 ? S = 3.75, ? G = 4 58.2 74.5 72.7 24.7 70.3 42.7 28.8 0.6 38.4 13.0 51.9 23.2 11.9 2.9 6.1 12.3 16.9 0.0 40.9 20.0 48.5 26.1 15.6 ? C = 0 56.9 72.0 73.0 25.0 67.7 40.5 30.2 0.5 38.0 13.3 50.8 25.9 7.8 5.4 2.3 15.7 28.1 0.0 40.7 18.8 47.6 26.4 14.9</figDesc><table><row><cell>Local-DIFs (Baseline) 57.8 73.1 73.4 26.1 71.2 43.8 Arch. Local-DIFs-CBN 55.4 71.9 70.8 Cell size Cell size 75.0 % 54.1 70.6 Simplified sem. 57.8 74.1 72.3 (38.8) 70.0 69.8 Data Without sem. 57.9 73.6 73.1 (57.9)</cell><cell>45.0</cell><cell>38.0 51.9</cell><cell>30.2</cell><cell>57.9</cell><cell>11.8</cell><cell>40.5</cell><cell>46.7</cell><cell>15.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 7</head><label>7</label><figDesc>Cross-evaluation of LiDAR segmentation performance when trained for scene completion. Categories sorted by gain in IoU.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For a stable computation of log p i from logits z (?) i we are going to use the definition? z (?) = max z (?) 1 , . . . , z (?) n (21)</p><p>of maximum vectors? 1 , . . . ,? n and? <ref type="bibr" target="#b0">(1)</ref> , . . . ,? (M ) and s (?) as shorthand notation for the softmax denominator of the shifted logits.? (?) shifts the softmax calculation of each individual output logit vector. Thereafter,? i shifts the computation of the exponential over the given vector entry i. Again, this ensures that the argument of log(exp(.)) is within an acceptable range. Thus, log p i is written as</p><p>to gain a formulation that is stable against over or underflow. We insert Eq. (28) into Eq. <ref type="bibr" target="#b19">(19)</ref> and implement the resulting statement as our consistency loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C QUANTITATIVE RESULTS OF ABLATION STUDY</head><p>The quantitative results of the ablation study of section 4.5 in the manuscript are listed in <ref type="table">Table 6</ref>. The listing includes the class-individual IoU values.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niebner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4578" to="4587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured Prediction of Unobserved Voxels From a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two Stream 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic Scene Completion From a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning 3D Shape Completion From Laser Scan Data With Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1955" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rold?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<title level="m">LMSCNet: Lightweight Multiscale 3D Semantic Completion</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<ptr target="http://arxiv.org/abs/2008.10559" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>of the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Agia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bingbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CORL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Implicit Fields for Generative Shape Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Occupancy Networks: Learning 3D Reconstruction in Function Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit Surface Representations As Layers in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SCSSnet: Learning Spatially-Conditioned Scene Segmentation LiDAR Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intelligent Vehicles Symposium</title>
		<meeting>of the Intelligent Vehicles Symposium</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Best Paper Award</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on 3D Vision (3DV)</title>
		<meeting>of the International Conf. on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep marching cubes: Learning explicit surface representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Donn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2916" to="2925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Surface Prediction for 3D Object Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on 3D Vision (3DV</title>
		<meeting>of the International Conf. on 3D Vision (3DV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OctNet: Learning Deep 3D Representations at High Resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Octree Generating Networks: Efficient Convolutional Architectures for High-Resolution 3D Outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local Implicit Grid Representations for 3D Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional Occupancy Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chibane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local Deep Implicit Functions for 3D Shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Poisson Surface Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Eurographics Symposium on Geometry Processing</title>
		<meeting>the Fourth Eurographics Symposium on Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning 3D Shape Completion under Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PCN: Point Completion Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on 3D Vision (3DV)</title>
		<meeting>of the International Conf. on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3D Shape Completion in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">3D Semantic Scene Completion: a Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rold?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.07466" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matterport3D: Learning from RGB-D Data in Indoor Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on 3D Vision (3DV)</title>
		<meeting>of the International Conf. on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">YOLOv3: An Incremental Improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<ptr target="http://arxiv.org/abs/1804.02767" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tangent Convolutions for Dense Prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">RangeNet++: Fast and Accurate LiDAR Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PointPillars: Fast Encoders for Object Detection From Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-Sensor Deep Domain Adaptation for LiDAR Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symposium (IV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1535" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Point-Voxel CNN for Efficient 3D Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">TORNADO-Net: mulTiview tOtal vaRiatioN semAntic segmentation with Diamond inceptiOn module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerdzhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.10544" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">See and Think: Disentangling Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the adequacy of untuned warmup for adaptive optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.04209" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
