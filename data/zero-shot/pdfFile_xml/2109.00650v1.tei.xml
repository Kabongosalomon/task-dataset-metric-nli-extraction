<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dash: Semi-Supervised Learning with Dynamic Thresholding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-03">September 3, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?2</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><forename type="middle">Sun</forename><surname>?1</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dash: Semi-Supervised Learning with Dynamic Thresholding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-03">September 3, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While semi-supervised learning (SSL) has received tremendous attentions in many machine learning tasks due to its successful use of unlabeled data, existing SSL algorithms use either all unlabeled examples or the unlabeled examples with a fixed high-confidence prediction during the training progress. However, it is possible that too many correct/wrong pseudo labeled examples are eliminated/selected. In this work we develop a simple yet powerful framework, whose key idea is to select a subset of training examples from the unlabeled data when performing existing SSL methods so that only the unlabeled examples with pseudo labels related to the labeled data will be used to train models. The selection is performed at each updating iteration by only keeping the examples whose losses are smaller than a given threshold that is dynamically adjusted through the iteration. Our proposed approach, Dash, enjoys its adaptivity in terms of unlabeled data selection and its theoretical guarantee. Specifically, we theoretically establish the convergence rate of Dash from the view of non-convex optimization. Finally, we empirically demonstrate the effectiveness of the proposed method in comparison with state-of-the-art over benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In spite of successful use in a variety of classification and regression tasks, supervised learning requires large amount of labeled training data. In many machine learning applications, labeled data can be significantly more costly, time-consuming and difficult to obtain than the unlabeled data <ref type="bibr" target="#b77">[Zhu, 2005]</ref>, since they usually require experienced human labors from experts (e.g., a doctor in detection of covid-19 by using X-ray images). Typically, only a small amount of labeled data is available, but there is a huge amount of data without label. This is one of key hurdles in the development and deployment of machine learning models.</p><p>Semi-supervised learning (SSL) is designed to improve learning performance by leveraging an abundance of unlabeled data along with limited labeled data <ref type="bibr" target="#b10">[Chapelle et al., 2006]</ref>. In much recent work, SSL can be categorized into several main classes in terms of the use of unlabeled data: consistency regularization, pseudo labeling, generic regularization (e.g., large margin regularization, Laplacian regularization, etc [Chapelle  <ref type="bibr" target="#b3">[Baird, 1992</ref><ref type="bibr" target="#b55">, Schmidhuber, 2015</ref> based on the condition that the model predictions between different perturbed versions of the same image are similar. Another line of work is to produce artificial label for unlabeled data based on prediction model and add them to the training data set. With different approaches of artificial label production, varies of SSL methods have been proposed in the literature including self-training <ref type="bibr" target="#b68">[Yarowsky, 1995</ref><ref type="bibr" target="#b33">, Lee, 2013</ref><ref type="bibr" target="#b53">, Rosenberg et al., 2005</ref><ref type="bibr" target="#b54">, Sajjadi et al., 2016</ref><ref type="bibr" target="#b32">, Laine and Aila, 2017</ref><ref type="bibr" target="#b67">, Xie et al., 2020b</ref> and co-training <ref type="bibr" target="#b9">[Blum and Mitchell, 1998</ref><ref type="bibr" target="#b75">, Zhou and Li, 2005</ref><ref type="bibr" target="#b56">, Sindhwani and Rosenberg, 2008</ref><ref type="bibr" target="#b63">, Wang et al., 2008</ref><ref type="bibr" target="#b70">, Yu et al., 2008</ref><ref type="bibr" target="#b64">, Wang and Zhou, 2010</ref><ref type="bibr" target="#b12">, Chen et al., 2011</ref>. Due to its capability to handle both labeled data and unlabeled data, SSL has been widely studied in diverse machine learning tasks such as image classification <ref type="bibr" target="#b54">[Sajjadi et al., 2016</ref><ref type="bibr" target="#b32">, Laine and Aila, 2017</ref><ref type="bibr" target="#b59">, Tarvainen and Valpola, 2017</ref><ref type="bibr" target="#b66">, Xie et al., 2020a</ref><ref type="bibr" target="#b8">, Berthelot et al., 2019b</ref>, natural language processing <ref type="bibr" target="#b61">[Turian et al., 2010]</ref>, speech recognition <ref type="bibr" target="#b69">[Yu et al., 2010]</ref>, and object detection <ref type="bibr" target="#b46">[Misra et al., 2015]</ref>. Numerous empirical evidences show that unlabeled data in SSL can help to improve the learning performance <ref type="bibr" target="#b8">[Berthelot et al., 2019b</ref><ref type="bibr" target="#b58">,a, Sohn et al., 2020</ref>, however, a series of theoretical studies <ref type="bibr" target="#b6">[Ben-David et al., 2008</ref><ref type="bibr" target="#b57">, Singh et al., 2009</ref><ref type="bibr" target="#b36">, Li and Zhou, 2011b</ref><ref type="bibr" target="#b4">, Balcan and Blum, 2005</ref>] have demonstrated that this success is highly relying on a necessary condition that labeled data and unlabeled data with pseudo label come from the same distribution during the training process <ref type="bibr">[Zhu, 2005, Van Engelen and</ref><ref type="bibr" target="#b62">Hoos, 2020]</ref>. Unfortunately, it has been shown that this condition does not always hold in real applications and thus it could hurt the performance <ref type="bibr" target="#b38">[Li et al., 2017</ref><ref type="bibr" target="#b49">, Oliver et al., 2018</ref>. For example, the pseudo label of an unlabeled example generated by conventional SSL methods during the training progress is not correct <ref type="bibr" target="#b28">[Hataya and</ref><ref type="bibr">Nakayama, 2019, Li et al., 2020]</ref>. In this case, the degradation of model performance has been observed when using unlabeled data compared to the simple supervised learning model not using any unlabeled data at all <ref type="bibr" target="#b10">[Chapelle et al., 2006</ref><ref type="bibr" target="#b49">, Oliver et al., 2018</ref>. Thus, not all unlabeled data are needed in SSL.</p><p>To improve SSL performance, multiple studies <ref type="bibr" target="#b25">[Guo et al., 2020</ref><ref type="bibr" target="#b52">, Ren et al., 2020</ref>   <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref> uses the unlabeled examples with a fixed high-confidence prediction (e.g., 0.95) in classification tasks using cross entropy loss. However, the fixed threshold may lead to eliminate too many unlabeled examples with correct pseudo labels (see <ref type="figure" target="#fig_0">Figure 1</ref> (a)) and may lead to select too many unlabeled examples with wrong pseudo labels (see <ref type="figure" target="#fig_0">Figure 1</ref> (b)). That is to say, the fixed threshold is possible not good enough during the training progress and thus it could degrade the overall performance.</p><p>Unlike the previous work, we aim to the proposed approach enjoys its adaptivity in terms of unlabeled data selection and its theoretical guarantee. This inspires us to consider answering the following question in this study.</p><p>Can we design a provable SSL algorithm that selects unlabeled data with dynamic thresholding?</p><p>To this end, we propose a generic SSL algorithm with Dynamic Thresholding (Dash) that can dynamically select unlabeled data during the training process. Specifically, Dash firstly runs over labeled data and obtains a threshold for unlabeled data selection. It then selects the unlabeled data whose loss values are smaller than the threshold to the training data-set. The value of threshold is gradually decreased over the optimization iterations. It can be integrated with existing SSL methods like FixMatch <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref>. From the view of optimization, we show that eventually the proposed Dash can non-asymptotically converge with theoretical guarantee. Empirical evaluations on image benchmarks validate the effectiveness of Dash comparing with the state-of-the-art SSL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been growing interest in semi-supervised learning for training machine learning and deep learning <ref type="bibr" target="#b20">[Flach, 2012</ref><ref type="bibr" target="#b23">, Goodfellow et al., 2016</ref>. A number of SSL methods have been studied by leveraging the structure of unlabeled data including consistency regularization <ref type="bibr" target="#b2">[Bachman et al., 2014</ref><ref type="bibr" target="#b54">, Sajjadi et al., 2016</ref><ref type="bibr" target="#b32">, Laine and Aila, 2017</ref><ref type="bibr" target="#b59">, Tarvainen and Valpola, 2017</ref><ref type="bibr" target="#b47">, Miyato et al., 2018</ref><ref type="bibr" target="#b66">, Xie et al., 2020a</ref>, entropy minimization <ref type="bibr" target="#b24">[Grandvalet and</ref><ref type="bibr">Bengio, 2005, Lee, 2013]</ref>, and other interesting approaches <ref type="bibr">[Berthelot et al., 2019b,a]</ref>. In addition, several studies on SSL have been proposed to keep SSL performing safe when using unlabeled data, which is known as safe SSL <ref type="bibr" target="#b37">[Li and Zhou, 2015</ref>]. An non-exhaustive list of those studies include <ref type="bibr" target="#b16">[Cozman et al., 2003</ref><ref type="bibr" target="#b57">, Singh et al., 2009</ref><ref type="bibr" target="#b35">, Li and Zhou, 2011a</ref><ref type="bibr" target="#b5">, Balsubramani and Freund, 2015</ref><ref type="bibr" target="#b41">, Loog, 2015</ref><ref type="bibr" target="#b38">, Li et al., 2017</ref><ref type="bibr" target="#b30">, Krijthe and Loog, 2017</ref><ref type="bibr" target="#b39">, Li et al., 2021</ref><ref type="bibr" target="#b45">, Mey and Loog, 2019</ref><ref type="bibr" target="#b25">, Guo et al., 2020</ref>. For example, <ref type="bibr" target="#b52">Ren et al. [2020]</ref> proposed a new SSL framework that uses an individual weight for each unlabeled example, and it updates the individual weights and models iteratively by solving a bi-level optimization problem approximately. In this paper, we mainly focus on improved deep SSL methods with the use of unlabeled data selection. Comprehensive surveys on SSL methods could be refer to <ref type="bibr" target="#b77">[Zhu, 2005</ref><ref type="bibr" target="#b10">, Chapelle et al., 2006</ref><ref type="bibr" target="#b76">, Zhu and Goldberg, 2009</ref><ref type="bibr">, Hady and Schwenker, 2013</ref><ref type="bibr" target="#b62">, Van Engelen and Hoos, 2020</ref>.</p><p>The use of unlabeled data selection by a threshold is not new in the literature of SSL. As a simple yet widely used heuristic algorithm, pseudo-labeling <ref type="bibr" target="#b33">[Lee, 2013]</ref> (a.k.a. self-training <ref type="bibr" target="#b44">[McLachlan, 1975</ref><ref type="bibr" target="#b68">, Yarowsky, 1995</ref><ref type="bibr" target="#b53">, Rosenberg et al., 2005</ref><ref type="bibr" target="#b54">, Sajjadi et al., 2016</ref><ref type="bibr" target="#b32">, Laine and Aila, 2017</ref><ref type="bibr" target="#b67">, Xie et al., 2020b</ref>) uses the prediction model itself to generate pseudo labels for unlabeled images. Then the unlabeled images whose corresponding pseudo label's highest class probability is larger than a predefined threshold will be used for the training. Nowadays, pseudo-labeling has been become an important component of many modern SSL methods <ref type="bibr" target="#b66">[Xie et al., 2020a</ref><ref type="bibr" target="#b58">, Sohn et al., 2020</ref>.</p><p>With the use of weak and strong data augmentations, several recent works such as UDA <ref type="bibr" target="#b66">[Xie et al., 2020a]</ref>, ReMixMatch <ref type="bibr" target="#b7">[Berthelot et al., 2019a]</ref> and FixMatch <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref> have been proposed in image classification problems. Generally, they use a weakly-augmented 1 unlabeled image to generate a pseudo label and enforce consistency against strongly-augmented 2 version of the same image. In particular, UDA and FixMatch use a fixed threshold to retain the unlabeled example whose highest probability in the predicted class distribution for the pseudo label is higher than the threshold. For example, UDA sets this threshold to be 0.8 for CIFAR-10 and SVHN, and FixMatch sets the threshold to be 0.95 for all data-sets. To encourage the model to generate high-confidence predictions, UDA and ReMixMatch sharpen the guessed label distribution by adjusting its temperature and then re-normalize the distribution. <ref type="bibr" target="#b58">Sohn et al. [2020]</ref> have shown that the sharpening and thresholding pseudo-labeling have a similar effect.</p><p>By contrast, the proposed Dash method selects a subset of unlabeled data to be used in training models by a data-dependent dynamic threshold, and its theoretical convergence guarantee is established for stochastic gradient descent under the non-convex setting, which is applicable to deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary and Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>We study the task of learning a model to map an input x ? X ? R d onto a label y ? Y. In many machine learning applications, x refers to the feature and y ? Y refers to the label for classification or regression. For simplicity, let ? denote the input-label pair (x, y), i.e. ? := (x, y). We denote by P the underlying distribution of data pair ?, then ? ? P. The goal is to learn a model w ? R d via minimizing an optimization problem whose objective function F (w) is the expectation of random loss function f (w; ?):</p><formula xml:id="formula_0">min w?R d F (w) := E ??P [f (w; ?)] ,<label>(1)</label></formula><p>where E ? [?] is an expectation taking over random variable ? ? P. The optimization problem (1) covers most machine learning and deep learning applications. In this paper, we consider the classification problem with K-classes, whose loss function is the cross-entropy loss given by</p><formula xml:id="formula_1">f (w; ? i ) = H(y i , p(w; x i )) := K k=1 ?y i,k log exp(p k (w; x i )) K j=1 exp(p j (w; x i )) ,<label>(2)</label></formula><p>where p(w; x) is the prediction function and H(q, p) is the cross-entropy between q and p. In this paper, we do not require the function f (w; ?) to be convex in terms of w, which is applicable to various deep learning tasks.</p><p>In SSL, it consists of labeled examples and unlabeled examples. Let</p><formula xml:id="formula_2">D l := {(x i , y i ), i = 1, . . . , N l }<label>(3)</label></formula><p>be the labeled training data. Given unlabeled training examples {x u i , i = 1, 2, . . . , N u }, one can generate pseudo label y u i based on the predictions of a supervised model on labeled data. Different SSL methods such as pseudo-labeling <ref type="bibr" target="#b33">[Lee, 2013]</ref>, Adversarial Training <ref type="bibr" target="#b47">[Miyato et al., 2018]</ref>, UDA <ref type="bibr" target="#b66">[Xie et al., 2020a]</ref>, and FixMatch <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref> have been proposed to generate pseudo labels. We denote by</p><formula xml:id="formula_3">D u := {(x u i , y u i ), i = 1, . . . , N u }<label>(4)</label></formula><p>the unlabeled data, where y u i ? Y is the pseudo label. Although it contains pseudo label, we still call D u unlabeled data for simplicity in our analysis. Usually, the number of unlabeled examples is much larger than the number of labeled examples, i.e., N u N l . Finally, the training data consists of labeled data D l and unlabeled data with pseudo label D u , and thus the training loss of an SSL algorithm usually contains supervised loss F s and unsupervised loss F u with a weight ? u &gt; 0:</p><formula xml:id="formula_4">F s + ? u F u , where F s is constructed on D l and F u is constructed on D u .</formula><p>In image classification problems, F s is just the standard cross-entropy loss:</p><formula xml:id="formula_5">F s (w) := 1 N l N l i=1 f (w; ? i ),<label>(5)</label></formula><p>where ? i ? D l and f is defined in (2). Thus, different constructions of the unsupervised loss F u lead to different SSL methods. Typically, there are two ways of constructing F u : one is to use pseudo labels to formulate a "supervised" loss such as cross-entropy loss (e.g., FixMatch), and another one is to optimize a regularization that does not depend on labels such as consistency regularization (e.g., ?-Model). Next, we will introduce a recent SSL work to interpret how to generate pseudo labels and construct unsupervised loss F u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FixMatch: An SSL Algorithm with Fixed Thresholding</head><p>Due to its simplicity yet empirical success, we select FixMatch <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref> as an SSL example in this subsection. Besides, we consider FixMatch as a warm-up of the proposed algorithm, since FixMatch uses a fixed threshold to ratain unlabeled examples and it will be used as a pipeline in the proposed algorithm.</p><p>The key idea of FixMatch is to use a separate weak and strong augmentation when generating model's predicted class distribution and one-hot label in unsupervised loss. Specifically, based on a supervised model w and a weak augmentation ?, FixMatch predict the class distribution</p><formula xml:id="formula_6">h i = p(w, ?(x u i ))<label>(6)</label></formula><p>for a weakly-augmented version of a unlabeled image x u i , where p(w, x) is the prediction function. Then it creates a pseudo label by</p><formula xml:id="formula_7">y u i = arg max(h i ).<label>(7)</label></formula><p>Following by <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref>, the arg max applied to a probability distribution produces a "one-hot" probability distribution. To construct the unsupervised loss, it computes the model prediction for a strong augmentation T of the same unlabeled image x u i :</p><formula xml:id="formula_8">p(w, T (x u i )).<label>(8)</label></formula><p>The unsupervised loss is defined as the cross-entropy between y u i and p i :</p><formula xml:id="formula_9">H( y u i , p(w, T (x u i ))).<label>(9)</label></formula><p>Eventually, FixMatch only uses the unlabeled examples with a high-confidence prediction by selecting based on a fixed threshold ? = 0.95. Therefore, in FixMatch the cross-entropy loss with pseudo-label and confidence for unlabeled data is given by</p><formula xml:id="formula_10">F u (w) = 1 N u Nu i=1 I(max(h i ) ? ? )H( y u i , p(w, T (x u i ))),<label>(10)</label></formula><p>where I(?) is an indicator function. As we discussed in introduction, this fixed threshold may lead to eliminate/select too many unlabeled examples with correct/wrong pseudo labels (see <ref type="figure" target="#fig_0">Figure 1</ref>), which eventually could drop off overall performance. It is a natural choice: the threshold is not fixed across the optimization iterations. Thus, in the next section, we are going to propose a new SSL scheme having a dynamic threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dash: An SSL Algorithm with Dynamic Thresholding</head><p>Before introducing the proposed method, we would like to point out the importance of unlabeled data selection in SSL from the theoretical view of optimization. Classical SSL methods <ref type="bibr" target="#b77">[Zhu, 2005</ref><ref type="bibr" target="#b10">, Chapelle et al., 2006</ref><ref type="bibr" target="#b76">, Zhu and Goldberg, 2009</ref><ref type="bibr">, Hady and Schwenker, 2013</ref><ref type="bibr" target="#b62">, Van Engelen and Hoos, 2020</ref> assume that labeled data and unlabeled data are from the same distribution. That is to say, ? ? P holds for ? ? D l ? D u . Then SSL methods aim to solve the optimization problem (1) by using a standard stochastic optimization Algorithm 1 Dash: Semi-Supervised Learning with Dynamic Thresholding Input: learning rate ? 0 and mini-batch size m 0 for stage one, learning rate ? and parameter m of mini-batch size for stage two, two parameters C &gt; 1 and ? &gt; 1 for computing threshold, and violation probability ?. // Warm-up Stage: run SGD in T 0 iterations.</p><formula xml:id="formula_11">Initialization: u 0 = w 0 for t = 0, 1, . . . , T 0 ? 1 do Sample m 0 examples ? t,i (i = 1, . . . , m 0 ) from D l , u t+1 = u t ? ? 0gt whereg t = 1 m0 m0 i=1 ?f s (u t ; ? t,i ) end for // Selection Stage: run SGD in T iterations. Initialization: w 1 = u T0 .</formula><p>Compute the value of ? as in <ref type="formula" target="#formula_0">(16)</ref>. // In practice, ? can be obtained as in <ref type="formula" target="#formula_0">(17)</ref>.</p><formula xml:id="formula_12">for t = 1, . . . , T do 1) Sample n t = m? t?1 examples from D u , where the pseudo labels in D u are generated by FixMatch 2) Set the threshold ? t = C? ?(t?1) ?.</formula><p>3) Compute truncated stochastic gradient g t as (18). 4) Update solution by SGD using stochastic gradient g t and learning rate ?:</p><formula xml:id="formula_13">w t+1 = w t ? ?g t . end for Output: w T +1</formula><p>algorithm like mini-batch stochastic gradient descent (SGD). Specifically, at iteration t, mini-batch SGD updates intermediate solutions by</p><formula xml:id="formula_14">w t+1 = w t ? ? m m i=1 ?f (w t ; ? t,i ),<label>(11)</label></formula><p>where m is the mini-batch size, ? t,i is sampled from training data D l ? D u , ?f (w; ?) is the gradient of f (w; ?) in terms of w. In this situation, the theoretical convergence guarantee of SSL algorithms can be simply established under mild assumptions on objective function f (w; ?) such as smoothness and bounded variance <ref type="bibr" target="#b22">[Ghadimi et al., 2016]</ref>. If the labeled data and unlabeled data are not from the same distribution such as some of pseudo labels are not correct, classical SSL methods with standard stochastic optimization algorithm may lead to the performance drops <ref type="bibr" target="#b10">[Chapelle et al., 2006</ref><ref type="bibr" target="#b49">, Oliver et al., 2018</ref>. Besides, the theoretical guarantee of the optimization algorithm for this case is not clear. This inspires us to design a new algorithm to overcome this issue. To this end, we proposed an SSL method that can dynamically select unlabeled examples during the training progress. First, let us define the loss function for the proposed method. Same as FixMatch, the supervised loss F s (w) for the proposed method is the standard cross-entropy loss on labeled data D l :</p><formula xml:id="formula_15">F s (w) := 1 N l N l i=1 f s (w; ? i ),<label>(12)</label></formula><p>where</p><formula xml:id="formula_16">? i = (x i , y i ) is sampled from D l , f s (w; ? i ) = H(y i , p(w; ?(x i )))</formula><p>, and ?(x) is the weakly-augmented version of x. Since the new dynamic threshold is not fixed, we let it rely on the optimization iteration t and it is denoted by ? t . Then the unsupervised loss is given by</p><formula xml:id="formula_17">F u (w) = 1 N u Nu i=1 I(f u (w; ? u i ) ? ? t )f u (w; ? u i ),<label>(13)</label></formula><p>where</p><formula xml:id="formula_18">? u i = (x u i , y u i ) is sampled from D u , f u (w; ? u i ) = H( y u i , p(w; T (x u i ))), T (x)</formula><p>is the strongly-augmented version of x, and the pseudo label y u i is generated based on <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_7">(7)</ref>  unlabeled image x u i . The unsupervised loss <ref type="formula" target="#formula_0">(13)</ref> shows that the Dash will retain the unlabeled example whose loss is smaller than the threshold ? t . If we rewrite the indicator function I(max(h i ) ? ? ) in (10) to an equivalent expression</p><formula xml:id="formula_19">I(? log(max(h i )) ? ? log(? )),<label>(14)</label></formula><p>we can consider ? log(max(h i )) as a cross-entropy loss for one-hot label. Roughly speaking, FixMatch retains the unlabeled images with the loss ? log(max(h i )) smaller than ?log(0.95) ? 0.0513. It is worth nothing that the loss ? log(max(h i )) contains the information of weakly-augmented images, while the loss f u (w; ? u i ) in (13) includes the information of both weakly-augmented and strongly-augmented images, meaning that the proposed method considers the entire loss function.</p><p>We then need to construct ? t . Intuitively, with the increase of the optimization iteration t, the loss function would decrease in general, so that ? t is also required to decrease. Mathematically, we set the dynamic threshold ? t as a decreasing function of t, which is given by</p><formula xml:id="formula_20">? t := C? ?(t?1) ?,<label>(15)</label></formula><p>where C &gt; 1, ? &gt; 1 are two constants. For example, we set C = 1.0001 in our experiments and thus at first iteration (i.e., t = 1) the unlabeled examples whose loss values are smaller than ? t = 1.0001 ? ? will be used in the training. <ref type="figure" target="#fig_1">Figure 2</ref> shows a comparison of fixed threshold used in FixMatch and dynamic threshold ? t in (15) with C = 1.0001, ? = 1 and different ?, where the threshold in FixMatch is in the scale of negative log. It seems our thresholding strategy matches the curve of training loss in many real applications (e.g., see <ref type="figure" target="#fig_0">Figure 1</ref> (a) of <ref type="bibr" target="#b74">[Zhang et al., 2017]</ref>). Next, it is important to estimate the value of ?. In theory, it can be estimated by</p><formula xml:id="formula_21">? = max a, 4G 2 (1 + ?b 0 m) ??a 0 m ,<label>(16)</label></formula><p>where contains several parameters related to the property of considered problem (1) whose detailed definitions can be found in Theorem 1. Please note that the estimation of ? in (16) is for the use of convergence analysis only. In practice, we can use the following averaged loss from the training set D l as the approximate ?:</p><formula xml:id="formula_22">? ? 1 |D l | ?i?D l f (w 1 ; ? i ),<label>(17)</label></formula><p>where |D l | is the number of examples in D l , and w 1 can be learned on D l . We can see from <ref type="formula" target="#formula_0">(17)</ref> with <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(15)</ref> that the unlabeled examples whose losses are smaller than the averaged loss of labeled examples will be maintained during the training process. Finally, it is ready to describe the proposed Dash algorithm in details that contains two stages: warm-up stage and selection stage. In the warm-up stage, it runs SGD to train a model over labeled data D l in certain steps. Not only for warm-up, this stage is also used for estimating ? in (17). In the selection stage, we conduct SGD against D u using w 1 as the initial solution. At each iteration t, we sample n t = m? t?1 training examples from D u , where m &gt; 1 is a parameter defined in (24). We compute the stochastic gradients according to <ref type="formula" target="#formula_0">(13)</ref>:</p><formula xml:id="formula_23">g t = nt i=1 I(f u (w t ; ? u t,i ) ? ? t )?f u (w t ; ? u t,i ) nt i=1 I(f u (w t ; ? u t,i ) ? ? t ) .<label>(18)</label></formula><p>Since N l is small, in practice we can also construct the stochastic gradient by using all labeled data as</p><formula xml:id="formula_24">g t = nt?N l i=1 I(f u (w t ; ? u t,i ) ? ? t )?f u (w t ; ? u t,i ) N l + nt?N l i=1 I(f u (w t ; ? u t,i ) ? ? t ) + N l i=1 ?f s (w t ; ? t,i ) N l + nt?N l i=1 I(f u (w t ; ? u t,i ) ? ? t ) ,<label>(19)</label></formula><p>where ? u t,i = (x u t,i , y u t,i ) ? D u and ? t,i = (x t,i , y t,i ) ? D l . The solution is then updated by mini-batch SGD, whose update step is given by</p><formula xml:id="formula_25">w t+1 = w t ? ?g t .<label>(20)</label></formula><p>The detailed updating steps of the proposed algorithm are presented in Algorithm 1, which called SSL with Dynamic Thresholding (Dash).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Convergence Result</head><p>To establish the convergence result of the proposed Dash algorithm, we need to give some preliminaries.</p><p>Recall that the training examples for the labeled data D l follow the distribution P, and we aim to minimize the optimization problem (1). For the examples coming from the unlabeled data D u , suppose it is a mixture of two distributions, P and Q. More specifically, with a probability q, we will sample an example from P and with a probability 1 ? q sample from Q:</p><formula xml:id="formula_26">? ? qP + (1 ? q)Q, where ? ? D u , q ? (0, 1).<label>(21)</label></formula><p>We define the objective function B(w) as the expected loss for distribution Q, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B(w)</head><formula xml:id="formula_27">:= E ??Q [f (w; ?)] .<label>(22)</label></formula><p>For the simplicity of convergence analysis, we do not consider the weak and strong augmentations, i.e., let f s = f u = f in <ref type="formula" target="#formula_0">(12)</ref> and <ref type="formula" target="#formula_0">(13)</ref>. Without loss of generality, we assume that our loss function is non-negative and is bounded by 1, i.e. f (w; ?) ? [0, 1] for any w and ?. Then by <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(22)</ref>, we have F (w) ? [0, 1] and B(w) ? [0, 1]. In order to differentiate the two distribution, we follow the idea of Tsybakov noisy condition <ref type="bibr" target="#b43">[Mammen et al., 1999</ref><ref type="bibr" target="#b60">, Tsybakov, 2004</ref>, and assume, for any solution w, if F (w) ? a, then</p><formula xml:id="formula_28">E ??Q I {?:f (w;?)?F (w)} (?) ? bA ? (w),<label>(23)</label></formula><p>where I S (?) is an indicator function, ? ? 1, and b is constant. Finally, we made a few more assumptions that are commonly used in the studies of non-convex optimization (e.g., deep learning) <ref type="bibr">Lan, 2013, Yuan et al., 2019]</ref>. Throughout this paper, we also make the assumptions on the problem (1) as follows.</p><p>Assumption 1. Assume the following conditions hold:</p><p>(i) The stochastic gradient ?f (w; ?) is unbiased, i.e., E ??P [?f (w; ?)] = ?F (w), and there exists a constant G &gt; 0, such that ?f (w; ?) ? G.</p><p>(ii) F (w) is smooth with a L-Lipchitz continuous gradient, i.e., it is differentiable and there exists a constant L &gt; 0 such that</p><formula xml:id="formula_29">?F (w) ? ?F (u) ? L w ? u , ?w, u ? R d .</formula><p>Assumption 1 (i) assures that the stochastic gradient of the objective function is unbiased and the gradient of f (w; ?) in terms of w is upper bounded. Assumption 1 (ii) says the objective function is L-smooth, and it has an equivalent expression which is ?w, u ? R d ,</p><formula xml:id="formula_30">F (w) ? F (u) ? ?F (u), w ? u + L 2 w ? u 2 .</formula><p>We now introduce an important property regarding F (w), i.e. the Polyak-Lojasiewicz (PL) condition <ref type="bibr" target="#b50">[Polyak, 1963]</ref> of F (w).</p><p>Assumption 2. There exists ? &gt; 0 such that</p><formula xml:id="formula_31">2?(F (w) ? F (w * )) ? ?F (w) 2 , ?w ? R d .</formula><p>This PL property has been theoretically and empirically observed in training deep neural networks [Allen- <ref type="bibr" target="#b0">Zhu et al., 2019</ref><ref type="bibr" target="#b71">, Yuan et al., 2019</ref>. This condition is widely used to establish convergence in the literature of non-convex optimization, please see <ref type="bibr" target="#b71">[Yuan et al., 2019</ref><ref type="bibr" target="#b29">, Karimi et al., 2016</ref><ref type="bibr" target="#b40">, Li and Li, 2018</ref><ref type="bibr" target="#b11">, Charles and Papailiopoulos, 2018</ref> and references therein. Now, we are ready to provide the theoretical result for Dash. Without loss of generality, let F (w * ) = 0 in the analysis. Please note that this is a common property observed in training deep neural networks <ref type="bibr" target="#b74">[Zhang et al., 2017</ref><ref type="bibr" target="#b0">, Allen-Zhu et al., 2019</ref><ref type="bibr" target="#b1">, Arora et al., 2019</ref><ref type="bibr" target="#b13">, Chizat et al., 2019</ref><ref type="bibr" target="#b27">, Hastie et al., 2019</ref><ref type="bibr" target="#b72">, Yun et al., 2019</ref>. The following theorem states the convergence guarantee of the proposed Dash algorithm. We include its proof in the Appendix. Theorem 1. Under Assumptions 1 and 2, suppose that C &gt; 1 and F (w * ) = 0, for any ? ? (0, 1), ? 0 L ? 1, ?L ? 1, let T 0 = log(2F (w0)/a) log(1/(1??0?)) , m 0 = 4G 2 ??a ,</p><formula xml:id="formula_32">m = max log(2/?) q 2 , log(2/?) (1 ? q) 2 , log(2/?) q(1 ? C ?1 ) 2 ,<label>(24)</label></formula><formula xml:id="formula_33">? = max a, 4G 2 (1 + ?b 0 m) ??a 0 m<label>(25)</label></formula><p>in Algorithm 1, then with a probability 1 ? (4T + 1)?, we have F (w T +1 ) ? ?? ?T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark.</head><p>We can see from the above result that one can set the iteration number T to be large enough to ensure the convergence of Dash. Specifically, in order to have an optimization error, one can set</p><formula xml:id="formula_34">T = log( ?/ )/ log(?), then F (w T +1 ) ? . The total sample complexity of Dash is T 0 m 0 + T t=1 m? t?1 ? T 0 m 0 + m? T ??1 = T 0 m 0 + m ? (??1) = O(1/ )</formula><p>. This rate matches the result of supervised learning in <ref type="bibr" target="#b29">[Karimi et al., 2016]</ref> when analyzing the standard SGD under Assumptions 1 and 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we present some experimental results for image classification tasks. To evaluate the efficacy of Dash, we compare it with several state-of-the-art (SOTA) baselines on several standard SSL image classification benchmarks including CIFAR-10, CIFAR-100 <ref type="bibr" target="#b31">[Krizhevsky and Hinton, 2009]</ref>, SVHN <ref type="bibr" target="#b48">[Netzer et al., 2011]</ref>, and STL-10 . Specifically, SOTA baselines are MixMatch <ref type="bibr" target="#b8">[Berthelot et al., 2019b]</ref>, UDA <ref type="bibr" target="#b66">[Xie et al., 2020a]</ref>, ReMixMatch <ref type="bibr" target="#b7">[Berthelot et al., 2019a]</ref>, FixMatch <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref> and the algorithm RYS from <ref type="bibr" target="#b52">[Ren et al., 2020]</ref> 3 . Besides, ?-Model <ref type="bibr" target="#b51">[Rasmus et al., 2015]</ref>, Pseudo-Labeling <ref type="bibr" target="#b33">[Lee, 2013]</ref> and Mean Teacher <ref type="bibr" target="#b59">[Tarvainen and Valpola, 2017]</ref> are included in the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data-sets</head><p>The original CIFAR data-sets have 50,000 training images and 10,000 testing images of 32?32 resolutions, and CIFAR-10 has 10 classes containing 6,000 images each, while CIFAR-100 has 100 classes containing 600 images each. The original SVHN data-set has 73,257 digits for training and 26,032 digits for testing, and the total number of classes is 10. The original STL-10 data set has 5,000 labeled images from 10 classes and 100,000 unlabeled images, which contains out-of-distribution unlabeled images. Following by <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref>, we train ten benchmarks with different settings: CIFAR-10 with 4, 25, or 400 labels per class, CIFAR-100 with 4, 25, or 100 labels per class, SVHN with 4, 25, or 100 labels per class, and the STL-10 data set. For example, the benchmark CIFAR-10 with 4 labels per class means that there are 40 labeled images in CIFAR-10 and the remaining images are unlabeled, and then we denote this data set by CIFAR-10 with 40 labels. For fair comparison, same sets of labeled images from CIFAR, SVHN and STL-10 were used for the proposed Dash method and other baselines in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Models and Hyper-parameters</head><p>We use the Wide ResNet-28-2 model <ref type="bibr" target="#b73">[Zagoruyko and Komodakis, 2016]</ref> as the backbone for CIFAR-10 and SVHN, Wide ResNet-28-8 for CIFAR-100, and Wide ResNet-37-2 for STL-10. In the proposed Dash, we use FixMatch 4 as our pipeline to generate pseudo labels and to construct supervised and unsupervised losses. We employ CTAugment (CTA) <ref type="bibr" target="#b17">[Cubuk et al., 2019]</ref> and RandAugment (RA)  for the strong augmentation scheme following by <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref>. Similar to <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref>, we use the same training protocol such as optimizer, learning rate schedule, data preprocessing, random seeds, and so on. The total number of training epochs is set to be 1024 and the mini-bach size is fixed as 64. For the value of weight decay, we use 5 ? 10 ?4 for CIFAR-10, SVHN and STL-10, 1 ? 10 ?3 for CIAR-100. The SGD with momentum parameter of 0.9 is employed as the optimizer. The cosine learning rate decay schedule <ref type="bibr" target="#b42">[Loshchilov and Hutter, 2017]</ref> is used as <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref>. The initial learning rate is set to be 0.06 for all data-sets. We use (18) to compute stochastic gradients.</p><p>At the first 10 epochs, we do not implement the selection scheme and thus the algorithm uses all selected training examples, meaning that the threshold is infinite, i.e., ? t = ?. After that, we use the threshold to select unlabeled examples, and we choose ? = 1.27 in ? t to reduce the dynamic threshold until its value to be 0.05. That is to say, in practice we give a minimal value of dynamic threshold, which is 0.05 5 . We fix the constant C as 1.0001 and estimate the value of ? by using (17). We decay the dynamic threshold every 9 epochs. We use the predicted label distribution as soft label during the training and it is sharpened by adjusting its temperature of 0.5, which is similar to MixMatch. Once the dynamic threshold is reduced to 0.05, we turn it to one-hot label in the training since the largest label probability is close to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>We report the top-1 testing error rates of the proposed Dash along within other baselines for CIFAR in <ref type="table" target="#tab_1">Table 1</ref> and for SVHN and STL-10 in <ref type="table" target="#tab_2">Table 2</ref>, where all the results of baselines are from <ref type="bibr" target="#b58">[Sohn et al., 2020]</ref> except that the results of RYS are from <ref type="bibr" target="#b52">[Ren et al., 2020]</ref>. All top-1 testing error rates are averaged over 5 independent random trails with their standard deviations using the same random seeds as baselines used.</p><p>We can see from the results that the proposed Dash method has the best performance on CIFAR-10, SVHN and STL-10. For CIFAR-100, the proposed Dash is comparable to ReMixMatch, where ReMixMatch performs a bit better on 400 labels and Dash using RA is a bit better on 2500 labels and 10000 labels. This reason is that the proposed Dash uses FixMatch as its pipeline, and ReMixMatch uses distribution alignment (DA) to encourages the model to predict balanced class distribution (the class distribution of CIFAR-100  is balanced), while FixMatch and Dash do not use DA. We further conduct Dash with DA technique on CIFAR-100 with 400 labels, and the top-1 testing error rate is 43.31%, which is better than ReMixMatch (44.28%). We also find that Dash performs well on the data set with out-of-distribution unlabeled images, i.e., STL-10. The result in <ref type="table" target="#tab_2">Table 2</ref> shows that Dash with CTA has the SOTA performance of 3.96% on top-1 testing error rate. Besides, the proposed Dash can always outperform FixMatch, showing that the use of dynamic threshold is important to the overall performance. We find the proposed Dash has large improvement when the labeled examples is small (the data-sets with 4 labels per classes), comparing to FixMatch. By using CTA, on CIFAR-100 with 400 labels, on CIFAR-10 with 40 labels, and on SVHN with 40 labels, the proposed Dash method outperforms FixMatch result more than 19%, 10%, and 58% in the terms of top-1 testing error rate, respectively. While by using RA, the corresponding improved rates are 4%, 8%, and 23% respectively. These results reveal that the dynamic unlabeled example selection is an important term in SSL when the labeled data is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation study</head><p>In this subsection, we provide two ablation studies using data sets CIFAR-10 with 250 labels and CIFAR-10 with 4000 labels. The first one is to use different ? in the dynamic threshold, and the second one is to change FixMatch to Pseudo-Labeling as the pseudo label generator in Dash.</p><p>Different values of ?. Since ? is a key component of the dynamic threshold, we conduct an ablation study on different values of ? in Dash. For simplicity, we only implement the CTA case. We try four different values of ? ? {1.01, 1.1, 1.2, 1.3} and summarize the results in <ref type="table" target="#tab_3">Table 3</ref>. Comparing these results with that in <ref type="table" target="#tab_1">Table 1</ref>, we will find that the choice of ? = 1.27 in the previous subsection is not the best one. The results also show that Dash is not so sensitive to ? in a certain range.</p><p>Dash with Pseudo-Labeling. Since Dash can be integrated with many existing SSL methods, we use Pseudo-Labeling (PL) <ref type="bibr" target="#b33">[Lee, 2013]</ref> as the pipeline to generate pseudo labels in Dash. The results are listed in <ref type="table" target="#tab_4">Table 4</ref>, showing that Dash can improve PL, especially when the labeled images is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a method Dash that dynamically selects unlabeled data examples to train learning models. Its selection strategy keeps the unlabeled data whose loss value does not exceed a dynamic threshold at each optimization step. The proposed Dash method is a generic scheme that can be easily integrated with existing SSL methods. We demonstrate the use of dynamically selecting unlabeled data can help to the performance of existing SSL method FixMatch in the semi-supervised image classification benchmarks, indicating the importance of dynamic threshold in SSL. The theoretical analysis shows the convergence guarantee of the proposed Dash under the non-convex optimization setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>In this section, we present the proof of our main theoretical result. To this end, we divide the analysis into two part, with the first part devoted to examining the properties of w 1 learned in the first step and the second part devoted to the convergence for the iterations. As the setting of SSL, we assume that the number of unlabeled data is large, i.e., N u is sufficiently large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Properties of Solution w 1</head><p>We first give the property of w 1 in the following lemma, whose proof can be found in the Appendix.</p><p>Lemma 1. Given ? ? (0, 1), run the Warm-up Stage of Algorithm 1 with ? 0 ? 1 L , T 0 = log(2F (w0)/a) log(1/(1??0?)) and m 0 = 4G 2 ??a , then with a probability 1 ? 5? we have F (w 1 ) ? ?.</p><p>Proof. Since the objective function F (u) has a Lipchitz continuous gradient in Assumption 1 (ii), we have</p><formula xml:id="formula_35">F (u t+1 ) ? F (u t ) ? ?F (u t ), u t+1 ? u t + L 2 u t+1 ? u t 2 (a) = ? 0 2 ?F (u t ) ?g t 2 ? ? 0 2 ?F (u t ) 2 + (1 ? ? 0 L) g t 2 (b) ? ? 0 2 ?F (u t ) ?g t 2 ? ? 0 ?F (u t ),<label>(26)</label></formula><p>where (a) follows the update of u t+1 = u t ? ? 0gt ; (b) follows from Assumption 2 and ? 0 L ? 1. Since E[g t ] = ?F (u t ) and</p><formula xml:id="formula_36">E g t ? ?F (u t ) 2 = 1 m 2 0 m0 i=1 E ?f (u t ; ? t i ) ? ?F (u t ) 2 ? 4G 2 m 0 , (Assumption 1 (i))<label>(27)</label></formula><p>using concentration inequality in Lemma 4 of <ref type="bibr" target="#b22">[Ghadimi et al., 2016]</ref>, we have with a probability 1 ? 5?,</p><formula xml:id="formula_37">g t ? ?F (u t ) 2 ? 4G 2 ?m 0 .<label>(28)</label></formula><p>Using the above bound g t ? ?F (u t ) , we can further bound F (u T ) by using <ref type="formula" target="#formula_1">(26)</ref> and <ref type="formula" target="#formula_1">(27)</ref> as</p><formula xml:id="formula_38">F (u T0 ) ?(1 ? ? 0 ?)F (u T0?1 ) + 2? 0 G 2 ?m 0 ?(1 ? ? 0 ?) T0 F (u 0 ) + 2? 0 G 2 ?m 0 T0?1 i=0 (1 ? ? 0 ?) i ?(1 ? ? 0 ?) T0 F (u 0 ) + 2G 2 ?m 0 ? , which implies F (w 1 ) ?(1 ? ? 0 ?) T0 F (w 0 ) + 2G 2 ?m 0 ? .<label>(29)</label></formula><p>Be selecting T 0 ? log(2F (w0)/a) log(1/(1??0?)) and m 0 ? 4G 2 ??a , we have</p><formula xml:id="formula_39">F (w 1 ) ? a.<label>(30)</label></formula><p>Therefore the condition in <ref type="formula" target="#formula_1">(23)</ref> is applicable in this case. On the other hand, by the definition of ? in (25), we know, with a probability 1 ? 5?, that</p><formula xml:id="formula_40">F (w 1 ) ? ?.<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Analysis of Iterative Algorithm</head><p>The key to our analysis is to show that for each iteration t, with a high probability, we have F (w t ) ? ?? ?(t?1) . We will prove this statement by induction. Before we carry out our analysis, we define a few important constants ? = max log(2/?) 2q 2 m , log(2/?) 2(1 ? q) 2 m ,</p><formula xml:id="formula_41">? = log(2/?) qm(1 ? C ?1 ) 2 ? log(2/?) 2qm(1 ? ?)(1 ? C ?1 ) 2 ,<label>(32)</label></formula><formula xml:id="formula_42">a 0 =(1 ? C ?1 )(1 ? ?)(1 ? ?)q,<label>(33)</label></formula><formula xml:id="formula_43">b 0 =2 (1 ? q)(1 + ?)b ? ? + log(1/?) ,<label>(34)</label></formula><formula xml:id="formula_44">b 1 = b 0 a 0 .<label>(35)</label></formula><p>When t = 1, we have F (w 1 ) ? ? according to Lemma 1. At each iteration t, given the solution w t , according to our inductive assumption, with a probability 1 ? (4t + 1)?, we have</p><formula xml:id="formula_46">F (w t ) ? ?? ?(t?1) ,<label>(37)</label></formula><p>For the n t = m? t?1 training examples sampled from D u , we divide it into two sets for the analysis use only, i.e. set A t that includes examples sampled from P and set B t that includes examples sampled from Q. We furthermore denote by A ? t and B ? t the subset of examples in A t and B t whose loss is smaller than the given threshold ? t , i.e.</p><formula xml:id="formula_47">A ? t = {? ? A t : f (w t ; ?) ? ? t } ,<label>(38)</label></formula><formula xml:id="formula_48">B ? t = {? ? B t : f (w t ; ?) ? ? t } ,<label>(39)</label></formula><p>where ? t = C ?? ?(t?1) with C &gt; 1. Evidently, the samples used for computing g t is the union of A ? t and B ? t . The following result bounds the size of A ? t and B ? t . With a probability 1 ? 4?, we have</p><formula xml:id="formula_49">|A ? t | ? a 0 m? (t?1) , |B ? t | ? b 0 m,<label>(40)</label></formula><p>where a 0 and b 0 are defined in <ref type="formula" target="#formula_2">(34)</ref> and <ref type="formula" target="#formula_2">(35)</ref>. Using the Hoeffding's inequality, we have, with a probability 1 ? 2?, |A t | ?qn t 1 ? log(2/?) 2q 2 n t (32)</p><formula xml:id="formula_50">? qn t (1 ? ?) ,<label>(41)</label></formula><p>|B t | ?(1 ? q)n t 1 + log(2/?) 2(1 ? q) 2 n t (32) ? (1 ? q)n t (1 + ?) .</p><p>Using the above bound g a t ? ?F (w) , we can further bound F (w t+1 ) ? F (w t ) as</p><formula xml:id="formula_52">F (w t+1 ) ? F (w t ) ? ? 2 (1 ? b t ) 4G 2 ?a 0 m? t?1 + 4b t G 2 ? ??F (w t ) (48) ? ? 2 4G 2 ?a 0 m? t?1 + 4G 2 b 1 ? ?(t?1) ? ??F (w t ) =2?G 2 1 ?a 0 m + b 1 ? ?(t?1) ? ??F (w t )<label>(49)</label></formula><p>where b 1 is defined in (36). Hence, we have</p><formula xml:id="formula_53">F (w t+1 ) ?(1 ? ??)F (w t ) + 2?G 2 1 ?a 0 m + b 1 ? ?(t?1)<label>(50)</label></formula><p>Let select ? = 1 1???/2 , we know by using (37) the inequality (50) will become</p><formula xml:id="formula_54">F (w t+1 ) ??(1 ? ??) ?? ?t + 2??G 2 1 ?a 0 m + b 1 ? ?t = 1 ? ??/2 1 ? ??/2 ?? ?t + ??/2 1 ? ??/2 4G 2 ? 1 ?a 0 m + b 1 ? ?t</formula><p>By the setting of ? we have</p><formula xml:id="formula_55">F (w t+1 ) ? ?? ?t .<label>(51)</label></formula><p>Therefore, we complete the proof by induction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of experimental results on Wide ResNet-28-8 for CIFAR-100 with 400 labeled images illustrates the reason of dynamically selecting unlabeled data to train learning models. Pseudo labels are generated based on the prediction models. FixMatch selects unlabeled example if its confidence prediction is greater than 0.95, while the proposed Dash algorithm selects unlabeled example based on a dynamic threshold through optimization iterations.(a) The proposed Dash selects more examples with correct pseudo labels than that of FixMatch. (b) The proposed Dash maintains much more examples with wrong pseudo labels at the beginning but it will drop off more examples with wrong pseudo labels after several epochs, comparing to FixMatch. et al., 2006]), and their combinations. With the image data augmentation technique, consistency regularization uses unlabeled data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>using prediction model w and a Comparison of fixed threshold and dynamic threshold. Fixed threshold is in the scale of negative log: ? log(0.95), dynamic threshold ? t = 1.0001? ?(t?1) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>examined the strategies of weighting different training unlabeled examples by solving a bi-level optimization problem. It is also a popular idea to select a subset of training examples from unlabeled examples for SSL. For example, FixMatch</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of top-1 testing error rates for different methods using Wide ResNet-28-2 for CIFAR-10, Wide ResNet-28-8 for CIFAR-100 (in %, mean ? standard deviation).</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Algorithm</cell><cell>40 labels</cell><cell cols="2">250 labels 4000 labels</cell><cell>400 labels</cell><cell cols="2">2500 labels 10000 labels</cell></row><row><cell>?-Model</cell><cell>-</cell><cell cols="2">54.26?3.97 14.01?0.38</cell><cell>-</cell><cell>57.25?0.48</cell><cell>37.88?0.11</cell></row><row><cell>Pseudo-Labeling</cell><cell>-</cell><cell cols="2">49.78?0.43 16.09?0.28</cell><cell>-</cell><cell>57.38?0.46</cell><cell>36.21?0.19</cell></row><row><cell>Mean Teacher</cell><cell>-</cell><cell cols="2">32.32?2.30 9.19?0.19</cell><cell>-</cell><cell>53.91?0.57</cell><cell>35.83?0.24</cell></row><row><cell>MixMatch</cell><cell cols="3">47.54?11.50 11.05?0.86 6.42?0.10</cell><cell>67.61?1.32</cell><cell>39.94?0.37</cell><cell>28.31?0.33</cell></row><row><cell>UDA</cell><cell>29.05?5.93</cell><cell>8.82?1.08</cell><cell>4.88?0.18</cell><cell>59.28?0.88</cell><cell>33.13?0.22</cell><cell>24.50?0.25</cell></row><row><cell>ReMixMatch</cell><cell>19.10?9.64</cell><cell>5.44?0.05</cell><cell cols="3">4.72?0.13 44.28?2.06 27.43?0.31</cell><cell>23.03?0.56</cell></row><row><cell>RYS (UDA)</cell><cell>-</cell><cell>5.53?0.17</cell><cell>4.75?0.28</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RYS (FixMatch)</cell><cell>-</cell><cell>5.05?0.12</cell><cell>4.35?0.06</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FixMatch (CTA)</cell><cell>11.39?3.35</cell><cell>5.07?0.33</cell><cell>4.31?0.15</cell><cell>49.95?3.01</cell><cell>28.64?0.24</cell><cell>23.18?0.11</cell></row><row><cell>Dash (CTA, ours)</cell><cell>9.16?4.31</cell><cell>4.78?0.12</cell><cell>4.13?0.06</cell><cell>44.83?1.36</cell><cell>27.85?0.19</cell><cell>22.77?0.21</cell></row><row><cell>FixMatch (RA)</cell><cell>13.81?3.37</cell><cell>5.07?0.65</cell><cell>4.26?0.05</cell><cell>48.85?1.75</cell><cell>28.29?0.11</cell><cell>22.60?0.12</cell></row><row><cell>Dash (RA, ours)</cell><cell>13.22?3.75</cell><cell cols="5">4.56?0.13 4.08?0.06 44.76?0.96 27.18?0.21 21.97?0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of top-1 testing error rates for different methods using Wide ResNet-28-2 for SVHN and Wide ResNet-37-2 for STL-10 (in %, mean ? standard deviation).</figDesc><table><row><cell></cell><cell></cell><cell>SVHN</cell><cell></cell><cell>STL-10</cell></row><row><cell>Algorithm</cell><cell>40 labels</cell><cell cols="3">250 labels 1000 labels 1000 labels</cell></row><row><cell>?-Model</cell><cell>-</cell><cell cols="3">18.96?1.92 7.54?0.36 26.23?0.82</cell></row><row><cell>Pseudo-Labeling</cell><cell>-</cell><cell cols="3">20.21?1.09 9.94?0.61 27.99?0.83</cell></row><row><cell>Mean Teacher</cell><cell>-</cell><cell>3.57?0.11</cell><cell cols="2">3.42?0.07 21.43?2.39</cell></row><row><cell>MixMatch</cell><cell cols="2">42.55?14.53 3.98?0.23</cell><cell cols="2">3.50?0.28 10.41?0.61</cell></row><row><cell>UDA</cell><cell cols="2">52.63?20.51 5.69?2.76</cell><cell>2.46?0.24</cell><cell>7.66?0.56</cell></row><row><cell>ReMixMatch</cell><cell>3.34?0.20</cell><cell>2.92?0.48</cell><cell>2.65?0.08</cell><cell>5.23?0.45</cell></row><row><cell>RYS (UDA)</cell><cell>-</cell><cell>2.45?0.08</cell><cell>2.32?0.06</cell><cell>-</cell></row><row><cell>RYS (FixMatch)</cell><cell>-</cell><cell>2.63?0.23</cell><cell>2.34?0.15</cell><cell>-</cell></row><row><cell>FixMatch (CTA)</cell><cell>7.65?7.65</cell><cell>2.64?0.64</cell><cell>2.36?0.19</cell><cell>5.17?0.63</cell></row><row><cell>Dash (CTA, ours)</cell><cell>3.14?1.60</cell><cell>2.38?0.29</cell><cell>2.14?0.09</cell><cell>3.96?0.25</cell></row><row><cell>FixMatch (RA)</cell><cell>3.96?2.17</cell><cell>2.48?0.38</cell><cell>2.28?0.11</cell><cell>7.98?1.50</cell></row><row><cell>Dash (RA, ours)</cell><cell>3.03?1.59</cell><cell cols="2">2.17?0.10 2.03?0.06</cell><cell>7.26?0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of top-1 testing error rates for different values of ? on CIFAR-10 (in %).</figDesc><table><row><cell>?</cell><cell>1.01</cell><cell>1.1</cell><cell>1.2</cell><cell>1.3</cell></row><row><cell cols="5">250 labels 4.85 4.76 4.99 4.82</cell></row><row><cell cols="5">4000 labels 4.39 4.28 4.11 4.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of top-1 testing error rates for PL and Dash with PL on CIFAR-10 (in %).</figDesc><table><row><cell>Algorithm</cell><cell>PL</cell><cell>Dash-PL</cell></row><row><cell cols="2">250 labels 49.78</cell><cell>46.90</cell></row><row><cell cols="2">4000 labels 16.09</cell><cell>15.59</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both UDA and ReMixMatch use crop and flip as "weak" augmentation while FixMatch uses flip and shift. 2 For "strong" augmentation, UDA uses RandAugment [Cubuk et al., 2020], ReMixMatch uses CTAugment [Cubuk et al., 2019], and FixMatch uses both.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since the authors did not name their algorithm, we use RYS to denote their algorithm for simplicity, where RYS is the combination of initials for last names of the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In our experiments, the FixMatch codebase is used: https://github.com/google-research/fixmatch 5 In practice, we use ?t = max{?t, 0.05}.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous ICML 2021 reviewers for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Furthermore, using Markov inequality, we have</p><p>where (a) uses the fact that F (w t ) ? ?? ?(t?1) . Using the property in (43), we can bound the size of A ? t by Hoeffding's inequality, i.e., with a probability 1 ? 2?, we have</p><p>where a 0 is defined in (34). Using the inequality in (23) we know</p><p>where (a) is due to C &gt; 1; (b) and (c) are due to F (w t ) ? ?? ?(t?1) . Using the property in (45), we can bound the expectation of |B ? t | given |B t |, i.e.,</p><p>We can bound the size of B ? t by a concentration inequality in Theorem 8 of <ref type="bibr" target="#b14">[Chung and Lu, 2006]</ref>, with a probability 1 ? 2?,</p><p>where b 0 is defined in (35). Therefore, we complete the proof of (40). As indicated in <ref type="formula">(44)</ref> and <ref type="formula">(47)</ref>, |A ? t | increases exponentially over iteration while |B ? t | remains upper bounded by a constant. It implies that our dynamically adjusted threshold help us select more and more examples from the unlabeled data that are relevant to the labeled data. In the same time, the number of mistakes we made in the selection process remain to be at most a constant. As a result, our optimization is able to make significant progress by using the selected examples from the unlabeled data. Below we will show that with a high probability, F (w t+1 ) ? ?? ?t , the key step of the inductive analysis. Finally, we will prove that with a probability 1 ? (4t + 1)?, we have</p><p>To this end, using the notation of A ? t and B ? t , we can rewrite g t as</p><p>where g a t = 1</p><p>Following the classical analysis of non-convex optimization, since F (w) is L-smooth by Assumption 1 (ii), we have</p><p>where (a) follows the update of w t+1 = w t ? ?g t ; (b) is due to the definition of g t and the convexity of ? 2 ; (c) follows from Assumption 2, Assumption 1 (i), and ?L ? 1. Since E ??P [?f (w; ?)] = ?F (w) and</p><p>, (Assumption 1 (i)) using concentration inequality in Lemma 4 of <ref type="bibr" target="#b22">[Ghadimi et al., 2016]</ref>, we have with a probability 1 ? 5?,</p><p>? 4G 2 ?a 0 m? t?1 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Document image defect models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structured Document Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A pac-style model for learning from labeled and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimally combining classifiers using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D?vid</forename><surname>P?l</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Conference on Computational Learning Theory</title>
		<meeting>Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<title level="m">Semi-Supervised Learning</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stability and generalization of learning algorithms that converge to global optima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic feature decomposition for single view co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="953" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On lazy training in differentiable programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2937" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Concentration inequalities and martingale inequalities: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="127" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Fabio G Cozman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">C</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cirelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Machine learning: the art and science of algorithms that make sense of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="267" to="305" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Safe deep semi-supervised learning for unseen-class unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan-Zhe</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
	</analytic>
	<monogr>
		<title level="m">Handbook on Neural Information Processing</title>
		<editor>Mohamed Farouk Abdel Hady and Friedhelm Schwenker</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="215" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08560</idno>
		<title level="m">Surprises in high-dimensional ridgeless least squares interpolation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unifying semi-supervised and robust learning by mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Hataya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Projected estimators for robust semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Krijthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="993" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Tronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving semi-supervised support vector machines through unlabeled instances selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards making unlabeled data never hurt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards making unlabeled data never hurt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="188" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning safe prediction for semi-supervised regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Wen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2217" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards safe weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan-Zhe</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="334" to="346" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A simple proximal stochastic gradient method for nonsmooth nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhize</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5564" to="5574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive pessimistic likelihood estimation for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="475" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Smooth discrimination analysis. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enno</forename><surname>Mammen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1808" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">350</biblScope>
			<biblScope unit="page" from="365" to="369" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Improvability through semi-supervised learning: a survey of theoretical results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Loog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09574</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning for object detectors from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3593" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gradient methods for minimizing functionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Teodorovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polyak</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zhurnal Vychislitel&apos;noi Matematiki i Matematicheskoi Fiziki</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="653" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Not all unlabeled data are equal: learning to weight data in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE Workshops on Application of Computer Vision</title>
		<meeting>the 7th IEEE Workshops on Application of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An rkhs for multi-view learning and manifold co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Optimal aggregation of classifiers in statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="166" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A random subspace method for co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si-Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Hua</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Joint Conference on Neural Networks</title>
		<meeting>IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A new analysis of co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1135" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spiderboost and momentum: Faster variance reduction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Tarokh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2403" to="2413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6256" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Active learning and semi-supervised learning for speech recognition: A unified framework using the global entropy reduction maximization criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="444" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bayesian co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mer</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stagewise training accelerates convergence of testing error over sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoning</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2604" to="2614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Small relu networks are powerful memorizers: a tight analysis of memorization capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15558" to="15569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Introduction to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="130" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
