<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Moving Object Detection via Contextual Information Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">UCLA Vision Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">UCLA Vision Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Moving Object Detection via Contextual Information Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This paper has been accepted for publication at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach (CA), 2019. c IEEE</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an adversarial contextual model for detecting moving objects in images. A deep neural network is trained to predict the optical flow in a region using information from everywhere else but that region (context), while another network attempts to make such context as uninformative as possible. The result is a model where hypotheses naturally compete with no need for explicit regularization or hyper-parameter tuning. Although our method requires no supervision whatsoever, it outperforms several methods that are pre-trained on large annotated datasets. Our model can be thought of as a generalization of classical variational generative region-based segmentation, but in a way that avoids explicit regularization or solution of partial differential equations at run-time. We publicly release all our code and trained networks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider <ref type="figure">Fig. 1</ref>: Even relatively simple objects, when moving in the scene, cause complex discontinuous changes in the image. Being able to rapidly detect independently moving objects in a wide variety of scenes from images is functional to the survival of animals and autonomous vehicles alike. We wish to endow artificial systems with similar capabilities, without the need to pre-condition or learn similar-looking backgrounds. This problem relates to motion segmentation, foreground/background separation, visual attention, video object segmentation as we discuss in Sect. <ref type="bibr" target="#b2">3</ref>. For now, we use the words "object" or "foreground" informally 2 to mean (possibly multiple) connected regions of the image domain, to be distinguished from their surrounding, which we call "background" or "context," according to some criterion.</p><p>Since objects exist in the scene, not in the image, a method to infer them from the latter rests on an operational 1 http://rpg.ifi.uzh.ch/unsupervised_detection.html *These two authors contributed equally.</p><p>Correspondence to yanchao.yang@cs.ucla.edu and loquercio@ifi.uzh.ch <ref type="bibr" target="#b1">2</ref> The precise meaning of these terms will be formalized in Sect. 2. <ref type="figure">Figure 1</ref>: An encounter between a hawk and a drone (top). The latter will not survive without being aware of the attack. Detecting moving objects is crucial to the survival of animal and artificial systems alike. Note that the optical flow (middle row) is quite diverse within the region where the hawk projects: It changes both in space and time. Grouping this into a moving object (bottom row) is our goal in this work. Note the object is detected by our algorithm across multiple scales, partial occlusions from the viewpoint, and complex boundaries.</p><p>definition based on measurable image correlates. We call moving objects regions of the image whose motion cannot be explained by that of their surroundings. In other words, the motion of the background is uninformative of the motion of the foreground and vice-versa. The "information separation" can be quantified by the information reduction rate (IRR) between the two as defined in Sect. 2. This naturally translates into an adversarial inference criterion that has close connections with classical variational regionbased segmentation, but with a twist: Instead of learning a generative model of a region that explains the image in that region as well as possible, our approach yields a model that tries to explain it as poorly as possible using measurements from everywhere else but that region. In generative model-based segmentation, one can always explain the image with a trivial model, the image itself. To avoid that, one has to impose model complexity bounds, bottlenecks or regularization. Our model does not have access to trivial solutions, as it is forced to predict a region without looking at it. What we learn instead is a contextual adversarial model, without the need for explicit regularization, where foreground and background hypotheses compete to explain the data with no pre-training nor (hyper)parameter selection. In this sense, our approach relates to adversarial learning and self-supervision as discussed in Sect. <ref type="bibr" target="#b2">3</ref>.</p><p>The result is a completely unsupervised method, unlike many recent approaches that are called unsupervised but still require supervised pre-training on massive labeled datasets and can perform poorly in contexts that are not well represented in the training set. Despite the complete lack of supervision, our method performs competitively even compared with those that use supervised pre-training (Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Contributions</head><p>Our method captures the desirable features of variational region-based segmentation: Robustness, lack of thresholds or tunable parameters, no need for training. However, it does not require solving a partial differential equation (PDE) at run-time, nor to pick regularizers or Lagrange multipliers, nor to restrict the model to one that is simpleenough to be tractable analytically. It also exploits the power of modern deep learning methods: It uses deep neural networks as the model class, optimizes it efficiently with stochastic gradient descent (SGD), and can be computed efficiently at run time. However, it requires no supervision whatsoever.</p><p>While our approach has close relations to both classical region-based variational segmentation and generative models, as well as modern deep learning-based self-supervision, discussed in detail in Sect. 3, to the best of our knowledge, it is the first adversarial contextual model to detect moving objects in images. It achieves better or similar performance compare to unsupervised methods on the three most common benchmarks, and it even edges out methods that rely on supervised pre-training, as described in Sect. 4. On one of the considered benchmarks, it outperforms all methods using supervision, which illustrates the generalizability of our approach. In Sect. 5 we describe typical failure modes and discuss limitations of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We call "moving object(s)" or "foreground" any region of an image whose motion is unexplainable from the context. A "region of an image" ? is a compact and multiplyconnected subset of the domain of the image, discretized into a lattice D. "Context" or "background" is the complement of the foreground in the image domain, ? c " Dz?. Given a measured image I and/or its optical flow to the next (or previous) image u, foreground and background are uncertain, and therefore treated as random variables. A random variable u 1 is "unexplainable" from (or "uninformed" by) another u 2 if their mutual information Ipu 1 ; u 2 q is zero, that is if their joint distribution equals the product of the marginals, P pu 1 , u 2 q " P pu 1 qP pu 2 q.</p><p>More specifically, the optical flow u : D 1 ? R 2 maps the domain of an image I 1 : D 1 ? R 3 onto the domain D 2 of I 2 , so that if x i P D 1 , then x i`ui P D 2 , where u i " upx i q up to a discretization into the lattice and cropping of the boundary. Ideally, if the brightness constancy constraint equation that defines optical flow was satisfied, we would have I 1 " I 2?u point-wise.</p><p>If we consider the flow at two locations i, j, we can formalize the notion of foreground as a region ? that is uninformed by the background:</p><formula xml:id="formula_0"># Ipu i , u j |Iq ? 0, i, j P ? Ipu i , u j |Iq " 0, i P ?, j P Dz?.<label>(1)</label></formula><p>As one would expect, based on this definition, if the domain of an object is included in another, then they inform each other (see appendix <ref type="bibr" target="#b41">[40]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Loss function</head><p>We now operationalize the definition of foreground into a criterion to infer it. We use the information reduction rate (IRR) ?, which takes two subsets x, y ? D as input and returns a non-negative scalar:</p><formula xml:id="formula_1">?px|y; Iq " Ipu x , u y |Iq Hpu x |Iq " 1?H pu x |u y , Iq Hpu x |Iq<label>(2)</label></formula><p>where H denotes (Shannon) entropy. It is zero when the two variables are independent, but the normalization prevents the trivial solution (empty set). <ref type="bibr" target="#b2">3</ref> As proven in the appendix <ref type="bibr" target="#b41">[40]</ref>, objects as we defined them are the regions that minimize the following loss function</p><formula xml:id="formula_2">Lp?; Iq " ?p?|? c ; Iq`?p? c |?; Iq.<label>(3)</label></formula><p>Note that L does not have a complexity term, or regularizer, as one would expect in most region-based segmentation methods. This is a key strength of our approach, that involves no modeling hyperparameters, as we elaborate on in Sect. 3. Tame as it may look, (3) is intractable in general. For simplicity we indicate the flow inside the region(s) ? (foreground) with u in " tu i , i P ?u, and similarly for u out , the flow in the background ? c . The only term that matters in the IRR is the ratio Hpu in |u out , Iq{Hpu in |Iq, which is ? log P pu in |u out , IqdP pu in |u out , Iq ? log P pu in |IqdP pu in |Iq (4) that measures the information transfer from the background to the foreground. This is minimized when knowledge of   <ref type="figure">Figure 3</ref>: The two diagrams illustrate the learning process of the mask generator (G), after the inpainter (I) has learned how to accurately inpaint a masked flow. The upper diagram shows a poorly trained mask generator which does not precisely detect the object. Due to the imprecise detection, the inpainter can observe part of the object's flow, and perform an accurate reconstruction. At the same time, the inpainter partially observes the background's flow in the complementary mask. Consequently, it can precisely predict missing parts of the background's flow. In contrast, the lower diagram shows a fully trained mask generator which can precisely tell apart the object from the background. In this case, the inpainter observes the flow only outside the object and has no information to predict the flow inside it. At initialization time the inpainter does not know the conditionals to inpaint masked flows. Therefore, we propose to train both the generator and the inpainter jointly in an adversarial manner (see Sect. 2). the background flow is sufficient to predict the foreground. To enable computation, we have to make draconian, yet common, assumptions on the underlying probability model, namely that P pu in " x|Iq 9 exp??} x} 2 ? 2?( 5) P pu in " x|u out " y, Iq 9 exp??} x??p?, y, Iq} 2 ? 2?</p><p>here ?p?, y, Iq " ? u in dP pu in |u out , Iq is the conditional mean given the image and the complementary observation.</p><p>Here we assume ?p?, H, Iq " 0, since given a single image the most probable guess of the flow is zeros. With these assumptions, (4) can be simplified, to</p><formula xml:id="formula_3">? }u in?? p?, u out , Iq} 2 dP pu in |u out , Iq ? }u in } 2 dP pu in |Iq ? ? ? N i"1 }u i in?? p?, u i out , Iq} 2 ? N i"1 }u i in } 2<label>(6)</label></formula><p>where N " |D| is the cardinality of D, or the number of flow samples available. Finally, our loss (3) to be minimized can be approximated as</p><formula xml:id="formula_4">Lp?; Iq " 1?? N i"1 }u i in?? p?, u i out , Iq} 2 ? N i"1 }u i in } 2` `1?? N i"1 }u i out?? p? c , u i in , Iq} 2 ? N i"1 }u i out } 2` .<label>(7)</label></formula><p>In order to minimize this loss, we have to choose a representation for the unknown region ? and for the function ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Function class</head><p>The region ? that minimizes <ref type="bibr" target="#b6">(7)</ref> belongs to the power set of D, that is the set of all possible subsets of the image domain, which has exponential complexity. <ref type="bibr" target="#b3">4</ref> We represent it with the indicator function</p><formula xml:id="formula_5">? : D ? t0, 1u i ? ? 1 if i P ?; 0 otherwise<label>(8)</label></formula><p>so that the flow inside the region ? can be written as u i in " ?u i , and outside as u i out " p1??qu i . Similarly, the function ? is non-linear, non-local, and high-dimensional, as it has to predict the flow in a region of the image of varying size and shape, given the flow in a different region. In other words, ? has to capture the context of a region to recover its flow.</p><p>Characteristically for the ages, we choose both ? and ? to be in the parametric function class of deep convolutional neural networks, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the specifics of which are in Sect. 4.1. We indicate the parameters with w, and the corresponding functions ? w1 and ? w2 . Accordingly, after discarding the constants, the negative loss <ref type="formula" target="#formula_4">(7)</ref> can be written as a function of the parameters</p><formula xml:id="formula_6">Lpw 1 , w 2 ; Iq " ? i }? w2 pu i??w1 p? w2 , u i out , Iqq} 2 ? i }u i in } 2 ? i }p1?? w2 qpu i??w1 p1?? w2 , u i in , Iq} 2 ? i }u i out } 2<label>(9)</label></formula><p>? w1 is called the inpainter network, and must be chosen to minimize the loss above. At the same time, the region ?, represented by the parameters w 2 of its indicator function ? w2 called mask generator network, should be chosen so that u out is as uninformative as possible of u in , and therefore the same loss is maximized with respect to w 2 . This naturally gives rise to a minimax problem:</p><formula xml:id="formula_7">w " arg min w1 max w2 Lpw 1 , w 2 ; Iq.<label>(10)</label></formula><p>This loss has interesting connections to classical regionbased segmentation, but with a twist as we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>To understand the relation of our approach to classical methods, consider the simplest model for region-based segmentation <ref type="bibr" target="#b7">[8]</ref> </p><formula xml:id="formula_8">Lp?, c i , c o q " ? ? |u in pxq?c i | 2 dx`? ? c |u out pxq?c o | 2 dx (11)</formula><p>typically combined with a regularizing term, for instance the length of the boundary of ?. This is a convex infinitedimensional optimization problem that can be solved by numerically integrating a partial differential equation (PDE). The result enjoys significant robustness to noise, provided the underlying scene has piecewise constant radiance and is measured by image irradiance, to which it is related by a simple "signal-plus-noise" model. Not many scenes of interest have piecewise constant radiance, although this method has enjoyed a long career in medical image analysis. If we enrich the model by replacing the constants c i with smooth functions, ? i pxq, we obtain the celebrated Mumford-Shah functional <ref type="bibr" target="#b26">[25]</ref>, also optimized by integrating a PDE. Since smooth functions are an infinitedimensional space, regularization is needed, which opens the Pandora box of regularization criteria, not to mention hyperparameters: Too much regularization and details are missed; too little and the model gets stuck in noise-induced minima. A modern version of this program would replace ?pxq with a parametrized model ? w pxq, for instance a deep neural network with weights w pre-trained on a dataset D. In this case, the loss is a function of w, with natural model complexity bounds. Evaluating ? w at a point inside, x P ?, requires knowledge of the entire function u inside ?, which we indicate with ? w px, u in q:</p><formula xml:id="formula_9">? ? |u in pxq?? w px, u in q| 2 dx`? ? c |u out pxq?? w px, u out q| 2 dx.<label>(12)</label></formula><p>Here, a network can just map ? w px, u in q " u in providing a trivial solution, avoided by introducing (architectural or information) bottlenecks, akin to explicit regularizers. We turn the table around and use the outside to predict the inside and vice-versa:</p><formula xml:id="formula_10">? ? |u in pxq?? w px, u out q| 2 dx`? ? c |u out pxq?? w px, u in q| 2 dx (13)</formula><p>After normalization and discretization, this leads to our loss function <ref type="bibr" target="#b6">(7)</ref>. The two regions compete: for one to grow, the other has to shrink. In this sense, our approach relates to region competition methods, and specifically Motion Competition <ref type="bibr" target="#b13">[12]</ref>, but also to adversarial training, since we can think of ? as the "discriminator" presented in a classification problem (GAN <ref type="bibr" target="#b0">[1]</ref>), reflected in the loss function we use. This also relates to what is called "self-supervised learning," a misnomer since there is no supervision, just a loss function that does not involve externally annotated data. Several variants of our approach can be constructed by using different norms, or correspondingly different models for the joint and marginal distributions <ref type="bibr" target="#b4">(5)</ref>.</p><p>More broadly, the ability to detect independently moving objects is primal, so there is a long history of motion-based segmentation, or moving object detection. Early attempts to explicitly model occlusions include the layer model <ref type="bibr" target="#b39">[38]</ref> with piecewise affine regions, with computational complexity improvements using graph-based methods <ref type="bibr" target="#b31">[30]</ref> and variational inference <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b44">43]</ref> to jointly optimize for motion estimation and segmentation; <ref type="bibr" target="#b27">[26]</ref> use of long-term temporal consistency and color constancy, making however the optimization more difficult and sensitive to parameter choices. Similar ideas were applied to motion detection in crowds <ref type="bibr" target="#b4">[5]</ref>, traffic monitoring <ref type="bibr" target="#b3">[4]</ref> and medical image analysis <ref type="bibr" target="#b15">[14]</ref>. Our work also related to the literature on visual attention <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>More recent data-driven methods <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b32">31]</ref> learn discriminative spatio-temporal features and differ mainly for the type of inputs and architectures. Inputs can be either image pairs <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b10">9]</ref> or image plus dense optical flow <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b36">35]</ref>. Architectures can be either time-independent <ref type="bibr" target="#b36">[35]</ref>, or with recurrent memory <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b32">31]</ref>. Overall, those methods outperform traditional ones on benchmark datasets <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b30">29]</ref>, but at the cost of requiring a large amount of labeled training data and with evidence of poor generalization to previously unseen data.</p><p>It must be noted that, unlike in Machine Learning at large, it is customary in video object segmentation to call "unsupervised" methods that do rely on massive amounts of manually annotated data, so long as they do not require manual annotation at run-time. We adopt the broader use of the term where unsupervised means that there is no supervision of any kind both at training and test time.</p><p>Like classical variational methods, our approach does not need any annotated training data. However, like modern learning methods, our approach learns a contextual model, which would be impossible to engineer given the complexity of image formation and scene dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare our approach to a set of state-of-the-art baselines on the task of video object segmentation to evaluate the accuracy of detection. We first present experiments on a controlled toy-example, where the assumptions of our model are perfectly satisfied. The aim of this experiment is to get a sense of the capabilities of the presented approach in ideal conditions. In the second set of experiments, we evalu-ate the effectiveness of the proposed model on three public, widely used datasets: Densely Annotated VIdeo Segmentation (DAVIS) <ref type="bibr" target="#b30">[29]</ref>, Freiburg-Berkeley Motion Segmentation (FBMS59) <ref type="bibr" target="#b27">[26]</ref>, and SegTrackV2 <ref type="bibr" target="#b38">[37]</ref>. Provided the high degree of appearance and resolution differences between them, these datasets represent a challenging benchmark for any moving object segmentation method. While the DAVIS dataset has always a single object per scene, FBMS and SegTrackV2 scenes can contain multiple objects per frame. We show that our method not only outperforms the unsupervised approaches, but even edges out other supervised algorithms that, in contrast to ours, have access to a large amount of labeled data with precise manual segmentation at training time. For quantitative evaluation, we employ the most common metric for video object segmentation , i.e. the mean Jaccard score, a.k.a. intersection-overunion score, J . Given space constraints, we add additional evaluation metrics in the appendix <ref type="bibr" target="#b41">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and Networks Details</head><p>Generator, G: Depicted on the left of <ref type="figure">Fig. 3</ref>, the generator architecture is a shrunk version of SegNet <ref type="bibr" target="#b1">[2]</ref>. Its encoder part consists of 5 convolutional layers each followed by batch normalization, reducing the input image to <ref type="bibr">1 4</ref> of its original dimensions. The encoder is followed by a set of 4 atrous convolutions with increasing radius <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">16)</ref>. The decoder part consists of 5 convolutional layers, that, with upsampling, generate an output with the same size of the input image. As in SegNet <ref type="bibr" target="#b1">[2]</ref>, a final softmax layer generates the probabilities for each pixel to be foreground or background. The generator input consists of an RGB image I t and the optical flow u t:t`?T between I t and I t`?T , to introduce more variations in the optical flows conditioned on image I t . At training time, ?T is randomly sampled from the uniform distribution U " r?5, 5s, with ?T ? 0. The optical flow u t:t`?T is generated with the pretrained PWC network <ref type="bibr" target="#b34">[33]</ref>, given its state-of-the-art accuracy and efficiency. The generator network has a total of 3.4M parameters.</p><p>Inpainter, I: We adapt the architecture of CPN <ref type="bibr" target="#b42">[41]</ref> to build our inpainter network. Its structure is depicted on the right of <ref type="figure">Fig. 3</ref>. The input to this network consists of the input image I t and the flow masked according to the generator output, ?u, the latter concatenated with ?, to make the inpainter aware of the region to look for context. Differently from the CPN, these two branches are balanced, and have the same number of parameters. The encoded features are then concatenated and passed to the CPN decoder, that outputs an optical flow? " ?p?, p1??qu, I t q of the same size of the input image, whose inside is going to be used for the difference between u in and the recovered flow inside. Similarly, we can run the same procedure for the complement part. Our inpainter network has a total of 1.5M parameters.</p><p>At test time, only the generator G is used. Given I t DAVIS <ref type="bibr" target="#b30">[29]</ref> FBMS59 <ref type="bibr" target="#b27">[26]</ref> SegTrackV2 <ref type="bibr" target="#b38">[37]</ref> J ? 92.5 88.5 92.1 <ref type="table">Table 1</ref>: Performance under ideal conditions: When the assumptions made by our model are fully satisfied, our approach can successfully detect moving objects.. Indeed, our model reaches near maximum Jaccard score in all considered datasets. and u t:t`?T , it outputs a probability for each pixel to be foreground or background, P t p?T q. To encourage temporal consistency, we compute the temporal average:</p><formula xml:id="formula_11">P t " ?T "5 ? ?T "?5,?0 P t p?T q<label>(14)</label></formula><p>The final mask ? is generated with a CRF <ref type="bibr" target="#b22">[21]</ref> postprocessing step on the final P t . More details about the postprocessing can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments in Ideal Conditions</head><p>Our method relies on basic, fundamental assumptions: The optical flow of the foreground and of the background are independent. To get a sense of the capabilities of our approach in ideal conditions, we artificially produce datasets where this assumption is fully satisfied. The datasets are generated as a modification of DAVIS2016 <ref type="bibr" target="#b30">[29]</ref>, FMBS <ref type="bibr" target="#b27">[26]</ref>, and SegTrackV2 <ref type="bibr" target="#b38">[37]</ref>. While images are kept unchanged, ground truth masks are used to artificially perturb the optical flow generated by PWC <ref type="bibr" target="#b34">[33]</ref> such that foreground and background are statistically independent. More specifically, a different (constant) optical flow field is sampled from a uniform distribution independently at each frame, and associated to the foreground and the background, respectively. More details about the generation of those datasets and the visual results can be found in the Appendix. As it is possible to observe in <ref type="table">Table 1</ref>, our method reaches very high performance in all considered datasets. This confirms the validity of our algorithm and that our loss function <ref type="formula" target="#formula_0">(10)</ref> is a valid and tractable approximation of the functional (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance on Video Object Segmentation</head><p>As previously stated, we use the term Unsupervised with a different meaning with respect to its definition in literature of video object segmentation. In our definition and for what follows, the supervision refers to the algorithm's usage of ground truth object annotations at training time. In contrast, the literature usually defines methods as semisupervised, if at test time they assume the ground-truth segmentation of the first frame to be known <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">24]</ref>. This could be posed as tracking problem <ref type="bibr" target="#b43">[42]</ref> since the detection of the target is human generated. Instead, here we focus on moving object detection and thus we compare our approach to the methods that are usually referred to as "unsupervised" in the video object segmentation domain. However we make further differentiation on whether the ground truth object segmentation is needed (supervised) or not (truly unsupervised) during training.</p><p>In this section we compare our method with other 8 methods that represent the state of the art for moving object segmentation. For comparison, we use the same metric defined above, which is the Jaccard score J between the real and predicted masks. <ref type="table">Table 2</ref> shows the performance of our method and the baseline methods on three popular datasets, DAVIS2016 <ref type="bibr" target="#b30">[29]</ref>, FBMS59 <ref type="bibr" target="#b27">[26]</ref> and SegTrackV2 <ref type="bibr" target="#b38">[37]</ref>. Our approach is top-two in each of the considered datasets, and even outperforms baselines that need a large amount of labelled data at training time, i.e. FSEG <ref type="bibr" target="#b18">[17]</ref>.</p><p>As can be observed in <ref type="table">Table 2</ref>, unsupervised baselines typically perform well in one dataset but significantly worse in others. For example, despite being the best performing unsupervised method on DAVIS2016, the performance of ARP <ref type="bibr" target="#b21">[20]</ref> drops significantly in the FBMS59 <ref type="bibr" target="#b27">[26]</ref> and Seg-TrackV2 <ref type="bibr" target="#b27">[26]</ref> datasets. ARP outperforms our method by 6.5% on DAVIS, however, our method outperforms ARP by 6.3% and 8.4%, on FBMS59 and SegTrackV2 respectively. Similarly, NLC <ref type="bibr" target="#b16">[15]</ref> and SAGE <ref type="bibr" target="#b40">[39]</ref> are extremely competitive in the Segtrack and FBMS59 benchmarks, respectively, but not in others. NLC outperforms us on SegTrackV2 by 8.4%, however we outperform NLC by 29.8% and 24.7%, on DAVIS and FBMS respectively.</p><p>It has been established that being second-best in multiple benchmarks is more indicative of robust performance than being best in one <ref type="bibr" target="#b28">[27]</ref>. Indeed, existing unsupervised approaches for moving object segmentation are typically highly-engineered pipeline methods which are tuned on one dataset but do not necessarily generalize to others. Also, consisting of several computationally intensive steps, extant unsupervised methods are generally orders of magnitude slower than our method <ref type="table">(Table 3)</ref>.</p><p>Interestingly, a similar pattern is observable for supervised methods. This is particularly evident on the Seg-TrackV2 dataset <ref type="bibr" target="#b38">[37]</ref>, which is particularly challenging since several frames have very low resolution and are motion blurred. Indeed, supervised methods have difficulties with the covariate shift due to changes in the distribution between training and testing data. Generally, supervised methods alleviate this problem by pre-training on image segmentation datasets, but this solution clearly does not scale to every possible case. In contrast, our method can be finetuned on any data without the need for the latter to be annotated. As a result, our approach outperforms the majority of unsupervised methods as well as all the supervised ones, in PDB <ref type="bibr" target="#b32">[31]</ref> FSEG <ref type="bibr" target="#b18">[17]</ref> LVO <ref type="bibr" target="#b37">[36]</ref> ARP <ref type="bibr" target="#b21">[20]</ref> FTS <ref type="bibr" target="#b29">[28]</ref> NLC <ref type="bibr" target="#b16">[15]</ref>   <ref type="table">Table 2</ref>: Moving Object Segmentation Benchmarks: We compare our approach with 8 different baselines on the task of moving object segmentation. In order to do so, we use three popular datasets, i.e. DAVIS2016 <ref type="bibr" target="#b30">[29]</ref>, FBMS59 <ref type="bibr" target="#b27">[26]</ref>, and SegTrackV2 <ref type="bibr" target="#b38">[37]</ref>. Methods in blue require ground truth annotations at training time and are pre-trained on image segmentation datasets. In contrast, methods in red are unsupervised and not require any ground-truth annotation. Our approach is top-two in all the considered benchmarks, comparing to the other unsupervised methods. Bold indicates best among all methods, while Bold Red and red represent the best and second best for unsupervised methods, respectively.</p><p>terms of segmentation quality and training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative experiments and Failure Cases</head><p>In <ref type="figure">Fig. 4</ref> we show a qualitative comparison of the detection generated by our and others' methods on the DAVIS dataset. Our algorithm can segment precisely the moving object regardless of cluttered background, occlusions, or large depth discontinuities. The typical failure case of our method is the detection of objects whose motion is due to the primary object. An example is given in the last row of <ref type="figure">Fig. 4</ref>, where the water moved by the surfer is also classified as foreground by our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training and Runtime Analysis</head><p>The generator and inpainter network's parameters are trained at the same time by minimizing the functional (10). The optimization time is approximately 6 hours on a single GPU Nvidia Titan XP 1080i. Since both our generator and inpainter networks are relatively small, we can afford very fast training/finetuning times. This stands in contrast to larger modules, e.g. PDB <ref type="bibr" target="#b32">[31]</ref>, that require up to 40 hrs of training.</p><p>At test time, predictions P t (defined in eq. 14) are generated at 3.15 FPS, or with an average time of 320ms per frame, including the time to compute optical flow with PWC <ref type="bibr" target="#b34">[33]</ref>. Excluding the time to generate optical flow, our model can generate predictions at 10.2 FPS, or 98ms per frame. All previous timings do not include the CRF postprocessing step. <ref type="table">Table 3</ref> compares the inference time of our method with respect to other unsupervised methods. Since our method at test time requires only a pass through a relatively shallow network, it is orders of magnitude faster than other unsupervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our definition of objects and the resulting inference criterion are related to generative model-based segmentation and region-based methods popular in the nineties. However, there is an important difference: Instead of using the evidence inside a region to infer a model of that region which is as accurate as possible, we use evidence everywhere else but that region to infer a model within the region, and we seek the model to be as bad as possible. This relation, explored in detail in Sect. 3, forces learning a contextual model of the image, which is not otherwise the outcome of a generative model in region-based segmentation. For instance, if we choose a rich enough model class, we can trivially model the appearance of an object inside an image region as the image itself. This is not an option in our model: We can only predict the inside of a region by looking outside of it. This frees us from having to impose modeling assumptions to avoid trivial solutions, but requires a much richer class of function to harvest contextual information.</p><p>This naturally gives rise to an adversarial (min-max) optimization: An inpainter network, as a discriminator, tries to hallucinate the flow inside from the outside, with the reconstruction error as a quality measure of the generator network, which tries to force the inpainter network to do the lousiest possible job.</p><p>The strengths of our approach relate to its ability to learn complex relations between foreground and background without any annotation. This is made possible by using modern deep neural network architectures like Seg-Net <ref type="bibr" target="#b1">[2]</ref> and CPN <ref type="bibr" target="#b42">[41]</ref> as function approximators.</p><p>Not using ground-truth annotations can be seen as a strength but also a limitation: If massive datasets are available, why not use them? In part because even massive is not large enough: We have shown that models trained on large amount of data still suffer performance drops whenever tested on a new benchmark significantly different from the training ones. Moreover, our method does not require any pre-training on large image segmentation datasets, and it can adapt to any new data, since it does not require any supervision. This adaptation ability is not only important for computer vision tasks, but can also benefit other applications, e.g. robotic navigation <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b14">13]</ref> or manipulation <ref type="bibr" target="#b20">[19]</ref>.</p><p>Another limitation of our approach is that, for the task of ARP <ref type="bibr" target="#b21">[20]</ref> FTS <ref type="bibr" target="#b29">[28]</ref> NLC <ref type="bibr" target="#b16">[15]</ref>   <ref type="table">Table 3</ref>: Run-time analysis: Our method is not only effective (top-two in each considered dataset), but also orders of magnitude faster than other unsupervised methods. All timings are indicated without optical flow computation.</p><p>GT SFL <ref type="bibr" target="#b11">[10]</ref> LMP <ref type="bibr" target="#b36">[35]</ref> PDB <ref type="bibr" target="#b32">[31]</ref> CVOS <ref type="bibr" target="#b35">[34]</ref> FTS <ref type="bibr" target="#b29">[28]</ref> ELM <ref type="bibr" target="#b23">[22]</ref> Ours <ref type="figure">Figure 4</ref>: Qualitative Results: We qualitatively compare the performance of our approach with several state-of-the-art baselines as well as the Ground-Truth (GT) mask. Our prediction are robust to background clutter, large depth discontinuities and occlusions. The last row shows a typical failure case of our method, i.e. objects which are moved by the primary objects are detected as foreground (water is moved by the surfer in this case).</p><p>motion-based segmentation, we require the optical flow between subsequent frames. One could argue that optical flow is costly, local, and error-prone. However, our method is general and could be applied to other statistics than optical flow. Such extensions are part of our future work agenda. In addition, our approach does not fully exploit the intensity image, although we use it as a conditioning factor for the inpainter network. An optical flow or an image can be ambiguous in some cases, but the combination of the two is rarely insufficient for recognition <ref type="bibr" target="#b44">[43]</ref>. Again, our framework allows in theory exploitation of both, and in future work we intend to expand in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>Notation ? ? D: ? is a subset of the image domain D. i P ?: pixel i in region ?.</p><p>u ? : optical flow u restricted to region ?. Ipu i , u j |Iq: conditional mutual information between u i and u j given image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">More on the definition of objects</head><p>Our definition Eq. (1) of object is in terms of mutual information between optical flows restricted to different regions. It is helpful to analyze some simple cases that follow directly from the definition. statement 1: a subset of an object informs the remaining part of this object. If the object is ?, and there is a subset? ? ?, suppose i P?, j P ?z? respectively, then: Ipu?, u ?z? |Iq ? Ipu i , u ?z? |Iq ? Ipu i , u j |Iq ? 0 by Eq. (1). statement 2: a subset of the foreground does not inform a subset of the background. Suppose ? is the foreground, if ? ? ?, and ? 1 ? Dz?, then Ipu?, u ? 1 |Iq " 0. Otherwise, we can find at least two pixels i P?, j P ? 1 such that Ipu i , u j |Iq ? 0, which is contradictory to definition Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">The optimality and uniqueness of objects</head><p>In the main paper, we formalize the notion of foreground as a region ? that is uninformed by the background, see Eq. (1). Objects as we defined them in Eq. (1) are the regions that minimize the loss function Eq. (3).</p><p>Proof: First we show that the estimate ??right on the object achieves the minimum value of the loss function, since:</p><p>Lp??; Iq " ?p??|Dz??; Iq`?pDz??|??; Iq</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"</head><p>Ipu ??, u Dz??| Iq Hpu ??| Iq`I pu Dz??, u ??| Iq Hpu Dz??| Iq " 0 (15) by statement <ref type="formula" target="#formula_1">(2)</ref> above. Thus ??achieves the minimum value of the loss Eq. (3). Now we need to show that ?i s unique, for which, we just need to check the following two mutually exclusive and collectively inclusive cases for ? ? ??(note that LpH; Iq " LpD; Iq " 1.0 as 0 ? ! 1 is added to the denominator):</p><p>?? is either a subset of foreground or a subset of background:? X Dz??" H or? X ??" H.</p><p>?? is neither a subset of foreground nor a subset of background:? X Dz??? H and? X ??? H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In both cases Lp?;</head><p>Iq is strictly larger than 0 with some set operations under statements (1,2) above. Thus the object satisfies the definition Eq. (1) is a unique optima of the loss Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Extra quantitative evaluations</head><p>The performance of our algorithm compared with stateof-the-art systems on DAVIS2016 <ref type="bibr" target="#b30">[29]</ref> can be found in <ref type="table" target="#tab_3">Table 4</ref>. The metrics used to perform the quantitative evaluation are the Jaccard score J , and the mean boundary measure F. For more details see <ref type="bibr" target="#b30">[29]</ref>. According to the same metrics, we also provide the per-category score for DAVIS and FBMS59 in <ref type="table" target="#tab_4">Table 5</ref> and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Details on CRF</head><p>We use the online implementation 5 of the CRF algorithm <ref type="bibr" target="#b22">[21]</ref> for post-processing the mask P t . We only use 5 https://github.com/lucasb-eyer/pydensecrf the pairwise bilateral potential with the parameters: sxy=25, srgb=5, compat=5 as defined in the corresponding function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Experiments in ideal conditions</head><p>The experiments under ideal conditions have been specifically designed to test the performance of our algorithm when its assumptions are fully satisfied. In particular, the derivation presented in Section 2 assumes that the foreground and the background motion are completely independent given the image. In real datasets, this conditions is not always true: during the generation of the benchmark datasets, many times the camera tracks the moving object, creating a relation between the two motions. To analyze the performance of our method when background and foreground are completely independent, we artificially modify our considered benchmark datasets. To generate an ideal sample, we proceed as follows: We first take two image, I t and I t`?T , with ?T P Ur?5, 5s. Then, we generate the optical flow between the two frame u t:t`?T using PWC Net <ref type="bibr" target="#b34">[33]</ref>. Now, using the ground-truth mask, we artificially add to the latter a random optical flow, different between the foreground and background. The random optical flows are generated from a rotation r P Ur?1, 1s radians and translations t x , t y P Ur?30, 30s pixels. Since the ground-truth mask is used to generate the flow, in several case a solution is easy to find (last column of <ref type="figure" target="#fig_1">Fig. 5 and Fig. 6</ref>). However, as the original optical flow can be complex, it is not always possible to easily observe the mask in the flow (first three columns of <ref type="figure" target="#fig_1">Fig. 5 and Fig. 6</ref>). Nonetheless, since the background and foreground are fully independent, our algorithm can accurately segment the objects.</p><p>PDB <ref type="bibr" target="#b32">[31]</ref> LVO <ref type="bibr" target="#b37">[36]</ref> FSEG <ref type="bibr" target="#b18">[17]</ref> LMP <ref type="bibr" target="#b36">[35]</ref> ARP <ref type="bibr" target="#b21">[20]</ref>     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>During training, our method entails two modules. One is the generator (G) which produces a mask of the object by looking at the image and the associated optical flow. The other module is the inpainter (I) which tries to inpaint back the optical flow masked out by the corresponding mask. Both modules employ the encoder-decoder structure with skip connections. However, the inpainter (I) is equipped with two separate encoding branches. See Sect. 4.1 for network details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Experiment on FBMS in ideal conditions: The first row shows some samples of input images, the second row their idealized optical flows, and the latter row the segmentation generated by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Experiment on SegTrackV2 in ideal conditions: The first row shows samples of input images, the second row the idealized optical flows, and the latter row the segmentation generated by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>SFL<ref type="bibr" target="#b11">[10]</ref> FTS<ref type="bibr" target="#b29">[28]</ref> NLC<ref type="bibr" target="#b16">[15]</ref> Ours Performance on DAVIS2016<ref type="bibr" target="#b30">[29]</ref>: Methods in blue require ground truth annotations at training time, while the ones in red are fully unsupervised. Our approach reaches comparable performance to the state-of-the-art, outperforming several supervised methods.Category J mean ? J recall ? J decay ? F mean ? F recall ? F decay ?</figDesc><table><row><cell>J mean ?</cell><cell>77.2</cell><cell>75.9</cell><cell>70.7</cell><cell>70.0</cell><cell>76.2</cell><cell>67.4</cell><cell>55.8</cell><cell>55.1</cell><cell>71.5</cell></row><row><cell>J recall ?</cell><cell>90.1</cell><cell>89.1</cell><cell>83.5</cell><cell>85.0</cell><cell>91.1</cell><cell>81.4</cell><cell>64.9</cell><cell>55.8</cell><cell>86.5</cell></row><row><cell>J decay ?</cell><cell>0.9</cell><cell>0.0</cell><cell>1.5</cell><cell>1.3</cell><cell>7.0</cell><cell>6.2</cell><cell>0.0</cell><cell>12.6</cell><cell>9.5</cell></row><row><cell>F mean ?</cell><cell>74.5</cell><cell>72.1</cell><cell>65.3</cell><cell>65.9</cell><cell>70.6</cell><cell>66.7</cell><cell>51.1</cell><cell>52.3</cell><cell>70.5</cell></row><row><cell>F recall ?</cell><cell>84.4</cell><cell>83.4</cell><cell>73.8</cell><cell>79.2</cell><cell>83.5</cell><cell>77.1</cell><cell>51.6</cell><cell>51.9</cell><cell>83.5</cell></row><row><cell>F decay ?</cell><cell>-0.2</cell><cell>1.3</cell><cell>1.8</cell><cell>1.5</cell><cell>7.9</cell><cell>5.1</cell><cell>2.9</cell><cell>11.4</cell><cell>7.0</cell></row><row><cell></cell><cell>blackswan</cell><cell></cell><cell>69.1</cell><cell>100.0</cell><cell>25.2</cell><cell>72.4</cell><cell>100.0</cell><cell>23.6</cell><cell></cell></row><row><cell></cell><cell>bmx-trees</cell><cell></cell><cell>59.2</cell><cell>74.4</cell><cell>12.6</cell><cell>84.1</cell><cell>100.0</cell><cell>-7.4</cell><cell></cell></row><row><cell></cell><cell>breakdance</cell><cell></cell><cell>82.4</cell><cell>100.0</cell><cell>-0.6</cell><cell>84.1</cell><cell>100.0</cell><cell>-3.3</cell><cell></cell></row><row><cell></cell><cell>camel</cell><cell></cell><cell>83.0</cell><cell>100.0</cell><cell>4.6</cell><cell>83.0</cell><cell>100.0</cell><cell>0.7</cell><cell></cell></row><row><cell></cell><cell>car-roundabout</cell><cell></cell><cell>87.6</cell><cell>100.0</cell><cell>-0.8</cell><cell>76.5</cell><cell>98.6</cell><cell>-5.5</cell><cell></cell></row><row><cell></cell><cell>car-shadow</cell><cell></cell><cell>78.6</cell><cell>100.0</cell><cell>15.4</cell><cell>74.3</cell><cell>100.0</cell><cell>6.2</cell><cell></cell></row><row><cell></cell><cell>cows</cell><cell></cell><cell>85.4</cell><cell>100.0</cell><cell>0.6</cell><cell>79.6</cell><cell>98.0</cell><cell>-1.3</cell><cell></cell></row><row><cell></cell><cell>dance-twirl</cell><cell></cell><cell>79.0</cell><cell>95.5</cell><cell>1.2</cell><cell>82.4</cell><cell>100.0</cell><cell>9.9</cell><cell></cell></row><row><cell></cell><cell>dog</cell><cell></cell><cell>80.0</cell><cell>100.0</cell><cell>10.2</cell><cell>76.1</cell><cell>94.8</cell><cell>17.8</cell><cell></cell></row><row><cell></cell><cell>drift-chicane</cell><cell></cell><cell>62.0</cell><cell>80.0</cell><cell>10.9</cell><cell>76.1</cell><cell>90.0</cell><cell>18.4</cell><cell></cell></row><row><cell></cell><cell>drift-straight</cell><cell></cell><cell>67.9</cell><cell>87.5</cell><cell>16.9</cell><cell>57.7</cell><cell>54.2</cell><cell>43.0</cell><cell></cell></row><row><cell></cell><cell>goat</cell><cell></cell><cell>26.9</cell><cell>17.0</cell><cell>-17.7</cell><cell>34.4</cell><cell>17.0</cell><cell>-14.7</cell><cell></cell></row><row><cell></cell><cell>horsejump-high</cell><cell></cell><cell>79.6</cell><cell>100.0</cell><cell>15.6</cell><cell>87.6</cell><cell>100.0</cell><cell>9.4</cell><cell></cell></row><row><cell></cell><cell>kite-surf</cell><cell></cell><cell>23.9</cell><cell>0.0</cell><cell>6.7</cell><cell>44.0</cell><cell>20.8</cell><cell>-5.2</cell><cell></cell></row><row><cell></cell><cell>libby</cell><cell></cell><cell>76.5</cell><cell>91.5</cell><cell>19.1</cell><cell>92.2</cell><cell>100.0</cell><cell>2.7</cell><cell></cell></row><row><cell></cell><cell>motocross-jump</cell><cell></cell><cell>70.6</cell><cell>78.9</cell><cell>-1.6</cell><cell>53.4</cell><cell>57.9</cell><cell>-7.0</cell><cell></cell></row><row><cell></cell><cell cols="2">paragliding-launch</cell><cell>72.7</cell><cell>100.0</cell><cell>25.0</cell><cell>44.3</cell><cell>43.6</cell><cell>33.3</cell><cell></cell></row><row><cell></cell><cell>parkour</cell><cell></cell><cell>83.4</cell><cell>98.0</cell><cell>5.4</cell><cell>88.9</cell><cell>100.0</cell><cell>7.9</cell><cell></cell></row><row><cell></cell><cell>scooter-black</cell><cell></cell><cell>76.5</cell><cell>95.1</cell><cell>-3.5</cell><cell>70.2</cell><cell>95.1</cell><cell>-2.0</cell><cell></cell></row><row><cell></cell><cell>soapbox</cell><cell></cell><cell>77.0</cell><cell>95.9</cell><cell>16.7</cell><cell>74.5</cell><cell>100.0</cell><cell>18.0</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell></cell><cell>71.5</cell><cell>86.5</cell><cell>9.5</cell><cell>70.5</cell><cell>83.5</cell><cell>7.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance on DAVIS2016<ref type="bibr" target="#b30">[29]</ref>: Per category performance on the DAVIS2016 dataset.Category J mean ? J recall ? J decay ? F mean ? F recall ? F decay ?</figDesc><table><row><cell>camel01</cell><cell>78.3</cell><cell>100.0</cell><cell>3.3</cell><cell>83.1</cell><cell>100.0</cell><cell>4.9</cell></row><row><cell>cars1</cell><cell>61.4</cell><cell>100.0</cell><cell>0.0</cell><cell>37.5</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>cars10</cell><cell>31.1</cell><cell>0.0</cell><cell>-9.4</cell><cell>22.9</cell><cell>0.0</cell><cell>0.8</cell></row><row><cell>cars4</cell><cell>83.6</cell><cell>100.0</cell><cell>7.6</cell><cell>79.5</cell><cell>100.0</cell><cell>11.6</cell></row><row><cell>cars5</cell><cell>79.0</cell><cell>100.0</cell><cell>9.2</cell><cell>78.5</cell><cell>100.0</cell><cell>14.1</cell></row><row><cell>cats01</cell><cell>90.0</cell><cell>100.0</cell><cell>5.1</cell><cell>90.0</cell><cell>100.0</cell><cell>19.2</cell></row><row><cell>cats03</cell><cell>75.8</cell><cell>100.0</cell><cell>-12.6</cell><cell>73.8</cell><cell>100.0</cell><cell>0.8</cell></row><row><cell>cats06</cell><cell>61.3</cell><cell>81.2</cell><cell>27.3</cell><cell>75.6</cell><cell>87.5</cell><cell>17.5</cell></row><row><cell>dogs01</cell><cell>73.4</cell><cell>77.8</cell><cell>35.9</cell><cell>74.3</cell><cell>88.9</cell><cell>21.3</cell></row><row><cell>dogs02</cell><cell>73.8</cell><cell>90.0</cell><cell>-9.7</cell><cell>74.4</cell><cell>90.0</cell><cell>10.9</cell></row><row><cell>farm01</cell><cell>82.4</cell><cell>91.7</cell><cell>-24.6</cell><cell>71.0</cell><cell>83.3</cell><cell>-29.0</cell></row><row><cell>giraffes01</cell><cell>38.5</cell><cell>46.7</cell><cell>-25.4</cell><cell>44.2</cell><cell>33.3</cell><cell>-3.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A small constant 0 ? ! 1 is added to the denominator to avoid singularities, and whenever x ? H, Hpux|Iq " , thus we will omit from now on.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In the continuum, it belongs to the infinite-dimensional set of compact and multiply-connected regions of the unit square.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatiotemporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A real-time computer vision system for measuring traffic parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>1997. 5</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational motion segmentation with level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="471" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards the quantitative evaluation of visual attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Degennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajalingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ruda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="258" to="268" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Active contours without edges</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Per category performance on FBMS59</title>
		<imprint/>
	</monogr>
	<note>Table 6: Performance on FBMS59 [26</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SegFlow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SegFlow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Motion competition: A variational approach to piecewise parametric motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion competition: A variational approach to piecewise parametric motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aggressive deep driving: Combining convolutional neural networks and model predictive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldfain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Medical image segmentation: a brief survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elnakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gimelfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi Modality State-of-the-Art Medical Image Segmentation and Registration Methodologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video object segmentation by nonlocal consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>British Machine Vision Association</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FusionSeg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time obstacle avoidance for manipulators and mobile robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Khatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1985 IEEE International Conference on Robotics and Automation</title>
		<meeting>1985 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1985" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="500" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extending layered models to 3d motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dronet: Learning to fly by driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R D</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1088" to="1095" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal approximations by piecewise smooth functions and associated variational problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="685" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding the best from the second bestsinhibiting subjective bias in evaluation of visual tracking algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2784" to="2791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper ConvLSTM for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A fully-connected layered model of foreground and background flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion coherent tracking with multi-label MRF optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC) 2010. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representing moving images with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised moving object detection via contextual information separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03360</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conditional prior networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shape tracking with occlusions via coarse-to-fine region-based sobolev descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1053" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-occlusions and disocclusions in causal video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4408" to="4416" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
