<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazmul</forename><surname>Karim</surname></persName>
							<email>nazmul.karim18@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayeem</forename><surname>Mamshad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rizve</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">UCF</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
							<email>nazanin.rahnavard@ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
							<email>ajmal.mian@uwa.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">UWA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">UCF</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">UCF</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised deep learning methods require a large repository of annotated data; hence, label noise is inevitable. Training with such noisy data negatively impacts the generalization performance of deep neural networks. To combat label noise, recent state-of-the-art methods employ some sort of sample selection mechanism to select a possibly clean subset of data. Next, an off-the-shelf semi-supervised learning method is used for training where rejected samples are treated as unlabeled data. Our comprehensive analysis shows that current selection methods disproportionately select samples from easy (fast learnable) classes while rejecting those from relatively harder ones. This creates class imbalance in the selected clean set and in turn, deteriorates performance under high label noise. In this work, we propose UNICON, a simple yet effective sample selection method which is robust to high label noise. To address the disproportionate selection of easy and hard samples, we introduce a Jensen-Shannon divergence based uniform selection mechanism which does not require any probabilistic modeling and hyperparameter tuning. We complement our selection method with contrastive learning to further combat the memorization of noisy labels. Extensive experimentation on multiple benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4% improvement over the current state-of-the-art on CIFAR100 dataset with a 90% noise rate. Our code is publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have proven to be highly effective in solving various computer vision tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b64">65]</ref>. Most state-of-the-art (SOTA) methods require supervised training with a large pool of annotated data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b59">60]</ref>. Collecting and manually an-  notating such data is challenging and oftentimes very expensive. Most large-scale data collection techniques rely on open-source web data that can be automatically annotated using search engine queries and user tags <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b53">54]</ref>. This annotation scheme inevitably introduces label noise <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b59">60]</ref>. Training with such noisy labels is challenging since DNNs can effectively memorize arbitrary (noisy) labels over the course of training <ref type="bibr" target="#b1">[2]</ref>. Combating label noise is one of the fundamental problems in deep learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b67">68]</ref>, and is the focus of this study.</p><p>Training with noisy label data has been the subject of many recent studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b72">73]</ref>. Existing techniques can be categorized into two dominant groups: i) label correction, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref> and ii) sample separation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b68">69]</ref>. The former approach requires the estimation of noise transition matrix, which is hard to estimate for high number of classes and in high noise scenarios. The latter approach tries to filter out the noisy samples from the clean ones based on the small-loss criterion <ref type="bibr" target="#b24">[25]</ref>, where the low-loss samples are assumed to have clean labels. Next, an off-the-shelf semisupervised learning (SSL) technique <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53]</ref> is used for training where the selected noisy samples are treated as unlabeled data. However, the selection process is usually biased towards easy classes as clean samples from the hard classes (e.g. cats and dogs can be considered as hard classes in CIFAR10 <ref type="bibr" target="#b20">[21]</ref>) may produce high-loss values. This is more prominent at the early stage of training and can introduce class-disparity among the selected clean samples. Severe class-imbalance may lead to poor precision of sample selection, hence, sub-par classification performance.</p><p>In this work, we revamp the selection process from a more fundamental perspective. Our goal is to simplify the selection process by introducing an effective and scalable Jensen-Shannon divergence based sample separation mechanism. To address the disproportionate selection of easy and hard samples, we enforce a class-balance prior by selecting an equal number of clean samples from each class. Such a prior improves the overall quality of pseudo-labels, and hence, significantly boosts the performance of subsequent semi supervised learning-based training. In addition, we opt to employ unsupervised contrastive learning (CL) because of its inherent resistance (as labels are not required for training) to label noise memorization. We empirically show that unsupervised feature learning lowers memorization risk and improves the sample separation performance; especially under severe noise levels. We call this combined technique of UNIform selection and CONtrastive learning UNICON (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>), which is found to be effective even in the presence of very high label noise (see <ref type="table" target="#tab_0">Table 1</ref>). Our contributions are summarized as follows:</p><p>? We propose a simple yet effective uniform selection mechanism that ensures class-balancing among the selected clean samples. Through empirical analysis, we observe that class-uniformity helps in generating higher quality pseudo-labels for samples from all classes irrespective of their difficulty level. ? We further minimize the risk of label noise memorization by performing unsupervised feature learning using contrastive loss. This in turn boosts the sample separation performance. ? Our extensive experimentation demonstrates that UNI-CON achieves significant performance improvement over state-of-the-art methods, especially on datasets with severe label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Noisy label training has been studied extensively in recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b74">75]</ref>. Wei et al. <ref type="bibr" target="#b58">[59]</ref> proposed a regularization technique to learn from noisy labels. Another method called MentorNet <ref type="bibr" target="#b16">[17]</ref> trains a student network by generating pseudo-labels using a pre-trained/mentor network. Based on their relationship in the feature space, Meta-cleaner <ref type="bibr" target="#b71">[72]</ref> learns the confidence scores of noisy samples which are then used for obtaining cleaner representations. To deal with noisy labels, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b66">67]</ref> gradually adjust the data labels based on the predicted labels given by the network. Some noisy label methods are based on loss correction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40]</ref> and noise-tolerant loss functions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b73">74]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, a noise transition matrix was estimated by correcting the loss obtained by a DNN trained on a noisy dataset. However, the performance of these methods deteriorates under high noise rates and large number of classes. Other approaches rely on the separation of clean samples from the noisy samples <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b68">69]</ref>. A notable difference between these methods is the selection criteria of clean samples. A selection technique was proposed in <ref type="bibr" target="#b9">[10]</ref> that utilizes prediction likelihoods to obtain separation.</p><p>Co-teaching <ref type="bibr" target="#b11">[12]</ref> opts to train two networks simultaneously such that one network separates clean samples for the other network based on the small-loss criterion. The smallloss criterion suggests that samples with smaller loss tend to have clean labels. Therefore, one could separate samples on the training set based on their loss-values. DMix <ref type="bibr" target="#b24">[25]</ref> proposed a hybrid framework to separate samples and uses a SSL technique <ref type="bibr" target="#b70">[71]</ref> to concurrently train two networks. A modified training scheme for <ref type="bibr" target="#b24">[25]</ref> was proposed in <ref type="bibr" target="#b34">[35]</ref>. However, even for the same dataset, these methods employ different training settings and constraints under different noise rates and types. This limits their practical applications as prior knowledge of noise rate may not be available. Recently, a joint semi-supervised and contrastive learningbased technique was proposed in MOIT <ref type="bibr" target="#b38">[39]</ref>. Jo-SRC <ref type="bibr" target="#b65">[66]</ref> initially partitions the samples into clean and noisy sets before detecting in-distribution (ID) and OOD samples in the noisy set. However, it requires manual threshold adjustment for the separation during different epochs of the training. Furthermore, both <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b38">[39]</ref> struggle to achieve good performance under high noise rates.</p><p>In contrast, our proposed method can handle severe label noise and requires minimal to no change in the hyperparameter settings under different label-noise scenarios (e.g. different noise rates, noise types etc.). We show how a minimalistic approach to the selection process can boost the classification performance significantly beating the sateof-the-art methods in most cases. Furthermore, we achieve comparable performance to SOTA across different datasets which hints at the generalizability of our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Let D = {X , Y} = {(x 0 , y 0 ), (x 1 , y 1 ), . . . , (x N , y N )} denote the training set, where x i is an image and y i is the corresponding ground-truth label, and N is the total number of training samples. We instantiate the DNN model with a feature extractor (CNN backbone), f (.; ?), with parameters ?; a classification layer, h(.; ?), with parameters ?, and a projection head, g(., ?), with parameters ? for incorporating contrastive learning. For supervised training with ground-truth labels, we minimize cross-entropy (CE) loss, L CE , over the entire training set D,</p><formula xml:id="formula_0">L CE = ? 1 N N i=1 y T i log? i ,<label>(1)</label></formula><p>where? i = softmax(h(f (x i ; ?); ?)) is the softmax probability score of the network prediction corresponding to x i . In this work, we consider the training set to be noisy i.e. some images are incorrectly labeled. It has been demonstrated that DNNs learn simpler patterns before memorizing the noisy labels <ref type="bibr" target="#b1">[2]</ref>. Several studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> utilize this observation and try to separate the clean samples from the noisy ones at the early stage of training. Such a separation scheme partitions the dataset into a clean subset, D clean , and a noisy subset, D noisy = D \ D clean . After that, D clean can be used for standard supervised training. To mitigate the impact of label noise, samples from D noisy can be used for training without the corresponding noisy ground-truth labels. This training is generally performed in a semi-supervised manner where pseudo-labels are generated for the samples in D noisy .</p><p>We conduct extensive empirical analysis to investigate the effectiveness of partitioning the dataset into D clean , and D noisy subsets. We find that the typical construction of D clean creates disparity or imbalance among classes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b65">66]</ref>. <ref type="figure">Fig. 2a</ref> (left bars) depicts such a case where the D clean for noisy CIFAR10 (90% noise rate) contain class imbalance when we employ a recently proposed method, DMix <ref type="bibr" target="#b24">[25]</ref>. To be specific, we observe that 1228 samples are selected from class-1, whereas only 10 samples from class-2 are selected. However, the imbalance among true positives (TPs) are of particular importance as the quality of pseudo-labels for D noisy relies heavily on them. Methods such as <ref type="bibr" target="#b24">[25]</ref> attempt to address this issue by selecting more clean samples which in turn increases the false positive or noisy labels count ( <ref type="figure">Fig. 2b</ref> (left bars)) while drastically decreasing the precision. As the selected clean set D clean contains many false positives, supervised training on such a set leads to memorization. Consequently, the recall of the subsequent pseudo-labels drops drastically; as shown by the pseudo-label recall in <ref type="figure">Fig. 2c</ref> (left bars). In this way, the selection mechanism negatively impacts the SSL-Training and reduces the average classification accuracy ( <ref type="figure">Fig. 2d</ref>).</p><p>We propose to address these problems by a simple and effective technique of uniform selection ( <ref type="figure">Fig. 2a</ref> (right bars)). Furthermore, we employ contrastive feature learning to learn better unsupervised features irrespective of the quality of ground-truth or pseudo-labels. Details of our proposed method are presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>We propose UNICON with a unique sample-selection approach as well as simple but effective modification to the SSL-Training. UNICON improves precision ( <ref type="figure">Fig. 2b</ref> (right bars)) as well as pseudo-label recall ( <ref type="figure">Fig. 2c</ref> (right bars)) over training. <ref type="figure">Fig. 2d</ref> shows that our hybrid framework of uniform selection and SSL training improves the classification performance significantly. Next, we present our uniform sample selection strategy in Sec. 4.1, and our proposed SSL training method with contrastive learning in Sec 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Uniform Sample Selection</head><p>During the partitioning of D, we opt to enforce classbalancing in D clean by selecting/filtering R portion of samples from each class, where we define R as the filter rate. <ref type="figure" target="#fig_2">Fig. 3</ref> shows our proposed selection mechanism in which we feed D to two networks with parameters (? <ref type="bibr" target="#b0">(1)</ref> , ? <ref type="bibr" target="#b0">(1)</ref> , ? <ref type="bibr" target="#b0">(1)</ref> ) and (? <ref type="bibr" target="#b1">(2)</ref> , ? <ref type="bibr" target="#b1">(2)</ref> , ? <ref type="bibr" target="#b1">(2)</ref> ). For x i , the av- erage prediction probabilities from both networks can be denoted as</p><formula xml:id="formula_1">p i = [p 1 i , p 2 i , . . . , p C i ]</formula><p>, and the corresponding ground-truth label as</p><formula xml:id="formula_2">y i = [y 1 i , y 2 i , . . . , y C i ];</formula><p>here, C is the total number of classes. To construct the clean, D clean , and noisy, D noisy , subsets, we compute the disagreement/divergence between the ground-truth labels, y i , and the predicted probabilities, p i . To this end, we use Jensen-Shannon divergence (JSD), d i , as a measure of disagreement. The JSD is defined as,</p><formula xml:id="formula_3">d i = JSD(y i , p i ) = 1 2 KLD(y i || y i + p i 2 ) + 1 2 KLD(p i || y i + p i 2 ),<label>(2)</label></formula><p>where KLD(.) is the Kullback-Leibler divergence function. Previous works use different divergence measures to construct the clean and noisy subsets. Authors in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> apply CE loss-based divergence measure for selection. <ref type="bibr" target="#b24">[25]</ref> uses a similar divergence measure and fits a Gaussian mixture model (GMM) on the normalized CE values for partitioning. In contrast, we opt to employ JSD-based selection since it does not require normalization and probabilistic modelling. Besides, unlike CE loss, JSD is symmetric by design and the value ranges from 0 to 1.</p><p>After measuring the divergence, d = {d i : i ? (1, . . . , N )}, for all the samples, we compute a cutoff divergence value, d cutof f , which can be expressed as,</p><formula xml:id="formula_4">d cutof f = d avg ? (d avg ? d min )/?, if d avg ? d ? d avg , otherwise<label>(3)</label></formula><p>where d avg is the average over all values in d, d min is the lowest divergence score, ? is the filter coefficient, and d ? is an adjustment threshold. Finally, we determine R as the percentage of samples that have JSDs lower than d cutof f . There are two major benefits of this particular design of d cutof f . First, we determine the value of d cutof f based on the network prediction scores (as JSD depends on prediction probabilities) which eliminates the requirement of</p><formula xml:id="formula_5">Algorithm 1: Uniform Clean Sample Selection Input: training set D = (X , Y), number of samples N , number of classes C for i = 1 to N do pi = ? (1) i +? (2) i /2 di = JSD(pi, yi) (see Eq. (2)) Determine the cutoff distance, d cutof f using Eq. (3) dR ? {di &lt; d cutof f : i ? (1, . . . , N )} Determine filter rate, R = |dR|/N D clean = {} // Uniform Selection for j = 1 to C do d (j) f iltered ? Lowest R portion of d (j) D (j) clean ? {(x (j) t , y (j) t ) : ? d (j) t ? d (j) f iltered } D clean ? D clean ? D (j) clean</formula><p>Dnoisy ? D \ D clean Output: Dnoisy, D clean manual per-dataset tuning. The second benefit stems from the same source, i.e., d cutof f is determined from prediction scores. This ensures that if the network prediction scores are consistently low (high d avg ), d cutof f will encourage a conservative selection of D clean ; which helps in avoiding noisy sample selection at the early stage of training.</p><p>In the next step, we create class-specific partitions,</p><formula xml:id="formula_6">{d (1) , d (2) , . . . , d (C) }, where d (j)</formula><p>indicates the JSDs for class j. Motivated by the small-loss criterion <ref type="bibr" target="#b24">[25]</ref>, we define UNICON selection criterion as follows:</p><p>UNICON Selection Criterion: For each class j, if the difference d </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROC-AUC</head><p>Without CL With CL <ref type="figure">Figure 4</ref>. ROC-AUC score of clean sample selection with and without contrastive learning. As CL helps preventing the memorization, the clean samples are being detected with better precision.</p><p>Here we considred CIFAR10 with 90% noise rate.</p><p>aggregate all the selected clean and noisy samples from each class to form D clean and D noisy with cardinalities of N R and N (1 ? R), respectively. In cases where the total number of available samples (both clean and noisy) for any class falls below N R/C, we take all the available samples in that class for D clean . Algorithm 1 summarizes our selection method. Note that a previous technique named Jo-SRC <ref type="bibr" target="#b65">[66]</ref> has employed JSD for clean sample detection. However, our sample selection process differs significantly from Jo-SRC. For instance, the selection threshold in <ref type="bibr" target="#b65">[66]</ref> needs to be manually fine-tuned during different epochs of the training while UNICON automatically adjusts the filter rate, R, based on the network prediction scores; making our proposed selection method hyperparameter independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SSL-Training</head><p>Fig <ref type="figure" target="#fig_2">. 3</ref> shows the details of our SSL-Training with semisupervised and contrastive loss. Following FixMatch <ref type="bibr" target="#b47">[48]</ref>, we perform semi-supervised learning with the samples from D noisy . To this end, we generate two copies of each sample with a weak and a strong augmentation. Pseudo-labels are generated from the weakly augmented copy for computing a semi-supervised loss, L semi , on the strongly augmented copy. We also apply MixUp <ref type="bibr" target="#b69">[70]</ref> augmentation between the samples from D clean and D noisy ; for the D noisy samples, we use the pseudo-labels obtained from weakly augmented copy. However, feature or representation learning in such a SSL manner still bears the risk of noise memorization. During training, DNNs memorize certain portion of noisy samples irrespective of the sample selection technique. The presence of such noisy samples in the clean subset, will lead to noisy SSL training. To address this issue, we incorporate contrastive learning (CL) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref> into our SSL training pipeline to facilitate feature learning without relying on labels/pseudo-labels. Such an unsupervised feature learning scheme further mitigates the risk of noisy label memorization since it does not rely on imperfect separation of clean and noisy samples as well as incorrect pseudo-labels generated during SSL training. Thus, incor-poration of CL improves the performance of our proposed selection technique, as shown by the area under the curve (AUC) of Receiver Operating Characteristics (ROC) in <ref type="figure">Fig.  4</ref>. In our work, we employ contrastive loss only for the samples in the unlabeled set, D noisy . To this end, we employ the projection head g(.; ?) to obtain feature projec-</p><formula xml:id="formula_7">tions z i = g(f (x i,1 ; ?); ?), and z j = g(f (x i,2 ; ?); ?) of the differently augmented copies (x i,1 , x i,2 ) of input x i .</formula><p>The contrastive loss function <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref> can be expressed as</p><formula xml:id="formula_8">? i,j = ? log exp(sim(z i , z j )/?) 2B b=1 1 b? =i exp(sim(z i , z b )/?) ,<label>(4)</label></formula><formula xml:id="formula_9">L C = 1 2B 2B b=1 [? 2b?1,2b + ? 2b,2b?1 ],<label>(5)</label></formula><p>where 1 b? =i is an indicator function that gives a 1 iff b ? = i, ? is a temperature constant, B is the number of samples in mini-batch, and sim(z i , z j ) can be expressed as the cosine similarity between z i and z j . The total loss function we minimize is</p><formula xml:id="formula_10">L tot = L semi + ? C L C ,<label>(6)</label></formula><p>where ? C is contrastive loss coefficient. Additional details of the contrastive learning as well as the rest of our SSL-Training scheme is provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>CIFAR10/100: The CIFAR-10/100 datasets <ref type="bibr" target="#b20">[21]</ref> contain 50K training and 10K test images. In general, it is difficult to control or determine the noise characteristics; e.g. noise rate, in natural datasets. Therefore, synthetic noise models are commonly used for the evaluation of noiserobust algorithms. In our work, we employ two types of noise models: symmetric and asymmetric. For symmetric noise model, an r portion of samples from one particular class are uniformly distributed to all other classes. On the other hand, the design of asymmetric label noise follows the structure of real mistakes that take place in CIFAR10 <ref type="bibr" target="#b25">[26]</ref>: "Truck? Automobile, Bird ? Airplane, Deer ? Horse, Cat ? Dog". For CIFAR100, we use label flips for each class to the next one within the super-classes.</p><p>Tiny-ImageNet <ref type="bibr" target="#b22">[23]</ref>: This dataset is a smaller version of the original ImageNet in terms of the number of classes and the image resolution. There are in total 200 classes containing 500 images per class. The image size is 64 ? 64.</p><p>Clothing1M: Clothing1M is a large-scale real-world dataset with noisy labels <ref type="bibr" target="#b59">[60]</ref>. It contains 1M images from 14 different cloth-related classes. Since the labels are produced by the seller provided surrounding texts of the images, a large portion of confusing classes (e.g., Knitwear and Sweater) are mislabeled.  <ref type="table">Table 2</ref>. Test accuracies (%) obtained by different techniques under symmetric noise. Our class balance with contrastive loss strategy improves performance at almost every noise level. Results for previous techniques were copied from their respective papers.</p><p>Webvision <ref type="bibr" target="#b26">[27]</ref>: This dataset contains 2.4 million images (obtained from Flickr and Google) that are categorized into the same 1,000 classes as in the ImageNet ILSVRC12. Following the previous studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>, we use the first 50 classes of the Google image subset as the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Details</head><p>We use the PreAct ResNet18 <ref type="bibr" target="#b12">[13]</ref> architecture for CI-FAR10, CIFAR100, and Tiny-ImageNet. For Clothing1M and WebVision datasets, we take a ResNet50 network <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet and a InceptionResNetV2 network <ref type="bibr" target="#b50">[51]</ref> which is trained from scratch. We modify these architectures with a projection head, that produces a embedding vector of size 128, to facilitate contrastive learning.</p><p>For CIFAR-10 and CIFAR-100, optimization is performed using stochastic gradient descent (SGD) optimizer with the following settings: an initial learning rate (LR) of 0.02, a weight decay of 5e ?4 , a value of 0.9 for the momentum, and a batch size of 64. For CIFAR-10 and CIFAR-100, we train each network for around 300 epochs while linearly decaying the learning rate (lr-decay) by 0.1 per 120 epochs. Following <ref type="bibr" target="#b24">[25]</ref>, a warmup period of 10 and 30 epochs was employed before starting the selection and SSL-Training. For Tiny-ImageNet, we use an initial LR of 0.01, a weight decay of 1e ?3 with a batch size of 32. We train the network for 350 epochs and the lr-decay rate is 0.1/100 epochs. The warmup period is 15 epochs. For Clothing1M, we choose an initial LR of 0.002 and a weight decay of 1e ?3 . We employ the same settings as Tiny-ImageNet for WebVision. The total number of training epochs is 100 and the lr-decay rate is 0.1/40 epochs.</p><p>For data augmentations, we follow Auto-augment policy described in <ref type="bibr" target="#b6">[7]</ref>. For CIFAR-10 and CIFAR100, we use CIFAR10-Policy and we apply ImageNet-Policy to Tiny-ImageNet. As these policies are transferable from one dataset to another, ImageNet-Policy is employed for both Clothing1M and Webvision dataset.  <ref type="table">Table 3</ref>. Experimental results on CIFAR10 and CIFAR100 with asymmetric noise. UNICON sees consistent improvement for CI-FAR100 dataset under different asymmetric noise settings. ( * ) indicates that we run the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>We present the performance of UNICON under different label noise scenarios. We start with the synthetic noisy label datasets (e.g. CIFAR10, CIFAR100 and TinyImageNet) and move on to the real world noisy datasets (e.g. WebVision, Clothing1M). For experiments, we consider symmetric noise rates of 20%, 50%, 80%, and 90% and asymmetric noise rates of 10%, 30%, and 40%.</p><p>CIFAR10 and CIFAR100 datasets: <ref type="table">Table 2</ref> shows the average test accuracies for these datasets. In case of CI-FAR10, from moderate to severe label noise, UNICON performs consistently better than the baseline methods. For 90% noise rate, we achieve a significantly better performance improvement over the state-of-the art. For high noise rate, techniques like <ref type="bibr" target="#b24">[25]</ref> usually fail due to high number of false positives. However, for low noise rate (20%), <ref type="bibr" target="#b24">[25]</ref> performs slightly better than ours. Low noise rate indicates more clean samples are available for supervised learning. One possible explanation could be that the scarcity of unlabeled data (i.e. |D noisy | &lt; |D clean |) makes contrastive feature learning less effective. We have also conducted experiments under the asymmetric noise scenario. In case of asymmetric noise, each class is not equally affected by label noise. This makes the selection of clean samples a bit more challenging. However, UNICON achieves similar performance gain as symmetric noise which is shown in <ref type="table">Table  3</ref>. Note that there is an exception at 10% noise rate as <ref type="bibr" target="#b29">[30]</ref> obtains 0.1% better accuracy than UNICON. <ref type="table">Table 2</ref> and 3 contain the average test accuracies for CIFAR100 dataset. UNICON shows similar effectiveness against label noise in CIFAR100 obtaining an accuracy improvement of 11.4% for 90% noise rate. This improvement is consistent under different noise settings. While ELR <ref type="bibr" target="#b29">[30]</ref>, DMix <ref type="bibr" target="#b24">[25]</ref> and MOIT <ref type="bibr" target="#b38">[39]</ref> show some level of resistance to noisy labels for low noise rate, the performances are not consistent for high noise rate. Furthermore, the asymmetric noise performance of our method are also superior than other baseline methods in <ref type="table">Table 3</ref>.  <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b38">[39]</ref>. MOIT <ref type="bibr" target="#b38">[39]</ref> does not evaluate their method on ILSVRC12 and did not provide top-5 accuracies.</p><p>TinyImageNet Dataset: <ref type="table">Table 4</ref> presents the performance comparison of UNICON and other state of the art methods. Even with no label noise, Tiny-ImageNet remains a challenging benchmark dataset to deal with. It becomes more challenging under the presence of label noise. One of the baseline methods M-correction <ref type="bibr">[</ref> a loss-correction technique to tackle noisy labels while NCT <ref type="bibr" target="#b44">[45]</ref> leverages from collaborative learning of two networks. However, both methods underperform compared to our method. <ref type="table">Table 4</ref> shows that UNICON gains around 1% performance improvement over SOTA for all noise rates. Clothing1M Dataset: <ref type="table">Table 5</ref> presents performance comparison on this real world noisy labeled dataset. We achieve 0.17% performance improvement over ELR <ref type="bibr" target="#b29">[30]</ref>. The performance improvement for clothing1M sometimes depends on the length of warmup, as longer period of standard CE-based training can lead to memorization. In our training, we use a warm-up period of 2,000 steps.</p><p>WebVision Dataset: We present our experimental results on this dataset in <ref type="table" target="#tab_4">Table 6</ref>. While validating, MOIT <ref type="bibr" target="#b38">[39]</ref> sees SOTA Top-1 accuracy while our method achieves the best Top-5 accuracy. We obtain around 1.5% improvement over SOTA (MOIT <ref type="bibr" target="#b38">[39]</ref> did not provide Top-5 accuracy.) Furthermore, UNICON secures SOTA Top-1 and Top-5 accuracies on ILSVRC12 validation set. While the gain in Top-1 accuracy is not significant, we achieve a performance improvement of 1.88% over DMix <ref type="bibr" target="#b24">[25]</ref> in Top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablation Studies</head><p>In this section, we conduct an ablation study of UNICON under different training settings.</p><p>Sample Selection Performance: In general, the precision of clean sample selection directly impacts the overall  <ref type="figure">Fig. 5a</ref> shows the the ROC-AUC score of our selection mechanism under different noise settings. It can be observed that UNICON sees a steady rise in the precision irrespective of the noise level.</p><p>In case of high noise rate, it is usual for the network to get confused between clean and noisy samples. However, our separation approach proves to be effective even under such scenario. With improved precision, the network learns better discriminative features from labeled data and generalizes well to the unlabeled data. Through the generation of quality pseudo-labels, UNICON improves the classification accuracy significantly <ref type="figure">(Fig. 5b)</ref>. Effect of Contrastive Learning: CL is one of the key components of our framework. <ref type="table" target="#tab_6">Table 7</ref> indicates the impact of CL in overall performance of our method. As CL is resistant to label noise memorization, it boosts the performance significantly even in high label noise scenarios. For CIFAR10 and CIFAR100, with 90% noise rate, UNICON without CL sees 3.53% and 2.99% drop in test accuracies respectively. We explain more on contrastive learning and its impact in the supplementary material.</p><p>Effect of Ensemble and balancing: During selection, we take the average of both network's predictions instead of depending on just one network <ref type="bibr" target="#b11">[12]</ref>. This seems to improve the performance significantly in case of high noise rate (see <ref type="table" target="#tab_6">Table 7</ref>). However, taking the feedback from both networks bears the risk of confirmation bias over the course of training <ref type="bibr" target="#b24">[25]</ref>. We prevent that by training one network at a time. During the same training epoch, we perform the separation again before training the other network. <ref type="table" target="#tab_6">Table 7</ref> also contains the performance of our method without balancing. The significant decrease in classification accuracies underlines the importance of class-balance prior. The effectiveness of UNICON in combating memorization can be observed in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations of UNICON</head><p>In this work, to combat label noise we employ a classbalance prior. The prior helps in combating artificial im-balance caused by current state-of-the-art selection methods. This prior can be restrictive in some extreme scenarios where the dataset itself exhibits extreme imbalance. However, in such cases, it is possible to update our prior accordingly based on the class distribution of the dataset. Since knowing the dataset distribution in advance is equally restrictive we do not explore this direction in this study. Additionally, even though we provide a general solution for combating label noise, our solution is particularly effective under high label noise. Therefore, it is possible to outperform our proposed method on datasets which do not contain a significant amount of label noise. However, we emphasize that such success can be attributed to superior training strategy and complicated design whereas our simple solution is more general and provides reasonable results even for such low noise rate scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we proposed UNICON, a simple yet effective solution for combating label noise. Our proposed uniform selection technique effectively addresses often overlooked but critical shortcoming of selection based stateof-the-art methods. Furthermore, our constrastive feature learning approach provides a fundamental solution to combat memorization of noisy label. Equipped with these two components, our method selects clean samples more precisely over the course of training by reducing the classdisparity among the true positives and CL-based unsupervised feature learning. Network trained on high precision clean samples generates higher quality pseudo-labels for the noisy label data and the overall process improves the high noise level performance significantly. UNICON achieves ?10% performance improvement over state-of-the-art on 90% noisy CIFAR10 and CIFAR100. Through extensive empirical analysis, we show the effectiveness of our method under different noise scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Overview</head><p>Section 10 describes our SSL-Training method in detail. Section 11 discusses the findings of our ablation studies. Section 13 has some details about hyperparameter settings and experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">SSL-Training Details</head><p>Before semi-supervised learning (SSL), we separate the training set into D clean and D noisy by applying uniform selection. A sample training set with 60% noise level is shown in <ref type="figure" target="#fig_4">Figure 7</ref>. We consider D clean and D noisy to be labeled and unlabeled data, respectively. At the beginning of SSL, we create four sets of weakly-augmented (WA) data:</p><p>? Two sets of weakly-augmented labeled data {x weak i,1 ,x weak i,2</p><p>: i ? (1, ..., N )}.</p><p>? Two sets for weakly-augmented unlabeled data {? weak i,1 ,? weak i,2</p><p>: i ? (1, ..., N )}.</p><p>In addition, we also generate four sets of stronglyaugmented (SA) data:</p><p>? Two sets of strongly-augmented labeled data {x strong i,1 ,x strong i,2</p><p>: i ? (1, ..., N )}.</p><p>? Two sets of strongly-augmented unlabeled {? strong i,1</p><p>,? strong i,2</p><p>: i ? (1, ..., N )}.</p><p>Here, weak augmentations are used for label updating (label-refinement and pseudo-label guessing). We employ strong augmentations for updating the network parameters using backpropagation. For label-refinement <ref type="bibr" target="#b24">[25]</ref>, we use the networks' prediction to a weakly-augmented sample x i for refining the given-label y i . For {x weak i,1 ,x weak i,2 }, the output probabilities can be written as,</p><formula xml:id="formula_11">p i = 1 2 2 m=1 h(f (x weak i,m ; ? (k) ); ? (k) ),<label>(7)</label></formula><p>where N is the number of data points in the training set and h(f (x weak i,m ; ? (k) ); ? (k) ) is the Softmax probabilities of network-k (k=1,2) corresponding tox weak i,m . After getting p i , we refine the label as follows:</p><formula xml:id="formula_12">y i = w i y i + (1 ? w i )p i ,<label>(8)</label></formula><p>where w i is the label refinement coefficient. However, w i can be calculated from the JSD values as,</p><formula xml:id="formula_13">w i = 1 ? d i , if d i ? d ? 1, otherwise<label>(9)</label></formula><p>where d ? is the label-refinement threshold that adjusts w i based on the JSD of sample x i . Next, we follow the temperature sharpening <ref type="bibr" target="#b24">[25]</ref> step given that gives us? i . Similarly, we calculate pseudo-label by averaging the predictions of both networks <ref type="bibr" target="#b24">[25]</ref>, i.e.</p><formula xml:id="formula_14">q b = 1 4 2 m=1</formula><p>h(f (? weak b,m ; ? (1) ); ? <ref type="bibr" target="#b0">(1)</ref> )+h(f (? weak b,m ; ? (2) ); ? (1) ) (10) and apply temperature sharpening on it to get q b .</p><p>We aggregate the labeled and unlabeled images with their ground-truth labels and pseudo-labels, respectively. That is,X = {(x strong i,m ,? i ); i ? (1, ..., N ), m = (1, 2)}, and? = {(? strong i,m , q i ); i ? (1, ..., N ), m = (1, 2)} are the labeled and unlabeled sets. We use MixMatch <ref type="bibr" target="#b2">[3]</ref> to have</p><formula xml:id="formula_15">W = Shuffle Concat(X ,?) ,<label>(11)</label></formula><formula xml:id="formula_16">X = MixUp(X i , W i ); i ? (1, . . . , |X |) ,<label>(12)</label></formula><formula xml:id="formula_17">U = MixUp(? i , W i+|X | ); i ? (1, . . . , |?|) .<label>(13)</label></formula><p>MixUp <ref type="bibr" target="#b70">[71]</ref> proposed a strategy for generating convex combination of two inputs: in this case, samples from labeled and unlabeled sets and their corresponding ground-truth labels and pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.">Loss Functions</head><p>After applying MixMatch, the semi-supervised losses are calculated as follows <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id="formula_18">L X = 1 |X | x,p?X H(p, h(f (y | x; ?); ?)),<label>(14)</label></formula><formula xml:id="formula_19">L U = 1 |?| u,q?? ?q ? h(f (y | u; ?); ?)? 2 2 ,<label>(15)</label></formula><p>where H(p, q) is the cross-entropy between distributions p and q with y as the given label. Additionally, to prevent single-class assignment of all samples, we use a regularization term based on a prior uniform distribution (? c = 1/C) to regularize the network's output across all samples in the mini-batch similar to Tanaka et al. <ref type="bibr" target="#b51">[52]</ref> ,</p><formula xml:id="formula_20">L reg = c ? c log ? c 1 |X +? | x?|X +? | h(f (x; ?); ?)<label>(16)</label></formula><p>This gives us our semi-supervised loss function as shown in <ref type="figure" target="#fig_6">Figure 8</ref>,</p><formula xml:id="formula_21">L semi = L X + ? U L U + ? r L reg .<label>(17)</label></formula><p>Here, ? U and ? r are unsupervised loss coefficient and regularization coefficient, respectively. Here, the noise rate is 60% (3/5). Note that these images are taken for demonstration purpose only and corresponding labels are not their original given labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly Augmented Data</head><p>Set-1 Set-2 . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set-1</head><p>Set-2</p><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Loss</head><p>Set-1 Set-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target MixUp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strongly Augmented Data</head><p>Set-1 Set- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input MixUp</head><p>Label Refining</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Label Guessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network-'k'</head><p>Model Prediction  . After separating the samples, we create total 8 sets of weak and strong augmented data. While weakly augmented data helps with target label generation, strongly augmented data are used for updating the parameters through backpropagation. There are two types of label generation here: pseudo label guessing (10) (represented by green color) and label-refinement (8) (represented by red color). We have semi-supervised (eq. 17) and contrastive (eq. 19) losses that are minimized during training. Note that for pseudo-label guessing we take the average of both network-(1,2) predictions which is not shown here (eq. 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Loss</head><p>We consider another loss function, contrastive loss, which is used only for the data points in D noisy . Let the projection head output corresponding to? strong i,1 and? strong i,2 be z i and z j , respectively. The contrastive loss function <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref> can be defined as</p><formula xml:id="formula_22">? i,j = ? log exp(sim(z i , z j )/?) 2B b=1 1 b? =i exp(sim(z i , z b )/?) ,<label>(18)</label></formula><formula xml:id="formula_23">L C = 1 2B 2B b=1 [? 2b?1,2b + ? 2b,2b?1 ],<label>(19)</label></formula><p>where 1 b? =i is an indicator function that gives a 1 iff b ? = i, ? is a temperature constant, B is the number of samples in mini-batch, and sim(z i , z j ) can be expressed as the cosine similarity between z i and z j . For each mini-batch, there are total 2B augmented samples, since we are creating a pair of augmented samples out of a single sample. Let us consider i and j as a positive pair, then the rest of the data points (2B ? 2) are treated as negative examples. We can compute the final contrastive loss L C across all the positive pairs, both (i, j) and (j, i) in a single mini-batch. The formulation of ? i,j does not require any labels (ground-truth or pseudo-labels). Since contrastive loss does not require labels, it mitigates the negative impact of noisy label memorization.</p><p>Finally, we accumulate all losses to get the total loss,</p><formula xml:id="formula_24">L tot = L semi + ? C L C ,<label>(20)</label></formula><p>where ? C is the contrastive loss coeffiecient. The summary of these steps is provided in Algorithm. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Ablation Studies</head><p>In this section, we analyze the performance of UNICON under different scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1.">Impact of Different Losses</head><p>We observe the contribution of each loss function on the performance of UNICON. It can be observed from <ref type="table" target="#tab_8">Table  8</ref> that each loss term helps in improving the performance while L U has the highest impact on performance. Training without L U indicates that we discard the selected noisy samples completely. The drop in accuracy shows the significance of pseudo-label based feature learning. Improving the quality of these pseudo-labels is one of the primary con- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: One epoch of SSL Training</head><p>Input: network-1 parameters ? (1) = (? <ref type="bibr" target="#b0">(1)</ref> , ? <ref type="bibr" target="#b0">(1)</ref> , ? <ref type="bibr" target="#b0">(1)</ref> ) and network-2 parameters ? (2) = (? <ref type="bibr" target="#b1">(2)</ref> , ? <ref type="bibr" target="#b1">(2)</ref> , ? <ref type="bibr" target="#b1">(2)</ref> ), training set D = (X , Y), number of samples N , number of classes C, sharpening temperature T , unsupervised loss coefficient ?U , contrastive loss coefficient ?C, and regularization coefficient ?r . for k = 1 to 2 do D clean , Dnoisy, d = Uniform-Selection D, (? <ref type="bibr" target="#b0">(1)</ref> , ? <ref type="bibr" target="#b0">(1)</ref> ), (? <ref type="bibr" target="#b1">(2)</ref> , ? <ref type="bibr" target="#b1">(2)</ref> ), N, C (see Alg. 1 of main paper) // Separation of clean and noisy set W = Weight-Estimation d (see eq. 9) // Weights for label-refinement for iter = 1 to num iters do // Second-strongly augmented copy</p><p>Get p b using Eq. 7 // Model Prediction Filter Rate <ref type="figure" target="#fig_0">Figure 11</ref>. Our designed filtering rate R adjusts itself based on the network predictions without manual tuning at each training iteration <ref type="bibr" target="#b65">[66]</ref>. As training progresses and the model gets confident about most of its predictions, UNICON selects more clean samples with better precision. For this graph we used CIFAR10 dataset with 50% symmetric noise.</p><formula xml:id="formula_25">y b = w b y b + (1 ? w b )p b // Label-refinement y b = Sharpen(? b , T ) // Temperature sharpening Getq b using Eq. 10 // Pseudo-label q b = Sharpen(q b , T ) // Temperature sharpenin? X = {(x strong b,m ,? b ); b ? (1, ..., B)} // labeled Set U = {(? strong b,m , q b ); b ? (1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.">Loss Coefficients</head><p>In <ref type="table" target="#tab_9">Table 9</ref>, we show the effect of different loss coefficients. We observe that the performance of UNICON is relatively stable over a large range of coefficient values. We select a value of 30 and 0.025 for ? U and ? C respectively since this set of values result in optimal performance on both CIFAR10 and CIFAR100 datasets. We apply the same loss coefficient value for all datasets irrespective of the class number, number of samples, noise type, noise rate etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.">T-SNE Visualization</head><p>A t-SNE visualization <ref type="bibr" target="#b54">[55]</ref> for features of test images is presented in <ref type="figure" target="#fig_7">Figure 9</ref>. The features are obtained from models trained under different label noise settings. We observe that class separation gets better as the noise level decreases. We further notice that UNICON obtains the best separation of test images at symmetric 50% noise. However, when the noise rate increases it becomes more challenging to learn the class distribution as shown in <ref type="figure">Figure.</ref> 9b and 9c. In addition, we compare the performance of our method with DMix <ref type="bibr" target="#b24">[25]</ref> in the presence of 95% label noise in <ref type="figure" target="#fig_0">Figure 10</ref>. It is a difficult task to separate clean samples from noisy samples under such high noise rate. Interestingly, we observe that our simple approach effectively learns better class distribution in comparison to DMix <ref type="bibr" target="#b24">[25]</ref>. We attribute this to the high precision of our uniform clean sample selection strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4.">Memorization of Noisy Labels</head><p>In case of standard training, the network memorizes the noisy labels leading to poor generalization performance. However, our proposed method UNICON demonstrates resistance to memorization of label noise. We show this phenomena in <ref type="figure">Figure 6</ref>. We observe that with standard training the accuracy improves consistently over different epochs suggesting the memorization of label noise. In sharp contrast to this, the training accuracy of UNICON saturates very quickly indicating that the network is resisting the memorization of noisy labels at later stage of training. For instance, an ideal scenario for 80% symmetric noise would be if the training accuracy is ?20%, i.e. the percentage of clean samples. Furthermore, we notice that our training accuracy deteriorates as we increase the rate of label noise in the training data. This further validates our claim that UNI-CON is effective in combating the memorization of label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.5.">Filter Rate</head><p>In <ref type="figure" target="#fig_0">Figure 11</ref>, we show that the filter rate steadily increases as the network generates more confident predictions (shown for 50% noise rate). At each epoch of training, the filter rate, R is selected based on network predictions. This design decision omits the requirement of manually tuning the selection parameter (filter rate) at each training epoch <ref type="bibr" target="#b65">[66]</ref>. For our experiments, we set d ? , and ? to 0.7, and 5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Baseline Methods</head><p>For CIFAR10 and CIFAR100, we compare UniCon with the following state-of-the art methods: LDMI <ref type="bibr" target="#b61">[62]</ref>, M-Up <ref type="bibr" target="#b70">[71]</ref>, PCIL <ref type="bibr" target="#b66">[67]</ref>, ELR <ref type="bibr" target="#b29">[30]</ref>, DMix <ref type="bibr" target="#b24">[25]</ref>, MOIT <ref type="bibr" target="#b38">[39]</ref>. Methods like ELR <ref type="bibr" target="#b29">[30]</ref> focus on the importance of the early learning regularization in preventing the memorization; MOIT <ref type="bibr" target="#b38">[39]</ref> porposes a multi-objective framework to deal with the noisy labels. For Clothing1M, we consider Joint-Optim <ref type="bibr" target="#b51">[52]</ref>, MetaCleaner <ref type="bibr" target="#b71">[72]</ref> along with ELR <ref type="bibr" target="#b29">[30]</ref> and DMix <ref type="bibr" target="#b24">[25]</ref> . Furthermore, D2L <ref type="bibr" target="#b31">[32]</ref>, MentrorNet <ref type="bibr" target="#b16">[17]</ref> Co-Teaching <ref type="bibr" target="#b11">[12]</ref>, Iterative-CV <ref type="bibr" target="#b57">[58]</ref> are among the methods we consider for WebVision. For TinyImageNet, we compare our method with Decoupling <ref type="bibr" target="#b33">[34]</ref>, MentorNet <ref type="bibr" target="#b16">[17]</ref>, Co-teaching+ <ref type="bibr" target="#b68">[69]</ref>, M-correction <ref type="bibr" target="#b0">[1]</ref>, NCT <ref type="bibr" target="#b44">[45]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1.">Hyper-parameter Settings</head><p>We describe the hyperparameter settings in <ref type="table" target="#tab_0">Table 10</ref>. Note that most of these hyperparameters are the same across all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.">WebVision and Clothing1M</head><p>For Clothing1M dataset, first, we resize the image to 256 ? 256 and then apply random crop to those images to obtain a 224 ? 224 image. On the other hand, each image of WebVision is resized to 320 ? 320 and a random crop of size 299 ? 299 is applied. For WebVision, we consider only 50 classes for training and validation. Similarly, only 50 classes are considered for ILSVRC12 validation set. The percentage of noisy labels in WebVision are estimated to be around 20%. It has been shown that our method obtains slightly lower Top-1 accuracy than state-of-the-art. In some scenarios (low noise level), the experimental results indicate that UNICON underperforms compared to the state-of-theart. Relatively low performance on WebVision dataset can be attributed to the presence of low label noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>UNICON training overview: At each iteration, we employ a uniform selection technique to partition the training set into clean and noisy sets. Upon separation, we perform SSL-training with an additional contrastive loss function. The uniform selection and subsequent SSL-training of two networks (with same architecture) is repeated until convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Proposed Selection Mechanism and SSL-Training: a) For selection, we ensemble the predictions of both networks to calculate JSD. After estimating the filter rate R from JSD distribution, we take equal number (N R/C) of samples from each class. b) We consider separated clean and noisy sets as labeled and unlabeled data only to employ Mix-up<ref type="bibr" target="#b69">[70]</ref> based SSL-training with contrastive loss. On top of classification (Cls.) layer, we add a projection (Proj.) head to facilitate contrastive learning. We train both networks sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>lowest R portion of all values in d (j) , we consider x(j) i to have a clean label. Here, i ? {1, 2, . . . , N j }, N j is the total number of samples in class j, and x (j) i is the i-th image belonging to the j-th class with JSD of d (j)i . Finally, following the UNICON selection criterion, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Sample images from Clothing1M [60] dataset. We show the given label (bottom) and indicate label noise (top) for each image. Noisy samples are marked as positive (red) while clean samples contain negative marks (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>SSL-Training of network-'k' (k=1,2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>T-SNE visualizations of network features of test images. The graphs show class distribution after training the network for 300 epochs on CIAFAR10 dataset with different noise types: (a) 50% symmetric, (b) 80% symmetric, (c) 90% symmetric, (d) 40% asymmetric. Even under extreme label-noise, UNICON effectively learns the true class distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Class distribution learned by DMix<ref type="bibr" target="#b24">[25]</ref> Class distribution learned by (a) the proposed UNICON and (b) DMix<ref type="bibr" target="#b24">[25]</ref> on CIFAR10 dataset with 95% symmetric noise. UNICON shows better class separation even when only 5% samples have correct labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>From (D clean , W), draw a mini-batch {(x b , y b , w b ); b ? (1, ..., B)} // Draw labeled data for SSL From Dnoisy, draw a mini-batch {u b ; b ? (1, ..., B)} // Draw unlabeled data for SSL for b = 1 to B do for m = 1 to 2 d? x weak b,m = Weak-Augment(x b ) // First weakly-augmented cop? u weak b,m = Weak-Augment(u b ) // Second weakly-augmented cop? x strong b,m = Strong-Augment(x b ) // First strongly-augmented cop? u strong b,m = Strong-Augment(u b )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>57.62 51.28 17.18 UNICON (Ours) 90.81 87.61 80.82 50.63 Classification performance (%) of the proposed method on CIFAR10 under severe label noise.</figDesc><table><row><cell>Noise Rate (%)</cell><cell>90%</cell><cell>92%</cell><cell>95%</cell><cell>98%</cell></row><row><cell>DMix [25]</cell><cell>76.08</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>79.4 62.9 42.7 62.0 46.7 19.9 10.1 LDMI [62] 88.3 81.2 43.7 36.9 58.8 51.8 27.9 13.7 M-Up [71] 95.6 87.1 71.6 52.2 67.8 57.3 30.8 14.6 PCIL [67] 92.4 89.1 77.5 58.9 69.4 57.5 31.1 15.3 JPL [20] 93.5 90.2 35.7 23.4 70.9 67.7 17.8 12.8 MOIT [39] 94.1 91.1 75.8 70.1 75.9 70.1 51.4 24.5 DMix [25] 96.1 94.6 92.9 76.0 77.3 74.6 60.2 31.5 ELR [30] 95.8 94.8 93.3 78.7 77.6 73.6 60.8 33.4 UNICON 96.0 95.6 93.9 90.8 78.9 77.6 63.9 44.8</figDesc><table><row><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>Method</cell><cell cols="2">20% 50% 80% 90% 20% 50% 80% 90%</cell></row><row><cell>CE</cell><cell>86.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>81.7 76.1 68.1 53.3 44.5 LDMI [62] 91.1 91.2 84.0 68.1 54.1 46.2 M-Up [71] 93.3 83.3 77.7 72.4 57.6 48.1 JPL [20] 94.2 92.5 90.7 72.0 68.1 59.5 PCIL [67] 93.1 92.9 91.6 76.0 59.3 48.3 DMix * [25] 93.8 92.5 91.7 71.6 69.5 55.1 ELR</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell cols="2">Method</cell><cell>10% 30% 40% 10% 30% 40%</cell></row><row><cell>CE</cell><cell cols="2">88.8 [30] 95.4 94.7 93.0 77.3 74.6 73.2</cell></row><row><cell cols="2">MOIT [39]</cell><cell>94.2 94.1 93.2 77.4 75.1 74.0</cell></row><row><cell cols="2">UNICON</cell><cell>95.3 94.8 94.1 78.2 75.6 74.8</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>Noise (%)</cell><cell></cell><cell>0</cell><cell>20</cell><cell></cell><cell>50</cell></row><row><cell>Alg.</cell><cell cols="5">Best Avg. Best Avg. Best Avg.</cell></row><row><cell>Standard CE</cell><cell cols="5">57.4 56.7 35.8 35.6 19.8 19.6</cell></row><row><cell>Decoupling [34]</cell><cell>-</cell><cell>-</cell><cell cols="3">37.0 36.3 22.8 22.6</cell></row><row><cell>F-correction [41]</cell><cell>-</cell><cell>-</cell><cell cols="3">44.5 44.4 33.1 32.8</cell></row><row><cell>MentorNet [17]</cell><cell>-</cell><cell>-</cell><cell cols="3">45.7 45.5 35.8 35.5</cell></row><row><cell cols="6">Co-teaching+ [69] 52.4 52.1 48.2 47.7 41.8 41.2</cell></row><row><cell>M-correction [1]</cell><cell cols="5">57.7 57.2 57.2 56.6 51.6 51.3</cell></row><row><cell>NCT [45]</cell><cell cols="5">62.4 61.5 58.0 57.2 47.8 47.4</cell></row><row><cell>UNICON</cell><cell cols="5">63.1 62.7 59.2 58.4 52.7 52.4</cell></row><row><cell cols="6">Table 4. Test accuracies (%) on Tiny-ImageNet dataset under sym-</cell></row><row><cell cols="6">metric noise settings. We report the results for other methods di-</cell></row><row><cell cols="6">rectly from [45] with the highest (Best) and the average (Avg.) test</cell></row><row><cell cols="3">accuracy (%) over the last 10 epochs.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="4">Backbone Test Accuracy</cell></row><row><cell>Standard CE</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>69.21</cell></row><row><cell cols="2">Joint-Optim [52]</cell><cell cols="2">ResNet-50</cell><cell>72.00</cell></row><row><cell cols="4">MetaCleaner [72] ResNet-50</cell><cell>72.50</cell></row><row><cell>MLNT [26]</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>73.47</cell></row><row><cell>PCIL [67]</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>73.49</cell></row><row><cell>JPL [20]</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>74.15</cell></row><row><cell>DMix [25]</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>74.76</cell></row><row><cell>ELR [30]</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>74.81</cell></row><row><cell>UNICON</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>74.98</cell></row><row><cell cols="6">Table 5. Experimental results on Clothing1M dataset. Results for</cell></row><row><cell cols="6">previous techniques were copied from their respective papers.</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">WebVision</cell><cell cols="2">ILSVRC12</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>D2L [32]</cell><cell></cell><cell cols="4">62.68 84.00 57.80 81.36</cell></row><row><cell cols="2">MentrorNet [17]</cell><cell cols="4">63.00 81.40 57.80 79.92</cell></row><row><cell cols="6">Co-Teaching [12] 63.58 85.20 61.48 84.70</cell></row><row><cell cols="6">Iterative-CV [58] 65.24 85.34 61.60 84.98</cell></row><row><cell>DivideMix [25]</cell><cell></cell><cell cols="4">77.32 91.64 75.20 90.84</cell></row><row><cell>ELR [30]</cell><cell></cell><cell cols="4">77.78 91.68 70.29 89.76</cell></row><row><cell>MOIT [39]</cell><cell></cell><cell>78.76</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UNICON</cell><cell></cell><cell cols="4">77.60 93.44 75.29 93.72</cell></row></table><note>Experimental results on Webvision and ILSVRC12. All methods are trained on the Webvision while evaluated on both We- bvsion and ILSVRC12 validation set. Results for baseline meth- ods are taken from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>/o balancing 94.28 94.06 91.41 91.16 85.49 85.28 75.26 75.01 60.51 60.16 39.87 39.02 UNICON w/o CL 94.92 94.24 91.67 91.21 87.28 86.34 75.75 75.09 60.54 60.17 41.83 41.11 UNICON w/o ensemble 95.20 94.91 92.38 92.11 88.84 88.18 76.28 76.10 62.98 62.11 42.36 41.56 UNICON 95.61 95.24 93.97 93.97 90.81 89.95 77.63 76.91 63.98 63.13 44.82 44.51 Ablation study with different training settings. Both contrastive loss and class-imbalance affects the performance significantly; especially for high noise rates. Ensembling the outputs of both network during separation seems to improve the performance as well. Test results at last epoch are also shown here. performance of any selection-based noisy label technique. Likewise, the success of UNICON depends on how well it can separate the clean samples.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">CIFAR10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR100</cell><cell></cell><cell></cell></row><row><cell>Noise Rate</cell><cell cols="2">50%</cell><cell cols="2">80%</cell><cell cols="2">90%</cell><cell cols="2">50%</cell><cell cols="2">80%</cell><cell cols="2">90%</cell></row><row><cell>Method</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell></row><row><cell>UNICON w</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>L U 94.89 94.70 87.82 87.10 74.99 74.73 56.94 56.04 UNICON w/o L reg 95.38 95.11 93.59 93.26 76.48 75.87 61.75 60.90 UNICON 95.61 95.24 93.97 93.97 77.63 76.91 63.98 63.13 Contribution of different loss functions on the performance of UNICON. While removing each loss term decreases the test accuracy, LU plays the most important role in obtaining SOTA performance. Test accuracies from the last epoch are also shown.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CIFAR10</cell><cell></cell><cell></cell><cell cols="2">CIFAR100</cell><cell></cell></row><row><cell>Noise Rate</cell><cell cols="2">50%</cell><cell cols="2">80%</cell><cell cols="2">50%</cell><cell cols="2">80%</cell></row><row><cell>Method</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell><cell>Best</cell><cell>Last</cell></row><row><cell>UNICON w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>.025 95.38 94.80 77.12 76.89 30 0.025 95.61 95.24 77.63 76.91 40 0.025 95.42 95.26 77.34 77.18 20 0.050 95.49 94.83 77.46 76.95 30 0.050 95.17 94.56 77.28 76.12 40 0.050 95.35 94.79 77.15 76.44 Performance analysis of UNICON with different loss coefficients (50% symmetric noise). We observe that our proposed method is stable over different values of ?U and ?C.</figDesc><table><row><cell>..., B)}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>etc. Hyperparameter Settings for UNICON. Most of the parameters are the same across different datasets. This shows the general applicability of the proposed UNICON method.</figDesc><table><row><cell>Hyper Parameters</cell><cell cols="4">CIFAR10/100 Tiny-ImageNet200 Clothing1M WebVision</cell></row><row><cell>Optimizer</cell><cell>SGD</cell><cell>SGD</cell><cell>SGD</cell><cell>SGD</cell></row><row><cell>Initial Learning Rate</cell><cell>0.02</cell><cell>0.01</cell><cell>0.002</cell><cell>0.01</cell></row><row><cell>Momentum</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Weight Decay</cell><cell>5e ?4</cell><cell>5e ?4</cell><cell>1e ?3</cell><cell>1e ?3</cell></row><row><cell>Mini-batch Size</cell><cell>64</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>Total Epochs</cell><cell>300/350</cell><cell>350</cell><cell>8</cell><cell>100</cell></row><row><cell>T</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>? C</cell><cell>0.025</cell><cell>0.025</cell><cell>0.025</cell><cell>0.025</cell></row><row><cell>? U</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell></row><row><cell>? r</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>?</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>d ?</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>MixUp, ?</cell><cell>4</cell><cell>2</cell><cell>0.5</cell><cell>0.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haw-Shiuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mc-Callum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zoph ; Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Dandelion Mane, Vijay Vasudevan, and</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integration of news content into web results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06872</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05300</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noisetolerant paradigm for training face recognition cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruirui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11887" to="11896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4804" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards open world object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Kj Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5830" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint negative and positive learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyounguk</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10588" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Noise-resistant deep metric learning with ranking-based instance selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Guo</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6226" to="6236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new baseline for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="316" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02613</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Augmentation strategies for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>H?llerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning with confident examples: Rank pruning for robust classification with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tailin</forename><surname>Curtis G Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01936</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Background-aware pooling and noise-aware loss for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6913" to="6922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-objective interpolation training for robustness to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03683</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dat: Training deep networks robust to label-noise by matching the feature distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6821" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mamshad Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Noisy concurrent training for efficient learning under label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Zonooz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Noiserank: Unsupervised label noise reduction with dependence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karishma</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yalniz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="737" to="753" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVII 16</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Noise-aware fully webly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11326" to="11335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Addersr: Towards energy efficient image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15648" to="15657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatically annotating images with keywords: A review of image annotation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Fong</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihli</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recent Patents on Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contrastive learning based hybrid networks for longtailed image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="943" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Co-mining: Deep face recognition with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9358" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">l d mi: An information-theoretic noise-robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust learning at noisy labeled medical images: Applied to skin lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cheng Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueying</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1280" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Joint noisetolerant learning and meta camera shift adaptation for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4855" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Nonsalient region object mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2623" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Jo-src: A contrastive approach for combating noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeren</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Divergence optimization for noisy universal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2515" to="2524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Metacleaner: Learning to hallucinate clean representations for noisy-labeled visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning with feature-dependent label noise: A progressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07756</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Meta label correction for noisy label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 35th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
