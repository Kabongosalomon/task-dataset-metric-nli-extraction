<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aligning Latent and Image Spaces to Connect the Unconnectable</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
							<email>iskorokhodov@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Elhoseiny KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Gradient, HSE, Skoltech</roleName><forename type="first">Grigorii</forename><surname>Sotnikov</surname></persName>
							<email>gdsotnikov@edu.hse.ru</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Elhoseiny KAUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Aligning Latent and Image Spaces to Connect the Unconnectable</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Our method can generate infinite images of diverse and complex scenes that transition naturally from one into another. It does so without any conditioning and trains without any supervision from a dataset of unrelated square images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this work, we develop a method to generate infinite high-resolution images with diverse and complex content. It is based on a perfectly equivariant generator with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an interpolation of the nearby style codes. We modify the AdaIN mechanism to work in such a setup and train the generator in an adversarial setting to produce images positioned between any two latent vectors. At test time, this allows for generating complex and diverse infinite images and connecting any two unrelated scenes into a single arbitrarily large panorama. Apart from that, we introduce LHQ: a new dataset of 90khigh-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of quality and diversity of the produced infinite images. The project website is located at https://universome.github.io/alis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern image generators are typically designed to synthesize pictures of some fixed size and aspect ratio. The real world, however, continues outside the boundaries of any captured photograph, and so to match this behavior, several recent works develop architectures to produce infinitely large images <ref type="bibr" target="#b13">[39,</ref><ref type="bibr">13,</ref><ref type="bibr" target="#b52">78]</ref>, or images that partially extrapolate outside their boundary <ref type="bibr" target="#b8">[34,</ref><ref type="bibr" target="#b35">61,</ref><ref type="bibr" target="#b52">78]</ref>.</p><p>Most of the prior work on infinite image generation focused on the synthesis of homogeneous texture-like patterns <ref type="bibr" target="#b1">[27,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b13">39]</ref> and did not explore the infinite generation of complex scenes, like nature or city landscapes. The critical challenge of generating such images compared to texture synthesis is making the produced frames globally consistent with one another: when a scene spans across several frames, they should all be conditioned on some shared information. To our knowledge, the existing works explored three ways to achieve this: 1) fit a separate model per scene, so the shared information is encoded in the model's weights (e.g., <ref type="bibr" target="#b13">[39,</ref><ref type="bibr" target="#b30">56,</ref><ref type="bibr" target="#b45">71,</ref><ref type="bibr">14]</ref>); 2) condition the whole generation process on a global latent vector <ref type="bibr" target="#b52">[78,</ref><ref type="bibr" target="#b8">34,</ref><ref type="bibr" target="#b35">61]</ref>; and 3) predict spatial latent codes autoregressively <ref type="bibr">[13]</ref>.</p><p>The first approach requires having a large-resolution photograph (like satellite images) and produces pictures whose variations in style and semantics are limited to the given imagery. The second solution can only perform some limited extrapolation since using a single global latent code cannot encompass the diversity of an infinite scenery (as arXiv:2104.06954v1 [cs.CV] 14 Apr 2021 = (( ? ? ) ? + ( + ) ? )/ Figure 2: Illustration of our alignment procedure. We position initial latent codes (anchors) on the 2D coordinate space and compute latent code w x for each position x as a linear interpolation between its two neighboring anchors. also confirmed by our experiments). The third approach is the most recent and principled one, but the autoregressive inference is dramatically slow <ref type="bibr">[19]</ref>: generating a single 256 2 image with [13]'s method takes us ?10 seconds on a single V100 GPU.</p><p>This work, like <ref type="bibr">[13]</ref>, also seeks to build a model with global consistency and diversity in its generation process. However, in contrast to <ref type="bibr">[13]</ref>, we attack the problem from a different angle. Instead of slowly generating local latent codes autoregressively (to make them coherent with one another), we produce several global latent codes independently and train the generator to connect them.</p><p>We build on top of the recently proposed coordinatebased decoders that produce images based on pixels' coordinate locations <ref type="bibr" target="#b12">[38,</ref><ref type="bibr" target="#b8">34,</ref><ref type="bibr" target="#b35">61,</ref><ref type="bibr">2,</ref><ref type="bibr">8]</ref> and develop the above idea in the following way. Latent codes, when sampled, are positioned on the 2D coordinate space -they constitute the "anchors" of the generation process. Next, each image patch is computed independently from the rest of the image, and the latent vector used for its generation is produced by linear interpolation between nearby anchors. If the distance d ? R between the anchors is sufficiently large, they cover large regions of space with the common global context and make the neighboring frames in these regions be semantically coherent. This idea of aligning latent and image spaces (ALIS) is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Our model is GAN-based [18] and the generator is trained to produce plausible images from any position in space. This is in high contrast to existing coordinate-based approaches that generate samples only from [0, 1] 2 coordinates area, which constitutes a single frame size. To make the model generalize to any position in space, we do not input global coordinates information. Instead, we input only its position relative to the neighboring anchors, which, in turn, could be located arbitrarily on the x-axis.</p><p>We utilize StyleGAN2 architecture <ref type="bibr" target="#b5">[31]</ref> for our method and modify only its generator component. Originally, Style-GAN2 passes latent vectors into the decoder by modulating and demodulating convolutional weights, an adjustment of adaptive instance normalization layer (AdaIN) <ref type="bibr">[25]</ref>. For our generator, we adjust AdaIN differently to make it work with the coordinate-based latent vectors and develop Spatially-Aligned AdaIN (SA-AdaIN), described in Sec 3. Our model is trained in a completely unsupervised way from a dataset of unrelated square image crops, i.e. it never sees full panorama images or even different parts of the same panorama during training. By training it to produce realistic images located between arbitrary anchors describing different content (for example, mountains and a forest), it learns to connect unrelated scenes into a single panorama. This task can be seen as learning to generate camera transitions between two viewpoints located at semantically very different locations.</p><p>We test our approach on several LSUN categories and Landscapes HQ (LHQ): a new dataset consisting of 90k high-resolution nature landscape images that we introduce in this work. We outperform the existing baselines for all the datasets in terms of infinite image quality by at least 4 times and at least 30% in generation speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>GANs. During the past few years, GAN-based models achieved photo-realistic quality in 2D image generation <ref type="bibr">[18,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b5">31]</ref>. Much work has been done to design better optimization objectives [4, 20, 33] and regularization schemes <ref type="bibr" target="#b20">[46,</ref><ref type="bibr" target="#b16">42,</ref><ref type="bibr" target="#b17">43,</ref><ref type="bibr" target="#b47">73]</ref>, make the training more compute <ref type="bibr">[23,</ref><ref type="bibr">15,</ref><ref type="bibr">3]</ref> and data <ref type="bibr" target="#b3">[29,</ref><ref type="bibr" target="#b49">75]</ref> efficient and develop robust evaluation metrics <ref type="bibr">[24,</ref><ref type="bibr" target="#b32">58,</ref><ref type="bibr" target="#b50">76,</ref><ref type="bibr" target="#b31">57]</ref>. Compared to VAEs <ref type="bibr" target="#b6">[32]</ref>, GANs do not provide a ready-to-use encoder. Thus much work has been devoted to either training a separate encoder component <ref type="bibr" target="#b28">[54,</ref><ref type="bibr" target="#b27">53]</ref> or designing procedures of embedding images into the generator's latent space <ref type="bibr">[1,</ref><ref type="bibr" target="#b15">41]</ref>.</p><p>Coordinates conditioning. Coordinates conditioning is the most popular among the NeRF-based <ref type="bibr" target="#b19">[45,</ref><ref type="bibr" target="#b14">40]</ref> and occupancy-modeling <ref type="bibr" target="#b18">[44,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b9">35]</ref> methods. <ref type="bibr" target="#b22">[48,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b33">59]</ref> trained a coordinate-based generator that models a volume which is then rendered and passed to a discriminator. However, several recent works demonstrated that providing positional information can help the 2D world as well. For example, it can improve the performance on several benchmarks <ref type="bibr" target="#b12">[38,</ref><ref type="bibr">2]</ref> and lead to the emergence of some attractive properties like extrapolation <ref type="bibr" target="#b8">[34,</ref><ref type="bibr" target="#b35">61]</ref> or super-resolution <ref type="bibr" target="#b35">[61,</ref><ref type="bibr">8]</ref>. An important question in designing coordinate-based methods is how to embed positional information into a model <ref type="bibr" target="#b42">[68,</ref><ref type="bibr">17]</ref>. Most of the works rely either on raw coordinates <ref type="bibr" target="#b12">[38,</ref><ref type="bibr" target="#b52">78]</ref> or periodic embeddings with log-linearly distributed frequencies <ref type="bibr" target="#b19">[45]</ref>. <ref type="bibr" target="#b34">[60,</ref><ref type="bibr" target="#b37">63]</ref> recently showed that using Gaussian distributed frequencies is a more principled approach. <ref type="bibr" target="#b25">[51]</ref> developed a progressive growing technique for positional embeddings.</p><p>Infinite image generation. Existing works on infinite image generation mainly consider the generation of only texture-like and pattern-like images <ref type="bibr" target="#b1">[27,</ref><ref type="bibr">5,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b13">39,</ref><ref type="bibr" target="#b29">55]</ref>, making it similar to procedural generation <ref type="bibr" target="#b24">[50,</ref><ref type="bibr" target="#b21">47]</ref>. SinGAN <ref type="bibr" target="#b30">[56]</ref> learn a GAN model from a single image and is able to produce its (potentially unbounded) variations. Generating infinite images with diverse, complex and globally coherent content is a more intricate task since one needs to seamlessly stitch both local and global features. Several recent works advanced on this problem by producing images with non-fixed aspect ratios <ref type="bibr" target="#b52">[78,</ref><ref type="bibr">13]</ref> and could be employed for the infinite generation. LocoGAN <ref type="bibr" target="#b52">[78]</ref> used a single global latent code to produce an image to make the generation globally coherent, but using a single global code leads to content saturation (as we show in Sec 5). Taming Transformers (TT) <ref type="bibr">[13]</ref> proposed to generate global codes autoregressively with Transformer <ref type="bibr" target="#b42">[68]</ref>, but since their decoder uses GroupNorm <ref type="bibr" target="#b44">[70]</ref> under the hood, the infinite generation is constrained by the GPU memory at one's disposal (see Appendix E for the details). <ref type="bibr" target="#b10">[36]</ref> proposes both a model and a dataset to generate from a single RGB frame a sequence of images along a camera trajectory. Our approach shares some resemblance to image stitching <ref type="bibr" target="#b36">[62]</ref> but intends to generate an entire scene that lies between the given two images instead of concatenating two existing pictures into a single one without seams. <ref type="bibr" target="#b2">[28]</ref> constructed an infinite image by performing image retrieval + stitching from a vast image collection.</p><p>Image extrapolation. Another close line of research is image extrapolation (or image "outpainting" <ref type="bibr" target="#b46">[72,</ref><ref type="bibr" target="#b41">67]</ref>), i.e., predicting the surrounding context of an image given only its part. The latest approaches in this field rely on using GANs to predict an outlying image patch [21, <ref type="bibr" target="#b38">64,</ref><ref type="bibr" target="#b43">69]</ref>. The fundamental difference of these methods compared to our problem design is the reliance on an input image as a starting point of the generation process.</p><p>Adaptive Normalization. Instance-based normalization techniques were initially developed in the style transfer literature <ref type="bibr">[16]</ref>. Instance Normalization <ref type="bibr" target="#b39">[65]</ref> was proposed to improve feed-forward style transfer <ref type="bibr" target="#b40">[66]</ref> by replacing content image statistics with the style image ones. <ref type="bibr">CIN [12]</ref> learned separate scaling and shifting parameters for each style. AdaIN [25] developed the idea further and used shift and scaling values produced by a separate module to perform style transfer from an arbitrary image. Similar to StyleGAN <ref type="bibr" target="#b4">[30]</ref>, our architecture uses AdaIN <ref type="bibr">[25,</ref><ref type="bibr" target="#b39">65]</ref> to input latent information to the decoder. Many techniques have been developed to reconsider AdaIN for different needs. <ref type="bibr" target="#b5">[31]</ref> observed that using AdaIN inside the generator leads to blob artifacts and replaced it with a hypernetwork-based [22] weights modulation. <ref type="bibr" target="#b26">[52]</ref> proposed to denormalize activations using embeddings extracted from a semantic mask. <ref type="bibr" target="#b51">[77]</ref> modified their approach by producing separate style codes for each semantic class and applied them on a per-region basis. <ref type="bibr" target="#b11">[37]</ref> performed few-shot image-to-image translation from a given source domain via a mapping determined by the target's domain style space. In our case, we do not use shifts and compute the scale weights by interpolating nearby latent codes using their coordinate positions instead of using global ones for the whole image as is done in the traditional AdaIN.</p><p>Equivariant models. <ref type="bibr" target="#b48">[74]</ref> added averaging operation to a convolutional block to improve its invariance/equivariance to shifts. <ref type="bibr" target="#b23">[49]</ref> explored natural equivariance properties inside the existing classifiers.</p><p>[11] developed a convolutional module equivariant to sphere rotations, and [10] generalized this idea to other symmetries. In our case, we manually construct a model to be equivariant to shifts in the coordinate space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We build upon StyleGAN2 <ref type="bibr" target="#b5">[31]</ref> architecture and modify only its generator G as illustrated in <ref type="figure" target="#fig_0">Fig 3.</ref> All the other components, including discriminator D, the loss terms, and the optimization procedure, are left untouched.</p><p>To produce an image with our generator, we first need to sample anchors: latent codes that define the context in the space region in which the image is located. In this work, we consider only horizontal infinite image generation. Thus  <ref type="figure">Figure 4</ref>: Our generator has the equivariance property by construction: we depict three samples with the coordinate shifts of 0, 1/2 and 1, respectively, and this makes the resulted output move accordingly. As highlighted by dash circles, pixel values are equal when their coordinates are equal (up to numerical precision) for different samples. Samples remain of the same quality and diversity for any shift s ? (??, ?).</p><p>we need only three anchors to define the context: left anchor w l , center anchor w c and right anchor w r . Following StyleGAN2, we produce a latent code w with the mapping network w ? F(z) where z ? N (0, I dz ).</p><p>To generate an image, we first need to define its location in space. It is defined relative to the left anchor w l by a position of its left border ? ? [0, 2 ? d ? W ], where d is the distance between anchors and W is the frame width. In this way, ? gives flexibility to position the image anywhere between w l and w r anchors in such a way that it lies entirely inside the region controlled by the anchors. During training, w l , w c , w r are positioned in the locations ?d, 0, d respectively along the x-axis and ? is sampled randomly. At test time, we position anchors w i at positions 0, d, 2d, 3d, ... and move with the step size of ? = W along the x-axis while generating new images. This is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>Traditional StyleGAN2 inputs a latent code into the decoder via an AdaIN-like [25] weight demodulation mechanism, which is not suited for our setup since we use different latent codes depending on a feature coordinate position. This forces us to modify it into Spatially-Aligned AdaIN (SA-AdaIN), which we describe in Sec 3.2.</p><p>Our generator architecture is coordinate-based and, inspired by <ref type="bibr" target="#b8">[34]</ref>, we generate an image as 16 independent vertical patches, which are then concatenated together (see B for the illustration). Independent generation is needed to make the generator learn how to stitch nearby patches using the coordinates information: at test-time, it will be stitching together an infinite amount of them. Such a design also gives rise to an attractive property: spatial equivariance, that we illustrate in <ref type="figure">Figure 4</ref>. It arises from the fact that each patch does not depend on nearby patches, but only on the anchors w l , w c , w r and its relative coordinates position ?. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Aligning Latent and Image Spaces</head><p>The core idea of our model is positioning global latent codes (anchors) on the image coordinates grid and slowly varying them with linear interpolation while moving along the plane. In this way, interpolation between the latent codes can be seen as the camera translation between two unrelated scenes corresponding to different independent anchors. We call those anchors global because each one of them influences many frames. The idea for 1D alignment (i.e., where we move only along a single axis) is illustrated on <ref type="figure">Figure 2</ref>.</p><p>Imagine that we need to generate a pixel or intermediate feature value v at position (x, y) ? R 2 . A traditional generator would produce a latent code w and generate the value based on it: v(x, y) = G(x, y; w). But in our case, we make the latent code be dependent on x (it's trivial to generalize it to be dependent on both x and y):</p><formula xml:id="formula_0">v(x, y) = G(x, y; w x ),<label>(1)</label></formula><p>where w x is computed as a linear interpolation between the nearby anchors w a and w b at positions a, b ? R:</p><formula xml:id="formula_1">w x = ?w a + (1 ? ?)w b ,<label>(2)</label></formula><p>and ? is the normalized distance from</p><formula xml:id="formula_2">x to b: ? = (b ? x)/(b ? a).</formula><p>In this way, latent and image spaces become aligned with one another: any movement in the coordinate space spurs movement in the latent space and vice versa. By training G to produce images in the interpolated regions, it implicitly learns to connect w a and w b with realistic inter-scene frames. At test time, this allows us to connect any two independent w a and w b into a single panorama by generating in-between frames, as can be seen in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatially Aligned AdaIN (SA-AdaIN)</head><p>Our G architecture is based on StyleGAN2's generator, which uses an AdaIN-like mechanism of modulating the decoder's convolutional weights. This mechanism is not suited for our setup since one cannot make convolutional weights be dependent on the coordinate position efficiently. That is why we develop a specialized AdaIN variation that can be implemented efficiently on modern hardware.</p><p>Classical AdaIN [25] works the following way: given an input h ? R c?s 2 of resolution s 2 (for simplicity, we consider it to be square), it first normalizes it across spatial dimensions and then rescales and shifts with parameters ?, ? ? R c :</p><formula xml:id="formula_3">AdaIN(h, ?, ?) = ? ? h ? ?(h) ?(h) + ? (3)</formula><p>where ?(h), ?(h) ? R c are mean and standard deviation computed across the spatial axes and all the operations are applied element-wise. It was recently shown that the performance does not degrade when one removes the shifting <ref type="bibr" target="#b5">[31]</ref>:</p><formula xml:id="formula_4">AdaIN (h, ?) = ? ? h/?(h)<label>(4)</label></formula><p>thus we build on top of this simplified version of AdaIN. Our Spatially-Aligned AdaIN (SA-AdaIN) is an analog of AdaIN' for a scenario where latent and image spaces are aligned with one other (as described in Sec 3.1), i.e. where the latent code is different depending on the coordinate position we compute it in. This section describes it for 1Dcase, i.e., when the latent code changes only across the horizontal axis, but our exposition can be easily generalized to the 2D case (actually, for any N -D).</p><p>While generating an image, ALIS framework uses w l , w c and w r to compute the interpolations w x for the required positions x. However, following StyleGAN2, we input to the convolutional layers not the "raw" latent codes w, but style vectors ? obtained through an affine transform ? = A w + b where denotes the layer index. Since a linear interpolation and an affine transform are interchangeable, from the performance considerations we first compute ? l , ? c , ? r from w l , w c , w r and then compute the interpolated style vectors ? x instead of interpolating w l , w c , w r into w x immediately.</p><p>SA-AdaIN works the following way. Given anchor style vectors ? l , ? c , ? r , image offset ?, distance d between the anchors and the resolution of the hidden representation s, it first computes the grid of interpolated styles ? = [? 1 , ? 2 , ..., ? s ] ? R s?c , where:</p><formula xml:id="formula_5">? k = d???k/s d ? l + ?+k/s d ? c , if ? + k/s &gt; d 2d???k/s d ? c + ?+k/s?d d ? r , otherwise<label>(5)</label></formula><p>This formulation assumes that anchors ? l , ? c , ? r are located at positions ?d, 0, d respectively. Then, just like AdaIN', it normalizes h to obtainh = h/?(h). After that, it element-wise multiplies ? and h, broadcasting the values along the vertical positions:</p><formula xml:id="formula_6">[SA-AdaIN(x, ? l , ? c , ? r , ?)] k = ? k ? [h/?(h)] k , (6)</formula><p>where we denote by [?] k the k-th vertical patch of a variable of size s ? 1 ? c. Note that since our G produces images in a patch-wise fashion, we normalize across patches instead of the full images. Otherwise, it will break the equivariance and lead to seams between nearby frames.</p><p>SA-AdaIN is illustrated in <ref type="figure" target="#fig_3">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Which datasets are "connectable"?</head><p>As mentioned in <ref type="bibr">[13]</ref>, to generate arbitrarily-sized images, we want the data statistics to be invariant to their spatial location in an image. This means that given an image patch, one should be unable to confidently predict which part of an image it comes from. However, many datasets either do not have this property (like FFHQ <ref type="bibr" target="#b4">[30]</ref>) or have it only for a small number of images. To check if images in a given dataset have spatially invariant statistics and to extract a subset of such images, we developed the following simple procedure. Given a dataset, we train a classifier on its patches to predict what part of an image a patch is coming from. If a classifier cannot do this easily (i.e., it has low accuracy for such predictions), then the dataset does have spatially invariant statistics. To extract a subset of such good images, we measure this classifier's confidence on each image and select those for which its confidence is low. This procedure is discussed in details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Landscapes HQ dataset</head><p>We introduce Landscapes HQ (LHQ): a dataset of 90k high-resolution (? 1024 2 ) nature landscapes that we crawled and preprocessed from two sources: Unsplash (60k) and Flickr (30k). We downloaded 500k of images in total using a list of manually constructed 450 search queries and then filtered it out using a blacklist of 320 image tags. After that, we ran a pretrained Mask R-CNN to remove the pictures that likely contain objects on them. As a result, we obtained a dataset of 91,693 high-resolution images. The details are described in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. We test our model on 4 datasets: LSUN Tower <ref type="table" target="#tab_4">Table 1</ref>: Scores for different models on different datasets in terms of FID and ?-FID. "N/A" denotes "not-applicable".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Bridge <ref type="formula" target="#formula_1">256</ref>   Style vectors ? , ? c , ? r are positioned on the 2D coordinate space and we compute a style vector in each location as a linear interpolation between its neighbors. For both AdaIN and SA-AdaIN, we broadcast styles' dimensions in the multiplication operation to match the input tensor shape.</p><p>preprocessed each dataset with the procedure described in Algorithm 1 in Appendix C to extract a subset of data with (approximately) spatially invariant statistics. We emphasize that we focus on infinite horizontal generation, but the framework is easily generalizable for the joint verti-cal+horizontal infinite generation. We also train ALIS on LHQ 1024 2 to test how it scales to high resolutions.</p><p>Evaluation. We use two metrics to compare the methods: a popular FID measure [24] and our additionally proposed ?-FID, which is used to measure the quality and diversity of an "infinite" image. It is computed in the following way. For a model with the output frame size of 256 2 , we first generate a very wide image of size 256 ? (50000 ? 256). Then, we slice it into 50000 frames of size 256 2 without overlaps or gaps and compute FID between the real dataset and those sliced frames. We also compute a total number of parameters and inference speed of the generator module for each baseline. Inference speed is computed as a speed to generate a single frame of size 256 2 on NVidia V100 GPU.</p><p>Baselines. For our main baselines, we use two methods: Taming Transformers (TT) <ref type="bibr">[13]</ref> and LocoGAN <ref type="bibr" target="#b52">[78]</ref>. For LocoGAN, since by default it is a small model (5.5M parameters vs ?50M parameters in StyleGAN2) and does not employ any StyleGAN2 tricks that boost the performance (like style mixing, equalized learning rate, skip/resnet connections, noise injection, normalizations, etc.) we reimplemented it on top of the StyleGAN2 code to make the comparison fair. We also replaced raw coordinates conditioning with Fourier positional embeddings since raw coordinates do not work for (??, ?) range and were recently shown to be inferior <ref type="bibr" target="#b34">[60,</ref><ref type="bibr" target="#b37">63,</ref><ref type="bibr" target="#b35">61,</ref><ref type="bibr">2]</ref>. We called this model Loco-GAN+SG2+Fourier. Besides the methods for infinite image generation, we compute the performance of the traditional StyleGAN2 as a lower bound on the possible image generation quality on a given dataset.</p><p>Each model was trained on 4 v100 GPUs for 2.5 days, except for TT, which was trained for 5 days since it is a two-stage model: 2.5 days for VQGAN and then 2.5 days for Transformer. For TT, we used the official implementation with the default hyperparameters setup for unconditional generation training. See Appendix E for the detailed remarks on the comparison to TT. For StyleGAN2based models, we used config-e setup (i.e. half-sized highresolution layers) from the original paper <ref type="bibr" target="#b5">[31]</ref>. We used precisely the same training settings (loss terms, optimizer parameters, etc.) as the original StyleGAN2 model. Ablations. We also ablate the model in terms of how vital the coordinates information is and how distance between anchors influences generation. For the first part, we replace all Coord-* modules with their non-coordconditioned counterparts. For the second kind of ablations, we vary the distance between the anchors in the coordinate space for values d = 1, 2, 4, 8. This distance can also be understood as an aspect ratio of a single scene.</p><p>Results. The main results are presented in <ref type="table" target="#tab_4">Table 1</ref>, and we provide the qualitative comparison on <ref type="figure" target="#fig_4">Figure 7</ref>. To measure ?-FID for TT, we had to simplify the procedure since it relies on GroupNorm in its decoder and generated   500 images of width 256 ? 100 instead of a single image of width 256 ? 50000. We emphasize, that it is an easier setup and elaborate on this in Appendix E. Note also, that the TT paper [13] mainly focused on conditional im- <ref type="figure">Figure 11</ref>: ALIS allows to resample any part of an image without breaking its "connectivity": frames are still consistent both locally and globally.</p><p>age generation from semantic masks/depths maps/class information/etc, that's why the visual quality of TT's samples is lower for our unconditional setup.</p><p>The ?-FID scores on <ref type="table" target="#tab_4">Table 1</ref> demonstrate that our proposed approach achieves state-of-the-art performance in the generation of infinite images. For the traditional FID measured on independent frames, its performance is on par with LocoGAN. However, the latter completely diverges in terms of infinite image generation because spatial noise does not provide global variability, which is needed to generate diverse scenes. Moreover, we noticed that LocoGAN has learned to ignore spatial noise injection <ref type="bibr" target="#b4">[30]</ref> to make it easier for the decoder to stitch nearby patches, which shuts off its only source of scene variability. For the model without coordinates conditioning, image quality drops: visually, they become blurry (see Appendix F) since it is harder for the model to distinguish small shifts in the coordinate space. When increasing the coordinate distance between anchors (the width of an image equals 1 coordinate unit), traditional FID improves. However, this leads to repetitious generation, as illustrated in <ref type="figure" target="#fig_7">Figure 10</ref>. It happens due to short periods of the periodic coordinate embeddings and anchors changing too slowly in the latent space. When most of the positional embeddings complete their cycle, the model has not received enough update from the anchors' movement and thus starts repeating its generation. The model trained on 1024 2 crops of LHQ achieves FID/?-FID scores of 10.11/10.53 respectively and we illustrate its samples in <ref type="figure">Figure 1</ref> and Appendix F.</p><p>As depicted in <ref type="figure" target="#fig_6">Figure 9</ref>, our model has two failure modes: sampling of too unrelated anchors and repetitious generation. The first issue could be alleviated at test-time by using different sampling schemes like truncation trick <ref type="bibr">[6,</ref><ref type="bibr" target="#b5">31]</ref> or clustering the latent codes. A more principled approach would be to combine autoregressive inference of TT [13] with our ideas, which we leave for future work.</p><p>One of our model's exciting properties is the ability to replace or swap any two parts of an image without breaking neither its local nor global consistency. It is illustrated in <ref type="figure">Figure 11</ref> where we resample different regions of the same landscape. For it, the middle parts are changing, while the corner ones remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed an idea of aligning the latent and image spaces and employed it to build a state-of-the-art model for infinite image generation. We additionally proposed a helpful ?-FID metric and a simple procedure to extract from any dataset a subset of images with approximately spatially invariant statistics. Finally, we introduced LHQ: a novel computer vision dataset consisting of 90k high-resolution nature landscapes. A. Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Limitations</head><p>Our method has the following limitations:</p><p>1. Shift-equivariance of our generator is limited only to the step size equal to the size of a patch, i.e., one cannot take a step smaller than the patch size. In our case, it equals 16 pixels for 256 2 -resolution generator and 64 pixels for 1024 2 -resolution one. In this way, our generator is periodic shift-equivariant <ref type="bibr" target="#b48">[74]</ref>.</p><p>2. As shown in 9, connecting too different scenes leads to seaming artifacts. There are two ways to alleviate this: sampling from the same distribution mode (after clustering the latent space, like in <ref type="figure" target="#fig_2">Figure 25</ref>) and increasing the distance between the anchors. The latter, however, leads to repetitions artifacts, like in 10.</p><p>3. As highlighted in 3.3, infinite image generation makes sense only for datasets of images with spatially invariant statistics and our method exploits this assumption. In this way, if one does not preprocess LSUN Bedroom with Algorithm 1, then the performance of our method will degrade by a larger margin compared to TT [13] for non-infinite generation. However, after preprocessing the dataset, our method can learn to connect those "unconnectable" scenes (see <ref type="figure">Figure 12</ref>). This limitation will not be an issue in practice because when one is interested in non-infinite image generation, then a noninfinite image generator is employed. And when on tries to learn infinite generation on a completely "unconnectable" dataset, then any method will fail to do so (unless some external knowledge is employed to rule out the discrepancies).</p><p>Figure 12: Connecting the "unconnectable" scenes of LSUN Bedroom. Though LSUN Bedroom is a dataset of images with spatially non-invariant statistics (see Appendix C), our method is still able to reasonably connect its scenes after the dataset is preprocessed with Algorithm 1.</p><p>4. Preprocessing the dataset with our proposed procedure in Section 3.3 changes the underlying data distribution since it filters away images with spatially non-invariant statistics. Note that we used the same processed datasets for our baselines in all the experiments to make the comparison fair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Questions and Answers</head><p>Q: What is spatial equivariance? Why is it useful? Is it unique to your approach, or other image generators also have it? A: We call a decoder spatially-equivariant (shift-equivariant) if shifting its input results in an equal shift of the output. It is not to be confused with spatial invariance, where shifting an input does not change the output. The equivariance property is interesting because it is a natural property that one seeks in a generator, i.e., by design, we want a decoder to move in accordance with the coordinate space movements. It was also shown that shift-equivariance improves the performance <ref type="bibr" target="#b48">[74]</ref> of both decoder and encoder modules. Traditional generators do not have it due to upsampling <ref type="bibr" target="#b48">[74]</ref>. While we have upsampling procedures too, in our case they do not break periodic shift-equivariance because they are patchwise.</p><p>Q: Why did you choose the grid size of 16? A: The grid size of 16 allows to perform small enough steps in the coordinate space while not losing the performance too much (as also confirmed by <ref type="bibr" target="#b8">[34]</ref>). Small steps in the coordinate space are needed from a purely design perspective to  <ref type="figure" target="#fig_0">Figure 13</ref>: Ablating the grid size of the ALIS generator on LHQ 256 2 for patchwise generation. Reducing the grid size improves the performance, but in the expense of not being able to do small steps in the coordinate space at test time. I.e. with 2 ? 2 grid, one would achieve the highest FID score but will be able to generate an image only by large chunks (of size W/2) which might be undesirable.  <ref type="bibr" target="#b52">[78]</ref> as our benchmark since it shares all the features of previously developed infinite-texture image generators and was additionally tested on non-texture datasets. N/A denotes "not applicable": while ?-GAN was trained on non-texture images, its generations are in the "texture-like" spirit. generate an image in smaller portions. In the ideal case, one would like to generate an image pixel-by-pixel, but this extreme scenario of having 1 ? 1 patches worsens image quality <ref type="bibr" target="#b35">[61]</ref>. We provide the ablation study for the grid size in <ref type="figure" target="#fig_0">Figure 13</ref>. Q: Why didn't you compare against other methods for infinite image generation? A: There are two reasons for it. First, to the best of our knowledge, there are no infinite generators of semantically complex images, like LSUN Tower or nature landscapes -only texture-like images. This is why we picked "any-aspect-ratio" image generators as our baselines: LocoGAN <ref type="bibr" target="#b52">[78]</ref> and Taming Transformers <ref type="bibr">[13]</ref>. Second, our LocoGAN+SG2+Fourier baseline encompasses the previous developments in infinite texture synthesis (like <ref type="bibr" target="#b1">[27,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b13">39]</ref>). It has the same architecture as the state-of-the-art PS-GAN [5], but is tested not only on textures, but also semantically complex datasets, like LSUN Bedrooms and FFHQ. See <ref type="table" target="#tab_2">Table 2</ref> for the benchmarks comparison in terms of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: How is your method different from panoramas generation?</head><p>A: A panorama is produced by panning, i.e. camera rotation, while our method performs tracking, i.e. camera transition. Also, a (classical) panorama is limited to 360 ? and will start repeating itself in the same manner as LocoGAN does in <ref type="figure" target="#fig_4">Figure 7</ref>. Q: How is your method different from image stitching? A: Image stitching connects nearby images of the same scene into a single large image, while our method merges entirely different scenes.</p><p>Q: Does your framework work for 2D (i.e., joint vertical + horizontal) infinite generation? Why didn't you explore it in the paper?</p><p>A: Yes, it can be easily generalized to this setup by positioning anchors not on a 1D line (as shown in <ref type="figure">Figure 2</ref>), but on a 2D grid instead and using bilinear interpolation instead of the linear one. The problem with 2D infinite generation is that there are no complex datasets for this, only pattern-like ones like textures or satellite imagery, where the global context is unlikely to span several frames. And the existing methods (e.g., LocoGAN <ref type="bibr" target="#b52">[78]</ref>, ?-GAN <ref type="bibr" target="#b13">[39]</ref>, PS-GAN [5], SpatialGAN <ref type="bibr" target="#b1">[27]</ref>, TileGAN [14], etc.) already mastered this setup to a good extent.</p><p>Q: How do you perform instance normalization for layers with 16 ? 16 resolution? Your patch size would be equal to 1 ? 1, so ?(x) = NaN (or 0) for it.</p><p>A: In contrast to CocoGAN <ref type="bibr" target="#b8">[34]</ref>, we use vertical patches (see Appendix B), so the minimal size of our patch is 16 ? 1, hence the std is defined properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: Why ?-FID for LocoGAN is so high?</head><p>A: LocoGAN generates a repeated scene and that's why it gets penalized for mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details B.1. Architecture and training</head><p>We preprocess all the datasets with the procedure described in Algorithm 1 with a threshold parameter of 0.7. We train all the methods on identical datasets to make the comparison fair.</p><p>As being said in the main paper, we develop our approach on top of the StyleGAN2 model. For this, we took the official StyleGAN2-ADA implementation and disabled differentiable augmentations <ref type="bibr" target="#b3">[29,</ref><ref type="bibr" target="#b49">75]</ref> since Taming Transformers [13] do not employ such kind of augmentations, so it would make the comparison unfair. We used a small version of the model (config-e from <ref type="bibr" target="#b5">[31]</ref>) for our 256 2 experiments and the large one (config-f from <ref type="bibr" target="#b5">[31]</ref>) for 1024 2 experiments. They differ in the number of channels in high-resolution layers: the config-f-model uses twice as big dimensionalities for them.</p><p>We preserve all the training hyperparameters the same. Specifically, we use ? = 10 for R1-regularization <ref type="bibr" target="#b16">[42]</ref> for all StyleGAN2-based experiments. We use the same probability of 0.9 for style mixing. In our case, since on each iteration we input 3 noise vectors w l , w c , w r we perform style mixing on them independently. Following StyleGAN2, we also apply the Perceptual Path Loss <ref type="bibr" target="#b5">[31]</ref> regularization with the weight of 2 for our generator G. We also use the Adam optimizer to train the modules with the learning rates of 0.0025 for G and D and betas of 0.0 and 0.99.</p><p>As being said in the main paper, CoordConst and CoordConv3x3 are analogs of StyleGAN's Const block and Conv3x3 respectively, but with coordinates positional embeddings concatenated to the hidden representations. We illustrate this in <ref type="figure" target="#fig_3">Figure 16</ref>. As being said in Section 3, our Conv3x3 is not modulated because one cannot make the convolutional weights be conditioned on a position efficiently and we use SA-AdaIN instead to input the latent code. Const block of StyleGAN2 is just a learnable 3D tensor of size 512 ? 4 ? 4 which is passed as the starting input to the convolution decoder. For CoordConst layer, we used padding mode of repeat for its constant part to make it periodic and not to break the equivariance.</p><p>To avoid numerical issues, we sampled ? only at discrete values of the interval [0, 2d ? W ], corresponding to a left border of each patch.</p><p>Each StyleGAN2-based model was trained for 2500 kimgs (i.e. until D saw 25M real images) with a batch size of 64 (distributed with individual batch size of 16 on 4 GPUs). Training cumulatively took 2.5 days on 4 V100 GPUs.</p><p>For Taming Transformer, we trained the model for 5 days in total: 2.5 days on 4 V100 GPUs for the VQGAN component and 2.5 days on 4 V100 GPUs for the Transformer part. VQGAN was trained with the cumulative batch size of 12 and the Transformer part -with 8. But note that in total it was consuming more GPU memory compared to our model: 10GB vs 6GB per a GPU card. We used the official implementation with the official hyperparameters 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Patchwise generation</head><p>As being said in Section 3, following the recent advances in coordinate-based generators <ref type="bibr" target="#b8">[34,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b35">61]</ref>, our method generates images via spatially independent patches, which is illustrated in <ref type="figure" target="#fig_2">Figure 15c</ref>. The motivation of it is to force the generator learn how to stitch nearby patches based on their positional information and latent codes. This solved the "padding" problem of infinite image generation: since traditional decoders extensively use padding in their implementations, this creates stitching artifacts when the produced frames are merged naively, because padded values are actually filled with neighboring context during the frame-by-frame infinite generation (see <ref type="figure" target="#fig_2">Figure 15a</ref>). A traditional way to solve this <ref type="bibr" target="#b1">[27,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b52">78,</ref><ref type="bibr" target="#b13">39]</ref> is to merge the produced frames with overlaps, like depicted in <ref type="figure" target="#fig_2">Figure 15b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Which datasets are connectable?</head><p>In this section, we elaborate on the importance of having spatially invariant statistics for training an infinite image generator. Since we consider only horizontal infinite image generation, we are concerned about horizontally invariant statistics, but the idea can be easily generalized to vertical+horizontal generation. As noted by <ref type="bibr">[13]</ref>, to train a successful "any-aspectratio" image generator from a dataset of just independent frames, there should be images containing not only scenes, but also transitions between scenes. We illustrate the issue in <ref type="figure" target="#fig_4">Figure 17</ref>. And this property does not hold for many computer vision  datasets, for example FFHQ, CelebA, ImageNet, LSUN Bedroom, etc. To test if a dataset contains such images or not and to extract a subset of good images, we propose the following procedure. Given a dataset, train a binary classifier to predict whether a given half of an image is the left one or right one. Then, if the classifier has low confidence on the test set, the dataset contains images with spatially invariant statistics, since image left/right patch locations cannot be confidently predicted. Now, to extract the subset of such good images, we just measure the confidence of a classifier and pick images with low confidence, as described in Algorithm 1.</p><p>To highlight the importance of this procedure, we trained our model on different subsets of LHQ and LSUN Tower,  The second problem is in too-close-to-camera objects. We propose Algorithm 1 to test if the dataset is connectable and to find a connectable subset of it. After preprocessing LSUN Bedroom, our algorithm can fit it, as shown in <ref type="figure">Figure 12</ref>. The importance of this procedure is also confirmed by FID scores in <ref type="table">Table 3.</ref> varying the confidence threshold t. As one can see from <ref type="table">Table 3</ref>, our procedure is essential to obtain decent results. Also the confidence threshold should not be too small, because too few images are left making the training unstable. As a classifier, we select an ImageNet-pretrained WideResnet50 and train it for 10 epochs with Adam optimizer and learning rates of 1e-5 for the body and 1e-4 for the head. We consider only square center crops of each image. Also, we tested our "horizontal invariance score" for different popular datasets and provide them in <ref type="table" target="#tab_6">Table 4</ref>. As one can see from this table, our proposed dataset contains images with much more spatially invariant statistics.</p><p>We used t = 0.95 for LHQ and t = 0.7 for LSUN datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Landscapes HQ dataset</head><p>To collect the dataset we used two sources: Unsplash 2 and Flickr 3 . For the both sources, the acquisition procedure consisted on 3 stages: constructing search queries, downloading the data, refining the search queries based on manually inspecting the subsets corresponding to different search queries and manually constructed blacklist of keywords.</p><p>After a more-or-less refined dataset collection was obtained, a pretrained Mask R-CNN model was employed to filter out the pictures which likely contained an object. The confidence threshold for "objectivity" was set to 0.2.  <ref type="table">Table 3</ref>: FID scores of ALIS for different confidence thresholds t in Algorithm 1 on LHQ 256 2 and LSUN Tower 256 2 . Preprocessing the dataset with the proposed procedure is essential to obtain decent results, since it filters out "bad" images, i.e. images with spatially non-invariant statistics. We select t = 0.7 for LSUN datasets and t = 0.95 for LHQ since decreasing t filters away too many images which also hurts the FID score. "Dataset size" is the number of images remaining after the filtering procedure. For t = 1, the dataset is untouched. For Unsplash, we used their publicly released dataset of photos descriptions 4 to extract the download urls. It was preprocessed by first searching the appropriate keywords with a whitelist of keywords and then filtering out images with a blacklist of keywords. A whitelist of keywords was constructed the following way. First, we collected 230 prefixes, consisting on geographic and nature-related places. Then, we collected 130 nature-related and object-related suffixes. A whitelist keyword was constructed as a concatenate of a prefix and a suffix, i.e. 230 ? 130 = 30k whitelist keywords. Then, we extracted the images and for each image enumerated all its keywords and if there was a keyword from our blacklist, then the image was removed. A blacklist of keywords was constructed by trial and error by progressively adding the keywords from the following categories: animals, humans, human body parts, human activities, eating, cities, buildings, country-specific sights, plants, drones images (since they do not fit our task), human belongings, improper camera positions. In total, it included 300 keywords. We attach both the whitelist and the blacklist that were used in the supplementary. In total, 230k images was collected and downloaded with this method. For Flickr, we constructed a set of 80 whitelist keywords and downloaded images using its official API 5 . They were much more strict, because there is no way to use a blacklist. In total, 170k images were collected with it.</p><formula xml:id="formula_7">left , r (N ) right ); Set D t ? {x (i) |x (i) ? D rest ? max{r (i) left , r (i) right } &lt; t}; Return D t ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>After Unsplash and Flickr images were downloaded, we ran a pretrained Mask R-CNN model and removed those photos, for which objectivity confidence score was higher than 0.2. In total, that left 60k images for Unsplash and 30k images for Flickr.</p><p>The images come in one of the following licenses: Unsplash License, Creative Commons BY 2.0, Creative Commons BY-NC 2.0, Public Domain Mark 1.0, Public Domain CC0 1.0, or U.S. Government Works license. All these licenes allow the use of the dataset for research purposes.</p><p>We depict 100 random samples from LHQ in <ref type="figure" target="#fig_5">Figure 18</ref> and a word map of keywords in <ref type="figure" target="#fig_6">Figure 19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Remarks on comparison to Taming Transformers</head><p>Using Taming Transformers (TT) [13] as a baseline is complicated for the following reasons:</p><p>? The original paper was mainly focused on conditional image generation. Since our setup is unconditional (and thus harder), the produced samples are of lower quality and it might confuse a reader when comparing them to conditionally generated samples from the original TT's paper [13].</p><p>? It is not a GAN-based model (though there is some auxiliary adversarial loss added during training) -thus, in contrast to LocoGAN or ALIS, it cannot be reimplemented in the StyleGAN2's framework. This makes the comparison to StyleGAN2-based models harder since StyleGAN2 is so well-tuned "out-of-the-box". However, it must be noted that TT is a very recent method and thus was developed with access to all the recent advances in generative modeling. As being said in Section 5, we used the official implementation with the official hyperparameters for unconditional generation training. In total, it was trained for twice as long compared to the rest of the models: 2.5 days on 4 V100 GPUs for the VQGAN part and 2.5 days on 4 V100 GPUs for the Transformer part.</p><p>? Autoregressivy inference makes it very slow at test time. For example, it took 8 days to compute FID and ?-FID scores on a single V100 GPU for a single experiment.</p><p>? Taming Transformer's decoder uses GroupNorm layer <ref type="bibr" target="#b44">[70]</ref>. GroupNorm, as opposed to other normalization layers (like BatchNorm <ref type="bibr" target="#b0">[26]</ref>) does not collect running statistics and computes from the hidden representation even at testtime. It becomes problematic when generating an "infinite" image frame by frame, because neighboring frames use different statistics at forward pass. This is illustrated in <ref type="figure" target="#fig_14">Figure 20</ref>. To solve the for ?-FID, instead of generating a long 256 ? (50000 ? 256) image, we generated 500 256 ? (100 ? 256) images. Note that it makes the task easier (and improves the score) because this increases the diversity of samples, which FID is very sensitive to.</p><p>F. Additional samples 5 https://www.flickr.com/services/api/ <ref type="figure" target="#fig_5">Figure 18</ref>: 100 random images from LHQ dataset, that we introduce in our work. It is a diverse dataset of very different scenes and in high resolution ? 1024 2 . We downsized the images for this figure to avoid performance issues.   <ref type="figure" target="#fig_2">Figure 15</ref>). Since TT's decoder uses GroupNorm <ref type="bibr" target="#b44">[70]</ref>, it computes activations statistics across the whole image, i.e. it needs the whole image to be available simultaneously during inference. If frame-by-frame generation is employed at test-time (to produce an infinite image, for example), then it leads to the illustrated artifacts.   <ref type="figure">Figure 24</ref>. To generate these images, we trained a Gaussian Mixture Model on 20k latent vectors with 8 components. Then, to generate a single long image, we sampled randomly from only a single mode. In this figure, samples from 4 different modes are presented. This improves sample quality since connecting too different anchors (like close-by water and far-away mountains) produces poor performance (see <ref type="figure" target="#fig_6">Figure 9</ref>). But this also decreases the diversity of samples. <ref type="figure" target="#fig_3">Figure 26</ref>: Ablating coordinate embeddings for ALIS on LHQ, LSUN Tower and LSUN Bridge. As being discussed in Section 5, this leads to blurry generations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of our proposed generator. In this illustration, we omit some standard StyleGAN2 layers that are not essential for our architecture to not clutter the exposition. The full architecture is provided in Appendix B. CoordConst and CoordConv3x3 are analogs of Style-GAN2's Const and Conv3x3 blocks, but with coordinates embeddings concatenated to the hidden representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Inference process of our model at test time for d = 2W (i.e. the distance between anchors is twice larger than the frame width). We sample new anchors w i on the fly at positions 0, d, 2d, 3d, .... Since only relative positional information is provided to the decoder, we can decode images at any location x ? (??, ?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top: AdaIN [25] without shifting (as explored in [31]). Bottom: Spatially-Aligned AdaIN (SA-AdaIN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>(a) Taming Transformers[13]  for unconditional generation (the original paper mainly focused on the conditional generation from semantic masks/depth maps/etc).(b) LocoGAN<ref type="bibr" target="#b52">[78]</ref> in the StyleGAN2<ref type="bibr" target="#b5">[31]</ref> framework + Fourier positional embeddings<ref type="bibr" target="#b34">[60,</ref><ref type="bibr" target="#b37">63]</ref> (c) ALIS (ours) Qualitative comparison between different methods on LHQ and LSUN Tower. More samples are in Appendix F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Varying distance d between the anchors for LSUN Tower and LHQ datasets. Larger distance leads to better per-frame image quality, but produces repetitions artifacts as depicted onFigure 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Failure cases of our method: 1) top 2 rowsconnecting too different anchors (like close-by water and far-away mountains); and 2) bottom 2 rows -content repetitions (which arise due to the usage of periodic positional embeddings<ref type="bibr" target="#b37">[63,</ref><ref type="bibr" target="#b34">60]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>A problem of using large distance d between the anchors (in the above case, model was trained with d = 16). Though it improves per-frame image quality, the model starts repeating itself during the generation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Full architecture of our G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Padding problem (b) Overlapped generation [27, 78, 5, 39, 13] (c) Patchwise generation [34, 61, 2] Figure 15: (a) A padding problem that occurs with infinite image generation; (b, c) two strategies to alleviate it. For ALIS, we use patchwise generation and show how to incorporate spatially varying global latent codes into it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>CoordConv3x3. Orange part of the hidden representation denotes coordinates embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Illustration of Conv3x3 and CoordConv3x3. For CoordConv3x3 there are different strategies on how to compute coordinates embeddings, for ALIS, we use periodic positional encoding [60, 63]. Random samples from LSUN Bedroom (top), LSUN Tower (middle) and LHQ (bottom). While LSUN Tower and LHQ have (approximately) horizontally invariant statistics, for LSUN Bedroom this does not hold, which is highlighted in red. For LSUN Bedroom, the first problem occurs due to walls: it is impossible to continue a sample to the left due to wall hitting the camera. And there are no samples in LSUN Bedroom which would have the transition between those walls directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 :</head><label>19</label><figDesc>A word map of keywords in the LHQ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 :</head><label>20</label><figDesc>Illustration of GroupNorm artifacts of TT [13] when frame-by-frame infinite generation is considered. Top: normal samples, bottom: overlapped generation (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 :</head><label>21</label><figDesc>Random samples from Taming Transformer [13] on LHQ, LSUN Tower and LSUN Bridge Figure 22: Random samples from LocoGAN+SG2+Fourier on LHQ, LSUN Tower and LSUN Bridge Figure 23: Random samples from ALIS on LHQ, LSUN Tower and LSUN Bridge Figure 24: Random samples from ALIS on LHQ. Compare to clustered sampling inFigure 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 25 :</head><label>25</label><figDesc>Random samples using "clustered sampling" on LHQ. Compare to random samples in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral cnn. In International Conference on Machine Learning, pages 1321-1330. PMLR, 2019. 3 [11] Taco S Cohen, Mario Geiger, Jonas K?hler, and Max Welling. Spherical cnns. arXiv preprint arXiv:1801.10130,</figDesc><table><row><cell>[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. age2stylegan: How to embed images into the stylegan latent Im-space? In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 4432-4441, 2019. 2 [2] Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, and Denis Korzhenkov. Image generators with conditionally-independent pixel synthesis. arXiv preprint arXiv:2011.13775, 2020. 2, 6, 14, 15 [3] Anonymous. Towards faster and stabilized {gan} training for high-fidelity few-shot image synthesis. In Submitted to In-ternational Conference on Learning Representations, 2021. under review. 2 [4] Martin Arjovsky, Soumith Chintala, and L?on Bottou. Wasserstein generative adversarial networks. In Interna-tional conference on machine learning, pages 214-223. PMLR, 2017. 2 [5] Urs Bergmann, Nikolay Jetchev, and Roland Vollgraf. Learn-ing texture manifolds with the periodic spatial gan. arXiv preprint arXiv:1705.06566, 2017. 1, 2, 13, 14, 15 [6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2, 8 [7] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. arXiv preprint arXiv:2012.00926, 2020. 2 [8] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning con-tinuous image representation with local implicit image func-tion. arXiv preprint arXiv:2012.09161, 2020. 2 [9] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5939-5948, 2019. 2 [10] 2018. 3 [12] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kud-lur. A learned representation for artistic style. arXiv preprint arXiv:1610.07629, 2016. 3 [13] Patrick Esser, Robin Rombach, and Bj?rn Ommer. Taming transformers for high-resolution image synthesis, 2020. 1, 2, 3, 5, 6, 7, 8, 9, 12, 13, 14, 15, 18, 20, 21 [14] Anna Fr?hst?ck, Ibraheem Alhashim, and Peter Wonka. Ti-legan: synthesis of large-scale non-homogeneous textures. ACM Transactions on Graphics (TOG), 38(4):1-11, 2019. 1, 2, 13 [15] Rinon Gal, Dana Cohen, Amit Bermano, and Daniel Cohen-Or. Swagan: A style-based wavelet-driven generative model. arXiv preprint arXiv:2102.06108, 2021. 2 [16] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-age style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2414-2423, 2016. 3 [17] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning, pages 1243-1252. PMLR, 2017. 2 [18] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014. 2 [19] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. 2 [20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017. 2 [21] Dongsheng Guo, Hongzhi Liu, Haoru Zhao, Yunhao Cheng, Qingwei Song, Zhaorui Gu, Haiyong Zheng, and Bing Zheng. Spiral generative network for image extrapolation. In European Conference on Computer Vision, pages 701-717. Springer, 2020. 3 [22] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016. 3 [23] Seungwook Han, Akash Srivastava, Cole Hurwitz, Prasanna Sattigeri, and David D Cox. not-so-biggan: Generating high-fidelity images on a small compute budget. arXiv preprint arXiv:2009.04433, 2020. 2 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib-rium. arXiv preprint arXiv:1706.08500, 2017. 2, 6 [25] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceed-ings of the IEEE International Conference on Computer Vi-</cell></row><row><cell>sion, pages 1501-1510, 2017. 2, 3, 4, 5, 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of existing infinite image generators. We used LocoGAN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Extract a subset with (approximately) horizontally invariant statistics. Input : Dataset D of (H ? W ? C)-sized images Input : Confidence threshold t. Output: Dataset D t ? D of horizontally invariant images. Initialize a binary classifier C ? ; Split D into subsets D train , D val , D rest with ratio 2:1:7; while not converged do Sample a training batch X ? D train ; for i ? 1 to |X| do Update C ? based on training batch {x (i) , y (i) } Evaluate C ? on D rest to obtain scores for each image half R = (r</figDesc><table><row><cell>Randomly sample side s (i) ? {left, right};</cell><cell></cell></row><row><cell>if s (i) = left then</cell><cell></cell></row><row><cell>x (i) ? x (i) [:, : W/2] ; // Select the left half</cell><cell></cell></row><row><cell>y (i) ? 0;</cell><cell></cell></row><row><cell>else</cell><cell></cell></row><row><cell>x (i) ? x (i) [:, W/2 :] ; // Select the right half</cell><cell></cell></row><row><cell>y (i) ? 1;</cell><cell></cell></row><row><cell>end</cell><cell></cell></row><row><cell>end</cell><cell></cell></row><row><cell>|X| i=1 ;</cell><cell></cell></row><row><cell>end</cell><cell></cell></row><row><cell>(1) left , r right , r (1) left , ..., r (2)</cell><cell>(N )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Spatial invariance scores for different datasets. We used Algorithm 1 to train a classifier and computed its mean accuracy on D rest . In all the cases, we used center crops.</figDesc><table><row><cell>Dataset</cell><cell>Mean Test Confidence</cell></row><row><cell>LSUN Bedroom</cell><cell>95.2%</cell></row><row><cell>LSUN Tower</cell><cell>92.7%</cell></row><row><cell>LSUN Bridge</cell><cell>88.6%</cell></row><row><cell>LSUN Church</cell><cell>96.1%</cell></row><row><cell>FFHQ</cell><cell>99.9%</cell></row><row><cell>ImageNet</cell><cell>99.8%</cell></row><row><cell>LHQ</cell><cell>82.5%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/CompVis/taming-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://unsplash.com/ 3 https://www.flickr.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://unsplash.com/data</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Texture synthesis with spatial generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Jetchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08207</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infinite images: Creating and exploring a large photorealistic virtual space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biliana</forename><surname>Kaneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">COCO-GAN: generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep meta functionals for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gidi</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1824" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Infinite nature: Perpetual view generation of natural scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09855</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10551" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03247</idno>
		<title level="m">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Interpreting spatially infinite generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duckworth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02268</idno>
		<title level="m">Nerf in the wild: Neural radiance fields for unconstrained photo collections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pulse: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2437" to="2445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10461</idno>
		<title level="m">The numerics of gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Procedural modeling of buildings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Haegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Papers</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12100</idno>
		<title level="m">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Naturally occurring equivariance in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<ptr target="https://distill.pub/2020/circuits/equivariance.3" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Procedural modeling of cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Yoav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Parish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo-Martin</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brualla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12948</idno>
		<title level="m">Deformable neural radiance fields</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Swapping autoencoder for deep image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial latent autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="14104" to="14113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Bigdeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or?un</forename><surname>G?ksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gramgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16112</idno>
		<title level="m">Deep 3d texture synthesis from 2d exemplars</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00035</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02442</idno>
		<title level="m">Generative radiance fields for 3d-aware image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial generation of continuous images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savva</forename><surname>Ignatyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Image alignment and stitching: A tutorial. Foundations and Trends? in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10739</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Boundless: Generative adversarial networks for image extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10521" to="10530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Image outpainting and harmonization using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basile Van Hoorick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10960</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Widecontext semantic image extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1399" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05217</idno>
		<title level="m">Positional encoding as spatial inductive bias in gans</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very long natural scenery image prediction by outpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10561" to="10570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12027</idno>
		<title level="m">Consistency regularization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hype: A benchmark for human eye perceptual evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Narcomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01121</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5104" to="5113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Locogan -locally convolutional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Struski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Knop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiktor</forename><surname>Daniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemys?aw</forename><surname>Spurek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
