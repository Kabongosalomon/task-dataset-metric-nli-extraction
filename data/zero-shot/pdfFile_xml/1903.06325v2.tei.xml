<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Person Re-identification by Soft Multil- abel Learning Unsupervised Person Re-identification by Soft Multilabel Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-08">8 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
							<email>wuancong@mail2.sysu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><forename type="middle">&amp;quot;</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unsupervised</forename><surname>Person</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>author={Yu</roleName><forename type="first">Multilabel</forename><surname>Learning}</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Ancong</roleName><forename type="first">Wu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang}</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Person Re-identification by Soft Multil- abel Learning Unsupervised Person Re-identification by Soft Multilabel Learning</title>
					</analytic>
					<monogr>
						<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2019} }</title>
						<imprint>
							<date type="published" when="2019-04-08">8 Apr 2019</date>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/KovenYu/MAR For reference of this work, please cite: Re-identification by Soft Multilabel Learning&quot; In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). 2019. Bib: @inproceedings{yu2019unsupervised, title={Unsupervised Person Re-identification by Soft</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although unsupervised person re-identification (RE-ID) has drawn increasing research attentions due to its potential to address the scalability problem of supervised RE-ID models, it is very challenging to learn discriminative information in the absence of pairwise labels across disjoint camera views. To overcome this problem, we propose a deep model for the soft multilabel learning for unsupervised RE-ID. The idea is to learn a soft multilabel (real-valued label likelihood vector) for each unlabeled person by comparing (and representing) the unlabeled person with a set of known reference persons from an auxiliary domain. We propose the soft multilabel-guided hard negative mining to learn a discriminative embedding for the unlabeled target domain by exploring the similarity consistency of the visual features and the soft multilabels of unlabeled target pairs. Since most target pairs are cross-view pairs, we develop the cross-view consistent soft multilabel learning to achieve the learning goal that the soft multilabels are consistently good across different camera views. To enable effecient soft multilabel learning, we introduce the reference agent learning to represent each reference person by a reference agent in a joint embedding. We evaluate our unified deep model on Market-1501 and DukeMTMC-reID. Our model outperforms the state-of-the-art unsupervised RE-ID methods by clear margins. Code is available at https://github.com/KovenYu/MAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Existing person re-identification (RE-ID) works mostly focus on supervised learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33]</ref>. However, they need substantial pairwise labeled data across every pair of camera views, limiting the scalability to large-scale applications where only unlabeled data * Corresponding author is available due to the prohibitive manual efforts in exhaustively labeling the pairwise RE-ID data <ref type="bibr" target="#b48">[49]</ref>. To address the scalability problem, some recent works focus on unsupervised RE-ID by clustering on the target unlabelled data <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b7">8]</ref> or transfering the knowledge from other labeled source dataset <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62]</ref>. However, the performance is still not satisfactory. The main reason is that, without the pairwise label as learning guidence, it is very challenging to discover the identity discriminative information due to the drastic cross-view intra-person appearance variation <ref type="bibr" target="#b51">[52]</ref> and the high inter-person appearance similarity <ref type="bibr" target="#b59">[60]</ref>.</p><p>To address the problem of lacking pairwise label guidance in unsupervised RE-ID, in this work we propose a novel soft multilabel learning to mine the potential label information in the unlabeled RE-ID data. The main idea is, for every unlabeled person image in an unlabeled RE-ID dataset, we learn a soft multilabel (i.e. a real-valued label likelihood vector instead of a single pseudo label) by comparing this unlabeled person with a set of reference persons from an existing labeled auxiliary source dataset. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates this soft multilabel concept.</p><p>Based on this soft multilabel learning concept, we propose to mine the potential discriminative information by the soft multilabel-guided hard negative mining, i.e. we leverage the soft multilabel to distinguish the visually similar but different unlabeled persons. In essence, the soft multilabel represents the unlabelled target person by the reference persons, and thus it encodes the relative comparative characteristic of the unlabeled person, which is from a different perspective than the absolute visual feature representation. Intuitively, a pair of images of the same person should be not only visually similar to each other (i.e. they should have similar absolute visual features), but also equally similar to any other reference person (i.e. they should also have similar relative comparative characteristics with respect to the reference persons). If this similarity consistency between the absolute visual representation and the relative soft multilabel representation is violated, i.e. the pair of images are visually similar but their comparative characteristics are dissimilar, it is probably a hard negative pair.</p><p>In the RE-ID context, most image pairs are cross-view pairs which consist of two person images captured by different camera views. Therefore, we propose to learn the soft multilabels that are consistently good across different camera views. We refer to this learning as the cross-view consistent soft multilabel learning. To enable the efficient soft multilabel learning which requires comparison between the unlabeled persons and the reference persons, we introduce the reference agent learning to represent each reference person by a reference agent which resides in a joint feature embedding with the unlabeled persons. Specifically, we develop a unified deep model named deep soft multilabel reference learning (MAR) which jointly formulates the soft multilabelguided hard negative mining, the cross-view consistent soft multilabel learning and the reference agent learning.</p><p>We summarize our contributions as follows: <ref type="bibr" target="#b0">(1)</ref>. We address the unsupervised RE-ID problem by a novel soft multilabel reference learning method, in which we mine the potential label information latent in the unlabeled RE-ID data by exploiting the auxiliary source dataset for reference comparison. <ref type="bibr" target="#b1">(2)</ref>. We formulate a novel deep model named deep soft multilabel reference learning (MAR). MAR enables simultaneously the soft multilabel-guided hard negative mining, the cross-view consistent soft multilabel learning and the reference agent learning in a unified model. Experimental results on Market-1501 and DukeMTMC-reID show that our model outperforms the state-of-the-art unsupervised RE-ID methods by significant margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised RE-ID. Unsupervised RE-ID refers to that the target dataset is unlabelled but the auxiliary source dataset is not necessarily unlabelled <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62]</ref>. Existing methods either transfer source label knowledge <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62]</ref> or assuming strong prior knowledge (i.e. either assuming the target RE-ID data has specific cluster structure <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b7">8]</ref> or assuming the hand-crafted features could be discriminative enough <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46]</ref>). Recently attempts have been made on exploiting video tracklet associations for unsupervised RE-ID <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. Another line of work focusing on reducing the labelling effort is to minimize the labelling budget on the target <ref type="bibr" target="#b31">[32]</ref> which is complementary to the unsupervised RE-ID. The most related works are the clustering-based models <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b7">8]</ref>, e.g. Yu et.al. <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> proposed an asymmetric metric clustering to discover labels latent in the unlabelled target RE-ID data. The main difference is that the soft multilabel could leverage the auxiliary reference information other than visual feature similarity, while the pseudo label only encodes the feature similarity of an unlabelled pair. Hence, the soft multilabel could mine the potential label information that cannot be discovered by directly comparing the visual features.</p><p>Some unsupervised RE-ID works also proposed to use the labeled source dataset by the unsupervised domain adaptation <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b47">48]</ref> to transfer the discriminative knowledge from the auxiliary source domain. Our model is different from them in that these models do not mine the discriminative information in the unlabeled target domain, which is very important because the transferred discriminative knowledge might be less effective in the target domain due to the domain shift <ref type="bibr" target="#b27">[28]</ref> in discriminative visual clues.</p><p>Unsupervised domain adaptation. Our work is also closely related to the unsupervised domain adaptation (UDA) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>, which also has a source dataset and an unlabeled target dataset. However, they are mostly based on the assumption that the classes are the same between both domains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>, which does not hold in the RE-ID context where the persons (classes) in the source dataset are completely different from the persons in the target dataset, rendering these UDA models inapplicable to the unsupervised RE-ID <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Multilabel classification. Our soft multilabel learning is conceptually different from the multilabel classification <ref type="bibr" target="#b53">[54]</ref>. The multilabel in the multilabel classification <ref type="bibr" target="#b53">[54]</ref> is a groundtruth binary vector indicating whether an instance belongs to a set of classes, while our soft multilabel is learned to represent an unlabeled target person by other different reference persons. Hence, existing multilabel classification models are for a different purpose and thus not suitable to model our idea.</p><p>Zero-shot learning. Zero-shot learning (ZSL) aims to recognize novel testing classes specified by semantic attributes but unseen during training <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b55">56]</ref>. Our soft multilabel reference learning is related to ZSL in that every unknown target person (unseen testing class) is represented by a set of known reference persons (attributes of training classes). However, the predefined semantic attributes are not available in unsupervised RE-ID. Nevertheless, the success of ZSL models validates/justifies the effectiveness of representing an unknown class (person) with a set of different classes. A recent work also explores a similar idea by representing an unknown testing person in an ID regression space which is formed by the known training persons <ref type="bibr" target="#b46">[47]</ref>, but it requires substantial labeled persons from the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Soft Multilabel Reference Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation and Overview</head><p>We have an unlabeled target RE-</p><formula xml:id="formula_0">ID dataset X = {x i } Nu i=1</formula><p>where each x i is an unlabeled person image collected in the target visual surveillance scenario, and an auxiliary RE-</p><formula xml:id="formula_1">ID dataset Z = {z i , w i } Na i=1</formula><p>where each z i is a person image with its label w i = 1, ? ? ? , N p where N p is the number of the reference persons. Note that the reference population is completely non-overlapping with the unlabeled target population since it is collected from a different surveillance scenario <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62]</ref>. Our goal is to learn a soft multilabel function l(?) such that y = l(x, Z) ? (0, 1) Np where all dimensions add up to 1 and each dimension represents the label likelihood w.r.t. a reference person. Simultaneously, we aim to learn a discriminative deep feature embdding f (?) under the guidance of the soft multilabels for the RE-ID task. Specifically, we propose to leverage the soft multilabel for hard negative mining, i.e. for visually similar pairs we determine they are positive or hard negative by comparing their soft multilabels. We refer to this part as the Soft multilabel-guided hard negative mining (Sec. 3.2). In the RE-ID context, most pairs are cross-view pairs which consist of two person images captured by different camera views. Therefore, we aim to learn the soft multilabels that are consistently good across different camera views so that the soft multilabels of the cross-view images are comparable. We refer to this part as the Cross-view consistent soft multilabel learning (Sec. 3.3). To effeciently compare each unlabeled person x to all the reference persons, we introduce the reference agent learning (Sec. 3.4), i.e. we learn a set of reference agents {a i } Np i=1 each of which represents a reference person in the shared joint feature embedding where both the unlabeled person f (x) and the agents {a i } Np i=1 reside (so that they are comparable). Therefore, we could learn the soft multilabel y for x by comparing f (x) with the reference</p><formula xml:id="formula_2">agents {a i } Np i=1 , i.e. the soft multilabel function is simplified to y = l(f (x), {a i } Np i=1 ).</formula><p>We show an overall illustration of our model in <ref type="figure">Figure</ref> 2. In the following, we introduce our deep soft multilabel reference learning (MAR). We first introduce the soft multilabel-guided hard negative mining given the reference agents {a i } Np i=1 and the reference comparability between f (x) and {a i } Np i=1 . To facilitate learning the joint embedding, we enforce a unit norm constraint, i.e. ||f (?)|| 2 = 1, ||a i || 2 = 1, ?i, to learn a hypersphere embedding <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22]</ref>. Note that in the hypersphere embedding, the cosine similarity between a pair of features f (x i ) and f (x j ) is simplified to their inner product f (x i ) T f (x j ), and so as for the reference agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Soft multilabel-guided hard negative mining</head><p>Let us start by defining the soft multilabel function. Since each entry/dimension of the soft multilabel y represents the label likelihood that adds up to 1, we define our soft multilabel function as</p><formula xml:id="formula_3">y (k) = l(f (x), {ai} Np i=1 ) (k) = exp(a T k f (x)) ?i exp(a T i f (x))<label>(1)</label></formula><p>where y (k) is the k-th entry of y.</p><p>It has been shown extensively that mining hard negatives is more important in learning a discriminative embedding than naively learning from all visual samples <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>. We explore a soft multilabel-guided hard negative mining, which focuses on the pairs of visually similar but different persons and aims to distinguish them with the guidance of their soft multilabels. Given that the soft multilabel encodes relative comparative characteristics, we explore a representation consistency: Besides the similar absolute visual features, images of the same person should also have similar relative comparative characteristics (i.e. equally similar to any other reference person). Specifically, we make the following assumption in our model formulation:</p><formula xml:id="formula_4">Assumption 1.</formula><p>If a pair of unlabeled person images x i , x j has high feature similarity f (x i ) T f (x j ), we call the pair a similar pair. If a similar pair has highly similar comparative characteristics, it is probably a positive pair. Otherwise, it is probably a hard negative pair.</p><p>For the similarity measure of the comparative characteristics encoded in the pair of soft multilabels, we propose the soft multilabel agreement A(?, ?), defined by:</p><formula xml:id="formula_5">A(y i , y j ) = y i ? y j = ? k min(y (k) i , y (k) j ) = 1 ? ||y i ? y j || 1 2 ,<label>(2)</label></formula><p>which is based on the well-defined L1 distance. Intuitively, the soft multilabel agreement is an analog to the voting by the reference persons: Every reference person k gives his/her conservative agreement min(y</p><formula xml:id="formula_6">(k) i , y (k)</formula><p>j ) on believing the pair to be positive (the more similar/related a reference person is to the unlabeled pair, the more important is his/her word), and the soft multilabel agreement is cumulated from all the reference persons. The soft multilabel agreement is defined based on L1 distance to treat fairly the agreement of every reference person by taking the absolute value. Now, we mine the hard negative pairs by considering both the feature similarity and soft multilabel agreement according to Assumption 1. We formulate the soft multilabelguided hard negative mining with a mining ratio p: We define the similar pairs in Assumption 1 as the pM pairs  that have highest feature similarities among all the M = N u ? (N u ? 1)/2 pairs within the unlabeled target dataset X . For a similar pair (x i , x j ), if it is also among the top pM pairs that have the highest soft multilabel agreements, we assign (i, j) to the positive set P, otherwise we assign it to the hard negative set N (see <ref type="figure" target="#fig_5">Figure 3</ref>). Formally, we construct:</p><formula xml:id="formula_7">P = {(i, j)|f (xi) T f (xj) ? S, A(yi, yj) ? T } N = {(k, l)|f (x k ) T f (x l ) ? S, A(y k , y l ) &lt; T }<label>(3)</label></formula><p>where S is the cosine similarity (inner product) of the pM -th pair after sorting all M pairs in an descending order according to the feature similarity (i.e. S is a similarity threshold), and T is the similarly defined threshold value for the soft multilabel agreement. Then we formulate the soft Multilabelguided Discriminative embedding Learning by:</p><formula xml:id="formula_8">LMDL = ? log P P + N ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_9">P = 1 |P| ? (i,j)?P exp(?||f (zi) ? f (zj)|| 2 2 ), N = 1 |N | ? (k,l)?N exp(?||f (z k ) ? f (z l )|| 2 2 ).</formula><p>By minimizing L M DL , we are learning a discriminative feature embedding using the mined positive/hard negative pairs. Note that the construction of P and N is dynamic during model training, and we construct them within every batch with the up-to-date feature embedding during model learning (in this case, we simply replace M by M batch = N batch ? (N batch ? 1)/2 where N batch is the number of unlabeled images in a mini-batch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-view consistent soft multilabel learning</head><p>Given the soft multilabel-guided hard negative mining, we notice that most pairs in the RE-ID problem context are  the cross-view pairs which consist of two person images captured by different camera views <ref type="bibr" target="#b51">[52]</ref>. Therefore, the soft multilabel should be consistently good across different camera views to be cross-view comparable. From a distributional perspective, given the reference persons and the unlabeled target dataset X which is collected in a given target domain, the distribution of the comparative characteristic should only depend on the distribution of the person appearance in the target domain and be independent of its camera views. For example, if the target domain is a cold open-air market where customers tend to wear dark clothes, the soft multilabels should have higher label likelihood in the entries which are corresponding to those reference persons who also wear dark, no matter in which target camera view. In other words, the distribution of the soft multilabel in every camera view should be consistent with the distribution of the target domain. Based on the above analysis, we introduce a Cross-view consistent soft Multilabel Learning loss 1 : LCML = ?vd(Pv(y), P(y)) 2 <ref type="bibr" target="#b4">(5)</ref> where P(y) is the soft multilabel distribution in the dataset X , P v (y) is the soft multilabel distribution in the v-th camera view in X , and d(?, ?) is the distance between two distributions. We could use any distributional distance, e.g. the KL divergence <ref type="bibr" target="#b10">[11]</ref> and the Wasserstein distance <ref type="bibr" target="#b1">[2]</ref>. Since we empirically observe that the soft multilabel approximately follows a log-normal distribution, in this work we adopt the simplified 2-Wasserstein distance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> which gives a very simple form (please refer to the supplementary material 2 for the observations of the log-normal distribution and the derivation of the simplified 2-Wasserstein distance):</p><formula xml:id="formula_10">LCML = ?v||?v ? ?|| 2 2 + ||?v ? ?|| 2 2<label>(6)</label></formula><p>where ?/? is the mean/std vector of the log-soft multilabels, ? v /? v is the mean/std vector of the log-soft multilabels in the v-th camera view. The form of L CM L in Eq. <ref type="formula" target="#formula_10">(6)</ref> is computationally cheap and easy-to-compute within a batch. We note that the camera view label is naturally available in the unsupervised RE-ID setting <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b61">62]</ref>, i.e. it is typically known from which camera an image is captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Reference agent learning</head><p>A reference agent serves to represent a unique reference person in the feature embedding like a compact "feature summarizer". Therefore, the reference agents should be mutually discriminated from each other while each of them should be representative of all the corresponding person images. Considering that the reference agents are compared within the soft multilabel function l(?), we formulate the Agent Learning loss as:</p><formula xml:id="formula_11">LAL = ? k ? log l(f (z k ), {ai}) (w k ) = ? k ? log exp(a T w k f (z k )) ?j exp(a T j f (z k ))<label>(7)</label></formula><p>where z k is the k-th person image in the auxiliary dataset with its label w k . By minimizing L AL , we not only learn discriminatively the reference agents, but also endow the feature embedding with basic discriminative power for the soft multilabelguided hard negative mining. Moreover, it reinforces implicitly the validity of the soft multilabel function l(?). Specifically, in the above L AL , the soft multilabel function learns to assign a reference person image f (z k ) with a soft mul-</p><formula xml:id="formula_12">tilabel? k = l(f (z k ), {a i } Np i=1</formula><p>) by comparing f (z k ) to all agents, with the learning goal that? k should have minimal cross-entropy with (i.e. similar enough to) the ideal one-hot label? k = [0, ? ? ? , 0, 1, 0, ? ? ? , 0] which could produce the ideal soft multilabel agreement, i.e. A(? i ,? j ) = 1 if z i and z j are the same person and A(? i ,? j ) = 0 otherwise. However, this L AL is minimized for the auxiliary dataset. To further improve the validity of the soft multilabel function for the unlabeled target dataset (i.e. the reference comparability between f (x) and {a i }), we propose to learn a joint embedding as follows.</p><p>2 https://kovenyu.com/papers/2019_CVPR_MAR_supp. pdf Joint embedding learning for reference comparability. A major challenge in achieving the reference comparability is the domain shift <ref type="bibr" target="#b27">[28]</ref>, which is caused by different person appearance distributions between the two independent domains. To address this challenge, we propose to mine the cross-domain hard negative pairs (i.e. the pair consisting of an unlabeled person f (x) and an auxiliary reference person a i ) to rectify the cross-domain distributional misalignment. Intuitively, for each reference person a i , we search for the unlabeled persons f (x) that are visually similar to a i . For a joint feature embedding where the discriminative distributions are well aligned, a i and f (x) should be discriminative enough to each other despite their high visual similarity. Based on the above discussion, we propose the Reference agent-based Joint embedding learning loss <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_13">LRJ = ?i?j?M i ? k?W i [m ? ai ? f (xj) 2 2 ]+ + ai ? f (z k ) 2 2<label>(8)</label></formula><p>where M i = {j| a i ? f (x j ) 2 2 &lt; m} denotes the mined data associated with the i-th agent a i , m = 1 is the agentbased margin which has been theoretically justified in <ref type="bibr" target="#b43">[44]</ref> with its recommaned value 1, [?] + is the hinge function, and</p><formula xml:id="formula_14">W i = {k|w k = i}. The center-pulling term ||a i ? f (z k )|| 2 2</formula><p>reinforces the representativeness of the reference agents to improve the validity that a i represents a reference person in the cross-domain pairs (a i , f (x j )).</p><p>We formulate the Reference Agent Learning by:</p><formula xml:id="formula_15">LRAL = LAL + ?LRJ<label>(9)</label></formula><p>where ? balances the loss magnitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model training and testing</head><p>To summarize, the loss objective of our deep soft multilabel reference learning (MAR) is formulated by:</p><formula xml:id="formula_16">LMAR = LMDL + ?1LCML + ?2LRAL<label>(10)</label></formula><p>where ? 1 and ? 2 are hyperparameters to control the relative importance of the cross-view consistent soft multilabel learning and the reference agent learning, respectively. We train our model end to end by the Stochastic Gradient Descent (SGD). For testing, we compute the cosine feature similarity of each probe(query)-gallery pair, and obtain the ranking list of the probe image against the gallery images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Evaluation benchmarks. We evaluate our model in two widely used large RE-ID benchmarks Market-1501 <ref type="bibr" target="#b58">[59]</ref> and DukeMTMC-reID <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b29">30]</ref>. The Market-1501 dataset has 32,668 person images of 1,501 identities. There are in total 6 camera views. The Duke dataset has 36,411 person images of 1,404 identities. There are in total 8 camera views. We show example images in <ref type="figure" target="#fig_6">Figure 4</ref>. We follow the standard protocol <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b60">61]</ref> where the training set contains half of the identities, and the testing set contains the other half. We do not use any label of the target datasets during training. The evaluation metrics are the Rank-1/Rank-5 matching accuracy and the mean average precision (MAP) <ref type="bibr" target="#b58">[59]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We set batch size B = 368, half of which randomly samples unlabeled images x and the other half randomly samples z. Since optimizing entropy-based loss L AL with the unit norm constraint has convergence issue <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b36">37]</ref>, we follow the training method in <ref type="bibr" target="#b43">[44]</ref>, i.e. we first pretrain the network using only L AL (without enforcing the unit norm constraint) to endow the basic discriminative power with the embedding and to determine the directions of the reference agents in the hypersphere embedding <ref type="bibr" target="#b43">[44]</ref>, then we enforce the constraint to start our model learning and multiply the constrained inner products by the average inner product value in the pretraining. We set ? 1 = 0.0002 which controls the relative importance of soft multilabel learning and ? 2 = 50 which controls the relative importance of agent reference learning. We show an evaluation on ? 1 and ? 2 in Sec. 4.6. We set the mining ratio p to 5?and set ? = 0.2. Training is on four Titan X GPUs and the total time is about 10 hours. We leave the evaluations on p/? and further details in the supplementary material due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the state of the art</head><p>We compare our model with the state-of-the-art unsupervised RE-ID models including: (1) the hand-crafted feature representation based models LOMO <ref type="bibr" target="#b19">[20]</ref>, BoW <ref type="bibr" target="#b58">[59]</ref>, DIC  <ref type="bibr" target="#b15">[16]</ref>, ISR <ref type="bibr" target="#b20">[21]</ref> and UDML <ref type="bibr" target="#b28">[29]</ref>; (2) the pseudo label learning based models CAMEL <ref type="bibr" target="#b51">[52]</ref>, DECAMEL <ref type="bibr" target="#b52">[53]</ref> and PUL <ref type="bibr" target="#b7">[8]</ref>; and (3) the unsupervised domain adaptation based models TJ-AIDL <ref type="bibr" target="#b47">[48]</ref>, PTGAN <ref type="bibr" target="#b49">[50]</ref>, SPGAN <ref type="bibr" target="#b6">[7]</ref> and HHL <ref type="bibr" target="#b61">[62]</ref>. We show the results in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>.</p><p>From <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table">Table 2</ref> we observe that our model could significantly outperform the state-of-the-art methods. Specifically, our model achieves an improvement over the current state of the art (HHL in ECCV'18) by 20.2%/20.8% on Rank-1 accuracy/MAP in the DukeMTMC-reID dataset and by 5.5%/8.6% in the Market-1501 dataset. This observation validates the effectiveness of MAR.</p><p>Comparison to the hand-crafted feature representation based models. The performance gaps are most significant when comparing our model to the hand-crafted feature based models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. The main reason is that these early works are mostly based on heuristic design, and thus they could not learn optimal discriminative features.</p><p>Comparison to the pseudo label learning based models. Our model significantly outperforms the pseudo label learning based unsupervised RE-ID models <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b7">8]</ref>. A key reason is that our soft multilabel reference learning could exploit the auxiliary reference information to mine the potential discriminative information that is hardly detectable when directly comparing the visual features of a pair of visually similar persons. In contrast, the pseudo label learning based models assign the pseudo label by direct comparison of the visual features (e.g. via K-means clustering <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b7">8]</ref>), rendering them blind to the potential discriminative information.</p><p>Comparison to the unsupervised domain adaptation based models. Compared to the unsupervised domain adaptation based RE-ID models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b47">48]</ref>, our model achieves superior performances. A key reason is that these models only focus on transfering/adapting the discriminative knowledge from the source domain but ignore the discriminative label information mining in the unlabeled target domain. The discriminative knowledge in the source domain could be less effective in the target domain even after adaptation, because the discriminative clues can be drastically different.</p><p>In contrast, our model mines the discriminative information in the unlabeled target data, which contributes direct effectiveness to the target RE-ID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>We perform an ablation study to demonstrate (1) the effectiveness of the soft multilabel guidance and (2) the indispensability of the cross-view consistent soft multilabel learning and the reference agent learning to MAR. For (1), we adopt the pretrained model (i.e. only trained by L AL using the auxiliary source MSMT17 dataset to have basic discriminative power, as mentioned in Sec. 4.2). We also adopt a baseline model that is feature similarity-guided instead of soft multilabel-guided. Specifically, after the same pretraining procedure, we replace the soft multilabel agreement with the feature similarity, i.e. in the hard negative mining we partition the mined similar pairs into two halves by a threshold of feature similarity rather than soft multilabel agreement, and thus regard the high/low similarity half as positive set P/hard negative set N . For (2), we remove the L CM L or L RAL . We show the results in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Effectiveness of the soft multilabel-guided hard negative mining. Comparing MAR to the pretrained model where the soft multilabel-guided hard negative mining is missing, we observe that MAR significantly improves the pretrained model (e.g. on Market-1501/DukeMTMC-reID, MAR improves the pretrained model by 21.5%/24.0% on Rank-1 accuracy). This is because the pretrained model is only dis-criminatively trained on the auxiliary source dataset without mining the discriminative information in the unlabeled target dataset, so that it is only discriminative on the source dataset but not the target. This comparison demonstrates the effectiveness of the soft multilabel-guided hard negative mining. Effectiveness of the soft multilabel agreement guidance. Comparing MAR to the baseline model, we observe that MAR also significantly outperforms the similarity-guided hard negative mining baseline model. (e.g. on Market-1501/DukeMTMC-reID, MAR outperforms the similarityguided hard negative mining baseline by 23.3%/17.1% on Rank-1 accuracy). Furthermore, even when the soft multilabel learning and reference agent learning losses are missing (i.e. "MAR w/o L CM L &amp;L RAL " where the soft multilabel is much worse than MAR), the soft multilabel-guided model still outperforms the similarity-guided model by 14.8%/7.9% on Rank-1 accuracy on Market-1501/DukeMTMC. These demonstrate the effectiveness of the soft multilabel guidance. Indispensability of the soft multilabel learning and the reference agent learning. When the cross-view consistent soft multilabel learning loss is absent, the performances drastically drop (e.g. drop by 7.7%/5.4% on Rank-1 accuracy/MAP in the Market-1501 dataset). This is mainly because optimizing L CM L improves the soft multilabel comparability of the cross-view pairs <ref type="bibr" target="#b51">[52]</ref>, giving more accurate judgement in the positive/hard negative pairs. Hence, the cross-view consistent soft multilabel learning is indispensable in MAR. When the reference agent learning loss is also absent, the performances further drop (e.g. drop by 13.8%/11.8% on Rank-1/MAP in the Market-1501 dataset). This is because in the absence of the reference agent learning, the soft multilabel is learned via comparing to the less valid reference agents (only pretrained). This observation validates the importance of the reference agent learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visual results and insight</head><p>To demonstrate how the proposed soft multilabel reference learning works, in <ref type="figure" target="#fig_7">Figure 5</ref> we show the similar target pairs with the lowest soft multilabel agreements (i.e. the mined soft multilabel-guided hard negative pairs) mined by our trained model. We make the following observations:</p><p>(1) For an unlabeled person image x, the maximal entries (label likelihood) of the learned soft multilabel are corresponding to the reference persons that are highly visually similar to x, i.e. the soft multilabel represents an unlabeled person mainly by visually similar reference persons.</p><p>(2) For a pair of visually similar but unlabeled person images, the soft multilabel reference learning works by discovering potential fine-grained discriminative clues. For example, in the upper-right pair in <ref type="figure" target="#fig_7">Figure 5</ref>, the two men are dressed similarly. A potential fine-grained discriminative clue is whether they have a backpack. For the man taking a backpack, the soft multilabel reference learning assigns maximal label likelihood to two reference persons who also take backpacks, while for the other man the two reference persons do not take backpacks, either. As a result, the soft multilabel agreement is very low, giving a judgement that this is a hard negative pair. We highlight the discovered fine-grained discriminative clues in the bottom of every pair. These observations lead us to conclude that the soft multilabel reference learning distinguishes visually similar persons by giving high label likelihood to different reference persons to produce a low soft multilabel agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Further evaluations</head><p>Various numbers of reference persons. We evaluate how the number of reference persons affect our model learning. In particular, we vary the number by using only the first N u reference persons (except that we keep all data used in L AL to guarantee that the basic discriminative power is not changed). We show the results in <ref type="figure" target="#fig_8">Figure 6</ref>.</p><p>From <ref type="figure" target="#fig_8">Figure 6</ref>(a) we observe that: (1) Empirically, the performances become stable when the number of reference persons are larger than 1,500, which is approximately two times of the number of the training persons in both datasets (750/700 training persons in Market-1501/DukeMTMC-reID). This indicates that MAR does not necessarily require  a very large reference population but a median size, e.g. two times of the training persons. <ref type="formula" target="#formula_5">(2)</ref> When there are only a few reference persons (e.g. 100), the performances drop drastically due to the poor soft multilabel representation capacity of the small reference population. In other words, this indicates that MAR could not be well learned using a very small auxiliary dataset. Hyperparameter evaluations. We evaluate how ? 1 (which controls the relative importance of the soft multilabel learning) and ? 2 (the relative importance of the reference agent learning) affect our model learning. We show the results in <ref type="figure">Figure 7</ref>. From <ref type="figure">Figure 7</ref> we observe that our model learning is stable within a wide range for both hyperparameters (e.g. 2 ? 10 ?5 &lt; ? 1 &lt; 5 ? 10 ?4 and 10 &lt; ? 2 &lt; 200), although both of them should not be too large to overemphasize the soft multilabel/reference agent learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we demonstrate the effectiveness of utilizing auxiliary source RE-ID data for mining the potential label information latent in the unlabeled target RE-ID data. Specifically, we propose MAR which enables simultaneously the soft multilabel-guided hard negative mining, the cross-view consistent soft multilabel learning and the reference agent learning in a unified model. In MAR, we leverage the soft multilabel for mining the latent discriminative information that cannot be discovered by direct comparison of the absolute visual features in the unlabeled RE-ID data. To enable the soft multilabel-guided hard negative mining in MAR, we simultaneously optimize the cross-view consistent soft multilabel learning and the reference agent learning. Experimental results in two benchmarks validate the effectiveness of the proposed MAR and each learning component of MAR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>reference of this work, please cite: Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong and Jian-Huang Lai. "Unsupervised Person Re-identification by Soft Multilabel Learning" In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). 2019. Bib: @inproceedings{yu2019unsupervised, title={Unsupervised Person Re-identification by Soft Multilabel Learning}, author={Yu, Hong-Xing and Zheng, Wei-Shi and Wu, Ancong and Guo, Xiaowei and Gong, Shaogang and Lai, Jian-Huang}, booktitle={Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2019} }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our soft multilabel concept. We learn a soft multilabel (real-valued label vector) for each unlabeled person by comparing to a set of known auxiliary reference persons (thicker arrowline indicates higher label likelihood). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>An illustration of our model MAR. We learn the soft multilabel by comparing each target unlabeled person image f (x) (red circle) to a set of auxiliary reference persons represented by a set of reference agents {ai} (blue triangles, learnable parameters) in the feature embedding. The soft multilabel judges whether a similar pair is positive or hard negative for discriminative embedding learning (Sec. 3.2). The soft multilabel learning and the reference learning are elaborated in Sec. 3.3 and Sec. 3.4, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the soft multilabel-guided hard negative mining. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Dataset examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Visual results of the soft multilabel-guided hard negative mining. Each pair surrounded by the red box is the similar pair mined by our model with the lowest soft multilabel agreements, and the images on their right are the reference persons corresponding to the first/second maximal soft multilabel entries. The first row is from the Market-1501 and the second from DukeMTMC-reID. We highlight the discovered fine-grained discriminative clues in the bottom text for each pair. Please view in the screen and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Evaluation on different numbers of reference persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>6 1 :? 2 Figure 7 .</head><label>127</label><figDesc>Relative importance of the multilabel learning Evaluation on important hyperparameters. For (a) we fix ?2 = 50 and for (b) we fix ?1 = 0.0002.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison to the state-of-the-art unsupervised results in the Market-1501 dataset. Red indicates the best and Blue the second best. Measured by %.</figDesc><table><row><cell>Methods</cell><cell>Reference</cell><cell>rank-1</cell><cell>Market-1501 rank-5</cell><cell>mAP</cell></row><row><cell>LOMO [20]</cell><cell>CVPR'15</cell><cell>27.2</cell><cell>41.6</cell><cell>8.0</cell></row><row><cell>BoW [59]</cell><cell>ICCV'15</cell><cell>35.8</cell><cell>52.4</cell><cell>14.8</cell></row><row><cell>DIC [16]</cell><cell>BMVC'15</cell><cell>50.2</cell><cell>68.8</cell><cell>22.7</cell></row><row><cell>ISR [21]</cell><cell>TPAMI'15</cell><cell>40.3</cell><cell>62.2</cell><cell>14.3</cell></row><row><cell>UDML [29]</cell><cell>CVPR'16</cell><cell>34.5</cell><cell>52.6</cell><cell>12.4</cell></row><row><cell>CAMEL [52]</cell><cell>ICCV'17</cell><cell>54.5</cell><cell>73.1</cell><cell>26.3</cell></row><row><cell>PUL [8]</cell><cell>ToMM'18</cell><cell>45.5</cell><cell>60.7</cell><cell>20.5</cell></row><row><cell>TJ-AIDL [48]</cell><cell>CVPR'18</cell><cell>58.2</cell><cell>74.8</cell><cell>26.5</cell></row><row><cell>PTGAN [50]</cell><cell>CVPR'18</cell><cell>38.6</cell><cell>57.3</cell><cell>15.7</cell></row><row><cell>SPGAN [7]</cell><cell>CVPR'18</cell><cell>51.5</cell><cell>70.1</cell><cell>27.1</cell></row><row><cell>HHL [62]</cell><cell>ECCV'18</cell><cell>62.2</cell><cell>78.8</cell><cell>31.4</cell></row><row><cell>DECAMEL [53]</cell><cell>TPAMI'19</cell><cell>60.2</cell><cell>76.0</cell><cell>32.4</cell></row><row><cell>MAR</cell><cell>This work</cell><cell>67.7</cell><cell>81.9</cell><cell>40.0</cell></row><row><cell cols="5">Table 2. Comparison to the state-of-the-art unsupervised results in</cell></row><row><cell cols="4">the DukeMTMC-reID dataset. Measured by %.</cell><cell></cell></row><row><cell>Methods</cell><cell>Reference</cell><cell cols="3">DukeMTMC-reID rank-1 rank-5 mAP</cell></row><row><cell>LOMO [20]</cell><cell>CVPR'15</cell><cell>12.3</cell><cell>21.3</cell><cell>4.8</cell></row><row><cell>BoW [59]</cell><cell>ICCV'15</cell><cell>17.1</cell><cell>28.8</cell><cell>8.3</cell></row><row><cell>UDML [29]</cell><cell>CVPR'16</cell><cell>18.5</cell><cell>31.4</cell><cell>7.3</cell></row><row><cell>CAMEL [52]</cell><cell>ICCV'17</cell><cell>40.3</cell><cell>57.6</cell><cell>19.8</cell></row><row><cell>PUL [8]</cell><cell>ToMM'18</cell><cell>30.0</cell><cell>43.4</cell><cell>16.4</cell></row><row><cell>TJ-AIDL [48]</cell><cell>CVPR'18</cell><cell>44.3</cell><cell>59.6</cell><cell>23.0</cell></row><row><cell>PTGAN [50]</cell><cell>CVPR'18</cell><cell>27.4</cell><cell>43.6</cell><cell>13.5</cell></row><row><cell>SPGAN [7]</cell><cell>CVPR'18</cell><cell>41.1</cell><cell>56.6</cell><cell>22.3</cell></row><row><cell>HHL [62]</cell><cell>ECCV'18</cell><cell>46.9</cell><cell>61.0</cell><cell>27.2</cell></row><row><cell>MAR</cell><cell>This work</cell><cell>67.1</cell><cell>79.8</cell><cell>48.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study. Please refer to the text in Sec. 4.4.</figDesc><table><row><cell>Methods</cell><cell>rank-1</cell><cell cols="2">Market-1501 rank-5 rank-10</cell><cell>mAP</cell></row><row><cell>Pretrained (source only)</cell><cell>46.2</cell><cell>64.4</cell><cell>71.3</cell><cell>24.6</cell></row><row><cell>Baseline (feature-guided)</cell><cell>44.4</cell><cell>62.5</cell><cell>69.8</cell><cell>21.5</cell></row><row><cell>MAR w/o L CM L</cell><cell>60.0</cell><cell>75.9</cell><cell>81.9</cell><cell>34.6</cell></row><row><cell>MAR w/o L CM L &amp;L RAL</cell><cell>53.9</cell><cell>71.5</cell><cell>77.7</cell><cell>28.2</cell></row><row><cell>MAR w/o L RAL</cell><cell>59.2</cell><cell>76.4</cell><cell>82.3</cell><cell>30.8</cell></row><row><cell>MAR</cell><cell>67.7</cell><cell>81.9</cell><cell>87.3</cell><cell>40.0</cell></row><row><cell>Methods</cell><cell>rank-1</cell><cell cols="2">DukeMTMC-reID rank-5 rank-10</cell><cell>mAP</cell></row><row><cell>Pretrained (source only)</cell><cell>43.1</cell><cell>59.2</cell><cell>65.7</cell><cell>28.8</cell></row><row><cell>Baseline (feature-guided)</cell><cell>50.0</cell><cell>66.4</cell><cell>71.7</cell><cell>31.7</cell></row><row><cell>MAR w/o L CM L</cell><cell>63.2</cell><cell>77.2</cell><cell>82.5</cell><cell>44.9</cell></row><row><cell>MAR w/o L CM L &amp;L RAL</cell><cell>60.1</cell><cell>73.0</cell><cell>78.4</cell><cell>40.4</cell></row><row><cell>MAR w/o L RAL</cell><cell>57.9</cell><cell>72.6</cell><cell>77.8</cell><cell>37.1</cell></row><row><cell>MAR</cell><cell>67.1</cell><cell>79.8</cell><cell>84.2</cell><cell>48.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For conciseness we omit all the averaging divisions for the outer summations in our losses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For brevity we omit the negative auxiliary term (i.e. w k = i) which is for a balanced learning in both domains, as our focus is to rectify the cross-domain distribution misalignment.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">Began: boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep association learning for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved selfsimilarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Wasserstein cnn: Learning invariant features for nir-vis face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person reidentification by unsupervised l1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dictionary learning with iterative laplacian regularisation for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised person reidentification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting transitivity for learning person re-identification models on a budget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Normface: l 2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Person reidentification in identity regression space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Transferable joint attribute-identity deep learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cross-scenario transfer person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised person reidentification by deep asymmetric metric embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">TPAMI(DOI10.1109/TPAMI.2018.2886878</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Person re-identification by saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
