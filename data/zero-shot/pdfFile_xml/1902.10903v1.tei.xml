<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bi-Directional Cascade Network for Perceptual Edge Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
							<email>m-yang4@u.northwestern.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
							<email>yanhu.shan@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
							<email>tjhuang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bi-Directional Cascade Network for Perceptual Edge Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploiting multi-scale representations is critical to improve edge detection for objects at different scales. To extract edges at dramatically different scales, we propose a Bi-Directional Cascade Network (BDCN) structure, where an individual layer is supervised by labeled edges at its specific scale, rather than directly applying the same supervision to all CNN outputs. Furthermore, to enrich multiscale representations learned by BDCN, we introduce a Scale Enhancement Module (SEM) which utilizes dilated convolution to generate multi-scale features, instead of using deeper CNNs or explicitly fusing multi-scale edge maps. These new approaches encourage the learning of multiscale representations in different layers and detect edges that are well delineated by their scales. Learning scale dedicated layers also results in compact network with a fraction of parameters. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and Multicue, and achieve ODS F-measure of 0.828, 1.3% higher than current state-of-the art on BSDS500. The code has been available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Edge detection targets on extracting object boundaries and perceptually salient edges from natural images, which preserve the gist of an image and ignore unintended details. Thus, it is important to a variety of mid-and high-level vision tasks, such as image segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>, object detection and recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, etc. Thanks to research efforts ranging from exploiting low-level visual cues with hand-crafted features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref> to recent deep models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref>, the accuracy of edge detection has been significantly boosted. For example, on the Berkeley Segmentation Data Set and Benchmarks 500 (BSDS500) <ref type="bibr" target="#b0">[1]</ref>, the detection performance has been boosted from 0.598 <ref type="bibr" target="#b6">[7]</ref> to 0.815 <ref type="bibr" target="#b46">[47]</ref> in ODS F-measure.</p><p>Nevertheless, there remain some open issues worthy of studying. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, edges in one image stem from both object-level boundaries and meaningful local details, e.g., the silhouette of human body and the shape of hand gestures. The variety of scale of edges makes it crucial to exploit multi-scale representations for edge detection. Recent neural net based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref> utilize hierarchal features learned by Convolutional Neural Networks (CNN) to obtain multi-scale representations. To generate more powerful multi-scale representation, some researchers adopt very deep networks, like ResNet50 <ref type="bibr" target="#b17">[18]</ref>, as the backbone model of the edge detector. Deeper models generally involve more parameters, making the network hard to train and expensive to infer. Another way is to build an image pyramid and fuse multi-level features, which may involve redundant computations. In another word, can we use a shallow or light network to achieve a comparable or even better performance?</p><p>Another issue is about the CNN training strategy for edge detection, i.e., supervising predictions of different network layers by one general ground truth edge map <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b29">30]</ref>. For instance, HED <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> and RCF <ref type="bibr" target="#b29">[30]</ref> compute edge prediction on each intermediate CNN output to spot edges at different scales, i.e., the lower layers are expected to detect more local image patterns while higher layers capture object-level information with larger receptive fields. Since different network layers attend to depict patterns at different scales, it is not optimal to train those layers with the same supervision. In another word, existing works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b29">30]</ref> enforce each layer of CNN to predict edges at all scales and ignore that one specific intermeadiate layer can only focus on edges at certain scales. Liu et al. <ref type="bibr" target="#b30">[31]</ref> propose to relax the supervisions on intermediate layers using Canny <ref type="bibr" target="#b3">[4]</ref> detectors with layer-specific scales. However, it is hard to decide layer-specific scales through human intervention.</p><p>Aiming to fully exploit the multiple scale cues with a shallow CNN, we introduce a Scale Enhancement Module (SEM) which consists of multiple parallel convolutions with different dilation rates. As shown in image segmentation <ref type="bibr" target="#b4">[5]</ref>, dilated convolution effectively increases the size of receptive fields of network neurons. By involving multiple dilated convolutions, SEM captures multi-scale spatial contexts. Compared with previous strategies, i.e., introducing deeper networks and explicitly fusing multiple edge detections, SEM does not significantly increase network parameters and avoids the repetitive edge detection on image pyramids.</p><p>To address the second issue, each layer in CNN shall be trained by proper layer-specific supervision, e.g., the shallow layers are trained to focus on meaningful details and deep layers should depict object-level boundaries. We propose a Bi-Directional Cascade Network (BDCN) architecture to achieve effective layer-specific edge learning. For each layer in BDCN, its layer-specific supervision is inferred by a bi-directional cascade structure, which propagates the outputs from its adjacent higher and lower layers, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In another word, each layer in BDCN predicts edges in an incremental way w.r.t scale. We hence call the basic block in BDCN, which is constructed by inserting several SEMs into a VGG-type block, as the Incremental Detection Block (ID Block). This bi-directional cascade structure enforces each layer to focus on a specific scale, allowing for a more rational training procedure.</p><p>By combining SEM and BDCN, our method achieves consistent performance on three widely used datasets, i.e., BSDS500, NYUDv2, and Multicue. It achieves ODS Fmeasure of 0.828, 1.3% higher than current state-of-the art CED <ref type="bibr" target="#b46">[47]</ref> on BSDS500. It achieves 0.806 only using the trainval data of BSDS500 for training, and outperforms the human perception (ODS F-measure 0.803). To our best knowledge, we are the first that outperforms human perception by training only on trainval data of BSDS500. Moreover, we achieve a better trade-off between model compactness and accuracy than existing methods relying on deeper models. With a shallow CNN structure, we obtain comparable performance with some well-known methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b1">2]</ref>. For example, we outperform HED <ref type="bibr" target="#b48">[49]</ref> using only 1/6 of its parameters. This shows the validity of our proposed SEM, which enriches the multi-scale representations in CNN. This work is also an original effort studying a rational training strategy for edge detection, i.e., employing the BDCN structure to train each CNN layer with layerspecific supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work is related to edge detection, multi-scale representation learning, and network cascade structure. We briefly review these three lines of works, respectively.</p><p>Edge Detection: Most edge detection methods can be categorized into three groups, i.e., traditional edge operators, learning based methods, and the recent deep learning, respectively. Traditional edge operators <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34]</ref>   <ref type="bibr" target="#b50">[51]</ref> introduce a hierarchical deep model to extract multi-scale features and a gated conditional random field to fuse them.</p><p>Multi-Scale Representation Learning: Extraction and fusion of multi-scale features are fundamental and critical for many vision tasks, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b5">6]</ref>. Multi-scale representations can be constructed from multiple re-scaled images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b10">11]</ref>, i.e., an image pyramid, either by computing features independently at each scale <ref type="bibr" target="#b11">[12]</ref> or using the output from one scale as the input to the next scale <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11]</ref>. Recently, innovative works DeepLab <ref type="bibr" target="#b4">[5]</ref> and PSPNet <ref type="bibr" target="#b54">[55]</ref> use dilated convolutions and pooling to achieve multi-scale feature learning in image segmentation. Chen et al. <ref type="bibr" target="#b5">[6]</ref> propose an attention mechanism to softly weight the multiscale features at each pixel location.</p><p>Like other image patterns, edges vary dramatically in scales. Ren et al. <ref type="bibr" target="#b38">[39]</ref> show that considering multi-scale cues does improve performance of edge detection. Multi-ple scale cues are also used in many approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">51]</ref>. Most of those approaches explore the scalespace of edges, e.g., using Gaussian smoothing at multiple scales <ref type="bibr" target="#b47">[48]</ref> or extracting features from different scaled images <ref type="bibr" target="#b0">[1]</ref>. Recent deep based methods employ image pyramid and hierarchal features. For example, Liu et al. <ref type="bibr" target="#b29">[30]</ref> forward multiple re-scaled images to a CNN independently, then average the results. Our approaches follow a similar intuition, nevertheless, we build SEM to learn multi-scale representations in an efficient way, which avoids repetitive computation on multiple input images.</p><p>Network Cascade: Network cascade <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b25">26]</ref> is an effective scheme for many vision applications like classification <ref type="bibr" target="#b36">[37]</ref>, detection <ref type="bibr" target="#b24">[25]</ref>, pose estimation <ref type="bibr" target="#b45">[46]</ref> and semantic segmentation <ref type="bibr" target="#b25">[26]</ref>. For example, Murthy et al. <ref type="bibr" target="#b36">[37]</ref> treat easy and hard samples with different networks to improve classification accuracy. Yuan et al. <ref type="bibr" target="#b53">[54]</ref> ensemble a set of models with different complexities to process samples with different difficulties. Li et al. <ref type="bibr" target="#b25">[26]</ref> propose to classify easy regions in a shallow network and train deeper networks to deal with hard regions. Lin et al. <ref type="bibr" target="#b28">[29]</ref> propose a top-down architecture with lateral connections to propagate deep semantic features to shallow layers. Different from previous network cascade, BDCN is a bidirectional pseudo-cascade structure, which allows an innovative way to supervise each layer individually for layerspecific edge detection. To our best knowledge, this is an early and original attempt to adopt a cascade architecture in edge detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Let (X, Y ) denote one sample in the training set T, where X = {x j , j = 1, ? ? ? , |X|} is a raw input image and Y = {y j , j = 1, ? ? ? , |X|}, y j ? {0, 1} is the corresponding groundtruth edge map. Considering the scale of edges may vary considerably in one image, we decompose edges in Y into S binary edge maps according to the scale of their depicted objects, i.e.,</p><formula xml:id="formula_0">Y = S s=1 Y s ,<label>(1)</label></formula><p>where Y s contains annotated edges corresponding to a scale s. Note that, we assume the scale of edges is in proportion to the size of their depicted objects. Our goal is to learn an edge detector D(?) capable of detecting edges at different scales. A natural way to design D(?) is to train a deep neural network, where different layers correspond to different sizes of receptive field. Specifically, we can build a neural network N with S convolutional layers. The pooling layers make adjacent convolutional layers depict image patterns at different scales.</p><p>For one training image X, suppose the feature map generated by the s-th convolutional layer is N s (X) ? R w?h?c . Using N s (X) as input, we design a detector D s (?) to spot edges at scale s. The training loss for D s (?) is formulated as</p><formula xml:id="formula_1">L s = X?T |P s ? Y s |,<label>(2)</label></formula><p>where P s = D s (N s (X)) is the edge prediction at scale s. The final detector D(?) hence is derived as the ensemble of detectors learned from scale 1 to S.</p><p>To make the training with Eq. (2) possible, Y s is required. It is not easy to decompose the groundtruth edge map Y manually into different scales, making it hard to obtain the layer-specific supervision Y s for the s-th layer. A possible solution is to approximate Y s based on ground truth label Y and edges predicted at other layers, i.e.,</p><formula xml:id="formula_2">Y s ? Y ? i =s P i .<label>(3)</label></formula><p>However, Y s computed in Eq. <ref type="formula" target="#formula_2">(3)</ref> is not an appropriate layer-specific supervision. In the following paragraph, we briefly explain the reason. According to Eq. (3), for a training image, its predicted edges P s at layer s should approximate Y s , i.e., P s ? Y ? i =s P i . In other words, we can pass the other layers' predictions to layer s for training, resulting in an equivalent formulation, i.e., Y ? i P i . The training objective can thus become L = L(? , Y ), where? = i P i . The gradient w.r.t the prediction P s of layer s is</p><formula xml:id="formula_3">?(L) ?(P s ) = ?(L(? , Y )) ?(P s ) = ?(L(? , Y )) ?(? ) ? ?(? ) ?(P s ) .<label>(4)</label></formula><p>According to Eq. (4), for edge predictions P s , P i at any two layers s and i, s = i, their loss gradients are equal because</p><formula xml:id="formula_4">?(? ) ?(Ps) = ?(? ) ?(Pi) = 1.</formula><p>This implies that, with Eq. <ref type="formula" target="#formula_2">(3)</ref>, the training process dose not necessarily differentiate the scales depicted by different layers, making it not appropriate for our layer-specific scale learning task.</p><p>To address the above issue, we approximate Y s with two complementary supervisions. One ignores the edges with scales smaller than s, and the other ignores the edges with larger scales. Those two supervisions train two edge detectors at each scale. We define those two supervisions at scale s as</p><formula xml:id="formula_5">Y s2d s = Y ? i&lt;s P i s2d , Y d2s s = Y ? i&gt;s P i d2s ,<label>(5)</label></formula><p>where the superscript s2d denotes information propagation from shallow layers to deeper layers, and d2s denotes the prorogation from deep layers to shallower layers. For scale s, the predicted edges P s s2d and P s d2s approximate to Y s2d s and Y s d2s , respectively. Their combination is a reasonable approximation to Y s , i.e.,</p><formula xml:id="formula_6">P s s2d + P s d2s ? 2Y ? i&lt;s P i s2d ? i&gt;s P i d2s ,<label>(6)</label></formula><p>where the edges predicted at scales i = s are depressed. Therefore, we use P s s2d + P s d2s to interpolate the edge prediction at scale s.</p><p>Because different convolutional layers depict different scales, the depth of a neural network determines the range of scales it could model. A shallow network may not be capable to detect edges at all of the S scales. However, a large number of convolutional layers involves too many parameters and makes the training difficult. To enable edge detection at different scales with a shallow network, we propose to enhance the multi-scale representation learned in each convolutional layer with the Scale Enhancement Module (SEM). The detail of SEM will be presented in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture of BDCN</head><p>Based on Eq. (6), we propose a Bi-Directional Cascade Network (BDCN) architecture to achieve layer-specific training for edge detection. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our network is composed of multiple ID Blocks, each of which is learned with different supervisions inferred by a bi-directional cascade structure. Specifically, the network is based on the VGG16 <ref type="bibr" target="#b43">[44]</ref> by removing its three fully connected layers and last pooling layer. The 13 convolutional layers in VGG16 are then divided into 5 blocks, each follows a pooling layer to progressively enlarge the receptive fields in the next block. The VGG blocks evolve into ID Blocks by inserting several SEMs. We illustrate the detailed architecture of BDCN and SEM in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>ID Block is the basic component of our network. Each ID block produces two edge predictions. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, an ID Block consists of several convolutional layers, each is followed by a SEM. The outputs of multiple SEMs are fused and fed into two 1?1 convolutional layers to generate two edges predictions P d2s and P s2d , respec-tively. The cascade structure shown in <ref type="figure" target="#fig_2">Fig. 3</ref> propagates the edge predictions from the shallow layers to deep layers. For the s-th block, P s2d s is trained with supervision Y s2d s computed in Eq. <ref type="bibr" target="#b4">(5)</ref>. P d2s s is trained in a similar way. The final edge prediction is computed by fusing those intermediate edge predictions in a fusion layer using 1?1 convolution.</p><p>Scale Enhancement Module is inserted into each ID Block to enrich the multi-scale representations in it. SEM is inspired by the dilated convolution proposed by Chen et al. <ref type="bibr" target="#b4">[5]</ref> for image segmentation. For an input twodimensional feature map x ? R H?W with a convolution filter w ? R h?w , the output y ? R H ?W of dilated convolution at location (i, j) is computed by</p><formula xml:id="formula_7">y ij = h,w m,n x [i+r?m,j+r?n] ? w [m,n] ,<label>(7)</label></formula><p>where r is the dilation rate, indicating the stride for sampling input feature map. Standard convolution can be treated as a special case with r = 1. Eq. <ref type="formula" target="#formula_7">(7)</ref> shows that dilated convolution enlarges the receptive field of neurons without reducing the resolution of feature maps or increasing the parameters. As shown on the right side of <ref type="figure" target="#fig_2">Fig. 3</ref>, for each SEM we apply K dilated convolutions with different dilation rates. For the k-th dilated convolution, we set its dilation rate as r k = max(1, r 0 ? k), which involves two parameters in SEM: the dilation rate factor r 0 and the number of convolution layers K. They are evaluated in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Training</head><p>Each ID Block in our network is trained with two layerspecific side supervisions. Besides that, we fuse the intermediate edge predictions with a fusion layer as the final result. Therefore, BDCN is trained with three types of loss. We formulate the overall loss L as,</p><formula xml:id="formula_8">L = w side ? L side + w f use ? L f use (P, Y ),<label>(8)</label></formula><formula xml:id="formula_9">L side = S s=1 L(P d2s s , Y d2s s ) + L(P s2d s , Y s2d s ),<label>(9)</label></formula><p>where w side and w f use are weights for the side loss and fusion loss, respectively. P denotes the final edge prediction. The function L(?) is computed at each pixel with respect to its edge annotation. Because the distribution of edge/nonedge pixels is heavily biased, we employ a class-balanced cross-entropy loss as L(?). Because of the inconsistency of annotations among different annotators, we also introduce a threshold ? for loss computation. For a groudtruth Y = (y j , j = 1, ..., |Y |), y j ? (0, 1), we define Y + = {y j , y j &gt; ?} and Y ? = {y j , y j = 0}. Only pixels corresponding to Y + and Y ? are considered in loss computation. We hence  define L(?) as</p><formula xml:id="formula_10">L ? , Y = ?? j?Y? log(1 ?? j ) ? ? j?Y+ log(? j ),<label>(10)</label></formula><p>where? = (? j , j = 1, ..., |? |),? j ? (0, 1) denotes a predicted edge map, ? = ? ? |Y + |/(|Y + | + |Y ? |), ? = |Y ? |/(|Y + | + |Y ? |) balance the edge/non-edge pixels. ? controls the weight of positive over negative samples. <ref type="figure" target="#fig_4">Fig. 4</ref> shows edges detected by different ID blocks. We observe that, edges detected by different ID Blocks correspond to different scales. The shallow ID Blocks produce strong responses on local details and deeper ID Blocks are more sensitive to edges at larger scale. For instance, detailed edges on the body of zebra and butterfly can be detected by shallow ID Block, but are depressed by deeper ID Block. The following section tests the validity of BDCN and SEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate the proposed approach on three public datasets: BSDS500 [1], NYUDv2 <ref type="bibr" target="#b42">[43]</ref>, and Multicue <ref type="bibr" target="#b34">[35]</ref>.</p><p>BSDS500 contains 200 images for training, 100 images for validation, and 200 images for testing. Each image is manually annotated by multiple annotators. The final groundtruth is the averaged annotations by the annotators. We also utilize the strategies in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref> to augment training and validation sets by randomly flipping, scaling and rotating images. Following those works, we also adopt the PASCAL VOC Context dataset <ref type="bibr" target="#b35">[36]</ref> as our training set.</p><p>NYUDv2 consists of 1449 pairs of aligned RGB and depth images. It is split into 381 training, 414 validation, and 654 testing images. NYUDv2 is initially used for scene understanding, hence is also used for edge detection in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30]</ref>. Following those works, we augment the training set by randomly flipping, scaling, and rotating training images.</p><p>Multicue <ref type="bibr" target="#b34">[35]</ref> contains 100 challenging natural scenes. Each scene has two frame sequences taken from left and right view, respectively. The last frame of left-view sequence is annotated with edges and boundaries. Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref>, we randomly split 100 annotated frames into 80 and 20 images for training and testing, respectively. We also augment the training data with the same way in <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement our network using PyTorch.</p><p>The VGG16 <ref type="bibr" target="#b43">[44]</ref> pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> is used to initialize the backbone. The threshold ? used for loss computation is set as 0.3 for BSDS500. ? is set as 0.3 and 0.4 for Multicue boundary and edges datasets, respectively. NYUDv2 provides binary annotations, thus does not need to set ? for loss computation. Following <ref type="bibr" target="#b29">[30]</ref>, we set the parameter ? as 1.1 for BSDS500 and Multicue, set ? as 1.2 for NYUDv2.</p><p>SGD optimizer is adopted to train our network. On BSDS500 and NYUDv2, we set the batch size to 10 for all the experiments. The initial learning rate, momentum and weight decay are set to 1e-6, 0.9, and 2e-4 respectively. The learning rate decreases by 10 times after every 10k iterations. We train 40k iterations for BSDS500 and NYUDv2, 2k and 4k iterations for Multicue boundary and edge, respectively. w side and w f use are set as 0.5, and 1.1, respectively. Since Multicue dataset includes high resolution images, we randomly crop 500?500 patches from each image in training. All the experiments are conducted on a NVIDIA GeForce1080Ti GPU with 11GB memory.</p><p>We follow previous works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>, and perform standard Non-Maximum Suppression (NMS) to produce the final edge maps. For a fair comparison with other work, we report our edge detection performance with commonly used evaluation metrics, including Average Precision (AP), as well as F-measure at both Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS). The maximum tolerance allowed for correct matches between edge predictions and groundtruth annotations is set to 0.0075 for BSDS500 and Multicue dataset, and is set to 0.011 for NYUDv2 dataset as in previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we conduct experiments on BSDS500 to study the impact of parameters and verify each component in our network. We train the network on the BSDS500 training set and evaluate on the validation set. Firstly, we test the impact of the parameters in SEM, i.e., the number of dilated convolutions K and the dilation rate factor r 0 . Experimental results are summarized in <ref type="table">Table 1</ref>.</p><p>Table 1 (a) shows the impact of K with r 0 =4. Note that, K=0 means directly copying the input as output. The results demonstrate that setting K larger than 1 substantially improves the performance. However, too large K does not constantly boost the performance. The reason might be that, large K produces high dimensional outputs and makes edge extraction from such high dimensional data difficult. <ref type="table">Table  1</ref> (b) also shows that larger r 0 improves the performance. But the performance starts to drop with too large r 0 , e.g., r 0 =8. In our following experiments, we fix K=3 and r 0 =4. <ref type="table">Table 2</ref> (a) shows the comparison among different cascade architectures, i.e., single direction cascade from shallow to deep layers (S2D), from deep to shallow layers (D2S), and the bi-directional cascade (S2D+D2S), i.e., the BDCN w/o SEM. Note that, we use the VGG16 network without fully connected layer as baseline. It can be observed that, both S2D and D2S structures outperform the baseline. This shows the validity of the cascade structure in network training. The combination of these two cascade structures, i.e., S2D+D2S, results in the best performance. We further test the performance of combining SEM and S2D+D2S and summarize the results in <ref type="table">Table 2</ref> (b), which shows that SEM and bi-directional cascade structure consistently improve the performance of baseline, e.g., improve the ODS Fmeasure by 0.7% and 0.8% respectively. Combining SEM and S2D+D2S results in the best performance. We can conclude that, the components introduced in our method are valid in boosting edge detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Other Works</head><p>Performance on BSDS500: We compare our approach with recent deep learning based methods including CED <ref type="bibr" target="#b46">[47]</ref>, RCF <ref type="bibr" target="#b29">[30]</ref>, DeepBoundary <ref type="bibr" target="#b22">[23]</ref>, DCD <ref type="bibr" target="#b26">[27]</ref>, <ref type="table">Table 3</ref>. Comparison with other methods on BSDS500 test set. ?indicates trained with additional PASCAL-Context data.</p><p>?indicates the fused result of multi-scale images.</p><p>Method ODS OIS AP Human .803 .803 -SCG <ref type="bibr" target="#b39">[40]</ref> .739 .758 .773 PMI <ref type="bibr" target="#b19">[20]</ref> .741 .769 .799 OEF <ref type="bibr" target="#b16">[17]</ref> .746 .770 .820 DeepContour <ref type="bibr" target="#b41">[42]</ref> .757 .776 .800 HFL <ref type="bibr" target="#b2">[3]</ref> .767 .788 .795 HED <ref type="bibr" target="#b48">[49]</ref> .788 .808 .840 CEDN <ref type="bibr" target="#b52">[53]</ref> ? .788 .804 -COB <ref type="bibr" target="#b31">[32]</ref> .793 .820 .859 DCD <ref type="bibr" target="#b26">[27]</ref> .799 .817 .849 AMH-Net <ref type="bibr" target="#b50">[51]</ref> .798 .829 .869 RCF <ref type="bibr" target="#b29">[30]</ref> .  <ref type="figure">Figure 5</ref>. The precision-recall curves of our method and other works on BSDS500 test set.</p><p>COB <ref type="bibr" target="#b31">[32]</ref>, HED <ref type="bibr" target="#b48">[49]</ref>, HFL <ref type="bibr" target="#b2">[3]</ref>, DeepEdge <ref type="bibr" target="#b1">[2]</ref> and Deep-Contour <ref type="bibr" target="#b41">[42]</ref>, and traditional edge detection methods, including SCG <ref type="bibr" target="#b39">[40]</ref>, PMI <ref type="bibr" target="#b19">[20]</ref> and OEF <ref type="bibr" target="#b16">[17]</ref>. The comparison on BSDS500 is summarized in <ref type="table">Table 3</ref> and <ref type="figure">Fig. 5</ref>, respectively. As shown in the results, our method obtains the Fmeasure ODS of 0.820 using single scale input, and achieves 0.828 with multi-scale inputs, both outperform all of these competing methods. Using a single-scale input, our method still outperforms the recent CED <ref type="bibr" target="#b46">[47]</ref> and Deep-Boundary <ref type="bibr" target="#b22">[23]</ref> that use multi-scale inputs. Our method also outperforms the human perception by 2.5% in F-measure ODS. The F-measure OIS and AP of our approach are also  <ref type="bibr" target="#b14">[15]</ref> .687 .716 .629 OEF <ref type="bibr" target="#b16">[17]</ref> .651 .667 -SE <ref type="bibr" target="#b9">[10]</ref> .695 .708 .679 SE+NG+ <ref type="bibr" target="#b15">[16]</ref> . <ref type="bibr">706</ref>   higher than the ones of the other methods.</p><p>Performance on NYUDv2: NYUDv2 has three types of inputs, i.e., RGB, HHA, and RGB-HHA, respectively. Following previous works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b29">30]</ref>, we perform experiments on all of them. The results of RGB-HHA are obtained by averaging the edges detected on RGB and HHA. <ref type="table" target="#tab_3">Table 4</ref> shows the comparison of our method with several recent approaches, including gPb-ucm <ref type="bibr" target="#b0">[1]</ref>, OEF <ref type="bibr" target="#b16">[17]</ref>, gPb+NG <ref type="bibr" target="#b14">[15]</ref>, SE+NG+ <ref type="bibr" target="#b15">[16]</ref>, SE <ref type="bibr" target="#b9">[10]</ref>, HED <ref type="bibr" target="#b48">[49]</ref>, RCF <ref type="bibr" target="#b29">[30]</ref> and AMH-Net <ref type="bibr" target="#b50">[51]</ref>. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the precision-recall curves of our method and other competitors. All of the evaluation results are based on a single scale input.</p><p>As shown in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure" target="#fig_5">Fig. 6</ref>, our performance is competitive, i.e., outperforms most of the compared works except AMH-Net <ref type="bibr" target="#b50">[51]</ref>. Note that, AMH-Net applies the deeper ResNet50 to construct the edge detector. With a <ref type="table">Table 5</ref>. Comparison with recent works on Multicue. ?indicates the fused result of multi-scale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cat.</head><p>Method ODS OIS AP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary</head><p>Human <ref type="bibr" target="#b34">[35]</ref> .760 (0.017) --Multicue <ref type="bibr" target="#b34">[35]</ref> .720 (0.014) --HED <ref type="bibr" target="#b49">[50]</ref> .814 (0.011) .822 (0.008) .869(0.015) RCF <ref type="bibr" target="#b29">[30]</ref> .817 (0.004) .825 (0.005) -RCF <ref type="bibr" target="#b29">[30]</ref> ? .825 (0.008) .836 (0.007) -BDCN</p><p>.836 (0.001) .846(0.003) .893(0.001) BDCN ?</p><p>.838(0.004) .853(0.009) .906(0.005)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge</head><p>Human <ref type="bibr" target="#b34">[35]</ref> .750 (0.024) --Multicue <ref type="bibr" target="#b34">[35]</ref> .830 (0.002) --HED <ref type="bibr" target="#b49">[50]</ref> .851 (0.014) .864 (0.011) -RCF <ref type="bibr" target="#b29">[30]</ref> .857 (0.004  shallower network, our method still outperforms AMH-Net on the RGB image, i.e., our 0.748 vs. 0.744 of AMH-Net in F-measure ODS. Compared with previous works, our improvement over existing works is actually more substantial, e.g., on NYUDv2 our gains over RCF <ref type="bibr" target="#b29">[30]</ref> and HED <ref type="bibr" target="#b48">[49]</ref> are 0.019 and 0.028 in ODS, higher than the 0.009 gain of RCF <ref type="bibr" target="#b29">[30]</ref> over HED <ref type="bibr" target="#b48">[49]</ref>.</p><p>Performance on Multicue: Multicue consists of two sub datasets, i.e., Multicue boundary and Multicue edge. As done in RCF <ref type="bibr" target="#b29">[30]</ref> and the recent version of HED <ref type="bibr" target="#b49">[50]</ref>, we average the scores of three independent experiments as the final result. We show the comparison with recent works in <ref type="table">Table 5</ref>, where our method achieves substantially higher performance than RCF <ref type="bibr" target="#b29">[30]</ref> and HED <ref type="bibr" target="#b48">[49]</ref>. For boundary detection task, we outperform RCF and HED by 1.3% and 2.4%, respectively in F-measure ODS. For edge detection task, our performance is 3.4% and 4.3% higher than the ones of RCF and HED. Moreover, the performance fluctuation of our method is considerably smaller than those two methods, which means our method delivers more stable results. Some edge detection results generated by our approach on Multicue are presented in <ref type="figure" target="#fig_6">Fig. 7</ref>.</p><p>Discussions: The above experiments have shown the competitive performance of our proposed method. We further test the capability of our method in learning multiscale representations with shallow networks. We test our  approach and RCF with different depth of networks, i.e., using different numbers of convolutional block to construct the edge detection model. <ref type="figure">Fig. 8</ref> presents the results on BSDS500. As shown in <ref type="figure">Fig. 8</ref>, the performance of RCF <ref type="bibr" target="#b29">[30]</ref> drops more substantially than our method as we decrease the depth of networks. This verifies that our approach is more effective in detecting edges with shallow networks. We also show the performance of our approach without the SEM and the BDCN structure. These ablations show that removing either BDCN or SEM degrades the performance. It is also interesting to observe that, without SEM, the performance of our method drops substantially. This hence verifies the importance of SEM to multi-scale representation learning in shallow networks. <ref type="figure" target="#fig_7">Fig. 9</ref> further shows the comparison of parameters vs. performance of our method with other deep net based methods on BSDS500. With 5 convolutional blocks in VGG16, HED <ref type="bibr" target="#b48">[49]</ref>, RCF <ref type="bibr" target="#b29">[30]</ref>, and our method use similar number of parameters, i.e., about 16M. As we decrease the number of ID Blocks from 5 to 2, our number of parameters decreases dramatically, drops to 8.69M, 2.26M, and 0.28M, respectively. Our method still achieves F-measure ODS of 0.766 using only two ID Blocks with 0.28M parameters. It also outperforms HED and RCF with a more shallow network, i.e., with 3 and 4 ID Blocks respectively. For example, it outperforms HED by 0.8% with 3 ID Blocks and just 1/6 parameters of HED. We thus conclude that, our method can achieve promising edge detection accuracy even with a compact shallow network. To further show the advantage of our method, we evaluate the performance of edge predictions by different intermediate layers, and show the comparison with HED <ref type="bibr" target="#b48">[49]</ref> and RCF <ref type="bibr" target="#b29">[30]</ref> in <ref type="table" target="#tab_7">Table 6</ref>. It can be observed that, the intermediate predictions of our network also consistently outperform the ones from HED and RCF, respectively. With 5 ID Blocks, our method runs at about 22fps for edge detection, on par with most DCNN-based methods. With 4, 3 and 2 ID Blocks, it accelerates to 29 fps, 33fps, and 37fps, respectively. <ref type="figure" target="#fig_0">Fig. 10</ref> compares some edge detection results generated by our approach and several recent ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper proposes a Bi-Directional Cascade Network for edge detection. By introducing a bi-directional cascade structure to enforce each layer to focus on a specific scale, BDCN trains each network layer with a layer-specific supervision. To enrich the multi-scale representations learned with a shallow network, we further introduce a Scale Enhancement Module (SEM). Our method compares favorably with over 10 edge detection methods on three datasets, achieving ODS F-measure of 0.828, 1.3% higher than current state-of-art on BSDS500. Our experiments also show that learning scale dedicated layers results in compact networks with a fraction of parameters, e.g., our approach outperforms HED <ref type="bibr" target="#b48">[49]</ref> with only 1/6 of its parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Some images and their ground truth edge maps in BSDS500 dataset. The scale of edges in one image varies considerably, like the boundaries of human body and hands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of BDCN. ID Block denotes the Incremental Detection Block, which is the basic component of BDCN. Each ID Block is trained by layer-specific supervisions inferred by a bi-directional cascade structure. This structure trains each ID Block to spot edges at a proper scale. The predictions of ID Blocks are fused as the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The detailed architecture of BDCN and SEM. For illustration, we only show 3 ID Blocks and the cascade from shallow to deep. The number of ID Blocks in our network can be flexibly set from 2 to 5 (seeFig. 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Examples of edges detected by different ID Blocks (IDB for short). Each ID Block generates two edge predictions, P s2d and P d2s , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The precision-recall curves of our method and compared works on NYUDv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Examples of our edge detection results before Non-Maximum Suppression on Multicue dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of parameters and performance with other methods. The number behind "BDCN" indicates the number of ID Block. ?means the multiscale results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>layers with skip-connections. Liu et al. [30] further learn richer deep representations by concatenating features derived from all convolutional lay- ers. Xu et al.</head><label></label><figDesc></figDesc><table><row><cell>de-</cell></row><row><cell>tect edges by finding sudden changes in intensity, color,</cell></row><row><cell>texture, etc. Learning based methods spot edges by uti-</cell></row><row><cell>lizing supervised models and hand-crafted features. For</cell></row><row><cell>example, Doll?r et al. [10] propose structured edge which</cell></row><row><cell>jointly learns the clustering of groundtruth edges and the</cell></row><row><cell>mapping of image patch to clustered token. Deep learning</cell></row><row><cell>based methods use CNN to extract multi-level hierarchical</cell></row><row><cell>features. Bertasius et al. [2] employ CNN to generate fea-</cell></row><row><cell>tures of candidate contour points. Xie et al. [49] propose</cell></row><row><cell>an end-to-end detection model that leverages the outputs</cell></row><row><cell>from different intermediate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Impact of SEM parameters to the edge detection performance on BSDS500 validation set. (a) shows the impact of K with r0=4. (b) shows the impact of r0 with K=3. Validity of components in BDCN on BSDS500 validation set. (a) tests different cascade architectures. (b) shows the validity of SEM and the bi-directional cascade architecture.</figDesc><table><row><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="3">K ODS OIS</cell><cell>AP</cell><cell></cell><cell>r0</cell><cell>rate</cell><cell>ODS OIS</cell><cell>AP</cell></row><row><cell>0</cell><cell cols="3">.7728 .7881 .8093</cell><cell></cell><cell>0</cell><cell>1,1,1</cell><cell>.7720 .7881 .8116</cell></row><row><cell>1</cell><cell cols="3">.7733 .7845 .8139</cell><cell></cell><cell>1</cell><cell>1,2,3</cell><cell>.7721 .7882 .8124</cell></row><row><cell>2</cell><cell cols="3">.7738 .7876 .8169</cell><cell></cell><cell>2</cell><cell>2,4,6</cell><cell>.7725 .7875 .8132</cell></row><row><cell>3</cell><cell cols="3">.7748 .7894 .8170</cell><cell></cell><cell cols="2">4 4,8,12</cell><cell>.7748 .7894 .8170</cell></row><row><cell>4</cell><cell cols="3">.7745 .7896 .8166</cell><cell></cell><cell cols="2">8 8,16,24 .7742 .7889 .8169</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="2">Architecture</cell><cell cols="2">ODS OIS</cell><cell>AP</cell><cell></cell><cell>Method</cell><cell>ODS OIS</cell><cell>AP</cell></row><row><cell cols="2">baseline</cell><cell cols="3">.7681 .7751 .7912</cell><cell></cell><cell>baseline</cell><cell>.7681 .7751 .7912</cell></row><row><cell>S2D</cell><cell></cell><cell cols="3">.7683 .7802 .7978</cell><cell></cell><cell>SEM</cell><cell>.7748 .7894 .8170</cell></row><row><cell cols="2">D2S S2D+D2S (BDCN w/o SEM)</cell><cell cols="3">.7710 .7816 .8049 .7762 .7872 .8013</cell><cell cols="2">S2D+D2S (BDCN w/o SEM) BDCN</cell><cell>.7762 .7872 .8013 .7765 .7882 .8091</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with recent works on NYUDv2.</figDesc><table><row><cell>Method</cell><cell>ODS OIS AP</cell></row><row><cell>gPb-UCM [1]</cell><cell>.632 .661 .562</cell></row><row><cell>gPb+NG</cell><cell></cell></row><row><cell>RGB</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figure 8. Comparison of edge detection accuracy as we decrease the number of ID Blocks from 5 to 2. HED learned with VGG16 is denoted as the solid line for comparison.</figDesc><table><row><cell cols="2">0.83</cell><cell></cell><cell cols="3">ODS</cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell>OIS</cell><cell></cell><cell></cell><cell>0.9</cell><cell>AP</cell></row><row><cell cols="2">0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.86</cell></row><row><cell cols="2">0.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.82</cell></row><row><cell cols="2">0.71 0.74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.74 0.78</cell></row><row><cell cols="2">0.68</cell><cell>5</cell><cell>4</cell><cell></cell><cell></cell><cell>3</cell><cell>2</cell><cell>0.7</cell><cell cols="2">5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>0.7</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours</cell><cell cols="5">Ours w/o BDCN</cell><cell></cell><cell></cell><cell cols="2">Ours w/o SEB</cell><cell></cell><cell cols="2">RCF[29]</cell><cell>HED[47]</cell></row><row><cell></cell><cell cols="2">120</cell><cell cols="3">0.828 0.82</cell><cell>0.812</cell><cell></cell><cell cols="4">0.815 0.813 0.811</cell><cell></cell><cell></cell><cell></cell><cell>119.6</cell><cell>0.83 0.82</cell></row><row><cell>Param.(M)</cell><cell cols="2">0 20 100 40 60 80</cell><cell cols="2">16.3 16.3</cell><cell cols="5">8.69 0.796 2.26 0.28 0.766 21.4</cell><cell cols="6">0.38 0.757 0.767 20 0.788 0.788 28.8 14.7 0.799 0.798 14.7 14.8 14.7 22 0.793</cell><cell>0.74 0.75 0.81 0.76 0.77 0.78 0.79 0.8</cell><cell>ODS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>The performance (ODS) of each layer in BDCN, RCF<ref type="bibr" target="#b29">[30]</ref>, and HED<ref type="bibr" target="#b48">[49]</ref> on BSDS500 test set.Figure 10. Comparison of edge detection results on BSDS500 test set. All the results are raw edge maps computed with a single scale input before Non-Maximum Suppression.</figDesc><table><row><cell></cell><cell cols="2">Layer ID.</cell><cell cols="2">HED [49]</cell><cell>RCF [30]</cell><cell>BDCN</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>0.595</cell><cell>0.595</cell><cell>0.727</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>0.697</cell><cell>0.710</cell><cell>0.762</cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell>0.750</cell><cell>0.766</cell><cell>0.771</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>0.748</cell><cell>0.761</cell><cell>0.802</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>0.637</cell><cell>0.758</cell><cell>0.815</cell></row><row><cell></cell><cell>fuse</cell><cell></cell><cell></cell><cell>0.790</cell><cell>0.805</cell><cell>0.820</cell></row><row><cell>image</cell><cell>GT</cell><cell cols="2">PMI [19]</cell><cell>HED [47]</cell><cell>RCF [29]</cell><cell>CED [45]</cell><cell>BDCN</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pkuCactus/BDCN.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by Peng Cheng Laboratory, in part by Beijing Natural Science Foundation under Grant No. JQ18012, in part by Natural Science Foundation of China under Grant No. 61620106009, 61572050, 91538111. We additionally thank NVIDIA for generously providing DGX-1 super-computer and support through the NVAIL program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to predict crisp boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="562" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1558" to="1570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Oriented edge forests for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1732" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-scale dense convolutional networks for efficient prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09844</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crisp boundary detection using pointwise mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Srn: Side-output residual network for object symmetry detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1068" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the accuracy of the sobel edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical edge detection: Learning and evaluating edge cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="74" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep-learningbased object-level contour detection with ccg and crf optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning relaxed deep supervision for better edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach Interll</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A systematic comparison between visual cues for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>M?ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep decision network for multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2240" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale improves boundary detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="533" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep crisp boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scale-space filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep structured multi-scale features using attention-gated crfs for contour prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3964" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Hierarchical multiscale attention networks for action recognition. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1611.05720</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
