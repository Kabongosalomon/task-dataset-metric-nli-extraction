<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
							<email>zhenyiwa@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo, ? Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo, ? Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bang</roleName><forename type="first">An</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo, ? Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo, ? Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
							<email>changyou@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo, ? Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text generation from a knowledge base aims to translate knowledge triples to naturallanguage descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformerbased generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a tabletext embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin. * Zhenyi Wang was a research intern student at Tencent AI Lab in Bellevue, WA when doing this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding structured knowledge, e.g., information encoded in tables, and automatically generating natural-language descriptions is an important task in the area of Natural Language Generation. <ref type="table">Table-</ref>to-text generation helps making knowledge elements and their connections in tables easier to comprehend by human. There have been a number of practical application scenarios in this field, for example, weather report generation, NBA news generation, biography generation and medical-record description generation <ref type="bibr" target="#b18">(Liang et al., 2009;</ref><ref type="bibr" target="#b1">Barzilay and Lapata, 2005;</ref><ref type="bibr" target="#b16">Lebret et al., 2016a;</ref><ref type="bibr" target="#b3">Cawsey et al., 1997)</ref>.</p><p>Most existing methods for table-to-text generation are based on an encoder-decoder framework <ref type="bibr" target="#b30">(Sutskever et al., 2014;</ref><ref type="bibr">Bahdanau et al., Figure 1</ref>: An example of table-to-text generation. This generation is unfaithful because there exists information in table not covered by generated text (marked in blue); At the same time, hallucinated information in text does not appear in table (marked in red). 2015), most of which are RNN-based Sequenceto-Sequence (Seq2Seq) models <ref type="bibr" target="#b17">(Lebret et al., 2016b;</ref><ref type="bibr" target="#b22">Liu et al., 2018;</ref><ref type="bibr" target="#b36">Wiseman et al., 2018;</ref><ref type="bibr" target="#b24">Ma et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2019a)</ref>. Though significant progress has been achieved, we advocate two key problems in existing methods. Firstly, because of the intrinsic shortage of RNN, RNN-based models are not able to capture longterm dependencies, which would lose important information reflected in a table. This drawback prevents them from being applied to larger tables, for example, a table describing a large Knowledge Base . Secondly, little work has focused on generating faithful text descriptions, which is defined, in this paper, as the level of matching between a generated text sequence and the corresponding table content. An unfaithful generation example is illustrated in <ref type="figure">Figure 1</ref>. The training objectives and evaluation metrics of existing methods encourage generating texts to be as similar as possible to reference texts. One problem with this is that the reference text often contains extra information that is not presented in the table because human beings have external knowledge beyond the input table when writing the text, or it even misses some important information in the table <ref type="bibr">(Dhingra et al., 2019)</ref> due to the noise from the dataset collection process. As a result, unconstrained training with such mis-matching information usually leads to hallucinated words or phrases in generated texts, making them unfaithful to the table and thus harmful in practical uses.</p><p>In this paper, we aim to overcome the above problems to automatically generate faithful texts from tables. In other words, we aim to produce the writing that a human without any external knowledge would do given the same table data as input. In contrast to existing RNN-based models, we leverage the powerful attention-based Transformer model to capture long-term dependencies and generate more informative paragraphlevel texts. To generate descriptions faithful to tables, two content-matching constraints are proposed. The first one is a latent-representationlevel matching constraint encouraging the latent semantics of the whole text to be consistent with that of the whole table. The second one is an explicit entity-level matching scheme, which utilizes Optimal-Transport (OT) techniques to constrain key words of a table and the corresponding text to be as identical as possible. To evaluate the faithfulness, we also propose a new PARENT-T metric evaluating the content matching between texts and tables, based on the recently proposed PARENT <ref type="bibr">(Dhingra et al., 2019)</ref> metric. We train and evaluate our model on a large-scale knowledge base dataset . Automatic and human evaluations both show that our method achieve the state-of-the-art performance, and can generates paragraph-level descriptions much more informative and faithful to input tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>The task of text generation for a knowledge base is to take the structured table, T = {(t 1 , v 1 ), (t 2 , v 2 ), , (t m , v m )}, as input, and outputs a natural-language description consisting of a  <ref type="table">table-</ref>to-text generation. To enhance the ability of generating multi-sentence faithful texts, our loss consists of three parts, including a maximum-likelihood loss (green), a latent matching disagreement loss (orange), and an optimal-transport loss (blue). sequence of words y = {y 1 , y 2 , , y n } that is faithful to the input table. Here, t i denotes the slot type for the i th row, and v i denotes the slot value for the i th row in a table.</p><p>Our model adopts the powerful Transformer model <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> to translate a table to a text sequence. Specifically, the Transformer is a Seq2Seq model, consisting of an encoder and a decoder. Our proposed encoder-to-decoder Transformer model learns to estimate the conditional probability of a text sequence from a source table input in an autoregressive way:</p><formula xml:id="formula_0">P (y|T ; ?) = n i=1 P (y i |y &lt;i , T ; ?) ,<label>(1)</label></formula><p>where ? is the Transformer parameters and y &lt;i denotes the decoded words from previous steps. Existing models for table-to-text generation either only focus on generating text to match the reference text <ref type="bibr" target="#b22">(Liu et al., 2018;</ref><ref type="bibr" target="#b24">Ma et al., 2019)</ref>, or only require a generated text sequence to be able to cover the input table . However, as the only input information is the table, the generated text should be faithful to the input table as much as possible. Therefore, we propose two constraint losses, including a tabletext disagreement constraint loss and a constrained content matching loss with optimal transport, to encourage the model to learn to match between the generated text and the input table faithfully. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the overall architecture of our model. In summary, our model loss contains three parts: 1) a maximum likelihood loss (green) that measures the matching between a model prediction and the reference text sequence; 2) a latent feature matching disagreement loss (orange) that measures the disagreement between a table encoding and the corresponding reference-text encoding; and 3) an optimal-transport loss (blue) matching the key words of an input table and the corresponding generated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Table Representation</head><p>The entities of a table simply consists of Slot Type and Slot Value pairs. To apply the Transformer model, we first linearize input tables into sequences. Slot types and slot values are separated by special tokens "&lt;" and "&gt;". As an example, the table in <ref type="figure">Figure 1</ref> is converted into a sequence: {&lt; Name ID &gt;, Willie Burden, &lt; date of birth &gt; , July 21 1951, ? ? ? }. We note that encoding a table in this way might lose some high-order structure information presented in the original knowledge graph. However, our knowledge graph is relatively simple. According to our preliminary studies, a naive combination of feature extracted with graph neural networks <ref type="bibr" target="#b2">(Beck et al., 2018)</ref> does not seem helpful. As a result, we only rely on the sequence representation in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Base Objective</head><p>Our base objective comes from the standard Transformer model, which is defined as the negative log-likelihood loss L mle of a target sentence y given its input T , i.e.,</p><formula xml:id="formula_1">L mle = ? log P (y|T ; ?)<label>(2)</label></formula><p>with P (y|T ; ?) defined in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Faithfulness Modeling with a Table-Text Disagreement Constraint Loss</head><p>One key element of our model is to enforce a generated text sequence to be consistent with (or faithful to) the table input. To achieve this, we propose to add some constraints so that a generated text sequence only contains information from the table.</p><p>Our first idea is inspired by related work in machine translation . Specifically, we propose to constrain a table embedding to be close to the corresponding target sentence embedding. Since the embedding of a text sequence (or the table) in our model is also represented as a sequence, we propose to match the mean embeddings of both sequences. In fact, the mean embedding has been proved to be an effective representation for the whole sequence in machine translation <ref type="bibr" target="#b34">Wang et al., 2017)</ref>. Let V table andV text be the mean embeddings of a table and the target text embeddings in our Transformerbased model, respectively. A table-target sentence disagreement loss L disagree is then defined as</p><formula xml:id="formula_2">L disagree = V table ?V text 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Faithfulness Modeling with Constrained Content Matching via Optimal Transport</head><p>Our second strategy is to explicitly match the key words in a table and the corresponding generated text. In our case, key words are defined as nouns, which can be easily extracted with existing tools such as NLTK <ref type="bibr" target="#b23">(Loper and Bird, 2002)</ref>.</p><p>To match key words, a mis-matching loss should be defined. Such a mis-matching loss could be non-differentiable, e.g., when the loss is defined as the number of matched entities. In order to still be able to learn by gradient descent, one can adopt the policy gradient algorithm to deal with the non-differentiability. However, policy gradient is known to exhibit high variance. To overcome this issue, we instead propose to perform optimization via optimal transport (OT), inspired by the recent techniques in <ref type="bibr" target="#b4">(Chen et al., 2019a)</ref>.</p><p>Optimal-Transport Distance In the context of text generation, a generated text sequence, y = (y 1 , ? ? ? , y n ), can be represented as a discrete distribution ? = n i=1 u i ? y i (?), where u i ? 0 and i u i = 1, ? x (?) denotes a spike distribution located at x. Given two discrete distributions ? and ?, written as ? = n i=1 u i ? x i and ? = m j=1 v j ? y j , respectively, the OT distance between ? and ? is defined as the solution of the following maximum network-flow problem:</p><formula xml:id="formula_3">L OT = min U ??(?,?) n i=1 m j=1 U ij ? d(x i , y j ) , (4)</formula><p>where d(x, y) is the cost of moving x to y (matching x and y). In this paper, we use the cosine distance between the two word-embedding vectors of x and y, defined as d(x, y) = 1 ? xy x 2 y 2 . ?(?, ?) is the set of joint distributions such that the two marginal distributions equal to ? and ?, respectively. <ref type="figure">Figure 3</ref>: Illustration of the OT loss, which is defined with OT distance to only match key words in both the table and the generated sentence. Left: the generated sentence not only contains extra information not presented in the table (shown as orange), but also lacks some information presented in the table (shown as red). This is unfaithful generation. The OT lost is thus high. Right: all information in the table is covered in the generated sentence, and the generated sentence does not contain extra information not presented in the table. This is faithful generation. The OT cost is thus low. This example is borrowed and modified from <ref type="bibr">(Dhingra et al., 2019)</ref>.</p><p>Exact minimization over U in the above problem is in general computational intractable <ref type="bibr" target="#b10">(Genevay et al., 2018)</ref>. Therefore, we adopt the recently proposed Inexact Proximal point method for Optimal Transport (IPOT) <ref type="bibr" target="#b37">(Xie et al., 2018)</ref> as an approximation. The details of the IPOT algorithm are shown in Appendix C.</p><p>Constrained Content Matching via OT To apply the OT distance to our setting, we need to first specify the atoms in the discrete distributions. Since nouns typically are more informative, we propose to match the nouns in both an input table and the decoded target sequence. We use NLTK <ref type="bibr" target="#b23">(Loper and Bird, 2002)</ref> to extract the nouns that are then used for computing the OT loss. In this way, the computational cost can also be significantly reduced comparing to matching all words.</p><p>The OT loss can be used as a metric to measure the goodness of the match between two sequences. To illustrate the motivation of applying the OT loss to our setting, we provide an example illustrated in <ref type="figure">Figure 3</ref>, where we try to match the table with the two generated text sequences. On the left plot, the generated text sequence contains "California brand Grateful Dead", which is not presented in the input table. Similarly, and the phrases "Seattle, Washington" and "Skokie Illinois" in the table are not covered by the generated text. Consequently, the resulting OT loss will be high. By contrast, on the right plot, the table contains all information in the text, and all the phrases in the table are also covered well by the generated text, leading to a low OT loss. As a result, optimizing over the OT loss in (4) would enforce faithful matching be-tween a table and its generated text.</p><p>Optimization via OT When optimizing the OT loss with the IPOT algorithm, the gradients of the OT loss is required to be able to propagate back to the Transformer component. In other words, this requires gradients to flow back from a generated sentence. Note that a sentence is generated by sampling from a multinomial distribution, whose parameter is the Transformer decoder output represented as a logit vector S t for each word in the vocabulary. This sampling process is unfortunately non-differentiable. To enable backpropagation, we follow <ref type="bibr" target="#b4">Chen et al. (2019a)</ref> and use the Soft-argmax trick to approximate each word with the corresponding soft-max output.</p><p>To further reduce the number of parameters and improve the computational efficiency, we adopt the factorized embedding parameterization proposed recently <ref type="bibr" target="#b15">(Lan et al., 2019)</ref>. Specifically, we decompose a word embedding matrix of size V ? D into the product of two matrices of sizes V ? H and H ? D, respectively. In this way, the parameter number of the embedding matrices could be significantly reduced as long as H is to be much smaller than D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">The Final Objective</head><p>Combing all the above components, the final training loss of our model is defined as:</p><formula xml:id="formula_4">L = L mle + ?L disagree + ?L OT ,<label>(5)</label></formula><p>where ? and ? controls the relative importance of each component of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Decoder with a Copy Mechanism</head><p>To enforce a generated sentence to stick to the words presented in the table as much as possible, we follow <ref type="bibr" target="#b28">(See et al., 2017)</ref> to employ a copy mechanism when generating an output sequence. Specifically, let P vocab be the output of the Transformer decoder. P vocab is a discrete distribution over the vocabulary words and denotes the probabilities of generating the next word. The standard methods typically generate the next word by directly sampling from P vocab . In the copy mechanism, we instead generate the next word y i with the following discrete distribution:</p><formula xml:id="formula_5">P (y i ) = p g P vocab (y i ) + (1 ? p g )P att (y i ) , where p g = ?(W 1 h i + b 1 )</formula><p>is the probability of switching sampling between P vocab and P att , with learnable parameters (W 1 , b 1 ) and h i as the hidden state from the Transformer decoder for the i-th word. P att is the attention weights (probability) returned from the encoder-decoder attention module in the Transformer. Specifically, when generating the current word y i , the encoder-decoder attention module calculates the probability vector P att denoting the probabilities of attending to each word in the input table. Note that the probabilities of the words not presented in the table are set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct experiments to verify the effectiveness and superiority of our proposed approach against related methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Our model is evaluated on the large-scale knowledge-base Wikiperson dataset released by . It contains 250,186, 30,487, and 29,982 table-text pairs for training, validation, and testing, respectively. Compared to the Wik-iBio dataset used in previous studies <ref type="bibr" target="#b17">(Lebret et al., 2016b;</ref><ref type="bibr" target="#b22">Liu et al., 2018;</ref><ref type="bibr" target="#b36">Wiseman et al., 2018;</ref><ref type="bibr" target="#b24">Ma et al., 2019)</ref> whose reference text only contains one-sentence descriptions, this dataset contains multiple sentences for each table to cover as many facts encoded in the input structured knowledge base as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>For automatic evaluation, we apply the widely used evaluation metrics including the standard BLEU-4 <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref>, METEOR  <ref type="bibr" target="#b6">(Denkowski and Lavie, 2014)</ref> and ROUGE <ref type="bibr" target="#b19">(Lin, 2004)</ref> scores to evaluate the generation quality.</p><p>Since these metrics rely solely on the reference texts, they usually show poor correlations with human judgments when the references deviate too much from the table. To this end, we also apply the PARENT <ref type="bibr">(Dhingra et al., 2019</ref>) metric that considers both the reference texts and table content in evaluations. To evaluate the faithfulness of the generated texts, we further modify the PARENT metric to measure the level of matching between generated texts and the corresponding tables. We denote this new metric as PARENT-T. Please see Appendix A for details. Note that the precision in PARENT-T corresponds to the percentage of words in a text sequence that co-occur in the table; and the recall corresponds to the percentage of words in a table that co-occur in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Models</head><p>We compare our model with several strong baselines, including</p><p>? The vanilla Seq2Seq attention model <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>.</p><p>? The method in : The stateof-art model on the Wikiperson dataset.</p><p>? The method in <ref type="bibr" target="#b22">(Liu et al., 2018)</ref>: The stateof-the-art method on the WikiBio dataset.</p><p>? The pointer-generator <ref type="bibr" target="#b28">(See et al., 2017</ref>): A Seq2Seq model with attention, copying and coverage mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Our implementation is based on OpenNMT <ref type="bibr" target="#b13">(Klein et al., 2017)</ref>. We train our models end-to-end to minimize our objective function with/without the copy mechanism. The vocabulary is limited to    <ref type="table">Table 3</ref>: Ablation study of our model components. means the corresponding column component is used. means do not use the corresponding column component. Specifically, "Copy" means using copy mechanism, "EF" means using embedding factorization, "OT" means using optimal transport constraint loss, "N" means extracting nouns from both the table and text, and "W" means using the whole table and text to compute OT. Lastly, "latent" means using latent similarity loss.</p><p>the 50, 000 most common words in the training dataset. The hidden units of the multi-head component and the feed-forward layer are set to 2048. The baseline embedding size is 512. Following <ref type="bibr" target="#b15">(Lan et al., 2019)</ref>, the embedding size with embedding factorization is set to be 128. The number of heads is set to 8, and the number of Transformer blocks is 3. Beam size is set to be 5. Label smoothing is set to 0.1.</p><p>For the optimal-transport based regularizer, we first train the model without OT for about 20,000 steps, then fine tune the network with OT for about 10,000 steps. We use the Adam (Kingma and Ba, 2015) optimizer to train the models. We set the hyper-parameters of Adam optimizer accordingly, including the learning rate ? = 0.00001, and the two momentum parameters, batch size = 4096 (tokens) and ? 2 = 0.998.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>Table 1 and 2 show the experiment results in terms of different evaluation metrics compared with different baselines. "Ours" means our proposed model with components of copy mechanism, embedding factorization, OT-matching with nouns, and latent similarity loss 1 . We can see that our model outperforms existing models in all of the automatic evaluation scores, indicating high quality of the generated texts. The superiority of the PARENT-T scores (in terms of precision and recall) indicates that the generated text from our model is more faithful than others. Example out-Precision Recall F-1 measure Fluency Grammar  76.    <ref type="figure" target="#fig_1">Figure 4</ref>. The blue color indicates the corresponding row appears in the input table, but not in the output generation text. The red color indicates that these entities appear in the text but do not appear in the input <ref type="table">table.</ref> puts from different models are shown in <ref type="table" target="#tab_5">Table 5</ref> with an input table shown in <ref type="figure" target="#fig_1">Figure 4</ref>. In this example, our model covers all the entities in the input, while all other models miss some entities. Furthermore, other models hallucinate some information that does not appear in the input, while our model generates almost no extra information other than that in the input. These results indicate the faithfulness of our model. More examples are shown in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>We also conduct extensive ablation studies to better understand each component of our model, including the copy mechanism, embedding factorization, optimal transport constraint loss, and latent similarity loss. <ref type="table">Table 3</ref> shows the results in different evaluation metrics.</p><p>Effect of copy mechanism The first and second rows in <ref type="table">Table 3</ref> demonstrate the impacts of the copy mechanism. It is observed that with the copy mechanism, one can significantly improve the performance in all of the automatic metrics, especially on the faithfulness reflected by the PARENT-T score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of embedding factorization</head><p>We compare our model with the one without embedding factorization. The comparisons are shown in the second and third rows of <ref type="table">Table 3</ref>. We can see that with embedding factorization, around half of the parameters can be reduced, while comparable performance can still be maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of table-text embedding similarity loss</head><p>We also test the model by removing the table-text embedding similarity loss component. The third and fourth rows in <ref type="table">Table 3</ref> summarize the results. With the table-text embedding similarity loss, the BLEU and METEOR scores drop a little, but the PARENT and PARENT-T scores improve over the model without the loss. This is reasonable because the loss aims at improving faithfulness of generated texts, reflected by the PARENT-T score.</p><p>Effect of the OT constraint loss We further compare the performance of the model (a) without using OT loss, (b) with using the whole table and text to compute OT, and (c) with using the extracted nouns from both table and text to compute OT. Results are presented in the third, fifth, and sixth rows of <ref type="table">Table 3</ref>, respectively. The model with the OT loss improve performance on almost all scores, especially on the PARENT-T score. Furthermore, with only using the nouns to compute the OT loss, one can obtain even better results. These results demonstrate the effectiveness of the proposed OT loss on enforcing the model to be faithful to the original table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Human Evaluation</head><p>Following <ref type="bibr" target="#b31">Tian et al., 2019)</ref>, we conduct extensive human evaluation on the generated descriptions and compare the results to the state-of-the-art methods. We design our evaluation criteria based on <ref type="bibr" target="#b31">Tian et al., 2019)</ref>, but our criteria differs from <ref type="bibr" target="#b31">(Tian et al., 2019)</ref> in several aspects. Specifically, for each group of generated texts, we ask the human raters to evaluate the grammar, fluency, and faithfulness. The human evaluation metrics of faithfulness is defined in terms of precision, recall and F1-score with respect to the reconstructed Knowledge-base  <ref type="table">Table-</ref>to-text generation has been widely studied, and Seq2Seq models have achieved promising performance. <ref type="bibr" target="#b17">(Lebret et al., 2016b;</ref><ref type="bibr" target="#b22">Liu et al., 2018;</ref><ref type="bibr" target="#b36">Wiseman et al., 2018;</ref><ref type="bibr" target="#b24">Ma et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2019a)</ref>. For Transformer-based methods, the Seq2Seq Transformer is used by <ref type="bibr" target="#b24">Ma et al. (2019)</ref> for tableto-text generation in low-resource scenario. Thus, instead of encoding an entire table as in our approach, only the predicted key facts are encoded in <ref type="bibr" target="#b24">(Ma et al., 2019)</ref>. Extended transformer has been applied to game summary <ref type="bibr" target="#b11">(Gong et al., 2019)</ref> and E2E NLG tasks <ref type="bibr" target="#b9">(Gehrmann et al., 2018)</ref>. However, their goals focus on matching the reference text instead of being faithful to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Another line of work attempts to use external knowledge to improve the quality of generated text <ref type="bibr" target="#b5">(Chen et al., 2019b)</ref>. These methods allow generation from an expanded external knowledge base that may contain information not relevant to the input table. Comparatively, our setting requires the generated text to be faithful to the input table. <ref type="bibr" target="#b25">Nie et al. (2018)</ref> further study fidelitydata-to-text generation, where several executable symbolic operations are applied to guide text generation. Both models do not consider the matching between the input and generated output.</p><p>Regarding datasets, most previous methods are trained and evaluated on much simpler datasets like WikiBio <ref type="bibr" target="#b17">(Lebret et al., 2016b</ref>) that contains only one sentence as a reference description. Instead, we focus on the more complicated structured knowledge base dataset  that aims to generate multi-sentence texts.  propose a model based on the pointer network that can copy facts directly from the input knowledge base. Our model uses a similar strategy but obtains much better performance.</p><p>In terms of faithfulness, one related parallel work is <ref type="bibr" target="#b31">Tian et al. (2019)</ref>. However, our method is completely different from theirs. Specifically, <ref type="bibr" target="#b31">Tian et al. (2019)</ref> develop a confidence oriented decoder that assigns a confidence score to each target position to reduce the unfaithful information in the generated text. Comparatively, our method enforces faithfulness by including the proposed table-text optimal-transport matching loss and table-text embedding similarity loss. Moreover, the faithfulness of <ref type="bibr" target="#b31">Tian et al. (2019)</ref> only requires generated texts to be supported by either a table or the reference; whereas ours constrains generated texts to be faithful only to the table.</p><p>Other related works are <ref type="bibr" target="#b27">(Perez-Beltrachini and Lapata, 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2019b)</ref>. For <ref type="bibr" target="#b27">(Perez-Beltrachini and Lapata, 2018)</ref>, the content selection mechanism training with multi-task learning and reinforcement learning is proposed. For <ref type="bibr" target="#b21">(Liu et al., 2019b)</ref>, they propose force attention and reinforcement learning based method. Their learning methods are completely different from our method that simultaneously incorporates optimaltransport matching loss and embedding similarity loss. Moreover, the REINFORCE algorithm <ref type="bibr" target="#b35">(Williams, 1992)</ref> and policy gradient method used in <ref type="bibr" target="#b27">(Perez-Beltrachini and Lapata, 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2019b)</ref> exhibits high variance when training the model.</p><p>Finally, the content-matching constraints between text and table is inspired by ideas in machine translation  and Seq2Seq models <ref type="bibr" target="#b4">(Chen et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel Transformerbased table-to-text generation framework to address the faithful text-generation problem. To enforce faithful generation, we propose a new tabletext optimal-transport matching loss and a tabletext embedding similarity loss. To evaluate the faithfulness of the generated texts, we further propose a new automatic evaluation metric specialized to the table-to-text generation problem. Extensive experiments are conducted to verify the proposed method. Both automatic and human evaluations show that our framework can significantly outperform the state-of-the-art methods.</p><p>A PARENT-T Metric PARENT-T evaluates each instance (T i , G i ) separately, by computing the precision and recall of generated text G i against table T i . In other words, PARENT-T is a table-focused version of PARENT <ref type="bibr">(Dhingra et al., 2019)</ref>.</p><p>When computing precision, we want to check what fraction of the n-grams in G i n are correct. We consider an n-gram g to be correct if it has a high probability of being entailed by the table. We use the word overlap model for entailment probability w(g). The precision score E p for one instance is computed as follows:</p><formula xml:id="formula_6">w(g) = n j=1 1(g j ?T i ) n (6) E n p = g?G i n w(g)# G i n (g) g?G i n # G i n (g) (7) E p = exp 4 n=1 1 4 logE n p<label>(8)</label></formula><p>whereT i denotes all the lexical items present in the table T i , n is the length of g, and g j is the jth token in g. w(g) is the entailment probability, and E n p is the entailed precision score for n-grams of order n. # G i n (g) denotes the count of n-gram g in G i n . The precision score E p is a combination of n-gram orders 1-4 using a geometric average.</p><p>For recall, we only compute it against table to ensure that texts that mention more information from the table get higher scores. E r (T i ) is computed in the same way as in <ref type="bibr">Dhingra et al. (2019)</ref>:</p><formula xml:id="formula_7">E r = E r (T i ) = 1 K K k=1 1 |r k | LCS(r k , G i ) (9)</formula><p>where a table is a set of records T i = {r k } K k=1 , r k denotes the value string of record r k , and LCS(x, y) is the length of the longest common subsequence between x and y. Higher values of E r (T i ) denote that more records are likely to be mentioned in G i .</p><p>Thus, the PARENT-T score (i.e. F score) for one instance is:</p><formula xml:id="formula_8">PARENT-T = 2E p E r E p + E r<label>(10)</label></formula><p>The system-level PARENT-T score for a model M is the average of instance-level PARENT-T scores across the evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Human Evaluation</head><p>The following are the details for instructing our human evaluation raters how to rate each generated sentence:</p><p>We only provide the input table and the generated text for the raters. There are 20 well-trained raters participating in the evaluation.</p><p>Fluency : 4: The sentence meaning is clear and flow naturally and smoothly.</p><p>3: The sentence meaning is clear, but there are a few interruptions.</p><p>2: The sentence does not flow smoothly but people can understand its meaning.</p><p>1: The sentence is not fluent at all and people cannot understand its meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar :</head><p>4 : There are no grammar errors.</p><p>3: There are a few grammar errors, but sentence meaning is clear.</p><p>2: There are some grammar errors, but not influencing its meaning.</p><p>1: There are many grammar errors. People cannot understand the sentence meaning.</p><p>Faithfulness A sentence is faithful if it contains only information supported by the table. It should not contain additional information other than the information provided by the table or inferred from the table. Also, the generated sentence should cover as much information in the given table as possible. The raters first manually extract entities from the generated sentences and then calculate the precision as the percentage of entities in the generated text also appear in the table; calculate the recall as the percentage of entities in the table also appear in the generated text. For each tabletext pair, its F-1 score is then calculated according to the precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IPOT algorithm</head><p>Given a pair of table and its corresponding text description, we can obtain table words embedding as S = {x i } i=n i=1 , and the model output for sentence words embedding as S = {y j } j=m j=1 . The cost matrix C is then computed as in Section 2.4. Both S and S are used as inputs to the IPOT algorithm in Algorithm 1 to obtain the OT-matching distance.</p><p>Algorithm 1 IPOT algorithm. <ref type="figure" target="#fig_2">Figure 5</ref> illustrates three matching cases from top to bottom, namely hard matching, soft bipartite matching, and optimal transport matching. The hard matching stands for exactly matching words between the table and the target sequences. This operation is non-differentiable. The soft bipartite matching, on the other hand, supposes the similarity between the word embedding v i k and v j k is d(v i k , v j k ), and finds the matching such that</p><formula xml:id="formula_9">Require: Feature vector S = {x i } i=n i=1 , S = {y j } j=m j=1 , and stepsize 1/? ? = 1 m 1 m T 1 = 1 n 1 T m C ij = d(x i , y j ), A ij = e ? C ij ? for t = 1 to N do Q = A T t for k = 1 to K do ? = 1 nQ? , ? = 1 mQ T ? end for T t+1 = diag(?)Qdiag(?) end for return T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of Optimal Transport Loss</head><formula xml:id="formula_10">L = k d(v i k , v j k )</formula><p>is minimized. This minimization can be solved exactly by the Hungarian algorithm <ref type="bibr" target="#b14">(Kuhn, 1955)</ref>. But, its objective is still non-differentiable. Our proposed optimal transport matching can be viewed as the relaxed problem of the soft bipartite matching by computing the distance between the distribution over the input table and the decoded text sentence. This distance in optimal transport matching is differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More generation examples</head><p>More generation examples from different models are shown in <ref type="figure" target="#fig_3">Figure 6</ref>, 7, and <ref type="table" target="#tab_8">Table 6</ref>, 7. Specifically, <ref type="table" target="#tab_10">Table 7</ref> and <ref type="figure" target="#fig_4">Figure 7</ref> show a more challenging example, as its table has 22 rows. In this example, we can observe that all the RNN-based models cannot capture such long term dependencies and miss most of the input records in the table. By contrast, our model miss much less input records.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miss</head><p>Generated texts     <ref type="figure" target="#fig_3">Figure 6</ref>. The "Miss" column indicates the corresponding row appears in the input table, but does not appear in the output generation text. The red color indicates that these entities appear in the text but do not appear in the input table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Miss Generated texts <ref type="bibr">(Wang et al., 2018) 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, 16, 21, 22?</ref> mile Mbouh ( born 30 May 1966 ) is a former Cameroon national football team Association football . he was born in Douala and played for the Tanjong Pagar United FC in the 1994 FIFA World Cup .</p><p>Pointer generator <ref type="bibr">2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 16, 20? mile Mbouh, (born 30 May 1966</ref>) is a Cameroon retired Association football who played as a Midfielder . he played for Cameroon national football team in the 1990 FIFA World Cup . he also played for Perlis FA and Liaoning Whowin F.C. .?mile was born in Douala, <ref type="bibr">2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20? mile Mbouh, (born 30 May 1966</ref>) is a retired Cameroonian Association football who played as a Midfielder . he was born in Douala . he was a member of the Cameroon national football team at the 1990 FIFA World Cup . he was a member of the Cameroon national football team at the 1990 FIFA World Cup . he was a member of the Cameroon national football team at the 1990 FIFA World Cup . he was a member of the Cameroon national football team at the 1990 FIFA World Cup .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Seq</head><p>Structure-Aware <ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">11,</ref><ref type="bibr">12,</ref><ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">16,</ref><ref type="bibr">17</ref>   <ref type="figure" target="#fig_4">Figure 7</ref>. The "Miss" column indicates the corresponding row appears in the input table, but does not appear in the output generation text. The red color indicates that these entities appear in the text but do not appear in the input table.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our proposed model for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Example input for different models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Hard matching (top), soft bipartite matching (middle), and optimal transport matching (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Example input for different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Example input for different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our model and baseline. PARENT and PARENT-T are the average of PARENT and PARENT-T scores of all table-text pairs.</figDesc><table><row><cell></cell><cell cols="4">P-recall P-precision PT-recall PT-precision</cell></row><row><cell>(Wang et al., 2018)</cell><cell>44.83</cell><cell>63.92</cell><cell>84.34</cell><cell>41.10</cell></row><row><cell>Seq2Seq (Bahdanau et al., 2015)</cell><cell>41.80</cell><cell>49.09</cell><cell>76.07</cell><cell>33.13</cell></row><row><cell>Pointer-Generator (See et al., 2017)</cell><cell>44.09</cell><cell>61.73</cell><cell>81.65</cell><cell>42.03</cell></row><row><cell>Structure-Aware Seq2Seq (Liu et al., 2018)</cell><cell>46.34</cell><cell>51.18</cell><cell>83.84</cell><cell>35.99</cell></row><row><cell>Ours</cell><cell>48.83</cell><cell>62.86</cell><cell>85.21</cell><cell>43.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our model and baseline. P-recall and P-precision refer to the average of PARENT precisions and recalls of all table-text pairs. Similarly, PT-recall and PT-precision are the average of PARENT-T precisions and recalls of all table-text pairs.</figDesc><table><row><cell cols="7">Copy EF OT (N/W) latent BLEU METEOR ROUGE PARENT PARENT-T params</cell></row><row><cell></cell><cell>24.49</cell><cell>22.01</cell><cell>40.98</cell><cell>48.31</cell><cell>49.89</cell><cell>98.92M</cell></row><row><cell></cell><cell>24.57</cell><cell>22.43</cell><cell>42.26</cell><cell>51.87</cell><cell>54.29</cell><cell>98.92M</cell></row><row><cell></cell><cell>25.07</cell><cell>22.38</cell><cell>42.37</cell><cell>51.76</cell><cell>54.36</cell><cell>45.94M</cell></row><row><cell></cell><cell>23.86</cell><cell>22.08</cell><cell>42.65</cell><cell>52.72</cell><cell>55.30</cell><cell>45.94M</cell></row><row><cell>W</cell><cell>24.64</cell><cell>22.39</cell><cell>42.52</cell><cell>52.77</cell><cell>55.46</cell><cell>45.94M</cell></row><row><cell>N</cell><cell>25.29</cell><cell>22.60</cell><cell>42.25</cell><cell>52.74</cell><cell>55.80</cell><cell>45.94M</cell></row><row><cell>N</cell><cell>24.56</cell><cell>22.37</cell><cell>42.40</cell><cell>53.06</cell><cell>56.10</cell><cell>45.94M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Human Evaluation of various aspects of generated text. British Physicist . Brompton Cemetery he was born in London the son of Sir Thomas and his wife Mary ( n?e Fleming ) . he was educated at University College School and University College London .</figDesc><table><row><cell>Model</cell><cell>Miss</cell><cell>Generated texts</cell></row><row><cell cols="2">(Wang et al., 2018) 9</cell><cell>William Edward Ayrton Fellow of the Royal Society ( 14 September 1847 -8</cell></row><row><cell cols="3">November 1908 ) was a Pointer generator 2, 9 William Edward Ayrton-Gould Fellow of the Royal Society (14 September</cell></row><row><cell></cell><cell></cell><cell>1847 -8 November 1908) was an English Physicist who was born in London</cell></row><row><cell></cell><cell></cell><cell>and was educated at Brompton College and University College London . he</cell></row><row><cell></cell><cell></cell><cell>died in London on 8 November 1908 . William was elected a Fellow of the</cell></row><row><cell></cell><cell></cell><cell>Royal Society in 1902.</cell></row><row><cell>Seq2Seq</cell><cell>1, 2, 3, 9</cell><cell></cell></row><row><cell>Structure-Aware</cell><cell>1, 2, 9</cell><cell></cell></row><row><cell>Ours</cell><cell>None</cell><cell></cell></row></table><note>William Edward Sandys Fellow of the Royal Society (14 September 1847 - 8 November 1908) was a British Physicist . he was educated at the University College London and the University College London . he was a Fellow of the Royal Society and a Fellow of the Royal Society.William Edward Keeler Fellow of the Royal Society (14 September 1847 - 8 November 1908) was a British Physicist and Physicist . he was elected a Fellow of the Royal Society in 1889 and was a member of the Royal Society of London and the Royal Society of London and the Royal Society of Lon- don . he was educated at the University College London and at the University College London where he was a pupil of the chemist William.William Edward Ayrton Fellow of the Royal Society (14 September 1847 -8 November 1908) was an English Physicist . William was born in London and educated at University College London. he is buried in Brompton Cemetery London . he was elected a Fellow of the Royal Society in 1901. he was the father of Barbara Ayrton-Gould .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Example outputs from different methods with an input table shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>table from</head><label>from</label><figDesc></figDesc><table><row><cell>a generated text se-</cell></row><row><cell>quence. To ensure accurate human evaluation,</cell></row><row><cell>the raters are trained with word instructions and</cell></row><row><cell>text examples of the grading standard beforehand.</cell></row><row><cell>During evaluation, we randomly sample 100 ex-</cell></row><row><cell>amples from the predictions of each model on the</cell></row><row><cell>Wikiperson test set, and provide these examples to</cell></row><row><cell>the raters for blind testing. More details about the</cell></row><row><cell>human evaluation are provided in the Appendix B.</cell></row><row><cell>The human evaluation results in Table 4 clearly</cell></row><row><cell>show the superiority of our proposed method.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Aaron Miller ( born August 11 1971 ) is an United States former professional Ice hockey Defenceman who played in the National Hockey League ( NHL ) for the Quebec Nordiques and the Colorado Avalanche . he was born in Buffalo, New York and played for the Quebec Nordiques and the Ottawa Senators . Defenceman who played in the National Hockey League . miller was born in Buffalo, New York . he was drafted by the Colorado Avalanche in the second round of the 1994 NHL Entry Draft . he was drafted in the sixth round of the 1994 NHL Entry Draft by the Colorado Avalanche . he was drafted in the sixth round of the 1994 NHL Entry Draft by the Colorado Avalanche . Aaron Miller (born August 11 1971 in Buffalo, New York New York) is a retired United States professional Ice hockey Defenceman who played in the National Hockey League (NHL) for the Quebec Nordiques Colorado Avalanche Colorado Avalanche Colorado Avalanche Colorado Avalanche and Quebec Nordiques. he was drafted in the 2nd round of overall of the 2002 NHL Entry Draft. None Aaron Miller (born August 11 1971 in Buffalo, New York) is an United States former professional Ice hockey Defenceman who played in the National Hockey League (NHL) for the Quebec Nordiques and Colorado Avalanche . he was a member of the United States men's national Ice hockey team at the 2002 Winter Olympics and 2006 Winter Olympics.</figDesc><table><row><cell></cell><cell>7, 8</cell><cell></cell></row><row><cell>Pointer generator</cell><cell>2, 7, 8</cell><cell>Aaron Miller (born August 11 1971) is a retired United States professional</cell></row><row><cell></cell><cell></cell><cell>Ice hockey Defenceman who played in the National Hockey League (NHL)</cell></row><row><cell></cell><cell></cell><cell>for the Quebec Nordiques Quebec Nordiques Quebec Nordiques and the</cell></row><row><cell></cell><cell></cell><cell>Quebec Nordiques . he was born in Buffalo, New York and grew up in</cell></row><row><cell></cell><cell></cell><cell>New York City,</cell></row><row><cell>Seq2Seq</cell><cell>3, 7, 8</cell><cell>Aaron Miller (born August 11 1971) is an United States former profes-</cell></row><row><cell cols="3">sional Ice hockey Structure-Aware 7, 8</cell></row><row><cell>Ours</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Example outputs from different methods with an input table shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, 21? mile Mbouh, (born 30 May 1966) is a Cameroonian retired Association football who played as a Midfielder . Le represented Cameroon national football team at the 1994 FIFA World Cup and 1994 FIFA World Cup . he played for Le FC Sport Yaound?, United Yaound? and Tanjong Pagar May 1966) is a Cameroonian retired Association football who played as a Midfielder . born in Douala?mile began his career with Sport Benfica e Castelo Branco and Tanjong Pagar United FC . he also represented Cameroon national football team at the 1994 FIFA World Cup and 1990 FIFA World Cup . he also played for Sabah FA and Liaoning Whowin F.C. in the Malaysia Super League . he also played for Tanjong Pagar United FC and Liaoning Whowin F.C. in the Chinese Super League.</figDesc><table><row><cell></cell><cell></cell><cell>United FC</cell></row><row><cell>Ours</cell><cell>2, 3, 5, 6, 8, 12,</cell><cell>mile Mbouh (born 30</cell></row><row><cell></cell><cell cols="2">13, 14?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Example outputs from different models with an input table shown in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The result of the method by is different from the score reported in their paper, as we use their publicly released code https://github.com/EagleW/Describing a Knowledge Base and data that is three times larger than the original 106,216 table-text pair data used in the paper. We have confirmed the correctness of our results with the author.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We sincerely thank all the reviewers for providing valuable feedback. We thank Linfeng Song, Dian Yu, Wei-yun Ma, and Ruiyi Zhang for the helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Brief review: Natural language generation in health care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Cawsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="473" to="482" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving sequence-to-sequence learning via optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing neural data-to-text generation models with external background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end content and plan selection for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Natural Language Generation</title>
		<meeting>International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced transformer model for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation (EMNLP)</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for Learning Representations</title>
		<meeting>the International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open-NMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text Summarization Branches Out</title>
		<meeting>Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical encoder with auxiliary supervision for neural table-to-text generation: Learning better representation for tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards comprehensive description generation from factual attribute-value tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/cs/0205028" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Key fact as pivot: A two-stage model for low resource table-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1908.03067.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Operation-guided neural networks for high fidelity data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bootstrapping generators from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.04368" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for amrto-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sticking to the facts: Confident decoding for faithful data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.08684" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Describing a knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Language Generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentence embedding for neural machine translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning neural templates for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A fast proximal point method for computing exact wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1802.04307.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sentence-level agreement for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
