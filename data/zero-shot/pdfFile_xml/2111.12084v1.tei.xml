<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Pre-Training for Transformer-Based Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Pre-Training for Transformer-Based Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based supervised pre-training achieves great performance in person re-identification (ReID). However, due to the domain gap between ImageNet and ReID datasets, it usually needs a larger pre-training dataset (e.g. ImageNet-21K) to boost the performance because of the strong data fitting ability of the transformer. To address this challenge, this work targets to mitigate the gap between the pre-training and ReID datasets from the perspective of data and model structure, respectively. We first investigate self-supervised learning (SSL) methods with Vision Transformer (ViT) pretrained on unlabelled person images (the LUPerson dataset), and empirically find it significantly surpasses ImageNet supervised pre-training models on ReID tasks. To further reduce the domain gap and accelerate the pre-training, the Catastrophic Forgetting Score (CFS) is proposed to evaluate the gap between pre-training and fine-tuning data. Based on CFS, a subset is selected via sampling relevant data close to the down-stream ReID data and filtering irrelevant data from the pre-training dataset. For the model structure, a ReID-specific module named IBN-based convolution stem (ICS) is proposed to bridge the domain gap by learning more invariant features. Extensive experiments have been conducted to fine-tune the pre-training models under supervised learning, unsupervised domain adaptation (UDA), and unsupervised learning (USL) settings. We successfully downscale the LUPerson dataset to 50% with no performance degradation. Finally, we achieve state-of-the-art performance on Market-1501 and MSMT17. For example, our ViT-S/16 achieves 91.3%/89.9%/89.6% mAP accuracy on Market1501 for supervised/UDA/USL ReID. Codes and models will be released to https://github.com/michuanhaohao/ TransReID-SSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">48]</ref> have attracted more and more attention and achieved great performance in person ReID. For example, the pure transformer-based method <ref type="figure">Figure 1</ref>. Performance of ViT-S on Market-1501 <ref type="bibr" target="#b49">[50]</ref> and MSMT17 <ref type="bibr" target="#b40">[41]</ref> in supervised, UDA and USL ReID. Our pretraining paradigm outperforms Baseline (supervised pre-training on ImageNet + vanilla ViT) by a large margin.</p><p>TransReID <ref type="bibr" target="#b18">[19]</ref> achieves a significant performance boost over state-of-the-art CNN-based methods. However, there exists a large domain gap between ImageNet and person ReID datasets because 1) The image content of ImageNet and ReID datasets is very different <ref type="bibr" target="#b13">[14]</ref>; 2) supervised pretraining on ImageNet is focused on category-level supervision which reduces the rich visual information <ref type="bibr" target="#b2">[3]</ref> while person ReID prefers fine-grained identity information. As a result, Transformers need to be pre-trained on a larger-scale dataset ImageNet-21K <ref type="bibr" target="#b11">[12]</ref> to avoid over-fitting on the pretraining dataset. To bridge the gap between pre-training and fine-tuning datasets for better transformer-based ReID models, this paper tackles the problem from the perspectives of data and model structure, respectively.</p><p>From the data view, we have seen a large-scale pretraining dataset named LUPerson being built by collecting unlabeled person images <ref type="bibr" target="#b13">[14]</ref>, which has demonstrated that CNN-based SSL pre-training on LUPerson dataset improves ReID performance compared with ImageNet-1k pretraininng. However, following the same paradigm and replacing the backbone with Vision Transformers (ViTs) <ref type="bibr" target="#b12">[13]</ref> would get poor performance due to the huge differences between training CNNs and ViTs. It motivates us to explore an effective SSL pre-training paradigm for transformer-based person ReID in the first place. After thorough comparison between several Transformer-based self-supervised methods (e.g. MocoV3 <ref type="bibr" target="#b7">[8]</ref>, MoBY <ref type="bibr" target="#b42">[43]</ref> and DINO <ref type="bibr" target="#b2">[3]</ref>) with ViTs on the LUPerson dataset, it is found that DINO significantly outperforms other SSL methods and supervised pre-training on ImageNet, thus it is used as our following setup. Next, it is brought to our attention that, though with better performance, pre-training on LUPersonn needs a larger amount of computational resources due to the large amount of training data (3X of ImageNet-1K). Therefore, we propose to adopt conditional pre-training to speed-up the training process and further reduce the domain gap. As LUPerson is collected from web videos, a portion of the images are of low-quality or have great domain bias with the downstream ReID datasets, so conditional filtering shall be performed to downscale LUPerson (source domain) by selecting a relevant subset close to the downstream ReID datasets (target domain). However, previous works on conditional filtering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> are mainly designed for close-set tasks by selecting data close to category labels or cluster centers of the target training data. Those methods will easily overfit the person IDs instead of the true target domain if directly applied to the open-set ReID task. We propose a metric named Catastrophic Forgetting Score (CFS) to evaluate the gap between pre-training and downstream data, inspired by the conclusions in catastrophic forgetting problems <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. For a pre-training image, the similarity between its features from the two proxy models (one is pre-trained on the source dataset and another is fine-tuned on the target dataset) can represent the similarity between the source and target domain. In this way, a subset of images with higher CFS scores can be selected from the pre-training data to perform efficient conditional pre-training. A preliminary theoretical analysis is conducted to justify the effectiveness of CFS.</p><p>From the perspective of model structure, some recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> have pointed out that, an important factor that affects performance and stability of ViTs is the patchify stem implemented by a stride-p p ? p convolution (p = 16 by default) on the input image. To address this problem, MocoV3 <ref type="bibr" target="#b7">[8]</ref> froze the patch projection to train ViTs, while Xiao et al. <ref type="bibr" target="#b41">[42]</ref> and Wang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a convolution stem stacked by several convolution, Batch Normalization (BN) <ref type="bibr" target="#b21">[22]</ref>, and ReLU <ref type="bibr" target="#b31">[32]</ref> layers to increase optimization stability and improve performance. Inspired by the success of integrating Instance Normalization (IN) and BN to learn domain-invariant representation in the ReID task <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>, we refer to IBN-Net <ref type="bibr" target="#b32">[33]</ref> and improve the convolution stem to the IBN-based convolution stem (ICS). ICS inherits the stability of the convolution stem and also introduces IBN to learn features with appearance invariance (e.g. viewpoint, pose and illumination invariance, etc.). ICS has similar computational complexity to convolution stem, but improves peak performance of ViTs significantly in person ReID.</p><p>Embraced with the above two improvements based on DINO, we conduct experiments with supervised learning, UDA and USL settings on Market1501 and MSMT17. Our pre-training paradigm helps ViTs achieve state-of-the-art performance on these benchmarks. For instance, as shown in <ref type="figure">Figure 1</ref>, our ViT-S achieves 91.3%/89.9%/89.6% mAP accuracy on Market-1501 for supervised/UDA/USL ReID, which exceed the supervised pre-training setting on Ima-geNet by a large margin. The pre-training cost on LUPerson is also reduced by 30% without any performance degradation, through the proposed CFS-based conditional filtering and downscaling the original LUPerson by 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-supervised Learning</head><p>Self-supervised learning (SSL) methods are proposed to learn discriminative features from large-scale unlabeled data without any human-annotated labels <ref type="bibr" target="#b22">[23]</ref>. One branch is developed from Momentum Contrast (MoCo) <ref type="bibr" target="#b5">[6]</ref> by treating a pair of augmentations of a sample as a positive pair and all other samples as negative pairs. Since the number of negative samples greatly affects the final performance, MoCo series <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> require large batches or memory banks. Among them, MoCoV3 <ref type="bibr" target="#b7">[8]</ref> is a Transformer-specific version. Fu et al. <ref type="bibr" target="#b13">[14]</ref> have verified ResNet50 can be well pre-trained by a modified MoCoV2 on human images in person ReID. Many recent works have shown models can learn feature representation without discriminating between images. In this direction, Ge et al. <ref type="bibr" target="#b17">[18]</ref> propose a new paradigm called BYOL, where the online network predicts the representation of the target network on the same image under a different augmented view. Large batch size is unnecessary in BYOL since negative samples are not needed. Many variants have successfully improved BYOL in various ways. One of them is DINO <ref type="bibr" target="#b2">[3]</ref>, where a centering and sharpening of the momentum teacher outputs is used to avoid model collapse. DINO achieves state-of-the-art performance with ViTs on both ImageNet classification and down-stream tasks. Xie et al. combines MoCo with BOYL to propose a Transformer-specific method called MoBY. Given that there have been more SSL methods designed specifically for Transformers, we will focus on several of the state-of-the-art options in the following experiments, e.g. MoCo series, MoBY and DINO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer-based ReID</head><p>CNN-based methods have dominated the ReID community for many years. However, pure-transformer models are gradually becoming a popular choice. He et al. <ref type="bibr" target="#b18">[19]</ref> are the first to successfully apply ViTs to ReID tasks by proposing TransReID which achieves state-of-the-art performance on both person and vehicle ReID. Auto-Aligned Transformer (AAformer) <ref type="bibr" target="#b50">[51]</ref> also uses ViT backbone with the additional learnable vectors of "part tokens" to learn the part representations and integrates the part alignment into the self-attention. Other works try to use Transformer to aggregate features or information from CNN backbones. For example, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48]</ref> integrate Transformer layers into the CNN backbone to aggregate hierarchical features and align local features. For video ReID, <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b48">49]</ref> exploit Transformer to aggregate appearance features, spatial features, and temporal features to learn a discriminative representation for a person tracklet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Conditional Transfer Learning</head><p>There are a few works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref> studying how to select relevant subsets from the pre-training dataset to improve the performance when transferred to target datasets. <ref type="bibr" target="#b9">[10]</ref> use a feature extractor trained on the JFT300M <ref type="bibr" target="#b19">[20]</ref> to select the most similar source categories to the target categories in a greedy way. For each image from the target domain, Ge et al. <ref type="bibr" target="#b14">[15]</ref> search a certain number of images with similar low-level characteristics from the source domain. Shuvam et al. <ref type="bibr" target="#b3">[4]</ref> train the feature extractor on the target data and individually select source images that are close to cluster centers in the target domain. Yan et al. <ref type="bibr" target="#b44">[45]</ref> propose a Neural Data Server (NDS) to train expert models on many subsets of the pre-training dataset. Source images used to train an expert with a good target task performance are assigned high importance scores. It is noted that conditional transfer learning has mainly been studied on close-set tasks such as image classification, fine-grained recognition, object detection, etc. Therefore, these methods may be not suitable for the open-set ReID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-supervised Pre-training</head><p>As far as we know, there has been no literature studying the SSL pre-training for transformer-based ReID. Therefore we first conduct an empirical study to gain better understanding on this problem. We investigate two backbones (CNN-based ResNet50 and Transformer-based ViT), four SSL methods (MoCoV2, MoCoV3, MoBY and DINO), and two pre-training datasets (ImageNet and LUPerson). Mo-CoV2 used here is a modified version proposed to adapt to person ReID on ResNet50 in <ref type="bibr" target="#b13">[14]</ref>, while the other three methods, i.e. MoCoV3, MoBy and DINO, are transformerspecific methods proposed on the ImageNet data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Supervised Fine-tuning</head><p>The baseline <ref type="bibr" target="#b18">[19]</ref> we used is pre-trained on ImageNet. We fine-tune pre-trained models on two ReID benchmarks including Market-1501 (Market) and MSMT17, and the peak fine-tuning performance of all models being compared are presented in <ref type="table">Table 1</ref>. For convenience, a pre-trained model is marked in the form of Model+Method+Data. For instance, R50+Supervised+IMG denotes the ResNet50 model pre-trained on the ImageNet in the supervised man- ner, which is the standard pre-training paradigm in most previous ReID methods. Some observations are made as follows. 1) MoCoV2 performs the best among all SSL methods for ResNet50, while it is much worse than the other three transformerspecific methods with ViT, which means that it is necessary to explore specific methods for transformer-based models. 2) ViT is more sensitive to a proper pre-training than ResNet50. For example, we can see that mAP using ResNet50 ranges from 51.9% to 53.3% on MSMT17 with different pre-training settings, while performance of ViT-S/16 differs more widely on MSMT17. 3) SSL methods on LUP consistently achieve better performance comparing to SSL with ImageNet. Even when we restrict the number of training images of LUP to be the same as IMG, ViT-S/16+DINO+LUP * still surpasses ViT-S/16+DINO+ IMG on both benchmarks, indicating that leveraging person images is a better choice to pre-train ReID models. 4) ViT-S/16+DINO+LUP achieves 64.2% mAP and 83.4% Rank-1 accuracy on the MSMT17, surpassing the baseline (ViT-S/16+Supervised+IMG) by 10.7% mAP and 8.2% Rank-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised Fine-tuning</head><p>Since there has been no transformer-based baseline for unsupervised ReID, the state-of-the-art CNN-based framework C-Contrast <ref type="bibr" target="#b10">[11]</ref> is selected for the following experiments. We reproduce C-Contrast with ResNet50 and ViT-S/16 on both USL ReID and UDA ReID 1 . Based on ob-  <ref type="table">Table 3</ref>.</p><p>Model performance in UDA ReID. MS2MA and MA2MS stands for MSMT17?Market (MS2MA) and Market?MSMT17 (MA2MS), respectively. servations made from <ref type="table">Table 1</ref>, we choose MoCoV2 for ResNet50 and DINO for ViT-S/16 in this section in <ref type="table" target="#tab_1">Table 2</ref>. SSL pre-training doesn't provide large gains compared with Sup when applied with ResNet50, not to mention that performance drop is observed on MSMT17. This is consistent with the aforementioned conclusion that Transformer is more sensitive to the pre-training than CNN. In contrast, SSL pre-training on LUPerson improves the performance by a large margin for ViT-S/16. The UDA performance on MS2MA and MA2MS also yields the similar conclusion. Conclusion. DINO is the most suitable SSL method among candidate methods for transformer-based ReID. Pretraining is more crucial for Transformers than CNN models. Transformers pre-trained on LUPerson can significantly improve the performance compared with ImageNet pre-training, indicating that bridging the domain gap between pre-training and fine-tuning datasets for transformers is more beneficial and worth doing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conditional Pre-training</head><p>This section introduces the efficient conditional pretraining where models are pre-trained on a subset closer to the target domain to speed up the pre-training process while maintaining or even improving the downstream finetuning performance. Catastrophic Forgetting Score (CFS) is proposed to evaluate the similarity between the pre-training data and the target domain. Theoretic analysis is provided to support the method. random seed and GPU machine to reduce randomness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Definition</head><formula xml:id="formula_0">Given a target dataset D t = (X t , Y t ) where X t = {x 1 t , x 2 t , x 3 t , ..., x M t } with their ID labels Y t . We target to select a subset D s from a large-scale source/pre-training dataset D s where X s = {x 1 s , x 2 s , x 3 s , ..., x N s }.</formula><p>The number of images in D s is N &lt; N . The efficient conditional pre-training is to pre-train models on D s , which should reduce the training cost of pre-training while maintaining or even improving performance on the target dataset. Some previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref> have shown that the solution is to select pre-training data close to the target domain, and we also provide theoretical analysis in Appendix to further verify this. Since ReID is a open set problem with different IDs in training and testing set, the key problem is how to design a metric to evaluate the 'similarity' between the pre-training data x i s , i ? [1, N ] and the target dataset D t (instead of the person IDs in D t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Catastrophic Forgetting Score</head><p>Algorithm 1 Our Proposed Conditional Filtering</p><formula xml:id="formula_1">1: procedure FILTER(D s , D t ) 2: ? s ? T RAIN (D s ) Source Proxy Model 3: ? t ? T RAIN (? s , D t ) Target Proxy Model 4: for i ? 1 to N do x i s ? D s 5: c i s ? CF S(x i s ) Compute CFS 6: c s ? SORT (c 1 s , c 2 s , ..., c N s ) Get Score Set 7: D s ? T OP (D s , N , c s ) Filter Source Dataset 8:</formula><p>return D s Return the Filtered Subset</p><p>We first pre-train a model ? s on the source dataset D s . ? s is transferred into ? t by fine-tuning on the target dataset D t . Since ? s and ? t only serve as proxy models to select data, there is no need for them to achieve the best ReID performance, i.e. they can be lightweight models trained with less epochs. In this paper, D t is the fusion of Market-1501 and MSMT17, so we only need to select one subset D s sharing between different ReID datasets.</p><p>Many previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> have observed that catastrophic forgetting in neural networks occurs during domain transferring, and the degree of forgetting is related to the gap between the source domain and the target domain. To evaluate the domain gap between the pre-training data and the target domain, a simple metric named Catastrophic Forgetting Score (CFS) is proposed as below, which computes the representation similarity between ? s (x i s ) and ? t (x i s ) for the pre-training data x i s :</p><formula xml:id="formula_2">c i s = ? s (x i s ), ? t (x i s ) ||? s (x i s )|| ||? t (x i s )|| .<label>(1)</label></formula><p>The greater the c i s , the smaller the degree of forgetting, i.e. the smaller the domain gap. c s = {c 1 s , c 2 s , ..., c N s } is sorted in descending order and the top N images are selected to get the subset D s ? D s . The advantage of CFS is that it does not compare x i s directly with the images in the target domain. That is, unlike previous methods designed for close-set tasks, it avoids scanning through all images from the target domain to compute c i s , which is computationally costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Theoretical Analysis of CFS</head><p>In this subsection, we give a theoretical analysis to show large CFS is a necessary condition for finding an image in the source domain that is close to the target domain. To this end, we first rigorously define some terminologies as follows, which will be used in the analysis.</p><formula xml:id="formula_3">Definition 1. The feature representation ? is a function of x ? X that maps to R d , i.e., ?(x) : X ? R d .</formula><p>The feature representation ? is a function extracting features of an image. Without loss of generality, we use the normalized representation function in our analysis, i.e.,</p><formula xml:id="formula_4">? := ? ? ,<label>(2)</label></formula><p>where ? is the norm.</p><formula xml:id="formula_5">Definition 2. We say x and x are ( , ? 1 , ? 2 )-similar, if ? 1 (x) ?? 2 (x ) ? , where ? (0, 1)</formula><p>is a small constant, ? 1 and ? 2 are two representation functions,? 1 and? 2 are defined as <ref type="bibr" target="#b1">(2)</ref>. In addition, if ? 1 = ? 2 := ?, we use ( , ?)-similar for simplicity.</p><p>The similarity of two images can be defined as the "closeness" of their features. This is reasonable since the feature is considered as the representation of an image. Assumption 1. If x ? D and x ? D are close, then they are ( /2, ?, ? )-similar, i.e., ? (x) ?? (x ) ? /2, where ? : D ? R d and ? : D ? R d are two representation functions,? and? are defined as <ref type="bibr" target="#b1">(2)</ref>.</p><p>Since our inference focuses on downstream tasks, it is interested in considering the representation for the target domain (i.e., ? t ). The main goal is to find an image in the source domain that is ( , ? t )-similar to an image from the target domain. Mathematically, we aim to find</p><formula xml:id="formula_6">x i s ? D s to be ( ,? t )-similar to x j t ? D t 2 , that is, ? t (x i s ) ?? t (x j t ) ? .<label>(3)</label></formula><p>The following theorem shows that the large CFS is a necessary condition for finding such an image in the source domain.</p><p>Theorem 1. For a given x i s ? D s , if there exists a x j t ? D t that satisfies Assumption 1, then we have the following result. If</p><formula xml:id="formula_7">c i s ? 1 ? 2 /8,<label>(4)</label></formula><p>then <ref type="formula" target="#formula_6">(3)</ref> holds, where c i s is defined in (1) and ? (0, 1) is a small constant.</p><p>Proof. Let define the distance</p><formula xml:id="formula_8">d s,t := ? t (x i s ) ?? s (x i s ) .<label>(5)</label></formula><p>We write the distance</p><formula xml:id="formula_9">? t (x i s ) ?? t (x j t ) as ? t (x i s ) ?? t (x j t ) = ? t (x i s ) ?? s (x i s ) +? s (x i s ) ?? t (x j t ) ?d s,t + ? s (x i s ) ?? t (x j t ) ,<label>(6)</label></formula><p>where the last inequality uses Triangle Inequality for norm. Furthermore, if x i s and x j t are close, then under Assumption 1 we have</p><formula xml:id="formula_10">? s (x i s ) ?? t (x j t ) ? /2.<label>(7)</label></formula><p>Next, we want to explain why the small distance d s,t can be used as the selection criterion, let consider two cases. When</p><formula xml:id="formula_11">d s,t ? ? s (x i s ) ?? t (x j t )</formula><p>, by <ref type="formula" target="#formula_8">(5)</ref> and <ref type="formula" target="#formula_9">(6)</ref> we have</p><formula xml:id="formula_12">? t (x i s ) ?? t (x j t ) ? 2d s,t .<label>(8)</label></formula><p>When</p><formula xml:id="formula_13">d s,t ? ? s (x i s ) ?? t (x j t )</formula><p>, by <ref type="formula" target="#formula_9">(6)</ref> and <ref type="formula" target="#formula_10">(7)</ref>, we have</p><formula xml:id="formula_14">? t (x i s ) ?? t (x j t ) ?<label>(9)</label></formula><p>Thus, if the distance d s,t ? /2, then by <ref type="formula" target="#formula_12">(8)</ref> and <ref type="formula" target="#formula_14">(9)</ref> we know x j t and x i s are ( ,? t )-similar, i.e., (3) holds. The above condition of d s,t ? /2 is equivalent to <ref type="bibr">(</ref></p><formula xml:id="formula_15">4) since d 2 s,t = ? t (x i s ) ?? s (x i s ) 2 = ? t (x i s ) 2 + ? s (x i s ) ? 2 ? s (x i s ),? t (x i s ) = 2 ? 2 ? t (x i s ),? s (x i s ) = 2 ? 2c i s . So the greater c i s is, the smaller d s,t is.</formula><p>Although larger value of CFS is proved to be only a necessary condition for finding a source domain image close to the downstream task, <ref type="table">Table 4</ref> also empirically shows that this metric is effective in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IBN-based Convolution Stem</head><p>Our architecture design is based on two important experiences. 1) The patchity stem implemented by a stridep p ? p convolution (p = 16 by default) in the standard ViT is the key reason of training instability <ref type="bibr" target="#b7">[8]</ref>. Recent works show the convolution stem <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42]</ref>   improve training stability and peak performance. 2) Data bias is a critical challenge for person ReID. By learning invariant representation, IBNNet-a block <ref type="bibr" target="#b32">[33]</ref> has achieved a huge success in many publications or academic challenges <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>When applying ViTs in ReID tasks, a straightforward way is to introduce IBNNet-a block into the convolution stem. Following the original design of IBNNet-a, we propose IBN-based convolution stem (ICS), as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, by applying BN for half of the channels and IN for the other half after shallow convolution layers, and applying only BN layers after deep convolution layers. In this paper, we choose to apply IN after the first two convolution layers. Another variant worth consideration but may slightly reduce performance is to only apply IN after the first convolution layer. The kernel sizes, channels and strides of the convolution stem are kept the same as in <ref type="bibr" target="#b41">[42]</ref>. Hereinafter, we denote ViT with patchify stem, convolution stem, and ICS as ViT, ViT C and ViT I , respectively 6. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>Datasets. The dataset used for pre-training, LUPerson <ref type="bibr" target="#b13">[14]</ref>, contains 4.18M unlabeled human images in total, collected from 50,534 online videos. To evaluate the performance on ReID tasks, we conduct experiments on two popular benchmarks, i.e. Market-1501 (Market) <ref type="bibr" target="#b49">[50]</ref> and MSMT17 <ref type="bibr" target="#b40">[41]</ref>. They contain 32,668 images of 1,501 identities and 126,441 images of 4,101 identities, respectively. Images in these two datasets are resized to 256 ? 128 during training and inference stages. Standard evaluation protocols are used with the metrics of mean Average Precision (mAP) and Rank-1 accuracy. Pre-training. Unless otherwise specified, we follow the default training settings of all self-supervised methods. Images are resized to 224 ? 224 and 256 ? 128 for ImageNet and LUPerson, respectively. For pre-training on LUPerson with DINO, the model is trained on 8?V100 GPUs for 100 epochs and a multi-crop strategy is applied to crop 8 images with 96 ? 48 resolution. Supervised ReID. The transformer-based baseline proposed in <ref type="bibr" target="#b18">[19]</ref> is used as our baseline in this paper. That is to say, none of the overlapping patch embedding, jigsaw patch module or side information embedding is included here. It is noticed that DINO models should be fine-tuned with a small learning rate and a longer warmup process, so we recommend setting the learning rate to lr = 0.0004 ? batchsize 64 and warm-up epochs to be 20. All other settings are same with the original paper. USL/UDA ReID. We follow most of the default settings in <ref type="bibr" target="#b10">[11]</ref>. USL ReID and UDA ReID share the same training settings in this paper, and the only difference is that the models for UDA ReID need to be first pre-trained on source datasets before training in an unsupervised manner on target datasets. The maximum distance d between two samples is set to 0.6 and 0.7 for Market1501 and MSMT17, respectively. We use SGD optimizer to train ViTs for 50 epochs. The initial learning rate is set to 3.5e-4, and is reduced 10 times for every 20 epochs. Each mini-batch contains 256 images of 32 person IDs, i.e. each ID contains 8 images. The rate of stochastic depth is set 0.3.  Effect of partial data. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, pre-training with 30% data achieves comparable performance with the full-data pre-training. It is surprising that 40%?60% pretraining data even improves the performance slightly(e.g. 66.3% vs 64.2% mAP on MSMT17). Therefore, filtering irrelevant data with CFS from the pre-training data is actually beneficial instead of harmful to the down-stream performance. For a better trade-off between accuracy and pretraining cost, 50% of the pre-training data will be sampled in our following experiments.  <ref type="table">Table 5</ref>. Consistent improvements can be observed for these two different SSL paradigms, which demonstrates the universality of our method. <ref type="table" target="#tab_4">Table 6</ref> also provides more results of different ViT-S/16 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results of Conditional</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Effectiveness of ICS</head><p>To evaluate the effectiveness of the proposed ICS, we perform a fair comparison between patchify stem, convolution stem and ICS with the ViT-S/16 backbone for both full-data pre-training and the conditional pre-retraining in <ref type="table" target="#tab_4">Table 6</ref>. Three settings (e.g. supervised, USL and UDA ReID) are all included for a comprehensive evaluation.</p><p>We can observe that ViT I -S/16 outperforms ViT-S/16 and ViT C -S/16 in most cases. For instance, for the fulldata pre-training, ViT I -S/16 improves the mAP by 10.4% and 9.1% in USL ReID on MSMT compared with ViT-S P /16 and ViT C -S/16, respectively. For the conditional pre-training, ICS can also consistently improve peak performance under supervised, UDA and USL settings. Therefore, ICS is proved to be an effective module for the ReID task. An interesting phenomenon is that the performance gain of ICS decreases in the conditional pre-training case, because the domain bias between pre-training and downstream datasets has been mitigated.</p><p>Model Complexity and Computational Costs. We keep same number of transformer blocks in <ref type="table" target="#tab_4">Table 6</ref> to clearly compare different patch embeddings. The difference in computational complexity between ICS and convolution stem is negligible. <ref type="bibr" target="#b41">[42]</ref> has shown that the convolution stem has approximately the same complexity as a single transformer block, and adding the convolution stem while removing one transformer block can maintain similar model complexity without affecting accuracy. To further verify this point, extra experiments are conducted by removing one transformer block in the CFS-based supervised setting ( ViT I -S/16 in "CFS" section of same as the numbers in <ref type="table" target="#tab_4">Table 6</ref>. Therefore, it is reasonable to expect that the performance of our other experiments will be unchanged if we remove a transformer block to maintain the complexity comparable to the vanilla ViT models.</p><p>Remark. We have also tried the trick proposed in Mo-coV3 <ref type="bibr" target="#b7">[8]</ref> to freeze patchify stem. It can bring in slight performance increase but is still worse than convolution stem and ICS. More experiments are included in the Appendix to show that ICS learns more invariant feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison to State-of-the-Art methods</head><p>Supervised ReID. We compare to some of the stateof-the-art methods on supervised ReID in <ref type="table">Table 7</ref>. Our ViT-S?384 outperforms MGN and ABDNet on both benchmarks by a large margin. It is worth noting that Tran-sReID is pre-trained on ImageNet-21K and integrates additional camera information and part features to achieve the current performance. With self-supervised pre-training on LUPerson, our ViT-B?384 obtains 93.2%/75.0% mAP and 96.7%/89.5% rank-1 accuracy on Market1501/MSMT17 datasets, significantly outperforming TransReID with no additional modules. It is also observed that MGN benefits from the self-supervised pre-training on LUPerson via MocoV2, but is still inferior to our results.</p><p>USL ReID. Our methods are compared to MMCL <ref type="bibr" target="#b45">[46]</ref>, HCT <ref type="bibr" target="#b46">[47]</ref>, IICS <ref type="bibr" target="#b43">[44]</ref>, SPCL <ref type="bibr" target="#b16">[17]</ref> and C-Contrast <ref type="bibr" target="#b10">[11]</ref> in <ref type="table">Table 8</ref>, where the last two methods also adopt the pre-trained model on LUPerson. Our best results boost the mAP performance by 3.2% (89.6% vs 86.4%) and 10.8% (50.6% vs 39.8%) on Market and MSMT17, respectively.</p><p>UDA ReID. Some latest UDA-ReID methods are compared in the <ref type="table" target="#tab_6">Table 9</ref>. Among all existing state-of-theart methods, C-Contrast achieves the best performance at </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Discussions</head><p>This paper mitigate the domain gap between pre-training and ReID datasets for transformer-based person ReID from aspects of data and model. After observing that the existing pre-training paradigms of person ReID cannot perform well for transformer-based backbones, we investigate SSL methods with ViTs on the LUPerson and find DINO is the most suitable pre-training method for transformerbased ReID. To further bridge the gap between pre-training and ReID datasets, we propose a conditional pre-training method based on CFS to select relevant pre-training data to the target domain. The proposed method can speed up pretraining without performance drop. For the model structure, a ReID-specific patch embedding called IBN-based convolution stem is proposed to improve the peak performance. We believe the promising performance will inspire more work to study the SSL pre-training for the transformerbased models towards person ReID, i.e., a more suitable ViT variant, or a ReID-specific pre-training framework, etc.</p><p>Limitations. The pre-trained models are only suitable for the person ReID but cannot perform well on other less related tasks such as vehicle ReID, human parsing, or image classification, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conditional Pre-training on ImageNet</head><p>In this section, our proposed conditional pre-training is further applied on ImageNet in <ref type="table">Table 10</ref>. Since most of the images in ImageNet are irrelevant to person images, it is difficult to select enough data for the ReID task. Therefore, the pre-training on 50% data is inferior to the full pretraining version, but pre-training on CFS-selected data still outperforms the random sampling strategy, which shows the effectiveness of the CFS-based selection strategy.   As <ref type="figure" target="#fig_3">Figure 4</ref> shows, the sampled images with higher CFS are more similar to the data in the target domain. In contrast, the filtered images with lower CFS are of low-quality or have great domain gap with the target datasets. These filtered images cannot provide enough discriminative ID information during pre-training. Therefore, our conditional pre-training can effectively mitigate the domain gap between the pre-training and target datasets by removing those irrelevant or harmful data. <ref type="table" target="#tab_8">Table 11</ref> shows an experiment on Market-1501 to analyze the feature invariance of pre-trained models. The original dataset is extended into six variations by applying six different augmentations to each image, to simulate some important appearance variances in the ReID task (examples shown in <ref type="figure" target="#fig_4">Figure 5</ref>). Then, Centered Kernel Alignment (CKA) <ref type="bibr" target="#b25">[26]</ref> scores can evaluate the similarity between the features of the origin dataset and the features of the each simulated dataset. The higher CKA score is, the features provided by the pre-trained model show better invariance on this type of augmentation. ViT I -S achieves the best CKA scores for all types of appearance variance, showing that ICS is beneficial for learning invariant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Feature Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrast Brightness</head><p>Cropping Flip Scaling Origin Saturation  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Theoretical Analysis of Generalization Performance</head><p>In this section, we give a theoretical analysis of generalization performance relating source and target data. In particular, we consider excess risk bound (ERB) as a measure of generalization performance, which is commonly used in the learning theory literature <ref type="bibr" target="#b37">[38]</ref>. The following informal theorem highlights the factors that influence the generalization of target task, which dramatically simplify Theorem 3.</p><p>Theorem 2 (Informal Version of Theorem 3). Under the condition that the sample size of source data is larger than that of target data, then we have the following tow propositions to help improve generalization performance of target task:</p><p>? use the examples from the source data that close to target data;</p><p>? use a small learning rate to slightly update the backbone during the fine-tuning process.</p><p>In the remaining of this section, we first introduce the problem setting, and then present the formal theoretical result with its proof. To simplify the presentation of the proof, we include some supporting lemmas at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Problem Setting</head><p>To give an insight of the proposed method from the view of theoretical side, we formalize the problem as follows. We consider binary classification for simplicity. A domain is defined as a pair consisting of a distribution D on inputs X and a labeling function f : X ? Y with Y := {0, 1}. The labeling function can have a fractional (expected) value when labeling occurs non-deterministically. We denote by D s , f s the source domain and D t , f t the target domain. A hypothesis is a function h ? H : X ? Y, where H is a hypothesis space on X with VC dimension <ref type="bibr" target="#b36">[37]</ref> d. The loss on source domain that a hypothesis h disagrees with a labeling function f is defined as</p><formula xml:id="formula_16">F s (h, f ) := E x?Ds [|h(x) ? f (x)|] ,<label>(10)</label></formula><p>where f can also be a hypothesis. Its empirical version is written as</p><formula xml:id="formula_17">F s (h) := 1 |U s | x?Us |h(x) ? f (x)|,<label>(11)</label></formula><p>where U s is sampled from D s and |U s | is the sample size. Similarly, for target domain we define</p><formula xml:id="formula_18">F t (h, f ) := E x?Dt [|h(x) ? f (x)|] ,<label>(12)</label></formula><formula xml:id="formula_19">F t (h) := 1 |U t | x?Ut |h(x) ? f (x)|,<label>(13)</label></formula><p>where U t is sampled from D t and |U t | is the sample size.. Since the label function is deterministic for target task, the risk of a hypothesis is defined as</p><formula xml:id="formula_20">F t (h) := E x?Dt [|h(x) ? f t (x)|] ,<label>(14)</label></formula><p>and the empirical risk is</p><formula xml:id="formula_21">F t (h) := 1 |U t | x?Ut |h(x) ? f t (x)|.<label>(15)</label></formula><p>Let define</p><formula xml:id="formula_22">h * := arg min h?H F t (h),</formula><p>h * , f * := arg min</p><formula xml:id="formula_23">h?H,f ?H F s (h, f ), F * :=F t (h * ) + F s ( h * , f * ).</formula><p>The excess risk is defined by</p><formula xml:id="formula_24">F t (h) ? F t (h * ),<label>(16)</label></formula><p>which can be considered as a measure of generalization performance. Our goal is to minimize the excess risk. For a hypothesis space H, we give the definitions of the symmetric difference hypothesis space H?H and its divergence following by <ref type="bibr" target="#b1">[2]</ref>.</p><formula xml:id="formula_25">Definition 3.</formula><p>[2] For a hypothesis space H, the symmetric difference hypothesis space H?H is the set of hypotheses g ? H?H ? g(x) = h(x) ? h (x) forsome h, h ? H, where ? is the XOR function. That is, every hypothesis g ? H?H is the set of disagreements between two hypotheses in H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.</head><p>[2] For a hypothesis space H, the H?H-distance between two distributions D 1 and D 2 is defined as</p><formula xml:id="formula_26">d H?H (D 1 , D 2 ) = sup h,h ?H |Pr x?D1 (h(x) = h (x)) ? Pr x?D2 (h(x) = h (x))| .</formula><p>Remark. The empirical version of H?H-distance, denoted by d H?H (U s , U t ), can be considered as the a measure of similarity between the sampled source data U s and the sampled target data U t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Main Result and its Proof</head><p>Theorem 3 (Formal Version). Let U s , U t finite samples of sizes m, n (m n) drawn i.i.d. according D s , D t respectively. d H?H (U s , U t ) is the empirical H?H-divergence between samples, then for any ? ? (0, 1), with probability at least 1 ? ?, we have </p><formula xml:id="formula_27">F t (h) ? F t (h * ) ? 3 2 d H?H (U s , U t ) + F t (h, h * ) + F t (h</formula><p>Remark. We make several comments of the ERB in Theorem 3. First, the bound shows that the unlabeled empirical H?H-divergence (i.e., d H?H (U s , U t ), the first term of the left hand side of bound <ref type="formula" target="#formula_2">(17)</ref>) is important to improve the generalization performance (reduce the excess risk), indicating that when the used source data and target data are close, it will have a good generalization performance in target task. Second, the bound needs the term F t (h, h * ) is small for a hypothesis h and the optimal hypothesis h * of source domain, which means that h should be not too far from h * . This is consistent with the practice that usually a small learning rate is used to slightly update the backbone during the fine-tuning process. Finally, the term F s ( h * , f * ) in the bound tells us that the source error obtained during the pre-training process is also important to the target performance. </p><p>Proof. By the definition of F(h, f ), we know this is symmetric. On the other hand, we have</p><formula xml:id="formula_30">F(h 1 , h 2 ) =E x?D [|h 1 (x) ? h 2 (x)|] ?E x?D [|h 1 (x) ? h 3 (x)| + |h 2 (x) ? h 3 (x)|] ?E x?D [|h 1 (x) ? h 3 (x)|] + E x?D [|h 2 (x) ? h 3 (x)|] =F(h 1 , h 3 ) + F(h 2 , h 3 ).</formula><p>Lemma 2. Let H be a hypothesis space on X with VC dimension d. D 1 and D 2 be two distribution on X and U 1 , U 2 finite samples of sizes n 1 , n 2 (n 1 n 2 ) drawn i.i.d. according D 1 , D 2 respectively. d H?H (U 1 , U 2 ) is the empirical H-divergence between samples, then for any ? ? (0, 1), with probability at least 1 ? ?, we have d H?H (D 1 , D 2 ) ? d H?H (U 1 , U 2 ) + 4 d log(2n 2 ) + log(2/?) n 2 .</p><p>Proof. This lemma can be proved by a slight modification of Theorem 3.4 of <ref type="bibr" target="#b23">[24]</ref>. To this end, by setting We then obtain (19) by using Theorem 3.4 of <ref type="bibr" target="#b23">[24]</ref>.</p><p>Lemma 3 (Hoeffding's inequality <ref type="bibr" target="#b20">[21]</ref>). Let X 1 , . . . , X n be i.i.d. random variables with bounded intervals: a i ? X i ? b i , i = 1, . . . , n. LetX = 1 n n i=1 X i , then we have</p><formula xml:id="formula_32">Pr X ? E[X] ? c ? 2 exp ? 2n 2 c 2 n i=1 (b i ? a i ) 2 .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of different modules. We refer to the IBNNet-a block to design the IBN-based Convolution Stem (ICS). (a) ResNet block, (b) IBNNet-a block<ref type="bibr" target="#b32">[33]</ref>, (c) vanilla convolution stem<ref type="bibr" target="#b41">[42]</ref>, (d) our proposed ICS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Supervised fine-tuning performance of our conditional pre-training with different percentages of pre-training data. All results are achieved by ViT-S. Left and right figures show mAP and Rank-1 accuracy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Sampled data with high CFS (b) Filtered data with low CFS (c) Data in the target domain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Some random examples of selected data with high CFS and filtered data with low CFS. Images in the first two rows are from the source domain (LUPerson). The last row shows images sampled from the target domain (Market and MSMT17). Images in each row have been rearranged for the best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples of generated images. Six different augmentations are included, i.e. brightness, contrast, saturation, cropping, flip, and scaling, which are all important for appearance variance in ReID. The first two types of augmentation are related to color information and the last four are related to texture information. Pre-train Brig Cont Sat Crop Flip Scale ViTI -S 0.809 0.818 0.998 0.927 0.996 0.820 ViTC -S 0.727 0.691 0.997 0.919 0.996 0.794 ViT-S 0.702 0.664 0.997 0.877 0.996 0.728</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>* , f * ) + F s ( h * , f * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Proof of Theorem 3 . 2 d 2 d 2 dLemma 1 .</head><label>32221</label><figDesc>By Lemma 1 we have F t (h) ? F t (h * ) ?F t (h, h * ) ?F s (h, h * ) + |F t (h, h * ) ? F s (h, h * )| ?F s (h, h * ) + sup h,h ?H |F t (h, h ) ? F s (h, h )| =F s (h, h * ) + sup h,h ?H |Pr x?Dt (h(x) = h (x)) ? Pr x?Ds (h(x) = h (x))| =F s (h, h * ) + 1 H?H (D s , D t ) (by Definition 3) ?F s (h * , f * ) + F s (h, h * ) + F s ( h * , f * ) + 1 H?H (D s , D t ) (a) ? F s (h * , f * ) + F s (h, h * ) + F s ( h * , f * ) + 1 H?H (U s , U t ) + 4 2d log(2n) + log(2/?) n (b) ?F s (h * , f * ) + F s (h, h * ) + log(4/?) 2m + F s ( h * , f * ) + 1 2 d H?H (U s , U t ) + 4 2d log(2n) + log(4/?) n .Here (a) uses Lemma 2 with the fact that since every g ? H?H can be represented as a linear threshold network of depth 2 with 2 hidden units, the VC dimension of H?H is at most 2d<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>; (b) uses Hoeffding's inequality in Lemma 3. Similarly, we haveF t (h) ? F t (h * ) ?F t (h * , f * ) + F t (h, h * ) + F s ( h * , f * ) + 3 2 d H?H (U s , U t ) + 12 2d log(2n) + log(8/?) n ?F t (h * , f * ) + F t (h, h * ) + F s ( h * , f * ) + 3 2 d H?H (U s , U t ) Define F(h, f ) := E x?D [|h(x) ? f (x)|].For any h 1 , h 2 , h 3 ? H, we have F(h 1 , h 2 ) = F(h 2 , h 1 ) andF(h 1 , h 2 ) ? F(h 1 , h 3 ) + F(h 2 , h 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>= 4 d 1 16 ? (2n 1 ) d exp ? 2 n 1 16 = ?/ 2 ,(2n 2 ) d exp ? 2 n 2 16</head><label>4111222</label><figDesc>log(2n2)+log(2/?) n2 and = 4 d log(2n1)+log(2/?) n1, then we know ? (since n 1 n 2 ) and(2n 1 ) d exp ? 2 n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>SSL IMG 68.9(+6.4) 84.3(+3.8) 14.9(+1.4) 31.0(+1.1) SSL LUP 87.8(+25.3) 94.4(+13.9) 38.4(+24.9) 63.8(+33.9) Model performance in USL ReID. Supervised pretraining and self-supervised pre-training are abbreviated as SUP and SSL, respectively.</figDesc><table><row><cell cols="2">Pre-training</cell><cell>Market</cell><cell></cell><cell cols="2">MSMT17</cell></row><row><cell cols="2">Models Methods Data</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell></row><row><cell>R50</cell><cell>Sup IMG SSL LUP</cell><cell>82.6 84.0</cell><cell>93.0 93.4</cell><cell>33.1 31.4</cell><cell>63.3 58.8</cell></row><row><cell></cell><cell>Sup IMG</cell><cell>62.5</cell><cell>80.5</cell><cell>13.5</cell><cell>29.9</cell></row><row><cell>ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-training</cell><cell cols="2">MS2MA</cell><cell cols="2">MA2MS</cell></row><row><cell cols="2">Models Methods Data</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell></row><row><cell>R50</cell><cell>Sup IMG SSL LUP</cell><cell>82.4 85.1</cell><cell>92.5 94.4</cell><cell>33.4 28.3</cell><cell>60.5 53.8</cell></row><row><cell></cell><cell>Sup IMG</cell><cell>68.5</cell><cell>85.4</cell><cell>13.6</cell><cell>29.5</cell></row><row><cell>ViT-S/16</cell><cell cols="5">SSL IMG 79.7(+11.2) 90.5(+5.1) 21.8(+8.2) 41.6(+12.1)</cell></row><row><cell></cell><cell cols="5">SSL LUP 88.5(+20.0) 95.0(+9.6) 43.9(+30.3) 67.7(+38.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>stacked with several convolution, BN and ReLU layers can significantly</figDesc><table><row><cell>1x1 conv, 64</cell><cell cols="2">1x1 conv, 64</cell><cell>7x7 conv, 64</cell><cell cols="2">7x7 conv, 64</cell></row><row><cell>BN, 64</cell><cell>IN, 32</cell><cell>BN, 32</cell><cell>BN, 64</cell><cell>IN, 32</cell><cell>BN, 32</cell></row><row><cell>ReLU</cell><cell cols="2">ReLU</cell><cell>ReLU</cell><cell cols="2">ReLU</cell></row><row><cell>3x3 conv, 64</cell><cell cols="2">3x3 conv, 64</cell><cell>3x3 conv, 64</cell><cell cols="2">3x3 conv, 64</cell></row><row><cell>BN, 64</cell><cell>IN, 32</cell><cell>BN, 32</cell><cell>BN, 64</cell><cell>IN, 32</cell><cell>BN, 32</cell></row><row><cell>ReLU</cell><cell cols="2">ReLU</cell><cell>ReLU</cell><cell cols="2">ReLU</cell></row><row><cell>1x1 conv, 256</cell><cell cols="2">1x1 conv, 256</cell><cell>3x3 conv, 64</cell><cell cols="2">3x3 conv, 64</cell></row><row><cell>BN, 256</cell><cell cols="2">BN, 256</cell><cell>BN, 64</cell><cell cols="2">BN, 32</cell></row><row><cell>ReLU</cell><cell cols="2">ReLU</cell><cell>ReLU</cell><cell cols="2">ReLU</cell></row><row><cell>ReLU</cell><cell cols="2">ReLU</cell><cell>16x16 conv, 384</cell><cell cols="2">16x16 conv, 384</cell></row><row><cell>(a) ResNet Block</cell><cell cols="2">(b) IBN-a Block</cell><cell>(c) Conv Stem</cell><cell cols="2">(d) ICS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">Pre-training</cell><cell cols="2">Market mAP R1</cell><cell cols="2">MSMT17 mAP R1</cell><cell></cell><cell></cell><cell cols="2">Pre-training</cell><cell cols="2">Market mAP R1</cell><cell cols="2">MSMT17 mAP R1</cell></row><row><cell cols="2">Full (100%)</cell><cell cols="4">90.3 95.4 64.2 83.4</cell><cell></cell><cell cols="7">MocoV2+R50 [14] 88.2 94.8 53.3 76.0</cell></row><row><cell cols="2">Random (50%)</cell><cell cols="4">89.9 95.2 62.9 82.6</cell><cell></cell><cell></cell><cell cols="2">+CFS (50%)</cell><cell cols="4">89.4 95.5 56.8 78.8</cell></row><row><cell cols="6">Cluster (50%) [4] 90.0 95.4 63.8 83.3</cell><cell></cell><cell></cell><cell cols="2">DINO+ViT-S</cell><cell cols="4">90.3 95.4 64.2 83.4</cell></row><row><cell cols="2">CFS (50%)</cell><cell cols="4">91.0 96.0 66.1 84.6</cell><cell></cell><cell></cell><cell cols="2">+CFS (50%)</cell><cell cols="4">91.0 96.0 66.1 84.6</cell></row><row><cell cols="7">Table 4. Comparison of different data selection strategies. Ran-</cell><cell cols="7">Table 5. Conditional pre-training with different SSL pre-training</cell></row><row><cell cols="7">dom sampling (Random) and selecting data close to cluster centers</cell><cell cols="7">models. We sample 50% pre-training data based on CFS.</cell></row><row><cell cols="2">(Cluster) are compared.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised ReID</cell><cell></cell><cell></cell><cell cols="2">USL ReID</cell><cell></cell><cell></cell><cell cols="2">UDA ReID</cell><cell></cell></row><row><cell cols="2">Pre-training</cell><cell cols="2">Market</cell><cell cols="2">MSMT17</cell><cell cols="2">Market</cell><cell cols="2">MSMT17</cell><cell cols="2">MS2MA</cell><cell cols="2">MA2MS</cell></row><row><cell>Data</cell><cell>Model</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell></row><row><cell></cell><cell>ViT-S/16</cell><cell>90.3</cell><cell>95.4</cell><cell>64.2</cell><cell>83.4</cell><cell>87.8</cell><cell>94.4</cell><cell>38.4</cell><cell>63.8</cell><cell>88.5</cell><cell>95.0</cell><cell>43.9</cell><cell>67.7</cell></row><row><cell>Full</cell><cell>ViTC -S/16</cell><cell>90.7</cell><cell>95.7</cell><cell>65.2</cell><cell>84.5</cell><cell>88.3</cell><cell>94.6</cell><cell>39.7</cell><cell>65.2</cell><cell>89.1</cell><cell>95.3</cell><cell>49.0</cell><cell>73.4</cell></row><row><cell>(100%)</cell><cell>ViTI -S/16</cell><cell cols="4">91.1 +0.8 +0.5 +2.6 +2.1 95.9 66.8 85.5</cell><cell cols="4">89.3 +1.5 +0.4 +10.4 +10.6 94.8 48.8 74.4</cell><cell cols="4">89.6 +1.1 +0.6 +11.1 +10.2 95.6 55.0 77.9</cell></row><row><cell></cell><cell>ViT-S/16</cell><cell>91.0</cell><cell>96.0</cell><cell>66.1</cell><cell>84.6</cell><cell>88.2</cell><cell>94.2</cell><cell>40.9</cell><cell>66.4</cell><cell>89.4</cell><cell>95.4</cell><cell>47.4</cell><cell>70.8</cell></row><row><cell>CFS</cell><cell>ViTC -S/16</cell><cell>91.2</cell><cell>95.8</cell><cell>67.8</cell><cell>85.7</cell><cell>89.3</cell><cell>95.0</cell><cell>42.5</cell><cell>67.6</cell><cell>89.7</cell><cell>95.5</cell><cell>55.7</cell><cell>75.5</cell></row><row><cell>(50%)</cell><cell>ViTI -S/16</cell><cell cols="4">91.3 +0.3 +0.2 +2.0 +2.5 96.2 68.1 86.1</cell><cell cols="2">89.6 +1.4 +0.9 95.3</cell><cell>50.6 +9.7</cell><cell>75.0 +8.6</cell><cell cols="3">89.9 +0.5 +0.1 +10.4 95.5 57.8</cell><cell>79.5 +8.7</cell></row></table><note>Time consumption of pre-training. Although our con- ditional pre-training is two-stage, it is still more efficient. Standard pre-training of a ViT-S on full LUPerson takes 107. Comparison of patchify stem (ViT-S/16), convolution stem (ViTC -S/16) and the proposed ICS (ViTI -S/16). Gray numbers present performance improvements from the proposed ICS.hours with 8?V100 GPUs. In our conditional pre-training, data selection step takes 21 hours because the proxy mod- els only need to be pre-trained for 20 epochs. For the sec- ond stage, pre-training the model on 50% of LUPerson only takes 51 hours. In total we can still save about 30% of pre- training time with a slight performance improvement. Selection strategies. Different selection strategies are com- pared in Table 4. Randomly sampling 50% data for pre- training does not help with the downnstream performance because it cannot reduce the noise in the original data. Although Cluster [4] is a state-of-the-art data selection method proposed for close-set tasks, it is not suitable for the open-set person ReID problem. Our conditional pre- training based on CFS performs the best on all</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 )Table 7 .</head><label>67</label><figDesc>, we achieve 91.2% mAP and 96.1% rank-1 accuracy on Market, almost the Comparison to state-of-the-art methods in supervised ReID. MGN is the improved version in fast-reid. * means that backbones are pre-trained on LUPerson. ?384 represents that images are resized to 384 ? 128.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Market</cell><cell cols="2">MSMT17</cell></row><row><cell>Methods</cell><cell>Backbone</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell></row><row><cell>BOT [31]</cell><cell>R50-IBN</cell><cell cols="2">88.2 95.0</cell><cell cols="2">57.8 80.7</cell></row><row><cell>BOT  *  [31]</cell><cell>R50-IBN</cell><cell cols="2">90.1 95.6</cell><cell cols="2">60.8 81.4</cell></row><row><cell>MGN [39]</cell><cell>R50?384</cell><cell cols="2">87.5 95.1</cell><cell cols="2">63.7 85.1</cell></row><row><cell>SCSN [7]</cell><cell>R50?384</cell><cell cols="2">88.5 95.7</cell><cell cols="2">58.5 83.8</cell></row><row><cell cols="2">ABDNet [5] R50?384</cell><cell cols="2">88.3 95.6</cell><cell cols="2">60.8 82.3</cell></row><row><cell cols="2">TransReID [19] ViT-B?384</cell><cell cols="2">89.5 95.2</cell><cell cols="2">69.4 89.2</cell></row><row><cell cols="2">MoCoV2  *  [14] MGN?384</cell><cell cols="2">91.0 96.4</cell><cell cols="2">65.7 85.5</cell></row><row><cell>Ours  *</cell><cell>ViTI -S</cell><cell cols="2">91.3 96.2</cell><cell cols="2">68.1 86.1</cell></row><row><cell>Ours  *</cell><cell>ViTI -S?384</cell><cell cols="2">91.7 96.3</cell><cell cols="2">70.5 87.8</cell></row><row><cell>Ours  *</cell><cell>ViTI -B?384</cell><cell cols="2">93.2 96.7</cell><cell cols="2">75.0 89.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">From the perspective of</cell></row><row><cell cols="6">computational complexity, ViTI -S and ViTI -B can be compared</cell></row><row><cell cols="6">with R50/R50-IBN and MGN, respectively. R50-IBN stands for</cell></row><row><cell>ResNet50-IBN-a.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Market</cell><cell cols="2">MSMT17</cell></row><row><cell>Methods</cell><cell cols="2">Backbone mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell></row><row><cell>MMCL [46]</cell><cell>R50</cell><cell cols="2">45.5 80.3</cell><cell cols="2">11.2 35.4</cell></row><row><cell>HCT [47]</cell><cell>R50</cell><cell cols="2">56.4 80.0</cell><cell>-</cell><cell>-</cell></row><row><cell>IICS [44]</cell><cell>R50</cell><cell cols="2">72.9 89.5</cell><cell cols="2">26.9 52.4</cell></row><row><cell>SPCL  *  [17]</cell><cell>R50</cell><cell cols="2">76.2 90.2</cell><cell>-</cell><cell>-</cell></row><row><cell>C-Contrast [11]</cell><cell>R50</cell><cell cols="2">82.6 93.0</cell><cell cols="2">33.1 63.3</cell></row><row><cell>C-Contrast  *  [11]</cell><cell>R50</cell><cell cols="2">84.0 93.4</cell><cell cols="2">31.4 58.8</cell></row><row><cell cols="2">C-Contrast  *  [11] R50-IBN</cell><cell cols="2">86.4 94.2</cell><cell cols="2">39.8 66.1</cell></row><row><cell>Ours  *</cell><cell>ViT-S</cell><cell cols="2">88.2 94.2</cell><cell cols="2">40.9 66.4</cell></row><row><cell>Ours  *</cell><cell>ViTI -S</cell><cell cols="2">89.6 95.3</cell><cell cols="2">50.6 75.0</cell></row><row><cell cols="6">Table 8. Comparison to state-of-the-art methods in USL ReID.  *</cell></row><row><cell cols="5">means that backbones are pre-trained on LUPerson.</cell></row><row><cell></cell><cell></cell><cell cols="2">MS2MA</cell><cell cols="2">MA2MS</cell></row><row><cell>Methods</cell><cell cols="2">Backbone mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell></row><row><cell>DG-Net++ [52]</cell><cell>R50</cell><cell cols="2">64.6 83.1</cell><cell cols="2">22.1 48.4</cell></row><row><cell>MMT [16]</cell><cell>R50</cell><cell cols="2">75.6 89.3</cell><cell cols="2">24.0 50.1</cell></row><row><cell>SPCL [17]</cell><cell>R50</cell><cell cols="2">77.5 89.7</cell><cell cols="2">26.8 53.7</cell></row><row><cell>SPCL [17]</cell><cell>R50-IBN</cell><cell cols="2">79.9 92.0</cell><cell cols="2">31.0 58.1</cell></row><row><cell>C-Contrast [11]</cell><cell>R50</cell><cell cols="2">82.4 92.5</cell><cell cols="2">33.4 60.5</cell></row><row><cell>C-Contrast  *  [11]</cell><cell>R50</cell><cell cols="2">85.1 94.4</cell><cell cols="2">28.3 53.8</cell></row><row><cell cols="2">C-Contrast  *  [11] R50-IBN</cell><cell cols="2">86.9 94.6</cell><cell cols="2">42.6 69.1</cell></row><row><cell>Ours  *</cell><cell>ViT-S</cell><cell cols="2">89.4 95.4</cell><cell cols="2">47.4 70.8</cell></row><row><cell>Ours  *</cell><cell>ViTI -S</cell><cell cols="2">89.9 95.5</cell><cell cols="2">57.8 79.5</cell></row><row><cell cols="6">86.9% and 42.6% mAP on MS2MA and MA2MS, respec-</cell></row><row><cell cols="6">tively. Our method with both ViT-S and ViT I -S surpasses</cell></row><row><cell cols="6">C-Contrast by a large margin. Especially, ViT I -S obtains</cell></row><row><cell cols="6">89.9% mAP on MS2MA and 57.8% mAP on MA2MS,</cell></row><row><cell cols="6">which are already comparable to many supervised methods.</cell></row></table><note>. Comparison to state-of-the-art methods in UDA ReID.* means that backbones are pre-trained on LUPerson.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 .</head><label>11</label><figDesc>Feature invariance of different patch embedding methods. The table compares the CKA scores of different pre-trained models for different augmentations. Brig, Cont, Sat, Crop, Flip, and Scale represent augmentation of brightness, contrast, saturation, cropping, flip, and scaling, respectively. Source codes of Patch Embedding for IBN-based Convolution Stem.</figDesc><table><row><cell cols="5">D. Improvements of LUPerson Pre-training</cell></row><row><cell></cell><cell>Pre-training</cell><cell></cell><cell>Market</cell><cell>MSMT17</cell></row><row><cell>Models</cell><cell>Data</cell><cell cols="3">#Images mAP R1 mAP R1</cell></row><row><cell>ViT-S</cell><cell>IMG-1K LUP</cell><cell>1.28M 4.18M</cell><cell cols="2">85.0 93.8 53.5 75.2 90.3 95.4 64.2 83.4</cell></row><row><cell>ViT-B</cell><cell>IMG-21K LUP</cell><cell>14.2M 4.18M</cell><cell cols="2">86.8 94.7 61.0 81.8 92.5 96.5 70.1 87.3</cell></row><row><cell cols="5">Table 12. SSL Pre-training on LUPerson can consistently im-</cell></row><row><cell cols="5">prove the performance of supervised pre-training on ImageNet-1k</cell></row><row><cell cols="5">or ImageNet-21k. Conditional pre-training or ICS is not included.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https : / / github . com / alibaba / cluster -contrastreid. All results are reproduced using the official code with the same</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please note that x j t could not be a true image from target domain and it is for the proof use only. In general, the image x j t can be a virtual one that follows the same distribution of the images of target domain.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural network learning: Theoretical foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>cambridge university press Cambridge</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient conditional pre-training for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuvam</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Tanmay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10231</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canmiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Selection via proxy: Efficient data selection for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cluster contrast for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11568</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="14750" to="14759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transreid: Transformer-based object reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15013" to="15022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wassily</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting change in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2019. 11</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diverse part discovery: Occluded person re-identification with part-aware transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2898" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A video is worth three views: Trigeminal transformers for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01745</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improved fine-tuning by leveraging pretraining data: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TMM</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anatomy of catastrophic forgetting: Hidden representations and task semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vinay Venkatesh Ramasesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanqiang</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05475</idno>
		<title level="m">Git: Graph interactive transformer for vehicle re-identification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting during domain adaptation of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Gwinnup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability and its Applications</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="264" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Springer science &amp; business media</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scaled relu matters for training vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03810</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Self-supervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Intra-inter camera similarity for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11926" to="11935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural data server: A large-scale search engine for transfer learning data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3893" to="3902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13657" to="13665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Hat: Hierarchical aggregation transformers for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05946</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Spatiotemporal transformer for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16469</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Aaformer: Auto-aligned transformer for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaopan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint disentangling and adaptation for crossdomain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="87" to="104" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
