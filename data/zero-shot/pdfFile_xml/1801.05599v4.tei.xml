<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Additive Margin Softmax for Face Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
							<email>feng.wff@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
							<email>wyliu@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
							<email>haijunliu@126.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>chengjian@uestc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UESTC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UESTC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UESTC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Additive Margin Softmax for Face Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax <ref type="bibr" target="#b9">[10]</ref> and Angular Softmax <ref type="bibr" target="#b8">[9]</ref> have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face verification is widely used for identity authentication in enormous areas such as finance, military, public security and so on. Nowadays, most face verification models are built upon Deep Convolutional Neural Networks and supervised by classification loss functions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref>, metric learning loss functions <ref type="bibr" target="#b15">[16]</ref> or both <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13]</ref>. Metric learning loss functions such as contrastive loss <ref type="bibr" target="#b16">[17]</ref> or triplet loss <ref type="bibr" target="#b15">[16]</ref> usually require carefully designed sample mining strategies and the final performance is very sensitive to these strategies, so increasingly more researchers shift their attentions to building deep face verification models based on improved classification loss functions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Current prevailing classification loss functions for deep face recognition are mostly based on the widely-used softmax loss. The softmax loss is typically good at optimizing <ref type="bibr">Figure 1</ref>. Comparison between the original softmax loss and the additive margin softmax loss. Note that, the angular softmax <ref type="bibr" target="#b8">[9]</ref> can only impose unfixed angular margin, while the additive margin softmax incorporates the fixed hard angular margin. the inter-class difference (i.e., separating different classes), but not good at reducing the intra-class variation (i.e., making features of the same class compact). To address this, lots of new loss functions are proposed to minimize the intraclass variation. <ref type="bibr" target="#b19">[20]</ref> proposed to add a regularization term to penalize the feature-to-center distances. In <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>, researchers proposed to use a scale parameter to control the "temperature" <ref type="bibr" target="#b1">[2]</ref> of the softmax loss, producing higher gradients to the well-separated samples to further shrink the intra-class variance. In <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, the authors introduced an conceptually appealing angular margin to push the classification boundary closer to the weight vector of each class. <ref type="bibr" target="#b8">[9]</ref> also provided a theoretical guidance of training a deep model for metric learning tasks using the classification loss functions. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> also improved the softmax loss by incorporating differnet kinds of margins.</p><p>In this work, we propose a novel and more interpretable way to import the angular margin into the softmax loss. We formulate an additive margin via cos ??m, which is simpler than <ref type="bibr" target="#b8">[9]</ref> and yields better performance. From Equation <ref type="formula" target="#formula_2">(3)</ref>, we can see that m is multiplied to the target angle ? yi in <ref type="bibr" target="#b8">[9]</ref>, so this type of margin is incorporated in a multiplicative manner. Since our margin is a scalar subtracted from cos?, we call our loss function Additive Margin Softmax (AM-Softmax).</p><p>Experiments on LFW BLUFR protocol <ref type="bibr" target="#b6">[7]</ref> and MegaFace <ref type="bibr" target="#b4">[5]</ref> show that our loss function with the same network architecture achieves better results than the current state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>To better understand the proposed AM-Softmax loss, we will first give a brief review of the original softmax and the A-softmax loss <ref type="bibr" target="#b8">[9]</ref>. The formulation of the original softmax loss is given by</p><formula xml:id="formula_0">L S = ? 1 n n i=1 log e W T y i fi c j=1 e W T j fi = ? 1 n n i=1 log e Wy i fi cos(?y i ) c j=1 e Wj fi cos(?j ) ,<label>(1)</label></formula><p>where f is the input of the last fully connected layer (f i denotes the the i-th sample), W j is the j-th column of the last fully connected layer. The W T yi f i is also called as the target logit <ref type="bibr" target="#b13">[14]</ref> of the i-th sample.</p><p>In the A-softmax loss, the authors proposed to normalize the weight vectors (making W i to be 1) and generalize the target logit from f i cos(? yi ) to f i ?(? yi ),</p><formula xml:id="formula_1">L AS = ? 1 n n i=1 log e fi ?(?y i ) e fi ?(?y i ) + c j=1,j =yi e fi cos(?j ) ,<label>(2)</label></formula><p>where the ?(?) is usually a piece-wise function defined as</p><formula xml:id="formula_2">?(?) = (?1) k cos(m?) ? 2k + ?cos(?) 1 + ? , ? ? [ k? m , (k + 1)? m ],<label>(3)</label></formula><p>where m is usually an integer larger than 1 and ? is a hyperparameter to control how hard the classification boundary should be pushed. During training, the ? is annealing from 1, 000 to a small value to make the angular space of each class become more and more compact. In their experiments, they set the minimum value of ? to be 5 and m = 4, which is approximately equivalent to m = 1.5 ( <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Additive Margin Softmax</head><p>In this section, we will first describe the definition of the proposed loss function. Then we will discuss about the intuition and interpretation of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Definition</head><p>[10] defines a general function ?(?) to introduce the large margin property. Motivated by that, we further propose a specific ?(?) that introduces an additive margin to the softmax loss function. The formulation is given by</p><formula xml:id="formula_3">?(?) = cos? ? m.<label>(4)</label></formula><p>Compared to the ?(?) defined in L-Softmax <ref type="bibr" target="#b9">[10]</ref> and Asoftmax <ref type="bibr" target="#b8">[9]</ref> (Equation <ref type="formula" target="#formula_2">(3)</ref>), our definition is more simple  </p><formula xml:id="formula_4">?(x) = x ? m.<label>(5)</label></formula><p>In this margin scheme, we don't need to calculate the gradient for back-propagation because ? (x) = 1. It is much easier to implement compared with SphereFace <ref type="bibr" target="#b8">[9]</ref>. Since we use cosine as the similarity to compare two face features, we follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> to apply both feature normalization and weight normalization to the inner product layer in order to build a cosine layer. Then we scale the cosine values using a hyper-parameter s as suggested in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Finally, the loss function becomes</p><formula xml:id="formula_5">L AM S = ? 1 n n i=1 log e s?(cos?y i ?m) e s?(cos?y i ?m) + c j=1,j =yi e s?cos?j = ? 1 n n i=1 log e s?(W T y i fi?m) e s?(W T y i fi?m) + c j=1,j =yi e sW T j fi .</formula><p>(6) In this paper, we assume that the norm of both W i and f are normalized to 1 if not specified. In <ref type="bibr" target="#b18">[19]</ref>, the authors propose to let the scaling factor s to be learned through backpropagation. However, after the margin is introduced into the loss function, we find that the s will not increase and the network converges very slowly if we let s to be learned. </p><formula xml:id="formula_6">, where W T 1 P0 = W T 2 P0. For AM-Softmax, the decision boundary for class 1 is at P1, where W T 1 P1 ? m = W T 2 P1 = W T 1 P2.</formula><p>Note that the distance marked on this figure doesn't represent the real distances. The real distance is a function of the cosine of the angle, while in this figure we use the angle as the distance for better visualization effect. Here we use the word "center" to represent the weight vector of the corresponding class in the last inner-product layer, even though they may not be exactly the mean vector of the features in the class. The relationship between the weight vector ("agent") and the features' mean vector ("center") is described in <ref type="figure" target="#fig_6">Figure 6</ref> of <ref type="bibr" target="#b18">[19]</ref>.</p><p>Thus, we fix s to be a large enough value, e.g. 30, to accelerate and stablize the optimization.</p><p>As described in Section 2, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> propose to use an annealing strategy to set the hyper-parameter ? to avoid network divergence. However, to set the annealing curve of ?, lots of extra parameters are introduced, which are more or less confusing for starters. Although properly tuning those hyper-parameters for ? could lead to impressive results, the hyper-parameters are still quite difficult to tune. With our margin scheme, we find that we no longer need the help of the annealing strategy. The network can converge flexibly even if we fix the hyper-parameter m from scratch. Compared to SphereFace <ref type="bibr" target="#b8">[9]</ref>, our additive margin scheme is more friendly to those who are not familiar with the effects of the hyper-parameters. Another recently proposed additive margin is also described in <ref type="bibr" target="#b5">[6]</ref>. Our AM-Softmax is different than <ref type="bibr" target="#b5">[6]</ref> in the sense that our feature and weight are normalized to a predefined constant s. The normalization is the key to the angular margin property. Without the normalization, the margin m does not necessarily lead to large angular margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Geometric Interpretation</head><p>Our additive margin scheme has a clear geometric interpretation on the hypersphere manifold. In <ref type="figure" target="#fig_2">Figure 3</ref>, we draw a schematic diagram to show the decision boundary of both conventional softmax loss and our AM-Softmax. For ex-ample, in <ref type="figure" target="#fig_2">Figure 3</ref>, the features are of 2 dimensions. After normalization, the features are on a circle and the decision boundary of the traditional softmax loss is denoted as the vector P 0 . In this case, we have W T 1 P 0 = W T 2 P 0 at the decision boundary P 0 .</p><p>For our AM-Softmax, the boundary becomes a marginal region instead of a single vector. At the new boundary</p><formula xml:id="formula_7">P 1 for class 1, we have W T 1 P 1 ? m = W T 2 P 1 , which gives m = (W 1 ? W 2 ) T P 1 = cos(? W1,P1 ) ? cos(? W2,P1 ).</formula><p>If we further assume that all the classes have the same intra-class variance and the boundary for class 2 is at P 2 , we can get cos(? W2,P1 ) = cos(? W1,P2 ) ( <ref type="figure" target="#fig_2">Fig. 3)</ref>. Thus, m = cos(? W1,P1 ) ? cos(? W1,P2 ), which is the difference of the cosine scores for class 1 between the two sides of the margin region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Angular Margin or Cosine Margin</head><p>In SphereFace <ref type="bibr" target="#b8">[9]</ref>, the margin m is multiplied to ?, so the angular margin is incorporated into the loss in a multiplicative way. In our proposed loss function, the margin is enforced by subtracting m from cos ?, so our margin is incorporated into the loss in an additive way, which is one of the most significant differences than <ref type="bibr" target="#b8">[9]</ref>. It is also worth mentioning that despite the difference of enforcing margin, these two types of margin formulations are also different in the base values. Specifically, one is ? and the other is cos ?. Although usually the cosine margin has an one-toone mapping to the angular margin, there will still be some difference while optimizing them due to the non-linearity induced by the cosine function.</p><p>Whether we should use the cosine margin depends on which similarity measurement (or distance) the final loss function is optimizing. Obviously, our modified softmax loss function is optimizing the cosine similarity, not the angle. This may not be a problem if we are using the conventional softmax loss because the decision boundaries are the same in these two forms (cos ? 1 = cos ? 2 ? ? 1 = ? 2 ). However, when we are trying to push the boundary, we will face a problem that these two similarities (distances) have different densities. Cosine values are more dense when the angles are near 0 or ?. If we want to optimize the angle, an arccos operation may be required after the value of the inner product W T f is obtained. It will potentially be more computationally expensive.</p><p>In general, angular margin is conceptually better than the cosine margin, but considering the computational cost, cosine margin is more appealing in the sense that it could achieve the same goal with less efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Feature Normalization</head><p>In the SphereFace model <ref type="bibr" target="#b8">[9]</ref>, the authors added the weight normalization based on Large Margin Softmax <ref type="bibr" target="#b9">[10]</ref>, leaving   The feature direction is selected as the mean vector of one selected target center and one nearest class center. Note that the y-axis is in logarithmic scale for better visualization. For softmax loss with feature normalization, we set s = 30. That is why the intersection of these two curves is at 30.</p><p>the feature still not normalized. Our loss function, following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>, applies feature normalization and uses a global scale factor s to replace the sample-dependent feature norm in SphereFace <ref type="bibr" target="#b8">[9]</ref>. One question arises: when should we add the feature normalization? Our answer is that it depends on the image quality. In <ref type="bibr" target="#b14">[15]</ref>'s <ref type="figure">Figure 1</ref>, we can see that the feature norm is highly correlated with the quality of the image. Note that back propagation has a property that,</p><formula xml:id="formula_8">y = x ? ? dy dx = 1 ? .<label>(7)</label></formula><p>Thus, after normalization, features with small norms will get much bigger gradient compared with features that have big norms ( <ref type="figure" target="#fig_5">Figure 5</ref>). By back-propagation, the network will pay more attention to the low-quality face images, which usually have small norms. Its effect is very similar with hard sample mining <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>. The advantages of feature normalization are also revealed in <ref type="bibr" target="#b10">[11]</ref>. As a conclusion, feature normalization is most suitable for tasks whose image quality is very low. From <ref type="figure" target="#fig_5">Figure 5</ref> we can see that the gradient norm may be extremely big when the feature norm is very small. This potentially increases the risk of gradient explosion, even though we may not come across many samples with very small feature norm. Maybe some re-weighting strategy whose feature-gradient norm curve is between the two curves in <ref type="figure" target="#fig_5">Figure 5</ref> could potentially work better. This is an interesting topic to be studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Feature Distribution Visualization</head><p>To better understand the effect of our loss function, we designed a toy experiment to visualize the feature distributions trained by several loss functions. We used Fashion MNIST <ref type="bibr" target="#b20">[21]</ref> (10 classes) to train several 7-layer CNN models which output 3-dimensional features. These networks are supervised by different loss functions. After we obtain the 3-dimensional features, we normalize and plot them on a hypersphere (ball) in the 3 dimensional space <ref type="figure" target="#fig_4">(Figure 4)</ref>.</p><p>From the visualization, we can empirically show that our AM-Softmax performs similarly with the best SphereFace <ref type="bibr" target="#b8">[9]</ref> (A-Softmax) model when we set s = 10, m = 0.2. Moreover, our loss function can further shrink the intra-class variance by setting a larger m. Compared to A-Softmax <ref type="bibr" target="#b8">[9]</ref>, the AM-Softmax loss also converges easier with proper scaling factor s. The visualized 3D features well demonstrates that AM-Softmax could bring the large margin property to the features without tuning too many hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we will firstly describe the experimental settings. Then we will discuss the overlapping problem of the modern in-the-wild face datasets. Finally we will compare the performance of our loss function with several previous state-of-the-art loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our loss function is implemented using Caffe framework <ref type="bibr" target="#b3">[4]</ref>. We follow all the experimental settings from <ref type="bibr" target="#b8">[9]</ref>, including the image resolution, preprocessing method and the network structure. Specifically speaking, we use MTCNN <ref type="bibr" target="#b23">[24]</ref> to detect faces and facial landmarks in images. Then LFW <ref type="bibr" target="#b2">[3]</ref> LFW BLUFR <ref type="bibr" target="#b6">[7]</ref> LFW BLUFR <ref type="bibr" target="#b6">[7]</ref> LFW BLUFR <ref type="bibr" target="#b6">[7]</ref> MegaFace <ref type="bibr" target="#b4">[5]</ref> MegaFace <ref type="bibr">[</ref>  <ref type="table">Table 1</ref>. Performance on modified ResNet-20 with various loss functions. Note that, for Center Loss <ref type="bibr" target="#b19">[20]</ref> and NormFace <ref type="bibr" target="#b18">[19]</ref>, we used modified ResNet-28 <ref type="bibr" target="#b19">[20]</ref> because we failed to train a model using Center Loss on modified ResNet-20 <ref type="bibr" target="#b8">[9]</ref> and the NormFace model was fine-tuned based on the Center Loss model.</p><p>the faces are aligned according to the detected landmarks.</p><p>The aligned face images are of size 112 ? 96, and are normalized by subtracting 128 and dividing 128. Our network structure follows <ref type="bibr" target="#b8">[9]</ref>, which is a modified ResNet <ref type="bibr" target="#b0">[1]</ref> with 20 layers that is adapted to face recognition. All the networks are trained from scratch. We set the weight decay parameter to be 5e?4. The batch size is 256 and the learning rate begins with 0.1 and is divided by 10 at the 16K, 24K and 28K iterations. The training is finished at 30K iterations. During training, we only use image mirror to augment the dataset.</p><p>In testing phase, We feed both frontal face images and mirror face images and extract the features from the output of the first inner-product layer. Then the two features are summed together as the representation of the face image. When comparing two face images, cosine similarity is utilized as the measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset Overlap Removal</head><p>The dataset we use for training is CASIA-Webface <ref type="bibr" target="#b21">[22]</ref>, which contains 494,414 training images from 10,575 identities. To perform open-set evaluations, we carefully remove the overlapped identities between training dataset (CASIA-Webface <ref type="bibr" target="#b21">[22]</ref>) and testing datasets (LFW <ref type="bibr" target="#b2">[3]</ref> and MegaFace <ref type="bibr" target="#b4">[5]</ref>). Finally, we find 17 overlapped identities between CASIA-Webface and LFW, and 42 overlapped identities between CASIA-Webface and MegaFace set1. Note that there are only 80 identities in MegaFace set1, i.e. over half of the identities are already in the training dataset. The effect of overlap removal is remarkable for MegaFace ( In our paper, we re-train some of the previous loss functions on the cleaned dataset as the baselines for comparison. Note that, we make our experiments fair by using the same network architecture and training dataset for every compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of Hyper-parameter m</head><p>There are two hyper-parameters in our proposed loss function, one is the scale s and another is the margin m. The scale s has already been discussed sufficiently in several previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>. In this paper, we directly fixed it to 30 and will not discuss its effect anymore.</p><p>The main hyper-parameter in our loss function is the margin m. In <ref type="table" target="#tab_1">Table 4</ref>, we list the performance of our proposed AM-Softmax loss function with m varies from 0.25 to 0.5. From the table we can see that from m = 0.25 to 0.3, the performance improves significantly, and the performance become the best when m = 0.35 to m = 0.4.</p><p>We also provide the result for the loss function without feature normalization (noted as w/o FN) and the scale s. As we explained before, feature normalization performs better on low quality images like MegaFace <ref type="bibr" target="#b4">[5]</ref>, and using the original feature norm performs better on high quality images like LFW <ref type="bibr" target="#b2">[3]</ref>.</p><p>In <ref type="figure" target="#fig_6">Figure 6</ref>, we draw both of the CMC curves to evaluate the performance of identification and ROC curves to evaluate the performance of verification. From this figure, we Note that for Center Loss and NormFace, the backend network is ResNet-28 <ref type="bibr" target="#b19">[20]</ref>, while others are based on ResNet-20 <ref type="bibr" target="#b8">[9]</ref>. Even though the curves of the Center Loss model and the NormFace model is close to the SphereFace model, please keep in mind that part of the performance comes from the bigger network structure.</p><p>can show that our loss function performs much better than the other loss functions when the rank or false positive rate is very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we propose to impose an additive margin strategy to the target logit of softmax loss with feature and weights normalized. Our loss function is built upon the previous margin schemes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, but it is more simple and interpretable. Comprehensive experiments show that our loss function performs better than A-Softmax [9] on LFW BLUFR <ref type="bibr" target="#b6">[7]</ref> and MegaFace <ref type="bibr" target="#b4">[5]</ref>.</p><p>There is still lots of potentials for the research of the large margin strategies. There could be more creative way of specifying the function ?(?) other than multiplication and addition. In our AM-Softmax loss, the margin is a manually tuned global hyper-parameter. How to automatically determine the margin and how to incorporate class-specific or sample-specific margins remain open questions and are worth studying.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>m=2, 6=0) Angular Softmax (m=4, 6=0) Angular Softmax (m=4, 6=5) Additive Margin Softmax (m=0.35)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>?(?) for conventional Softmax, Angular Softmax<ref type="bibr" target="#b8">[9]</ref> and our proposed Hard Margin Softmax. For Angular Softmax, we plot the logit curve for three parameter sets. From the curves we can infer that m = 4, ? = 5 lies between conventional Softmax and Angular Softmax with m = 2, ? = 0, which means it is approximately m = 1.5. Our proposed Additive Margin Softmax with optimized parameter m = 0.35 is also plotted and we can observe that it is similar with Angular Softmax with m = 4, ? = 5 in the range [0 ? , 90 ? ], in which most of the real-world ?s lie. and intuitive. During implementation, the input after normalizing both the feature and the weight is actually x = cos? yi = W T y i fi Wy i fi , so in the forward propagation we only need to compute</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Conventional Softmax's decision boundary and Additive Margin Softmax's decision boundary. For conventional softmax, the decision boundary is at P0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Feature distribution visualization of several loss functions. Each point on the sphere represent one normalized feature. Different colors denote different classes. For SphereFace<ref type="bibr" target="#b8">[9]</ref>, we have already tried to use the best hyper-parameters we could find.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The feature gradient norm w.r.t. the feature norm for softmax loss with and without feature normalization. The gradients are calculated using the weights from a converged network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Left: CMC curves of different loss functions with 1M distractors on MegaFace[5] Set 1. Right: ROC curves of different loss functions with 1M distractors on MegaFace[5] Set 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>.2). To be rigorous, all the experiments in this paper are based on the cleaned dataset. We have made our overlap checking code publicly available 2 to encourage researchers to clean 2 https://github.com/happynear/FaceDatasets their training datasets before experiments.</figDesc><table><row><cell>Loss</cell><cell>Overlap</cell><cell cols="2">MegaFace MegaFace</cell></row><row><cell>Function</cell><cell>Removal?</cell><cell>Rank1</cell><cell>VR</cell></row><row><cell>AM-Softmax</cell><cell>No</cell><cell>75.23%</cell><cell>87.06%</cell></row><row><cell>AM-Softmax</cell><cell>Yes</cell><cell>72.47%</cell><cell>84.44%</cell></row></table><note>Table 2. Effect of Overlap Removal on modified ResNet-20</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Softmargin softmax for deep classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A benchmark study of large-scale unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep hyperspherical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained softmax loss for discriminative face verification</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10284</idno>
		<title level="m">Feature incay for representation regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
