<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MlTr: Multi-label Classification with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
							<email>chengxing03@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
							<email>linhezheng@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
							<email>wuxiangyu@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>yangfan@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
							<email>shendong@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<email>wangzhongyuan@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Shi</surname></persName>
							<email>shinian@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Liu</surname></persName>
							<email>liuhonglin@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MlTr: Multi-label Classification with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of multi-label image classification is to recognize all the object labels presented in an image. Though advancing for years, small objects, similar objects and objects with high conditional probability are still the main bottlenecks of previous convolutional neural network(CNN) based models, limited by convolutional kernels' representational capacity. Recent vision transformer networks utilize the self-attention mechanism to extract the feature of pixel granularity, which expresses richer local semantic information, while is insufficient for mining global spatial dependence. In this paper, we point out the three crucial problems that CNN-based methods encounter and explore the possibility of conducting specific transformer modules to settle them. We put forward a Multi-label Transformer architecture(MlTr) constructed with windows partitioning, in-window pixel attention, cross-window attention, particularly improving the performance of multilabel image classification tasks. The proposed MlTr shows state-of-the-art results on various prevalent multi-label datasets such as MS-COCO, Pascal-VOC, NUS-WIDE with 88.5%, 95.8%, 65.5% respectively. The code will be available soon at https://github.com/starmemda/MlTr/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In general, multi-label image classification requires us to identify all the entities in one image and then label them accordingly. Compared to single-label image classification, such tasks have a wider range of applications in autonomous driving <ref type="bibr" target="#b0">[1]</ref>, medical diagnosis recognition <ref type="bibr" target="#b1">[2]</ref>, and industriallevel image content understanding. At present, mainstream multi-label classification schemes still take convolutional neural networks(CNN) as their primary frameworks. We analyzed several samples on MS-COCO <ref type="bibr" target="#b2">[3]</ref> where the resnet101 <ref type="bibr" target="#b3">[4]</ref> model performed poorly and grouped them into three:</p><p>? Small objects. There are fewer pixels representing small objects which will disappear in the process of downsampling the feature map. ? Different objects with high similarity. Similar categories have similar characteristics and are easily confused by model, e.g. backpack and handbag, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>(b) ? Different objects that co-occur frequently. For instance, the condition that buses and cars often appear simultaneously, leading to inaccurate inner connections between their labels and features. It may misguide the model to distinguish a single bus as a car and a bus.</p><p>Since convolution is only computed in one local area at a time with the property of inductive bias, the local information is easily ignored after adequate times of convolution, resulting in CNN-based models' dissatisfying performances in multi-label classification.  Transformer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, first created to excavate long-range dependencies among word embeddings in the field of Natural Language Processing(NLP), has also recently been found to be capable in various computer vision tasks. ViT <ref type="bibr" target="#b6">[7]</ref> makes it possible for the transformer to replace convolutional neural networks' long-term domination in the field of computer vision. Unlike convolution's inherent inductive bias <ref type="bibr" target="#b7">[8]</ref>, the transformer requires great amounts of data to learn long-range dependencies. Deit <ref type="bibr" target="#b8">[9]</ref>, swin transformer <ref type="bibr" target="#b9">[10]</ref> and Pit <ref type="bibr" target="#b10">[11]</ref> introduce knowledge distillation, windows partitioning, and pooling operations to alleviate massive data requirement effectively. However, they share the basic pattern to partition the image into separate patches, project the patches into pixels and then conduct self-attention among pixels. This pattern limits the transformer's further application in multi-label image classification, where inferring small parts from the perspective of global spatial location relationships is of great importance. In our work, we combine the pixel attention and the attention among windows to better excavate the transformer's activity in multi-label image classification. This new pattern of attention is what we call cross-window attention. In detail, We first project the input image into pixels within the feature map, and divide the pixels into window partitions, then alternate between pixel attention and cross window attention. With such a method, small objects can be detected by pixel attention and cross-window attention is capable of gathering overall information to make local conjecture more convincing. The object-aware token is added to our model's last layer to determine the number of labels, corresponding to the third problem. The proposed Multi-label Transformer architecture is named MlTr in short, and it achieves state-of-the-art performance on MS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively. Moreover, we show the visualization of the heatmaps in <ref type="figure">Fig.6</ref>, to elaborate that the proposed MlTr can indeed help each feature to capture implicit but crucial information globally. In summary, the contributions in this paper can be concluded as:</p><p>? Clearly pointing out the three crucial problems existing in the present multi-label image classification task.</p><p>? Firstly proposing the transformer based network designed for multi-label classification, which surpasses previous SOTA methods on general benchmarks.</p><p>? Offering a novel attention pattern of the transformer through putting forward cross-window attention.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN Based Networks</head><p>In the past many years, CNN models have brought the field of computer vision enormous progress. As general improvements of CNN were introduced to multi-label image classification tasks, there are some particular schemes being proposed according to their characteristics, mainly including Graph Convolution Networks(GCN) and weak supervision methods. GCN. In multi-label classification tasks, labels may share certain relationships as they normally co-occur in an image. A graph based on the conditional probability relation can be constructed as a prior to model the label dependencies. The papers based on GCN <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> utilize feature descriptions and correlation matrix with a graph convolutional neural network, and train a classifier to help the model infer. Although these series of methods can indeed bring some improvements, they are of little practical use. Mainly because they are not end-to-end, the graph relation among labels has to be extracted over training data, and the semantic priors have to be calculated by another NLP network in advance. Besides, the parameters of graph relation are the square of the number of categories, which is unaffordable when the number of categories is tremendous. But our proposed MlTr is endto-end and can be extended for a large number of categories. Weak supervision methods. In the case of a large number of categories, even for a professional data labeling team, it is inevitable to miss some labels for few samples. Weak supervision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> mainly solves the problem of incomplete labeling that may exist in every sample, and it can be applied to multi-label classification. Their principal means are to predict partial labels or calculate partial labels' loss during forward or backward propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer Networks</head><p>Since the paper <ref type="bibr" target="#b6">[7]</ref>, the transformer has been utilized in many tasks of computer vision. Recent TNT, PiT and CvT <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> have reached good performance in image classification. Swin Transformer <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> even reached state-of-the-art in both object detection and image segmentation. As for video understanding, the transformer's self-attention mechanism is fully utilized to achieve unprecedented performance <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. But primarily all transformer based networks notice only pixels' attention. For example, ViT directly conducts the transformer among global pixels, while TNT introduces local attention by alternating patch-level attention and pixel-level attention.</p><p>Though swin transformer elaborated on increasing pixels' receptive field from local to global, they couldn't cast off the pixel based limitation. How to mine spatial dependence relations in a feature map directly and combine the pixel information, it's what the proposed MlTr in this work does. And we find it particularly suited to solving the three issues raised in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Customized Loss Function</head><p>In multi-label classification, the commonly used loss functions are bce loss and multi-label soft margin loss <ref type="bibr" target="#b26">[27]</ref> in the early stage, considering the problem of positive-negative imbalance <ref type="bibr" target="#b27">[28]</ref>, asymmetric loss <ref type="bibr" target="#b28">[29]</ref> was proposed recently. It combines the best of label smoothing <ref type="bibr" target="#b29">[30]</ref> and focal loss <ref type="bibr" target="#b27">[28]</ref> that prevent overfitting and positive-negative imbalance by exploring an appropriate method to add margin in focal loss. It's so far the best loss function in multi-label classification, thus the loss optimization in this paper is based on asymmetric loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method: ML-Tr</head><p>In this section, we illustrate the architecture of the proposed transformer based multi-label classifier and its composition of each submodule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MlTr</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Overall Architecture</head><p>We design our model on scales: 224, 384, and 384, correspondingly named MlTr-s, MlTr-m, and MlTr-l with customized parameters respectively. <ref type="figure" target="#fig_1">Fig.2</ref> shows our overall architecture. Absorbing the ideas of ResNet <ref type="bibr" target="#b3">[4]</ref> and Swin transformer <ref type="bibr" target="#b9">[10]</ref>, we construct the network as a bottom-up model, which gradually increases the receptive field of each pixel from local to global. Firstly, the input 2D image matrix is expanded into C feature maps of size H P ? W P by convolution projection, where P denotes the patch size as paper <ref type="bibr" target="#b4">[5]</ref>. Then, each pixel of the feature map is regarded as a token with dimension C and is input into multiple layers comprised of different blocks successively. Four layers of different sizes are designed, and between every two layers, space to depth module <ref type="bibr" target="#b10">[11]</ref> is applied to reduce the feature size to <ref type="bibr">1 4</ref> and double the number of channels. In this way, we can gradually obtain more comprehensive information of the whole image through stacking the transformer module as typical convolution based networks do. Significantly, the objectaware token is stacked to the last layer to mine the number of distinct classes ever presented in  one image. Eventually, the pixel tokens after average pooling <ref type="bibr" target="#b30">[31]</ref> are followed by multi-layer perceptron(MLP) to calculate optimized asymmetric loss <ref type="bibr" target="#b28">[29]</ref>, object-aware token is followed by MLP directly to calculate cross entropy loss <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. In this section, we illustrate the pixel attention and the cross-window attention that constitute the MlTr block together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Attention Block</head><p>Pixel Attention. As <ref type="figure">Fig.3(a)</ref> shows, all pixels from the same position in the feature maps form a token and we set a certain window size to divide the feature map into different windows.</p><p>Then self-attention <ref type="bibr" target="#b4">[5]</ref> operation only works within windows. After utilizing the residual connection and multilayer perceptron, the calculation process can be expressed as:</p><formula xml:id="formula_0">x temp = P A(LN (x l?1 )) + x l?1 ,<label>(1)</label></formula><formula xml:id="formula_1">x l = M LP (LN (x temp )) + x temp<label>(2)</label></formula><p>Where x l?1 and x l denote the input and output of l-th attention module.LN, P A, and M LP represent the layer normalization, pixel attention, and multilayer perceptron. Shift window operations are also necessary to expand pixels' receptive field <ref type="bibr" target="#b9">[10]</ref>.</p><p>Why Pixel Attention? By partitioning windows, an artificial inductive bias was appended to the transformer. Based on the assumption that the most critical information for each object is primarily presented in and around the object, such inductive bias enables the network to converge efficiently on small datasets. And according to Section 3.1.3, the computational cost of pixel attention is far less than that of global attention adopted by ViT, TNT, and so on, which allows us to perform patch projecting in smaller patch size. Smaller patch size means denser image partitioning, retaining more local information, and achieving higher accuracy.</p><p>Cross-window Attention. Referred to <ref type="figure">Fig.3(b)</ref>. It turns different windows on one channel unfold into one-dimensional tokens whose length is w 2 s where w s denotes the window size. Then, the subsequent self-attention operation was only conducted among the tokens derived from one feature map. Likewise:</p><formula xml:id="formula_2">x temp = CW A(LN (x l?1 )) + x l?1 ,<label>(3)</label></formula><formula xml:id="formula_3">x l = M LP (LN (x temp )) + x temp<label>(4)</label></formula><p>Where CW A means cross-windows attention.</p><p>Why Cross-Window Attention? In <ref type="figure" target="#fig_3">Fig.4</ref>, with local pixel attention, backpack and skis are easily located. But backpack and handbag, skis and skateboard are so similar that the prediction score for them will aggregate around the threshold. Cross-window attention can exploit spatial dependence which may be location relations, scenes, and styles. Under the circumstances in <ref type="figure" target="#fig_3">Fig.4</ref>, the backpacks are on the back of persons and the overall ground environment is snow, which will assist backpack and skis be inferred precisely. MlTr first makes a preliminary local judgment and then coordinates the global information, so as to obtain the prediction results with high confidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Computational Cost Analysis</head><p>Supposing the feature map of C ? h ? w was partitioned by a window size of w s . If we take the global attention strategy <ref type="bibr" target="#b4">[5]</ref>, the computational cost is <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>:</p><formula xml:id="formula_4">F LOP GA = 4hwC 2 + 2(hw) 2 C (5) P aram GA ? C 2<label>(6)</label></formula><p>Where GA denotes global attention. As for pixel attention and cross-window attention:</p><formula xml:id="formula_5">F LOP P A = 4hwC 2 + 2w 2 s hwC (7) P aram P A ? C 2 (8) F LOP CW A = 4hww 2 s C + 2( hw ws ) 2 C (9) P aram CW A ? w 4 s<label>(10)</label></formula><p>For computational complexity, the first two are both quadratic to channel number C, while the proposed spatial attention is linear. That's why pure depth attention networks can't afford deeper channels that may represent more features. But with cross-window attention, we are capable of alleviating this problem to some extent. Otherwise, the parameter amount of pixel attention is irrelevant with channels C. Considering C &gt;&gt; w 2 s , adopting cross-window attention implies a significant reduction in the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Architecture Variants</head><p>We set up three MlTr models with different parameter amounts, computational complexity, and input resolutions to meet the needs of different application scenarios. Relevant variants are presented in <ref type="table" target="#tab_2">Table 1</ref>. P , C, and w s denote patch size, channels, and window size as <ref type="figure" target="#fig_1">Fig.2</ref> shows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Optimizations</head><p>In this section, two optimizations towards loss are illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Arc Sigmoid</head><p>The most widely used multi-label classification loss function, bce loss, is presented as follows:</p><formula xml:id="formula_6">p ij = 1 1+e x i W T j +b j<label>(11)</label></formula><formula xml:id="formula_7">BCE i = ? n j y ij log p ij ? n j (1 ? y ij ) log(1 ? p ij )<label>(12)</label></formula><p>where x ? R N ?d denotes the features extracted by the model in N samples and d means the feature dimension which is often set to 2048 in the paper following <ref type="bibr" target="#b3">[4]</ref>, n is the total number of distinct categories. i means i-th sample and j means j-th class. So W j ? R d and b j ? R d denotes the j-th column of the weight of W ? R d?n and bias item in the final fully connected layer. True labels we wish the model to learn is y, p ij is the confidence calculated by the sigmoid function that the i-th sample belongs to j-th category. We can infer that W j is the most typical feature of the j-th category, which is represented in the form of a vector in the d-dimensional hyperplane space. When the product of feature x i and W j learned by the model from an image reaches the maximum, we label the i-th sample as the j-th class. Referring to the paper <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>, it is more reasonable to describe the similarity among vectors by the angle on the hyperplane, so we put forward the arc sigmoid function which expresses as below:</p><formula xml:id="formula_8">p ij = 1 1 + e s cos ?ij<label>(13)</label></formula><p>where ? ij = arccos xiW T j ||xi||||W T j || exactly means the angle between vector x i and W T i , s is a hyperparameter used to keep the predicted value within [?s, s]. To alleviate the impact caused by samples with missing labels and positive-negative imbalance, we adopt asymmetric loss <ref type="bibr" target="#b28">[29]</ref> to replace bce loss:</p><formula xml:id="formula_9">ASL i = ? n j y ij L + ? n j (1 ? y ij )L ?<label>(14)</label></formula><p>L + and L ? are the positive and negative loss parts that can be calculated as:</p><formula xml:id="formula_10">L + = (1 ? p ij ) ?+ log(p ij ) (15) L ? = (p ij ? m) ?? log(1 ? p ij + m)<label>(16)</label></formula><p>where ? + , ? ? and m are the hyperparameters explored in <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Object-aware Token Loss (OTL)</head><p>Different from the labels' semantic priors extracted from the word2vec network in many papers <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, reinforced supervision can be summarized from the given labels, that is, the number of objects' distinct classes in an image. There are two ways to understand why such supervision works better, which requires no additional knowledge. On the one hand, it forces the model to extract the common features of the same class targets and enhances the generalization performance; on the other hand, it can make the model build a more robust corresponding projection between features and labels, especially for objects with high conditional probability. With the flexibility of transformer tokens, an object-aware token is stacked to MlTr's last layer to calculate the loss produced by the model's category numbers prediction. It can be derived from:</p><formula xml:id="formula_11">OT L i = ? log s z<label>(17)</label></formula><p>where z = n j=0 y ij represents the category numbers, and s is the softmax output of the fully connected layer following object-aware token. In this way, the problem of quantity prediction is transformed into a classification task, s z means the predicted confidence that there are z kinds of the object present in the image. Eventually, the loss of i-th sample consists of two parts:</p><formula xml:id="formula_12">L i = ASL i + OT L i<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we employ the proposed MlTr method to enforce plentiful experiments on several datasets universally token for multi-label classification. Then compare the results with the previous architectures and prove the effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets, Metrics, and Implementation Details</head><p>Datasets. We pretrain MlTr on ImageNet-22K <ref type="bibr" target="#b36">[37]</ref> which contains 14.2 million images from 22K classes and then fine-tune on multi-label classification benchmarks such as MS-COCO <ref type="bibr" target="#b2">[3]</ref>, Pascal-VOC <ref type="bibr" target="#b37">[38]</ref>, and NUS-WIDE <ref type="bibr" target="#b38">[39]</ref>. They consist of 122218 images, 9963 images, and 220000 images from 80, 20, and 81 classes respectively.</p><p>Metrics. Under the premise that label the sample with the class if its predicted score in one class is greater than a threshold (e.g. 0.5). The metric we're most interested in is mean average precision (mAP) <ref type="bibr" target="#b11">[12]</ref>, some additional indicator reported on previous work such as average per-class precision (CP), recall (CR), F1 (CF1), and average overall precision (OP), recall (OR), F1(OF1) are requisite, too.</p><p>Implementation Details. With mainstream fine-tune strategies like Adam <ref type="bibr" target="#b39">[40]</ref>, asymmetric loss <ref type="bibr" target="#b28">[29]</ref>, one-circle <ref type="bibr" target="#b40">[41]</ref> learning rate scheduler as tresnet <ref type="bibr" target="#b41">[42]</ref>, we carry out our experiment. s, m, ? + , and ? ? in Section 3.2 is set to be 8, 0.05, 0, and 4. Every image is transformed by cutout <ref type="bibr" target="#b42">[43]</ref>, randaugment when training, and the initial learning rate is 1e-4. The result will be shown after 200 epochs. More detailed configurations can be seen in our open source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Benchmark and Comparisons with State-of-the-Arts</head><p>MS-COCO. The results on the MS-COCO dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>. As we can see, our smallest design is 3.8% higher than resnet101 <ref type="bibr" target="#b3">[4]</ref> when they are both pretrained on imagenet1k with 224 resolution input. The medium-computational MlTr-m version with 384 resolution can achieve the same performance as the previous best method TresNet-l, which even needs a more intricate scale with 448 input size. When it comes to more complicated architecture like MlTr-l, the mAP can reach 88.5, it is 1.9% higher than the previous top one. Overall, the transformer based network we design can deliver more accurate results at a lower resolution input, and the large one MlTr-l demonstrates the ability to challenge higher precision.</p><p>Pascal-VOC. To be fair, we only take the models pretrained on ImageNet for comparison. As <ref type="table">Table  3</ref> shows, MlTr-l reaches a new state-of-the-art, it demonstrates that although the Pascal-VOC dataset is relatively small, it doesn't prevent MlTr from performing better. This also validates MLTr's strong capacity for transfer learning.  <ref type="table">Table 3</ref>: Comparison of mAP with previous excellent methods on Pascal-VOC dataset. mAP Atten-Reinforce <ref type="bibr" target="#b44">[45]</ref> 92.0 SSGRL <ref type="bibr" target="#b13">[14]</ref> 93.4 ML-GCN <ref type="bibr" target="#b11">[12]</ref> 94.0 Tresnet-l <ref type="bibr" target="#b28">[29]</ref> 94.6 MlTr-l 95.8  <ref type="table" target="#tab_4">Table 4</ref> shows, CF1 and OF1 have also been promoted, all of these indicate MlTr's better generalization performance. Main methods. To prove the efficiency of the refinements put forward in our work, we remove them in order and test the mAP on MS-COCO's validation set. The relevant result is shown in <ref type="table" target="#tab_5">Table 5</ref>, position embedding, shift window, crosswindow attention, arc sigmoid, and object-aware token produce an increase of 0.4, 0.9, 1.0, 0.2, and 0.2 separately. As expected, cross-window attention enables the model exploit the spatial dependence and significantly improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Arc sigmoid. Referred to Eq.(13), the proposed arc sigmoid connects the predicted score and the similarity described by cosine value between embeddings. Usually cosine? ? [?1, 1] <ref type="bibr" target="#b33">[34]</ref>, which indicates the result concentrates on 0.5 if feeding the cosine? into sigmoid function directly. In <ref type="table" target="#tab_6">Table 6</ref>, we assign scaling factors s as 4, 8, 16, 32, and it turns out that it performs best when s is equal to 8. Small Objects. According to the bounding box provided by MS-COCO and its criterion for small, medium, and large objects, we divide all labels of images in the validation dataset into these three and calculate their mAP as AP s , AP m , and AP l . In the case that two entities of different size is the same category, we boil the label down to the larger one. The results of ResNet101 and MlTr-s are compared in <ref type="table" target="#tab_7">Table 7</ref>. As it shows, the average precision for small objects(AP s ) increases by nearly 10 points, which is more obvious than AP m and AP l . Similar Objects And Objects with High Conditional Probability. The effectiveness of our approach towards similar objects and objects with high probability is visualized in this section. In <ref type="figure">Fig.5</ref>, MlTr-l and ResNet101 with the accuracy described in the last part are compared by t-SNE <ref type="bibr" target="#b46">[47]</ref>.</p><p>In the first two examples, MlTr performs significantly better in distinguishing similar couples such as skis and snowboard, backpack and clothing, remote and cell phone. For the last two rows display, due to the high probability of tennis rackets and sports balls, cars, and buses appearing in the same image, ResNet will identify any suspicious dot on the tennis court as a sports ball, and label the image of a bus with the car label. This is because the local feature extracted by ResNet isn't convincing enough, it learns an implicit graph relation to converge, which is the local optimum. With the object-aware token, the proposed MlTr is forced to wipe off the graph relation and maintain a more meaningful semantic topology.  <ref type="figure">Figure 5</ref>: Three picked out images from Top-5 returned ones with the query images. The leftmost column is the query images, our proposed MlTr-l returns with the left three, while the right three are based on ResNet101 baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we firstly analyse the three bottlenecks in multi-label image classification and then propose a novel transformer based architecture majoring in them. By effectively combining the pixel attention and cross-window attention, it makes a preliminary judgment of the target locally and then matches the relevant features globally. Besides, two loss optimizations have also been implemented to correct the feature distribution. The proposed MlTr is the first transformer framework to solve the problem of multi-label classification and reaches the state-of-the-art in various multi-label benchmarks, it further suggests the broad prospects of transformer thriving in the field of computer vision.</p><p>(a) The visualization of heatmap. The first column: original image, the second to the fourth ones: heatmap reflecting the attention to different objects.</p><p>(b) Two examples of spatial attention when recognizing the tennis rocket or sports ball. The first column: original image, the second column: heatmaps calculated from a same channel output, the third one: heatmaps taking the second column pictures' greatest concern regions as highlight. <ref type="figure">Figure 6</ref>: The comparison of heatmaps.</p><p>to kick the ball. To prove such ingenious spatial attention is no coincidence, we select another tennis playing image and feed it into the network with the same parameters, and draw the feature map from the same channel output. It turns out that the heatmap also focuses on the right hand, the thigh and the tennis racket.</p><p>The reason for such a phenomenon may be that, when the model detection preliminarily determines that the small target may be a tennis ball, the entity, scene or action related to tennis ball will be searched by cross-window attention in the global scope. If the relevant pixels are found, it can be more certain that the small target is a tennis ball, which is similar to the reasoning of human sometimes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Three bad cases token from the ResNet101 baseline results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of Multilabel Transformer(MlTr-s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Illustration of pixel attention (b) Illustration of cross-window attention Figure 3: A diagram of the MlTr block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The illustration of spatial dependence focused by cross-window attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>True label: person, kite, sports ball, umbrella Prediction: person, kite Missing prediction: sports ball, umbrella (a) Challenge of small objects</figDesc><table><row><cell>True label: person, suitcase, backpack</cell><cell>True label: bus</cell></row><row><cell>Prediction: person, suitcase, handbag</cell><cell>Prediction: bus, car, person</cell></row><row><cell>False prediction: handbag</cell><cell>Extra prediction: car, person</cell></row><row><cell>(b) Challenge of similar objects</cell><cell>(c) Confusion caused by objects</cell></row><row><cell></cell><cell>occurring together frequently</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Architecture Variants of MlTr.</figDesc><table><row><cell></cell><cell cols="2">input resolution P</cell><cell>C</cell><cell cols="2">w s block numbers in four layer</cell></row><row><cell>MlTr-s</cell><cell>224</cell><cell>4</cell><cell>96</cell><cell>7</cell><cell>{1,1,3,1}</cell></row><row><cell>MlTr-m</cell><cell>384</cell><cell>4</cell><cell>96</cell><cell>12</cell><cell>{1,1,9,1}</cell></row><row><cell>MlTr-l</cell><cell>384</cell><cell cols="3">4 128 12</cell><cell>{1,1,9,1}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of various metrics with previous state-of-the-art on MS-COCO. The default unit is %.The proposed MlTr-s, MlTr-m and MlTr-l use resolutions of 224,384,384 respectively. 1k and 22k denote the model is pretrained on imagenet1k and imagenet22k.</figDesc><table><row><cell></cell><cell>Resolution mAP</cell><cell>CP</cell><cell>CR CF1 OP</cell><cell>OR OF1 Params(M)</cell></row><row><cell>ResNet-101 [4]</cell><cell cols="4">224 ? 224 78.3 80.2 66.7 72.8 83.9 70.8 76.8</cell><cell>45</cell></row><row><cell>ML-GCN [12]</cell><cell cols="4">448 ? 448 83.0 85.1 72.0 78.0 85.8 75.4 80.3</cell><cell>46</cell></row><row><cell>SSGRL [14]</cell><cell cols="4">448 ? 448 83.8 89.9 68.5 76.8 91.3 70.8 79.9</cell><cell>-</cell></row><row><cell>KGGR [44]</cell><cell cols="4">448 ? 448 84.3 85.6 72.7 78.6 87.1 75.6 80.9</cell><cell>-</cell></row><row><cell>C-Tran [18]</cell><cell cols="4">448 ? 448 85.1 86.3 74.3 79.9 87.7 76.5 81.7</cell><cell>-</cell></row><row><cell>Tresnet-l [29]</cell><cell cols="4">448 ? 448 86.6 87.2 76.4 81.4 88.2 79.2 81.8</cell><cell>55</cell></row><row><cell>MlTr-s (1k)</cell><cell cols="4">224 ? 224 81.9 80.7 71.5 75.2 81.4 76.3 78.1</cell><cell>33</cell></row><row><cell>MlTr-s (22k)</cell><cell cols="4">224 ? 224 83.9 82.8 75.5 77.3 83.0 78.5 79.9</cell><cell>33</cell></row><row><cell>MlTr-m (22k)</cell><cell cols="4">384 ? 384 86.8 84.0 80.1 81.7 84.6 82.5 83.5</cell><cell>62</cell></row><row><cell>MlTr-l (22k)</cell><cell cols="4">384 ? 384 88.5 86.0 81.4 83.3 86.5 83.4 84.9</cell><cell>108</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Comparison of mAP, CF1, and OF1</cell></row><row><cell cols="2">with previous method on NUS-WIDE dataset.</cell></row><row><cell></cell><cell>mAP CF1 OF1</cell></row><row><cell cols="2">MS-CMA [13] 61.4 60.5 73.8</cell></row><row><cell>SRN [46]</cell><cell>62.0 58.5 73.4</cell></row><row><cell>ICME [12]</cell><cell>62.8 60.7 74.1</cell></row><row><cell>Tresnet-l [29]</cell><cell>65.2 63.6 75.0</cell></row><row><cell>MlTr-l</cell><cell>66.3 65.0 75.8</cell></row></table><note>NUS-WIDE. With the proposed MlTr-l, mAP reaches 66.3% on NUS-WIDE as</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Ablation study of MlTr-s</cell></row><row><cell>on the benchmark MS-COCO.</cell><cell></cell></row><row><cell></cell><cell>mAP</cell></row><row><cell>fixed window</cell><cell>79.2</cell></row><row><cell cols="2">+position embedding [10] 79.6(+0.4)</cell></row><row><cell>+shift window [10]</cell><cell>80.5(+0.9)</cell></row><row><cell cols="2">+cross-window attention 81.5(+1.0)</cell></row><row><cell>+arc sigmoid</cell><cell>81.7(+0.2)</cell></row><row><cell>+object-aware token</cell><cell>81.9(+0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation tests with different s values in Eq.(13).</figDesc><table><row><cell>All the results are obtained by MlTr-s on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The comparisons in three different size objects. AP s AP m AP l mAP</figDesc><table><row><cell cols="2">ResNet101 [4] 68.8 83.8 92.6</cell><cell>82.2</cell></row><row><cell>MlTr-m</cell><cell>77.6 90.2 95.9</cell><cell>88.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Visualization of Heatmaps</head><p>In this section, we visualize the heatmap <ref type="bibr" target="#b47">[48]</ref> reflecting the focus of each channel derived from the last layer's feature map. The specific approach is to upsample(bilinear) each feature map to the original input size. After normalization, draw the heatmap and the initial image in a same picture. For the tennis playing picture as <ref type="figure">Fig.6(a)</ref> shows, feature maps from different channels recognize clues of different objects.</p><p>Interestingly, we found that some feature maps employ rich spatial information when identifying tennis balls. Take <ref type="figure">Fig.6(b)</ref> as an example, with our cross-window attention, when discerning the sports ball, our model will simultaneously focus on the person's arm, hand and leg, who is preparing</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards fully autonomous driving: Systems and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Askeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Kammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaughan</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="163" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rahil Garnavi, and Rajib Chakravorty. Chest x-rays classification: A multi-label and fine-grained problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Sedai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilabel image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2801" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Order-free rnn with visual attention for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Shang-Fu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-driven dynamic graph convolutional network for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14027</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coco attributes: Attributes for people, animals, and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<title level="m">When does label smoothing help</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nuswide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emanuel Ben Baruch, Gilad Sharir, and Itamar Friedman. Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Knowledge-guided multi-label few-shot learning for general image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent attentional reinforcement learning for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
