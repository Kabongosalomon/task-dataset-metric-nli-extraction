<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Does Transliteration Help Multilingual Language Modeling?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><forename type="middle">Muhammad</forename><surname>Moosa</surname></persName>
							<email>ibraheemmoosa1347@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Independent Researcher</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmud</forename><forename type="middle">Elahi</forename><surname>Akhter</surname></persName>
							<email>mahmud.akhter01@northsouth.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">North South University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashfia</forename><forename type="middle">Binte</forename><surname>Habib</surname></persName>
							<email>ashfia.habib@northsouth.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">North South University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Does Transliteration Help Multilingual Language Modeling?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As there is a scarcity of large representative corpora for most languages, it is important for Multilingual Language Models (MLLM) to extract the most out of existing corpora. In this regard, script diversity presents a challenge to MLLMs by reducing lexical overlap among closely related languages. Therefore, transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs. In this paper, we pretrain two AL-BERT models to empirically measure the effect of transliteration on MLLMs. We specifically focus on the Indo-Aryan language family, which has the highest script diversity in the world. Afterward, we evaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test to rigorously verify whether the effect of transliteration is significant or not. We find that transliteration benefits the low-resource languages without negatively affecting the comparatively high-resource languages. We also measure the cross-lingual representation similarity (CLRS) of the models using centered kernel alignment (CKA) on parallel sentences of eight languages from the FLORES-101 dataset. We find that the hidden representations of the transliteration-based model have higher and more stable CLRS scores. Our code is available at Github 1 Hugging Face Hub 2 , 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last few years, we have seen impressive advances in many NLP tasks. These advances have been primarily led by the availability of large representative corpora and improvement in the architecture of large language models. While improving model architectures, training methods, regularization techniques, etc., can help advance the state of NLP in general, the unavailability of large, diverse corpora is the bottleneck for most languages <ref type="bibr">(Joshi et al., 2020)</ref>. Here we focus on the issue of diverse writing scripts used by closely related languages that may prevent MLLMs from learning good cross-lingual representations. For example, the Indo-Aryan family of languages uses at least six different scripts among them. Previous papers  have noted that lowresource languages that use unique scripts tend to have very few tokens representing them at the tokenizer. As a result, these languages tend to have more UNKnown tokens, and the words in these languages tend to be more split up by sub-word tokenizers. Often we can easily transliterate from one script to another using rule-based systems. For example, there are established standards that can be used to transliterate Greek (ISO 843), Cyrillic (ISO 9), Indic scripts (ISO 15919), and Thai (ISO 11940) to the Latin script. In this paper, We focus on the Indo-Aryan language family and empirically measure the effect of transliteration on the downstream performance of MLLMs. The linguistic relatedness in the Indo-Aryan language family is discussed in appendix A.</p><p>First, we pretrain two <ref type="bibr">ALBERT (Lan et al., 2020)</ref> models from scratch on the Indo-Aryan languages of the OSCAR corpus <ref type="bibr" target="#b11">(Ortiz Su'arez et al., 2019)</ref>. One of the models is pretrained with original scripts and the other after transliterating to a common script. Then, we evaluate the models on four diverse downstream tasks from the IndicGLUE benchmark dataset. We use nine random seeds on all downstream finetuning tasks. In order to rigorously compare the two models, we perform the <ref type="bibr">Mann-Whitney U test (MWU)</ref> between the uniscript model (group 1) and the multi-script model (group 2). MWU test lets us verify whether the performance differences of the two models are significant or not. Apart from statistical significance, we also report three different effect sizes to conclusively determine whether the magnitude of improvement due to transliteration is useful in practice. Using the MWU test, we conclude that transliteration significantly benefits the low-resource languages without negatively affecting the comparatively high-resource languages.</p><p>We also measure the tokenizer quality and the cross-lingual representation similarity (CLRS) to understand why the uni-script model performs better than the multi-script model. To measure tokenization quality, we use subword fertility and unbroken ratio <ref type="bibr" target="#b0">(?cs, 2019;</ref><ref type="bibr" target="#b20">Rust et al., 2021)</ref>. To measure the CLRS, we use the centered kernel alignment (CKA) (Kornblith et al., 2019) similarity score. We measure the CKA similarity score between the hidden representations of the models on the parallel sentences of eight Indo-Aryan languages from the FLORES-101 dataset <ref type="bibr">(Goyal et al., 2021)</ref>. We find that, compared to the multi-script model, the uni-script model achieves a higher CKA score and it is more stable throughout the hidden layers of the uni-script model. Based on this, we conclude that the uni-script model learns better cross-lingual representation than the multi-script model. In summary, our contributions are primarily three-fold:</p><p>1. We find that transliteration significantly benefits the low-resource languages without negatively affecting the comparatively highresource languages.</p><p>2. We establish this finding through rigorous experiments and show the statistical significance along with the effect size of transliteration using the Mann-Whitney U test.</p><p>3. Using CKA on the FLORES-101 dataset, we show that transliteration helps MLLMs learn better cross-lingual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation and Background</head><p>In their study, <ref type="bibr">Joshi et al. (2020)</ref> showed the resource disparity between low-resource and highresource languages, and <ref type="bibr" target="#b19">(Ruder, 2020)</ref> discussed the necessity of working with low-resource languages. A large body of work suggests that language-relatedness can help MLLMs achieve better performance on low-resource languages by leveraging related high-resource languages. For instance, <ref type="bibr" target="#b14">(Pires et al., 2019)</ref> found that lexical overlap improved mBERT's multilingual representation capability even though it learned to capture multilingual representations with zero lexical overlaps. <ref type="bibr">(Dabre et al., 2017)</ref> showed that transfer learning in the same or linguistically similar language family gives the best performance for NMT. <ref type="bibr">(Lauscher et al., 2020)</ref> found that language relatedness is crucial for POS-tagging and dependency parsing tasks. Although, corpus size is much more important for tasks like NLI and Question Answering. <ref type="bibr" target="#b29">(Wu and Dredze, 2020)</ref> demonstrated bilingual BERT can outperform monolingual BERT on low-resource languages when the bilingual languages are linguistically closely related. Nevertheless, mBERT outperforms bilingual BERT when it comes to lowresource languages.</p><p>However, one of the major challenges in leveraging transfer between high-resource and lowresource languages is overcoming the script barrier. Script barrier exists when multiple closely related languages use different scripts. <ref type="bibr">(Anastasopoulos and Neubig, 2019)</ref> found that for morphological inflection, script barrier between closely related languages impedes cross-lingual learning. Transliteration and phoneme-based techniques have been proposed to solve this issue. For example, <ref type="bibr">(Murikinati et al., 2020) expanded upon (Anastasopoulos and</ref><ref type="bibr" target="#b18">Neubig, 2019)</ref> and showed that both transliteration and grapheme to phoneme (g2p) conversion removes script barrier and improves crosslingual morphological inflection and <ref type="bibr" target="#b18">(Rijhwani et al., 2019)</ref> showed that pivoting low-resource languages to their closely related high-resource languages results in better zero shot entity linking capacity and used phoneme-based pivoting to overcome the script barrier. <ref type="bibr">(Bharadwaj et al., 2016)</ref> showed that phoneme representation outperformed orthographic representations for NER. <ref type="bibr">(Chaudhary et al., 2018)</ref> also used phoneme representation to resolve script barrier and adapt word embeddings to low-resource languages. <ref type="bibr">(Goyal et al., 2020)</ref> and <ref type="bibr" target="#b22">(Song et al., 2020)</ref> both utilized transliteration and showed that language relatedness was required for improving performance on NMT. (Amrhein and Sennrich, 2020) studied how transliter-ation improved NMT and came to the conclusion that transliteration offered significant improvement for low-resource languages with different scripts. <ref type="bibr">(Khemchandani et al., 2021)</ref> showed on Indo-Aryan languages that language relatedness could be exploited through transliteration along with bilingual lexicon-based pseudo-translation and aligned loss to incorporate low-resource languages into pretrained mBERT. <ref type="bibr" target="#b8">(Muller et al., 2021a)</ref> showed that for unseen languages, script barrier hindered transfer between low-resource and high-resource languages for MLLMs and transliteration removed this barrier. They showed that transliterating Uyghur, Buryat, Erzya, Sorani, Meadow Mari, and Mingrelian to Latin script and finetuning mBERT on the respective corpus with masked language modeling objective improved their downstream POS performance significantly. <ref type="bibr">(Dhamecha et al., 2021)</ref> showed that transliterating Indo-Aryan languages improved MLLM performance.</p><p>In contrast, <ref type="bibr">(K et al., 2020)</ref> and (Artetxe et al., 2020) seems to show that mBERT can learn crosslingual representations without any lexical overlap, a shared vocabulary, or joint training. However, these works focus on zero shot cross-lingual transfer learning. From the literature, it can be seen that apart from g2p, many in the community believe transliteration to be a potential solution for script barriers. However, most of the work shows the benefits of transliteration for NMT. Nevertheless, there is no solid empirical analysis of the effects of transliteration for MLLMs apart from <ref type="bibr">(Dhamecha et al., 2021;</ref><ref type="bibr" target="#b8">Muller et al., 2021a)</ref>, which are our contemporary studies. Hence, the motivation behind this paper is to provide a solid empirical analysis of the effect of transliteration for MLLMs with statistical analysis and determine if it helps models learn better cross-lingual representation.</p><p>Several techniques have recently been used to study the hidden representations of multilingual language models. (Kudugunta et al., 2019) study CLRS of NMT models using SVCCA <ref type="bibr" target="#b15">(Raghu et al., 2017)</ref>. <ref type="bibr" target="#b21">(Singh et al., 2019)</ref> used PWCCA <ref type="bibr" target="#b6">(Morcos et al., 2018)</ref> to study the CLRS of mBERT and found that it drastically falls with depth.  have used CKA to study the CLRS of bilingual BERT models. They found that similarity is highest in the first few layers and drops moderately with depth. <ref type="bibr">(Muller et al., 2021b)</ref> used CKA to study CLRS of mBERT before and after finetuning on downstream tasks. They found in all cases that CLRS increases steadily in the first five layers, then it decreases in the later layers. From this, they conclude that mBERT learns multilingual alignment in the early layers and preserves it throughout finetuning. A contemporary work <ref type="bibr">(Del and Fishel, 2021)</ref> has applied various similarity measures to understand CLRS of various multilingual masked language models. Their results also show that CLRS increases in the first half of the models, while in the later layers, this similarity steadily falls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The pretraining was done on a subset of the OS-CAR corpus. We use the unshuffled deduplicated version of OSCAR corpus available via Huggingface datasets library <ref type="bibr" target="#b2">(Lhoest et al., 2021)</ref>. We pretrain on Panjabi, Hindi, Bangla, Oriya, Assamese, Gujarati, Marathi, Sinhala, Nepali, Sanskrit, Goan Konkani, Maithili, Bihari, and Bishnupriya portion of the OSCAR corpus. The pretraining dataset details are given in appendix E. We evaluate the models on four downstream tasks from IndicGLUE, which are News Article Classification, WSTP, CSQA, and NER <ref type="bibr">(Kakwani et al., 2020)</ref>. In addition, we evaluate the models on other publicly available datasets. These are BBC Hindi News Classification, Soham Bangla News Classification, iNLTK Headlines Classification, IITP Movie, and Product Review Sentiment Analysis <ref type="bibr">(Akhtar et al., 2016)</ref>, and MIDAS Discourse Mode Classification(Dhanwal et al., 2020) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretraining</head><p>Corpus Preparation: Since the OSCAR corpus contains raw text from the Web, we apply a few filtering and normalization. First, we discard entries where the dominant script does not match the language tag provided by the OSCAR corpus. Then we use the IndicNLP normalizer(Kunchukuttan, 2020) to normalize the raw text. For the uniscript model, we then transliterate all the text to ISO-15919 format using the Aksharamukha(Rajan, 2015) library.</p><p>Tokenizer Training: Next, we train two Senten-cePiece tokenizers (Kudo and Richardson, 2018) on the transliterated and the non-transliterated corpus with a vocabulary size of 50,000.</p><p>ALBERT Model Training: We first pretrained an ALBERT base model from scratch on the nontransliterated corpus as our baseline. Afterward, we pretrained another ALBERT base from scratch on the transliterated corpus. We chose the base model due to computing constraints. We trained the models on a single TPUv3 VM. Both models were trained using the same hyperparameters. We followed the hyperparameters used in (Lan et al., 2020) except for batch size and learning rate. The pretraining objective is also the same as <ref type="bibr">(Lan et al., 2020)</ref>. The hyperparameter values and details are presented in the appendix C. Each model took about 7.5 days to train. We use the ALBERT implementation from the Huggingface Transformers Library <ref type="bibr" target="#b27">(Wolf et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistical Analysis</head><p>We perform Mann-Whitney U test (MWU) <ref type="bibr" target="#b5">(Mann and Whitney, 1947;</ref><ref type="bibr" target="#b26">Wilcoxon, 1945)</ref> to determine if the performance differences between the multiscript and the uni-script models are significant. In short, it tells us the effect of transliteration on model performance. MWU is a non-parametric hypothesis test between two groups/populations. MWU is chosen because it has weak assumptions. The only assumptions of MWU are that the samples of the two groups are independent of each other, and the samples are ordinal. Under the MWU, our null hypothesis or h 0 is that the performances of the uni-script (group 1) and the multi-script (group 2) models are equivalent, and the alternative hypothesis or h a is that the performances (groups) are different. We set our confidence interval ? at 0.05 and reject the h 0 for p-values &lt; ?. We also report three test statistics as p-value only gives statistical significance, which can be misleading at times <ref type="bibr" target="#b23">(Sullivan and Feinn, 2012)</ref>.</p><p>The test statistics are three different effect sizes that convey three different information. These test statistics are absolute effect size (?), common language effect size (?), and standardized effect size (r). The absolute effect size ? is the difference between the mean of the models' performance metric, which is given as, ? = ? uni-script -? multi-script for any given task and language. For any given task, if the h 0 is rejected, a positive ? indicates the uni-script is better, and a negative ? indicates the multi-script is better. ? gives us the probability of one group being better than the other group. That is the probability that a random performance sample of the the uni-script model is greater than a random performance sample of the multi-script model.</p><p>The last test statistic is r which indicates the magnitude of difference between the performance values of the uni-script model (group 1) and the multi-script model (group 2). r shows us how realistically significant the performance differences are between models even if the performance difference is statistically significant. It gives us a value between 0 to 1 and its ranges are: small effect ( 0 ? r ? 0.3) , medium effect ( 0.3 &lt; r ? 0.5) and large effect (0.5 &lt; r). We show the r only on the public datasets. We performed MWU on all downstream tasks except CSQA. On CSQA, we only report the ?. The MWU is performed using the SciPy library <ref type="bibr">(Virtanen et al., 2020)</ref>, and the results are further validated using <ref type="bibr">(2017,</ref><ref type="bibr">2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Downstream Finetuning</head><p>We finetune the models on each downstream task independently. The hyperparameters are selected based on the recommendations in <ref type="bibr" target="#b7">(Mosbach et al., 2021)</ref>. The specific hyperparameters used for each task are reported in the appendix D. On all tasks, we finetune with nine random seeds and report the average and standard deviation of the metrics. In <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_2">Table 3</ref>, we report the performances on IndicGLUE benchmark tasks and in <ref type="table" target="#tab_1">Table 2</ref> on other publicly available datasets. Here we discuss each of the tasks and compare the performance of the uni-script model with the multi-script model. We show the results from <ref type="bibr">IndicBERT(Kakwani et al., 2020)</ref> and <ref type="bibr">(Dhamecha et al., 2021)</ref> as independent baselines.</p><p>Wikipedia Section Title Prediction This is a MCQA task, where the task is to predict the title of a Wikipedia article section from four choices. However, we train a sequence classification model instead of a MCQA model for simplicity. This task is the same across all languages. Thus we simultaneously finetune the model on all languages. In <ref type="table" target="#tab_0">Table 1</ref>, we report the test set accuracy of different models on this task. Here, the uni-script model performs significantly better than the multi-script model. A ? value of 1 for all languages indicates that the uni-script model will always outperform the multi-script model. The ? value of Oriya (7.12), Gujarati (6.02), and Assamese (5.04) indicates that transliteration helps comparatively low-resource languages more compared to Hindi (4.06) and Bengali (3.02).</p><p>News Category Classification: This is a collection of news article classification tasks from the In-  <ref type="table" target="#tab_0">Table 1</ref>, we report the test set accuracy of different models on this task. We see our first failure to reject the null hypothesis case for the Bengali and Marathi classification tasks. Even though the ? is negative, we can not reject the null hypothesis for Bengali and Marathi as the p-values are 0.181 and 0.1683, respectively. The ? for these two tasks show that the uni-script can outperform the the multi-script model only 31% of the time. For Panjabi (? of 1.07) and Oriya (? of 0.68), the ?, indicates that the uni-script model can always outperform the multiscript model. However, for Gujarati (? of 0.60), the uni-script model can outperform the uni-script model 80% of the time.</p><p>Named Entity Recognition: We use the balanced Wikiann dataset from <ref type="bibr" target="#b16">Rahimi et al. (2019)</ref> for this task. Similar to WSTP, we simultaneously finetune the models on all languages. In <ref type="table" target="#tab_0">Table 1</ref>, we report the F1-scores of different models. As per the p-value, the uni-script model performs better and the difference is statistically significant. We specifically see that as per ?, the uni-script model makes remarkable improvements on Panjabi, Oriya, Assamese, and Gujarati NER and as per ? it always performs better on all languages. Similar to WSTP, the ? values show that transliteration helps lowresource languages more. For instance, the ? for Panjabi, Oriya, Assamese, and Gujarati are 8. <ref type="bibr">73, 9.36, 13</ref>.61, and 11.06, respectively. We can also see improvement on Hindi (1.13), Bengali (0.92), and Marathi (1.84) NER.</p><p>Article Genre, Sentiment &amp; Discourse Mode Classification: We evaluate the models on publicly available sequence classification datasets. Most of these classification tasks are highly skewed. Thus we report F1 scores in addition to accuracy on these tasks. As stated earlier, apart from ? and ?, we also report r on these datasets as their difference were not as pronounced as the IndicGLUE tasks. These tasks show one of the major reasons why multi-seeded results along statistical analysis are imperative. We present our results in <ref type="table" target="#tab_1">Table 2</ref>. We only show the test statistics for the F1-score here.  The test statistics for accuracy are provided in appendix F. For BBC News classification, we can see that even though the ? for Hindi is 2.12, there is no statistically significant difference in the models' performance. The null hypothesis can not be rejected due to the p-value being 0.4363, and the r is small (0.19). Both of these are due to the standard deviation of the multi-script model, which is (?4.6). However, ? indicates that uni-script model would outperform the multi-script model 62% of the time. For the Soham Articles classification the p-value is marginally rejected (0.05031). Nevertheless, the r is medium (0.46), and as per ?, uni-script model is better 78% of the time. For INLTK Headline classification task, we see that both models are equivalent for Gujarati and Marathi with a p-value of 0.5457 and 0.6665, respectively. Both of them have a small effect size (Gujarati 0.15 and Marathi 0.10). The ? indicates that uni-script model is better 59% of times for Gujarati and 43% of times for Marathi. On sentiment analysis tasks, we see a similar trend. Both models are equivalent performance wise with a p-value of 0.3865 for IITP Product Review and 0.5457 for IITP Movie review. The r for the tasks is 0.21 and 0.15, respectively, which indicates that the standardized effect is small for both tasks. Apart from that, as per ?, the uni-script model outperforms the multi-script model 63% of times for IITP Product review and 59% of the time for IITP Movie review. Lastly, for Discourse Mode Classification, we see that the uni-script model outperforms the multi-script with a ? value of 16.54 and a p-value of 0.00004. The standardized effect is large (0.83) and ? indicates that the uni-script model is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Transliteration:</head><p>In general, we can see that the effect of transliteration is mostly positive. However, it does not hurt the model's performance either, as we can see that the models performed equivalently in all the tasks where the null hypothesis could not be rejected. Transliteration mostly helps the comparatively low-resource languages (Panjabi, Oriya, Assamese), while highresource languages also (Hindi, Bengali) see good improvements as seen in WSTP and NER tasks. As for transliteration not hurting model performance, we can see that in <ref type="table" target="#tab_1">Table 2</ref> excluding Discourse Mode Classification, in all other tasks, the uniscript model performance was equivalent to the multi-script model. While Discourse Mode Classification had a statistically significant performance difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Zero Shot Capability Testing</head><p>We use the CSQA task to test the zero shot capability of the models as we can use the pretrained masked language models without finetuning on this task. On CSQA, we are given four entities to choose from to fill in a masked entity in a sentence. This task is designed to test whether language models can be used as knowledge bases <ref type="bibr" target="#b12">(Petroni et al., 2019)</ref>. In <ref type="table" target="#tab_2">Table 3</ref> we report the results. We see that the uni-script model performs better on all languages. The ? is particularly impressive in Gujarati (10.36), Assamese (3.44), and Marathi (3.39). This indicates that the uni-script model is better at representing entity knowledge than the multi-script model. We found some issue with the IndicBERT evaluation code for this task. We discuss this issue in appendix B. We evaluate IndicBERT with our code and report the results in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why Transliteration Works</head><p>In this section, we analyze why uni-script model performs better than the multi-script model from two perspectives, tokenization quality, and CLRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tokenizer Quality Analysis</head><p>As discussed in 1 and 2, we expect the transliteration model to exploit better tokenization across the languages. Whereas the same word written in six different scripts gets mapped to six different tokens in the multi-script model, these get mapped to the same token in the transliteration model. Thus we expect the tokenizer to more efficiently tokenize a given text in the case of the transliteration model.. Following <ref type="bibr">(?cs, 2019)</ref> and <ref type="bibr" target="#b20">(Rust et al., 2021)</ref>, we measure the subword fertility (average number of tokens per word) and the ratio of words unbroken by the tokenizer <ref type="figure" target="#fig_1">. From figure 1</ref>, we can see that transliteration reduces the splitting of words. This indicates that many words that were represented by different tokens in the multi-script model are represented by a single token in the transliteration model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-lingual Representation Similarity</head><p>Following <ref type="bibr">(Muller et al., 2021b)</ref>,  and <ref type="bibr">(Del and Fishel, 2021)</ref> we apply CKA to measure CLRS. We use the CKA implementation from the Ecco library <ref type="bibr">(Alammar, 2021)</ref>. We use parallel sentences on eight Indo-Aryan languages from the FLORES-101 <ref type="bibr">(Goyal et al., 2021)</ref> dataset. This is a dataset of professionally translated parallel  sentences on 101 languages. We only consider Panjabi, Hindi, Bengali, Oriya, Assamese, Gujarati, Marathi, and Nepali sentences from the FLORES-101 dataset as these are the only ones present in the pretraining corpus.</p><p>First, we extract the hidden state representations of the models on these parallel sentences. Then for each language pair, we calculate the CKA similarity score between the hidden state representations of the corresponding layers. For example, we calculate the CKA similarity score between layer eight hidden state representations of Bengali and Hindi parallel sentences. Then for each language, we average its CKA similarity scores with other languages per layer. In <ref type="figure" target="#fig_2">Figure 2</ref> we plot this average CKA similarity for each layer of the models for each language. We see that the multi-script model retains high CLRS score throughout the model. On the other hand, the CLRS score drops in the middle and end layers of the muti-script model. Overall the CLRS score of the uni-script model is more stable. This indicates that the uni-script model has learned better cross-lingual representations. We show the individual similarity figures for all language pairs in appendix G. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we found that transliterating closely related languages to a common script improves MLLM performance and leads to learning better CLRS. We established this by conducting rigorous statistical analysis to quantify the significance and effect size of transliteration on MLLMs. We found that transliteration especially improved the performance of comparatively low-resource languages and did not hurt the performance of high-resource languages. This findings are in agreement with contemporary literature <ref type="bibr">(Dhamecha et al., 2021;</ref><ref type="bibr" target="#b8">Muller et al., 2021a)</ref>. For the purpose of reproducibility, all our codes and models are available at Github and Hugging Face Hub. Our results indicate that in other scenarios where closely related languages from a language family use different scripts, transliteration can be used to improve MLLM performance. For example, the Dravidian, Slavic, and Turkic languages present similar scenarios. We would also like to implore our peers to include more hypothesis tests in their studies against a strong baseline. Non-parametric tests like MWU are easy to use and have a high level of interpretability. One limitation of our study is the relatively small models and the small number of languages. In future, we would like to extend our study to larger models and more languages. Also, character-based models hold more potential to exploit the lexical overlap enabled by transliteration. Efficient Transformer models that can efficiently handle long sequences which can be used to train these character-level models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ethical Considerations</head><p>In this paper, we empirically show the effect of transliteration on the performance of Multilingual Language Models.In their study, <ref type="bibr">Joshi et al. (2020)</ref> showed the resource disparity between lowresource and high-resource languages, and <ref type="bibr" target="#b19">(Ruder, 2020)</ref> also highlighted the necessity of working with low-resource languages. However, creating representative and inclusive corpora is a difficult task and an ongoing process and is not always possible for many low-resource languages. Thus to inclusively advance the state of NLP across languages, it is crucial to develop techniques for training MLLMs that can extract the most out of existing multilingual corpora. Hence, we believe our analysis might help MLLMS with low-resource languages in real-world applications. However, there are one ethical issues that we want to state explicitly. Even though we pretrain on a comparatively large multilingual corpus the model may exhibit harmful gender, ethnic and political bias. If the model is fine-tuned on a task where these issues are important, it is necessary to take special consideration when relying on the model's decisions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Linguistic relatedness in Indo-Aryan family</head><p>Many South Asian and Southeast Asian languages are intimately connected linguistically, historically, phonologically <ref type="bibr" target="#b3">(Littell et al., 2017)</ref> and phylogenetically. However, due to different scripts, it is difficult for MLLMs to fully exploit this shared information. Among the languages we considered in this study we encounter six different scripts. These are shown in <ref type="table" target="#tab_5">Table 5</ref>. Nevertheless, these scripts have shared ancestry from ancient Brahmic script <ref type="bibr">(Hockett et al., 1997;</ref><ref type="bibr">Coningham et al., 1996)</ref> and have similar structures that we can easily use to transliterate them to a common script. The ISO-15919 standard has been designed to perform this transliteration. Also, many of these languages heavily borrow from Sanskrit, and due to its influence, many words are shared among these languages. Therefore, due to their relatedness and highly diverse script barrier, this family of languages presents a unique opportunity to analyze the effects of transliteration on MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Cloze Style QA IndicBERT Evaluation Issue</head><p>Since a word can be tokenized to multiple tokens by the subword tokenizer, correctly evaluating the model on this task requires special care. Specifically, we have to use the same number of mask tokens as the number of subword tokens that a word gets split into. Then we calculate the probability for the word by multiplying the probability of the subword tokens predicted by the masked language model. We found that on the IndicBERT evaluation code, only a single mask token was used irrespective of the number of subword tokens that a word gets split into. We do not think this is the correct way to evaluate a masked language model on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Pretraining Hyperparameters</head><p>We used a batch size of 256, which is the highest that fits into TPU memory, whereas the ALBERT paper used a batch size of 4096. As our batch size is 1/16 th of the ALBERT paper, we use a learning rate of 1e-3/8, which is approximately 1/16 th of the learning rate used in the ALBERT paper (1.76e-2). Additionally, we use the Adam optimizer(Kingma and Ba, 2015) instead of the LAMB optimizer. The rest of the hyperparameters were the same as the ALBERT paper. Specifically, we use a sequence length of 512 with absolute positional encoding, weight decay of 1e-2, warmup steps of 5000, max gradient norm of 1.0, and Adam epsilon of 1e-6. The models were trained for 1M steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Downstream Hyperparameters</head><p>All of our hyperparameters for downstream tasks are presented in <ref type="table" target="#tab_4">Table 4</ref>. The batch size was chosen to be the maximum that fits in memory. This was done so that each batch contains approximately the same number of tokens. Otherwise the hyperparameters were chosen following the recommendations of <ref type="bibr" target="#b7">(Mosbach et al., 2021)</ref>. On the highly IITP Movie Review, IITP Product Review and MI-DAS Discourse we found that this default setting resulted in worse performance compared to the independent baselines. So we finetuned the learning rate and classifier dropout on the validation set of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Dataset Details</head><p>Here the corpus size details are provided in 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Public datasets accuracy test statistics</head><p>The MWU p-values and test statistics for public datasets accuracy is given in <ref type="table">Table 6</ref>. We can see that for BBC News (p-value 0.0088) and Soham Articles Classification (p-value 0.0090) the uni-script model is better than the multi-script model with a ? of 1.86 and 0.67, respectively. Both tasks have large r (0.62 for both). However, as per ?, the uni-script model outperforms the multi-script model 87% of the time. Whereas, the ? for Soham News Article Classification is 0.57. As for INLTK Headlines, the uni-script model and the multi-script model perform equivalently. On INLTK Headlines, the p-value for Gujarati (? of 0.32) is 0.6249 and Marathi (? of -0.17) is 0.3503. On IITP Product Reviews, the uni-script model outperforms the multiscript model with a ? of 0.85, p-value of 0.4099 and ? of 0.79.However, the r is medium (0.48) for the task. In contrast, on IITP Movie Reviews, both models are equivalent performance wise with a ? of 0.15, p-value of 0.8941, ? of 0.52 and a small (0.031) standardized effect. Finally, we can see that both models performing equally on Discourse Mode Classification. The ? is 0.15 with a p-value of 0.7561 and a small (0.073) r . However, as per the ?, the uni-script model outperforms the multiscript model 45% of the time.</p><p>G Cross-lingual Similarity of All Language Pairs  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Subword fertility (lower is better) and unbroken ratio (higher is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>CKA Similarity Score for the multi-script and uni-script models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on Classification Tasks from IndicGLUE Benchmark 55?0.61 82.24?0.18 84.38?0.29 81.47?0.99 81.74?0.82 82.39?0.27 82.74?0.52 81.78 multi-script and uni-script models are equal and blue indicates the uni-script model is better dicGLUE benchmark on five languages. In</figDesc><table><row><cell>Model</cell><cell>pa</cell><cell>hi</cell><cell>bn</cell><cell>or</cell><cell>as</cell><cell>gu</cell><cell>mr</cell><cell>avg</cell></row><row><cell>Wikipedia Section Title Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Independent Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT (Dhamecha et al., 2021)</cell><cell>76.48</cell><cell>80.81</cell><cell>82.85</cell><cell>28.29</cell><cell>-</cell><cell>78.58</cell><cell>84.38</cell><cell>71.90</cell></row><row><cell>XLM-R (Kakwani et al., 2020)</cell><cell>70.29</cell><cell>76.92</cell><cell>80.91</cell><cell>68.25</cell><cell>56.96</cell><cell>27.39</cell><cell>77.44</cell><cell>65.45</cell></row><row><cell>IndicBERT base (Kakwani et al., 2020)</cell><cell>67.39</cell><cell>74.02</cell><cell>80.11</cell><cell>57.14</cell><cell>65.82</cell><cell>68.79</cell><cell>72.56</cell><cell>69.40</cell></row><row><cell>IndicBERT large (Kakwani et al., 2020)</cell><cell>77.54</cell><cell>77.80</cell><cell>82.66</cell><cell>68.25</cell><cell>56.96</cell><cell>52.23</cell><cell>77.44</cell><cell>70.41</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>multi-script</cell><cell cols="3">74.33?0.83 78.18?0.33 81.18?0.28</cell><cell>74.35?1.2</cell><cell cols="4">76.70?0.83 76.37?0.53 79.10?0.84 77.17</cell></row><row><cell cols="2">uni-script 77.Test statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>3.22</cell><cell>4.06</cell><cell>3.20</cell><cell>7.12</cell><cell>5.04</cell><cell>6.02</cell><cell>3.64</cell><cell>4.61</cell></row><row><cell>p-value</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell><cell>-</cell></row><row><cell>?</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell>News Category Classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Independent Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R (Kakwani et al., 2020)</cell><cell>94.87</cell><cell>-</cell><cell>98.29</cell><cell>97.07</cell><cell>-</cell><cell>96.15</cell><cell>96.67</cell><cell>96.61</cell></row><row><cell>mBERT (Kakwani et al., 2020)</cell><cell>94.87</cell><cell>-</cell><cell>97.71</cell><cell>69.33</cell><cell>-</cell><cell>84.62</cell><cell>96.67</cell><cell>88.64</cell></row><row><cell>IndicBERT base (Kakwani et al., 2020)</cell><cell>97.44</cell><cell>-</cell><cell>97.14</cell><cell>97.33</cell><cell>-</cell><cell>100.00</cell><cell>96.67</cell><cell>97.72</cell></row><row><cell>IndicBERT large (Kakwani et al., 2020)</cell><cell>94.87</cell><cell>-</cell><cell>97.71</cell><cell>97.60</cell><cell>-</cell><cell>73.08</cell><cell>95.00</cell><cell>91.65</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>multi-script</cell><cell>96.83?0.19</cell><cell>-</cell><cell cols="2">98.14?0.14 98.09?0.16</cell><cell>-</cell><cell cols="3">98.80?0.43 99.58?0.25 98.30</cell></row><row><cell>uni-script</cell><cell>97.90?0.17</cell><cell>-</cell><cell cols="2">97.99?0.22 98.77?0.12</cell><cell>-</cell><cell cols="3">99.40?0.54 99.47?0.21 98.70</cell></row><row><cell>Test statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>1.07</cell><cell>-</cell><cell>-0.15</cell><cell>0.68</cell><cell>-</cell><cell>0.60</cell><cell>-0.18</cell><cell>0.40</cell></row><row><cell>p-value</cell><cell>0.0003</cell><cell>-</cell><cell>0.181</cell><cell>0.0004</cell><cell>-</cell><cell>0.03084</cell><cell>0.1683</cell><cell>-</cell></row><row><cell>?</cell><cell>1</cell><cell>-</cell><cell>0.31</cell><cell>1</cell><cell>-</cell><cell>0.80</cell><cell>0.31</cell><cell>-</cell></row><row><cell>NER (F1-Score)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Independent Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT (Dhamecha et al., 2021)</cell><cell>50.00</cell><cell>86.56</cell><cell>91.81</cell><cell>19.05</cell><cell>92.31</cell><cell>68.04</cell><cell>91.27</cell><cell>71.29</cell></row><row><cell>XLM-R (Kakwani et al., 2020)</cell><cell>17.86</cell><cell>89.62</cell><cell>92.95</cell><cell>25.00</cell><cell>66.67</cell><cell>55.32</cell><cell>87.86</cell><cell>62.18</cell></row><row><cell>IndicBERT base (Kakwani et al., 2020)</cell><cell>21.43</cell><cell>90.30</cell><cell>93.39</cell><cell>8.69</cell><cell>41.67</cell><cell>54.74</cell><cell>88.71</cell><cell>56.69</cell></row><row><cell>IndicBERT large (Kakwani et al., 2020)</cell><cell>44.44</cell><cell>86.81</cell><cell>91.85</cell><cell>35.09</cell><cell>43.48</cell><cell>70.21</cell><cell>87.73</cell><cell>65.66</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>multi-script</cell><cell>76.69?1.5</cell><cell cols="2">91.80?0.42 96.39?0.19</cell><cell>84.18?1.8</cell><cell>75.45?1.8</cell><cell>69.10?2.9</cell><cell cols="2">88.72?0.40 83.19</cell></row><row><cell>uni-script</cell><cell>85.42?1.9</cell><cell cols="3">92.93?0.21 97.31?0.22 93.54?0.58</cell><cell>89.06?2.2</cell><cell cols="3">80.16?0.15 90.56?0.44 89.85</cell></row><row><cell>Test statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>8.73</cell><cell>1.13</cell><cell>0.92</cell><cell>9.36</cell><cell>13.61</cell><cell>11.06</cell><cell>1.84</cell><cell>6.66</cell></row><row><cell>p-value</cell><cell>0.0004066</cell><cell>0.0004066</cell><cell>0.0003983</cell><cell>0.0004038</cell><cell>0.000401</cell><cell>0.0004066</cell><cell>0.0004095</cell><cell>-</cell></row><row><cell>?</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell>orange indicates the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on Public Datasets</figDesc><table><row><cell>Language</cell><cell>Dataset</cell><cell>IndicBERT-base</cell><cell cols="2">multi-script</cell><cell cols="2">uni-script</cell><cell></cell><cell cols="2">Test Statistics</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>F1-score</cell><cell>Accuracy</cell><cell>F1-Score</cell><cell>?</cell><cell>p-value</cell><cell>?</cell><cell>r F1-score</cell></row><row><cell cols="2">Article Genre Classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hi</cell><cell>BBC News</cell><cell>74.60</cell><cell>77.28?1.5</cell><cell>46.47?4.6</cell><cell cols="2">79.14?0.60 48.59?0.27</cell><cell>2.12</cell><cell>0.4363</cell><cell>0.62</cell><cell>0.19</cell></row><row><cell>bn</cell><cell>Soham News Article Classification</cell><cell>78.45</cell><cell cols="2">93.22?0.49 91.12?0.85</cell><cell>93.89?48</cell><cell>91.75?0.73</cell><cell>0.63</cell><cell cols="2">0.05031 0.78</cell><cell>0.46</cell></row><row><cell>gu</cell><cell>INLTK Headlines</cell><cell>92.91</cell><cell cols="4">90.41?0.69 88.82?0.73 90.73?0.75 89.14?0.88</cell><cell>0.32</cell><cell>0.5457</cell><cell>0.59</cell><cell>0.15</cell></row><row><cell>mr</cell><cell>INLTK Headlines</cell><cell>94.30</cell><cell cols="5">92.21?0.23 89.23?0.54 92.04?0.47 89.09?0.78 -0.14</cell><cell>0.6665</cell><cell>0.43</cell><cell>0.10</cell></row><row><cell cols="2">Sentiment Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hi</cell><cell>IITP Product Reviews</cell><cell>71.32</cell><cell cols="4">76.33?0.84 74.04?0.99 77.18?0.77 74.53?0.98</cell><cell>0.49</cell><cell>0.3865</cell><cell>0.63</cell><cell>0.21</cell></row><row><cell>hi</cell><cell>IITP Movie Reviews</cell><cell>59.03</cell><cell>65.91?2.2</cell><cell>65.26?2.2</cell><cell>66.34?0.16</cell><cell>65.87?1.6</cell><cell>0.61</cell><cell>0.5457</cell><cell>0.59</cell><cell>0.15</cell></row><row><cell cols="2">Discourse Mode Classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hi</cell><cell>MIDAS Discourse</cell><cell>78.44</cell><cell>78.39?0.33</cell><cell>47.26?6.2</cell><cell>78.54?0.91</cell><cell>63.80?3.2</cell><cell cols="2">16.54 0.00004</cell><cell>1</cell><cell>0.83</cell></row></table><note>orange indicates the multi-script and uni-script models are equal and blue indicates the uni-script model is better</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy on CSQA</figDesc><table><row><cell>Model</cell><cell>pa</cell><cell>hi</cell><cell>bn</cell><cell>or</cell><cell>as</cell><cell>gu</cell><cell>mr</cell><cell>avg</cell></row><row><cell>Cloze-style QA (Zero Shot)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Independent Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">IndicBERT base (Our Evaluation) 29.33 30.76 28.45 30.38 29.98 81.50 29.71</cell><cell>37.16</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>multi-script</cell><cell cols="7">31.04 36.72 35.19 34.63 33.92 59.86 36.14</cell><cell>38.21</cell></row><row><cell>uni-script</cell><cell cols="7">32.77 38.52 36.38 36.00 37.36 70.22 39.53</cell><cell>41.54</cell></row><row><cell>?</cell><cell>1.73</cell><cell>1.8</cell><cell>1.19</cell><cell>1.37</cell><cell>3.44</cell><cell>10.36</cell><cell>3.39</cell><cell>3.33</cell></row></table><note>orange indicates multi-script and uni-script models are equal and blue indicates uni-script is better than multi-script</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for all tasks Task TPU Batch Size Learning Rate Weight Decay Dropout Epochs Warmup Ratio</figDesc><table><row><cell>News Category Classification</cell><cell>False</cell><cell>16</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.1</cell><cell>20</cell><cell>0.10</cell></row><row><cell>Wikipedia Section-Title Prediction</cell><cell>True</cell><cell>256</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.1</cell><cell>3</cell><cell>0.10</cell></row><row><cell>Named Entity Recognition</cell><cell>True</cell><cell>512</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.1</cell><cell>20</cell><cell>0.10</cell></row><row><cell>BBC Hindi News Classification</cell><cell>False</cell><cell>16</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.1</cell><cell>20</cell><cell>0.10</cell></row><row><cell cols="2">Soham Bangla News Classification False</cell><cell>16</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.1</cell><cell>8</cell><cell>0.10</cell></row><row><cell>iNLTK Headlines Classification</cell><cell>False</cell><cell>256</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.1</cell><cell>20</cell><cell>0.10</cell></row><row><cell>IITP Movie Review</cell><cell>False</cell><cell>64</cell><cell>5e-5</cell><cell>0.01</cell><cell>0.25</cell><cell>20</cell><cell>0.10</cell></row><row><cell>IITP Product Review</cell><cell>False</cell><cell>16</cell><cell>5e-5</cell><cell>0.01</cell><cell>0.5</cell><cell>20</cell><cell>0.10</cell></row><row><cell>MIDAS Discourse Mode</cell><cell>False</cell><cell>32</cell><cell>2e-5</cell><cell>0.01</cell><cell>0.5</cell><cell>20</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Pretraining dataset details</figDesc><table><row><cell cols="2">Language Pretraining corpus size in GB</cell><cell>Language Sub-family</cell><cell>Script</cell></row><row><cell>hi</cell><cell>8.9</cell><cell>Central Indo-Aryan</cell><cell>Devanagari</cell></row><row><cell>bn</cell><cell>5.8</cell><cell>Eastern Indo-Aryan</cell><cell>Bengali-Assamese</cell></row><row><cell>mr</cell><cell>1.4</cell><cell>Southern Indo-Aryan</cell><cell>Devanagari</cell></row><row><cell>ne</cell><cell>1.2</cell><cell>Northern Indo-Aryan</cell><cell>Devanagari</cell></row><row><cell>si</cell><cell>0.783</cell><cell>Insular Indo-Aryan</cell><cell>Sinhala</cell></row><row><cell>gu</cell><cell>0.705</cell><cell>Western Indo-Aryan</cell><cell>Gujarati</cell></row><row><cell>pa</cell><cell>0.449</cell><cell>Northwestern Indo-Aryan</cell><cell>Gurmukhi</cell></row><row><cell>or</cell><cell>0.18</cell><cell>Eastern Indo-Aryan</cell><cell>Oriya</cell></row><row><cell>as</cell><cell>0.069</cell><cell>Eastern Indo-Aryan</cell><cell>Bengali-Assamese</cell></row><row><cell>sa</cell><cell>0.036</cell><cell>Sanskrit</cell><cell>Devanagari</cell></row><row><cell>bpy</cell><cell>0.0017</cell><cell>Eastern Indo-Aryan</cell><cell>Bengali-Assamese</cell></row><row><cell>gom</cell><cell>0.0017</cell><cell>Southern Indo-Aryan</cell><cell>Devanagari</cell></row><row><cell>bh</cell><cell>0.000034</cell><cell>Eastern Indo-Aryan</cell><cell>Devanagari</cell></row><row><cell>mai</cell><cell>0.000011</cell><cell>Eastern Indo-Aryan</cell><cell>Devanagari</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ibraheem-moosa/ XLM-Indic 2 https://huggingface.co/ibraheemmoosa/ xlmindic-base-uniscript 3 https://huggingface.co/ibraheemmoosa/ xlmindic-base-multiscript</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploring BERT&apos;s Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judit</forename><surname>?cs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Blog Post</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis: Category detection and sentiment classification for hindi</title>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 17th International Conference on Intelligent Text Processing and Computa</title>
		<editor>Md. Shad Akhtar, Asif Ekbal, and Pushpak Bhattacharyya</editor>
		<meeting>the 17th International Conference on Intelligent Text Processing and Computa</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Datasets: A community library for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>?a?ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavitvya</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delangue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran?ois Lagunas, Alexander Rush, and Thomas Wolf; Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Kairis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlisle</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<title level="m">URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Whitney</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177730491</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Insights on representational similarity in neural networks with canonical correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="448" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beno?t Sagot, and Djam? Seddah. 2021b. First align, then predict: Understanding the cross-lingual ability of multilingual bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transliteration for cross-lingual morphological inflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikitha</forename><surname>Murikinati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.sigmorphon-1.22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
		<meeting>the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su&amp;apos;arez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="DOI">10.14618/ids-pub-9021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019, Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7)</title>
		<meeting>the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019, the Workshop on Challenges in the Management of Large Corpora (CMLC-7)<address><addrLine>Mannheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-22" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Institut f&quot;ur Deutsche Sprache</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Language models as knowledge bases?</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unks everywhere: Adapting multilingual language models to new scripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli&amp;apos;c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Massively multilingual transfer for NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="151" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Aksharamukha transliteration tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinodh Rajan</surname></persName>
		</author>
		<ptr target="https://github.com/virtualvinodh/aksharamukha-python" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-shot neural transfer for cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiateng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="http://ruder.io/nlp-beyond-english" />
		<title level="m">Why You Should Do NLP Beyond English</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How good is your tokenizer? on the monolingual performance of multilingual language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3118" to="3135" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bert is not an interlingua and the bias of tokenization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasdeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pretraining via leveraging assisting languages for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-srw.37</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="279" to="285" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using Effect Sizeor Why the P Value Is Not Enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Grad Med Educ</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<imprint>
			<publisher>Pearu Peterson</publisher>
			<pubPlace>Warren</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<publisher>Paul van Mulbregt</publisher>
		</imprint>
	</monogr>
	<note>and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
		<idno type="DOI">10.2307/3001968</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Emerging crosslingual structure in pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are all languages created equal in multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
