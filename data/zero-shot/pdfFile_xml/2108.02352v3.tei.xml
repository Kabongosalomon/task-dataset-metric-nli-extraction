<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sentiment Analysis</term>
					<term>Entity Knowledge</term>
					<term>Aspect Level</term>
					<term>Memory Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-level sentiment classification (ASC) aims to predict the fine-grained sentiment polarity towards a given aspect mentioned in a review. Despite recent advances in ASC, enabling machines to preciously infer aspect sentiments is still challenging. This paper tackles two challenges in ASC: (1) due to lack of aspect knowledge, aspect representation derived in prior works is inadequate to represent aspect's exact meaning and property information; (2) prior works only capture either local syntactic information or global relational information, thus missing either one of them leads to insufficient syntactic information. To tackle these challenges, we propose a novel ASC model which not only end-to-end embeds and leverages aspect knowledge but also marries the two kinds of syntactic information and lets them compensate for each other. Our model includes four key components: (1) a knowledge-aware gated recurrent memory network recurrently integrates dynamically summarized aspect knowledge; (2) a dual syntax graph network combines both kinds of syntactic information to comprehensively capture sufficient syntactic information; (3) a knowledge integrating gate re-enhances the final representation with further needed aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the aspect-related semantics from all hidden states into the final representation. Experimental results on several benchmark datasets demonstrate the effectiveness of our model, which overpass previous state-of-the-art models by large margins in terms of both Accuracy and Macro-F1. To facilitate further research in the community, we have released our source code at https://github.com/XingBowen714/KaGRMN-DSG ABSA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A Spect-level sentiment classification (ASC) <ref type="bibr" target="#b0">[1]</ref> is a finegrained task of sentiment classification or emotion recognition <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. ASC aims to infer the fine-grained sentiment of a given aspect mentioned in a review. Generally, an aspect is a noun phrase included in a review sentence. For example, in a review "It took so long to get the check, while the dinner is great.", there are two aspects (check and dinner) of opposite sentiments. ASC has received increasing attention and interest from both academia and the industry due to its wide applications in real-life scenarios such as dialog systems <ref type="bibr" target="#b5">[6]</ref>, online reviews <ref type="bibr" target="#b6">[7]</ref> and social networks <ref type="bibr" target="#b7">[8]</ref>.</p><p>Prior works have noticed the importance of aspect-context interaction. Different kinds of attention mechanisms <ref type="bibr" target="#b9">[9]</ref>- <ref type="bibr" target="#b11">[11]</ref> are proposed to extract aspect-relevant semantics from the hidden states of context words. And more recently, syntactic information is widely leveraged to facilitate the interactions between the aspect and its related words that are distant in context sequence. Graph Convolutional Networks (GCN) <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref> and Graph Attention Networks (GAT) <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref> are adopted to encode the syntax graphs predicted by off-the-shelf dependency parsers. <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b16">[16]</ref> employed GCNs to capture the local syntactic information. <ref type="bibr" target="#b15">[15]</ref> proposed relational multi-head attention (Relational MHA) to capture the global relational information between aspect and each context word.</p><p>However, little attention has been spent on aspect representation and its conveyed semantics. Aspect representation and its semantics not only guide the aspect-context interaction but also provide important clues for ASC. Despite its importance, in previous works <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b17">[17]</ref>, aspect representation is simply derived by pooling the hidden states of aspect words. In Sec. IV-H we empirically study the aspect representation generated by BERT <ref type="bibr" target="#b19">[18]</ref>, and two cases are shown in <ref type="table" target="#tab_1">Table VIII</ref>. We can find that BERT cannot capture the exact meanings and property information of Mountain Lion OS and iTune, although it is one of the strongest language models. Merely relying on pre-trained large language models cannot obtain sufficiently effective and informative aspect representation, making it hard for machines to address ASC. In contrast, humans can easily handle ASC and we conjecture the key to master this task is to leverage the adequate aspect knowledge they often refer to as the clue. Thinking of and leveraging the aspect knowledge are instinctive reactions of humans when they read an aspect in a review. For example, there is a review "Just a not bad restaurant, because the cheese and chips are both very soft. <ref type="bibr">"</ref> With the knowledge of 'cheese' and 'chips', humans are aware that the former should be soft and the latter should be click (not soft). Hence it is easy for humans to infer the positive sentiment of 'cheese' and the negative sentiment of 'chips'. However, in contrast, in ASC models there is no such mechanism, and aspect knowledge has not been explored or leveraged. Inheriting this deficiency, the aspect representation and semantics derived by prior models may lose important aspect information, which hinders aspect sentiment reasoning and make ASC challenging for machines.</p><p>On the other hand, both GCN and Relational MHA are useful for modeling distinct syntax graphs, but they have respective arXiv:2108.02352v3 [cs.CL] 1 Mar 2022 shortages: GCN is hard to capture the global relations between aspect and its non-adjacent context words on the syntax graph; Relational MHA fails to capture the local syntactic information among context words because they are isolated from each other on the star-shaped aspect-oriented syntax graph. However, prior works only consider one of them, resulting in insufficient syntactic information.</p><p>To tackle the aforementioned two challenges, we suggest that (1) aspect knowledge should be explicitly leveraged in ASC models; <ref type="bibr" target="#b1">(2)</ref> both kinds of syntactic information should be combined to capture sufficient syntactic information. We observe that there is plenty of entity descriptions in popular and easily accessible knowledge bases, such as DBpedia 1 and Wikipedia 2 . From their statistics, there are about 6.6 Million and 50 Million entities in current DBpedia and Wikipedia datasets. These descriptions can sufficiently represent the entities' meanings and conveying a wealth of entities' knowledge. In ASC, aspects are always entities, making it more convenient to retrieve their descriptions.</p><p>In this work, we propose a Knowledge-aware Gated Recurrent Memory Network with Dual Syntax Graph Modeling (KaGRMN-DSG) model as our solution to the two challenges. Specifically, its novelty lies in three core modules. The first one is Knowledge-aware Gated Recurrent Memory Network (KaGR-MN) which recurrently integrates the aspect knowledge into aspect representation and then context memories. An aspect-todescription attention mechanism is devised to dynamically summarize the needed aspect knowledge from the aspect description regarding the current semantic state. An adaptive knowledge integrating gate is designed to adaptively integrate the summarized knowledge into aspect representation. Then a self multi-head attention is employed to contextualize the integrated knowledge and update the context memory bank. The second one is Dual Syntax Graph Network (DSG-Net), which marries the proposed Position-aware GCN and Relational MHA, then learns the dual syntactic interaction to comprehensively capture sufficient syntactic information. The third one is the knowledge integrating gate (KI Gate) which re-enhances the final representation with further needed knowledge.</p><p>We highlight our contributions as follows: (1) Based on plenty of informative entity descriptions from easily accessible knowledge bases, we end-to-end embed and leverage the aspect knowledge to address ASC.</p><p>(2) We propose a novel KaGR-MN, which combines the advantages of LSTM, Transformer, and Memory Networks. It recurrently embeds and integrates beneficial aspect knowledge into aspect representation and all context memories.</p><p>(3) We propose a dual syntax graph network, in which the local syntactic information and global relational information are combined to comprehensively capture sufficient syntactic information.</p><p>(4) We conduct extensive experiments on three benchmark datasets. Results show that our model achieves new stateof-the-art performances, significantly outperforming previous best results. Ablation study and further analysis validate the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In early studies <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, sentiment classifiers were built by traditional machine learning algorithms which demanded labor-intensive feature engineering. Most recently proposed ASC models are based on neural networks which can automatically learn representations. Conventionally, neural ASC model contains an aspect encoder, a context encoder and an aspect-to-context attention mechanism [9]- <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b22">[21]</ref>.</p><p>Different kinds of networks are adopted as the encoder. As for LSTM, <ref type="bibr" target="#b24">[22]</ref> employed two separated LSTMs to encode the aspect-left and -right word sequences and then combined the two last hidden sates for classification; <ref type="bibr" target="#b25">[23]</ref> proposed to leverage external document sentiment analysis corpus in a multi-task framework to enhance the context modeling of LSTM. Besides, convolutional neural networks (CNN) and Memory Networks (MNs) are also exploited as encoders. <ref type="bibr" target="#b26">[24]</ref> introduced parameterized filters and parameterized gates into CNN to integrate aspect information for context encoding. <ref type="bibr" target="#b27">[25]</ref> designed a gated CNN layer to extract the aspect-specific features from the context hidden states. Based on standard MNs, <ref type="bibr" target="#b29">[26]</ref> proposed the targetsensitive memory networks to focus on the impact of aspect semantics on the classification.</p><p>The attention mechanism is utilized to extract aspect-related sentiment features via assigning a weight to each context word regarding its relevance to the aspect. <ref type="bibr" target="#b9">[9]</ref> proposed an aspect-tocontext attention and a context-to-aspect attention to study the interactions between the aspect and context. <ref type="bibr" target="#b11">[11]</ref> proposed an algorithm to automatically mine useful supervised information for the attention mechanism through the training process.</p><p>However, attention mechanisms may hardly capture the important words which is far from the aspect in the input context. As the development of graph neural networks <ref type="bibr" target="#b30">[27]</ref>- <ref type="bibr" target="#b33">[30]</ref>, recent works utilize graph convolution network (GCN) <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b31">[28]</ref> and graph attention network (GAT) <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b30">[27]</ref>, <ref type="bibr" target="#b34">[31]</ref> to model the syntax graph for shortening the distance between the aspect and its sentiment trigger words and leveraging the syntactical information. <ref type="bibr" target="#b12">[12]</ref> employed LSTM as encoder and exploit GCN to capture local syntactic information via encoding the syntax graph produced by offthe-shelf dependency parsers. <ref type="bibr" target="#b15">[15]</ref> proposed Relational MHA, which can capture the global dependency between the aspect and each context word via operating on the star-shaped aspectoriented syntax graph.</p><p>To enhance the context modeling, <ref type="bibr" target="#b25">[23]</ref> and <ref type="bibr" target="#b35">[32]</ref> trained their models on both document-level sentiment classification and ASC tasks in the multi-task framework with a shared encoder. <ref type="bibr" target="#b36">[33]</ref> proposed an aspect-aware LSTM which introduces aspect information into LSTM cells to generate better context hidden states in which more aspect-related information is retained and aspect-irrelevant information is discarded. As BERT has proven its power of language modeling on hetergenuous NLP tasks, more recently proposed work <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b15">[15]</ref> adopted BERT as the context encoder to obtain high-quality hidden states.</p><p>However, prior models neglect to leverage aspect knowledge, resulting in inadequate aspect semantics. And the syntactic information they captured is insufficient. In this paper, we propose KaGRMN-DSG to solve these two challenges. There are two main differences from our model and previous works. The first one is leveraging aspect knowledge, which is achieved by a novel KaGR-MN. The other one is combining both of local syntactic information and global relational information, which is achieved by a DSG-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. KAGRMN-DSG</head><p>Overview The architecture of our KaGRMN-DSG model is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. To extract the beneficial clues for aspects, knowledge-aware gated recurrent memory network and knowledge integration gate incorporate summarized aspect knowledge to enrich aspect representation and all context memories. To capture sufficient syntactic information, dual syntax graph network combines local syntactic information and global relational information, then learns their mutual interaction. To comprehensively abstract high-level clues, aspect-to-context attention mechanism aggregates aspect-related semantics from all hidden states into the final representation. And we believe that these modules can effective cooperate to further improve aspect sentiment reasoning. Description Retrieval We use aspect (A) to query DBpedia first and then Wikipedia to get its description (D). If multiple descriptions are returned (polysemy), the one with the highest semantic similarity to context (C) is selected as D. The semantic similarity of a description candidate and review context is calculated as:</p><formula xml:id="formula_0">avg(C) = 1 N C N C i=1 e(c i )<label>(1)</label></formula><formula xml:id="formula_1">avg(D ) = 1 N D N D i=1 e(d i )<label>(2)</label></formula><formula xml:id="formula_2">sim(C, D ) = cos ? * avg(C) + (1 ? ?) * e(dl), avg(D )<label>(3)</label></formula><p>where N C and N D denotes the number of words in the context and description candidate respectively, e(w) denotes the word embedding 3 of word w, dl denotes domain label (e.g. the dl of Lap14 dataset is 'laptop'). Here we intuitively set ? as 0.5 because both of the context semantics avg(C) and domain information e(dl) are important in selecting the correct description candidate. The reason why we use domain label here is that sometimes there may be not enough words conveying domain-specific semantics for distinguishing the needed description. Besides, the retrieval is enhanced with some rules, such as soft matching with lemmatization and stop word filtering. Finally, about 70% aspects in the datasets can be equipped with retrieved descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Memory Bank Construction</head><p>In this work, we adopt BERT to encode the description and context to produce their hidden states. For description (D), the formal input is [CLS]; D; [SEP] , where ; denotes concatenation operation. The description is encoded in the single-sentence manner then a series of its hidden states is generated:</p><formula xml:id="formula_3">H D = {h i d ? R de } N D i=1</formula><p>, which is taken as the description memory bank M D .</p><p>As for context (C), we model the context-aspect pair in the sentence-pair manner to generate aspect-aware hidden states <ref type="bibr" target="#b36">[33]</ref>. The formal input is</p><formula xml:id="formula_4">[CLS]; C; [SEP]; A; [SEP] .</formula><p>In this way, we obtain the hidden state of [CLS]: h cls and a series of aspect-aware context hidden states:</p><formula xml:id="formula_5">H C = {h i c ? R de } N C i=1</formula><p>. As BERT has a strong capability of sentence-pair modeling, h cls contains not only the information from both of the aspect and the context but also their dependencies. Thus we take h cls as the initial contextualized aspect representation r 0 a . Then we use r 0 a to replace the hidden states of aspect</p><formula xml:id="formula_6">words (H A = {h i a ? R de } N A i=1 ) in H C , obtaining the initial context memory bank M 0 C = [h 1 c , h 2 c , ..., r 0 a , ..., h N c ], where N = N C ? N A + 1. M D and M 0</formula><p>C are two strands of input of KaGR-MN cell. Along time steps, M C is recurrently updated while M D remains identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Knowledge-aware Gated Recurrent Memory Network</head><p>As the series of context hidden states and description hidden states have been obtained, now the challenge is how to incorporate as much beneficial aspect knowledge as possible without losing the original semantics obtained from BERT?.</p><p>The first thing is to conserve the original semantics in the context memories obtained from BERT. To this end, we employ Memory Networks (MNs) as the backbone to store context memories, because that MNs can accurately remember original facts <ref type="bibr" target="#b38">[35]</ref>. Secondly, we are supposed to make the integrated knowledge beneficial. In other words, we should provide each sample the aspect knowledge it needs. Hence we propose an aspect-to-description attention (A2D Att) mechanism to summarize the needed aspect knowledge from the description memory bank. Thirdly, we should integrate the beneficial aspect knowledge into the aspect representation. Then we propose an adaptive knowledge integration gate, which borrows the idea of gating mechanisms in LSTM <ref type="bibr" target="#b39">[36]</ref>. Gate mechanism has proven its strong ability of information integration in many tasks <ref type="bibr" target="#b36">[33]</ref>, <ref type="bibr" target="#b40">[37]</ref>. However, only integrate knowledge into aspect representation is insufficient, not exploring the full value of aspect knowledge. It is intuitive that the aspect knowledge should be incorporated into all context memories. Besides, an appropriate mechanism should be devised to update the context memory bank. To achieve these two goals, inspired by Transformer <ref type="bibr" target="#b41">[38]</ref>, we utilize self multi-head attention to update the context memories, and in the meanwhile, the aspect knowledge in the aspect representation can be spread to all context memories. Finally, all the above mechanisms form the Knowledge-aware Gated Recurrent Memory Network (KaGR-MN), which combines the advantages of MNs, LSTM, and Transformer.</p><p>The architecture of KaGR-MN cell is illustrated in <ref type="figure">Fig. 2</ref>. In the following texts, we depict the details of KaGR-MN.</p><p>1) Dynamic Knowledge Summarizing: Intuitively, on the one hand, the context-aspect pair of each sample may demand individual aspect knowledge, even if they have the same aspect. On the other hand, at each time step, KaGR-MN should integrate specifically needed aspect knowledge according to the current cell state. Therefore, the aspect knowledge summarizing should be dynamic. To achieve this, we design an aspect-todescription attention (A2D Att) mechanism to dynamically summarize the specifically needed aspect knowledge from the description memory bank M D at each time step. The architecture of A2D Att is shown in <ref type="figure">Fig. 2</ref>. At each time step (t), the aspect representation of previous time step r t?1 a serves as the cell state and is used to query M D . Then an attention weight ? is assigned to each h d regarding its importance to r t?1 a :</p><formula xml:id="formula_7">? i =SoftMax(F(h i d , r t?1 a )) = exp(F(h i d , r t?1 a )) N D k=1 exp(F(h k d , r t?1 a ))<label>(4)</label></formula><p>where F(h i d , r t?1 a )) is a score function defined as: where W d and b d are weight matrix and bias respectively, and T denotes transposition. Then we can obtain the summarized knowledge representation as:</p><formula xml:id="formula_8">F(h i d , r t?1 a )) = (W d h i d + b d ) (r t?1 a ) T<label>(5)</label></formula><formula xml:id="formula_9">r t k = N D i=1 ? i h i d .</formula><p>2) Adaptive Knowledge Integration: As the specifically needed knowledge has been summarized, it should be integrated into the aspect representation regarding the current cell state. As the gate mechanisms <ref type="bibr" target="#b39">[36]</ref>, <ref type="bibr" target="#b40">[37]</ref>, <ref type="bibr" target="#b42">[39]</ref> have proven their ability of controlling information flow, here we design an Adaptive Knowledge Integration (AdaKI) Gate to integrate r t k into r t?1 a . Its architecture is shown in <ref type="figure">Fig. 2</ref>. AdaKI Gate can be formulated as:</p><formula xml:id="formula_10">r t * a = r t?1 a + r t k (W k [r t?1 a , r t k ])<label>(6)</label></formula><p>where denotes Hadamard product, [, ] denotes concatenation and W k is weight matrix. The core of AdaKI Gate is to produce a gate vector using r t k and r t?1 a . This gate vector achieves the fine-grained control on each dimension of r t k . There are two merits of this fine-grained control. First, AdaKI Gate can determine what knowledge and how much knowledge from r t k should be integrated into r t?1 a . Second, it can map the integrated knowledge into the same semantic space of r t?1 a and M t?1 C . This adaption helps maintain the semantics consistency of r t * a and M t?1 C , which is beneficial to later knowledge contextualizing. In Sec. IV-F, we investigate the effect of different knowledge gates used here. After r t * a is obtained, it replaces</p><formula xml:id="formula_11">r t?1 a in M t?1 C , forming M t * C .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Knowledge Contextualizing and Context Memory Bank</head><p>Updating: Although the needed beneficial knowledge has been integrated into r t * a , the other context memories in M t * C remain the same as the ones in M t?1 C . Intuitively, all context memories should benefit from aspect knowledge to facilitate aspect-related information aggregation. To achieve this, we propose a knowledge contextualizing mechanism to broadcast the newly-integrated knowledge in r t * a to all context memories in M t * C . Here we borrow the idea of self-attention <ref type="bibr" target="#b41">[38]</ref>, <ref type="bibr" target="#b43">[40]</ref>, which can effectively relate the different tokens in a sentence and capture the intra-sentence dependencies.</p><p>In this work, we adopt the self multi-head attention (Self MHA) formulation in <ref type="bibr" target="#b41">[38]</ref>. We first map M t * C to queries (Q), keys (K) and values (V) matrices by individual linear projections, where Q, K, V ? R N ?ds . And this process repeats H s n times, where H s n is the number of heads and H s n ?d s = d e . The scaled dot-product attention is used to produce the output of each head, then all of the H s n outputs are concatenated to form the updated context memory bank M t C :</p><formula xml:id="formula_12">M t C = H s n h=1 SoftMax QK T ? d s V<label>(7)</label></formula><p>This simple but effective knowledge contextualizing mechanism updates M t * C and r t * a by letting context memories (including r t * a ) exchange useful information with each other, which is beneficial to capture aspect-related information. Along time steps, r a and M C would contain more and more reasonable and beneficial semantics for ASC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dual Syntax Graph Network</head><p>As proven in prior works, GCN can capture local syntactic information and Relational MHA can capture the global relation between the aspect and each context word via operating on the star-shaped aspect-oriented syntax graph (as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). However, as we have discussed in Sec. I, they have respective shortages. They can only capture one of the two kinds of syntactic information and lose the other. Previous models only employ one of them, leading to insufficient syntactical information.</p><p>To this end, we propose DSG-Net (as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) which marries the proposed Position-aware GCN and Relational MHA and learns their interaction, capturing sufficient syntactic information.</p><p>1) Local Syntactic Information Modeling: Graph Construction Based on the original syntax graph G 4 , we first add a new aspect node A and merge all edges between nodes of aspect words and non-aspect context words to A. Then we delete all of the original nodes of aspect words and their edges. The obtained graph is similar to G, and only several context word nodes are connected to A. Thus we term it sparse graph G s (shown in <ref type="figure" target="#fig_0">Fig.1</ref>). Position-aware GCN In this work, we augment the standard GCN with a position weight w i p = 1? |i?? | N +1 , in which ? denotes the position of aspect, i denotes the i th context word. As the Self MHA in KaGR-MN does not consider the order of context memories, some positional and ordering information may be lost. w i p can supplement this information, which helps capture local syntactic information. Besides, it indicates the position of A and highlights the potential aspect-related words which are generally closer to A. In l th -layer, the local neighborhood information is aggregated as:</p><formula xml:id="formula_13">h l i = j?N s i W s g (w j p h l?1 j )/(d i + 1) + b s g<label>(8)</label></formula><p>in which N s i is the first-order neighbors of node i (including i) in G s , d i is the degree of node i, W s g and b s g are weight matrix and bias. <ref type="bibr" target="#b3">4</ref> obtained by spaCy toolkit: https://spacy.io/ 2) Global Relational Information Modeling: We obtain the star-shaped aspect-oriented syntax graph following <ref type="bibr" target="#b15">[15]</ref>. In this syntax graph, every context word directly connects to the aspect node A, so we term it dense graph G d . Then we employ the Relational MHA to model the global relational dependency between aspect and each context word. The node representation is:</p><formula xml:id="formula_14">h i = H d n m=1 j?N d i ? m ij W 1 m h j /H d n ? m ij = SoftMax(g m ij ) g m ij = ReLU r ij W 2 m + b 1 m W 3 m + b 2 m<label>(9)</label></formula><p>where H d n denotes the head number, r ij is the embedding of the relation between nodes i and j, W 1,2,3 m and b 1,2 m are weight matrices and biases.</p><p>3) Dual Syntactic Information Fusion: Now the Positionaware GCN has captured the important local syntactic information and the Relational MHA has captured the important global relational information. To integrate them together and let them compensate for each other, we concatenate the aspect node representations respectively derived by Positionaware GCN and Relational MHA, then we employ a multilayer perception (MLP), which can automatically abstract the integrated representation <ref type="bibr" target="#b44">[41]</ref>, <ref type="bibr" target="#b45">[42]</ref>, to generate the unified node representation sequence, which include the unified aspect representation R a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Knowledge Re-enhancement</head><p>After graph modeling, sufficient syntactic information has been integrated into R a . On the one hand, some new clues may be captured by DSG-Net and retained in R a . Thus R a may further need more aspect knowledge to collaborate with these new clues to support ASC. On the other hand, as the syntax graph may be imperfectly generated by the parser, some wrong connections and relations may be introduced. In this case, re-integrating some knowledge can help alleviate the influence of the imperfect syntax graph. To this end, we design a knowledge integrating gate (KI Gate) to re-enhance R a with further needed knowledge contained in r T k . The function of KI gate is given as:</p><formula xml:id="formula_15">R a = R a + r T k * W r k [ R a , r T k ]<label>(10)</label></formula><p>where W r k is weight matrix. Here R a and r T k produces a gate scalar rather than a gate vector. There is no subsequent contextualizing module thus r T k can be directly integrated into R a without fine-tuning for adaption. In Sec. IV-F, we investigate the effect of different knowledge gates used here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Aspect-related Semantics Aggregation</head><p>Here we employ an Aspect-to-Context Attention (A2C Att) mechanism to aggregate the aspect-related semantics retained in all hidden states into a final representation R f . Similar to A2D Att, A2C Att can be formulated as:</p><formula xml:id="formula_16">? i =SoftMax(F(h i c , R a )) (11) F(h i c , R a ) = (W ac h i c + b ac ) (R a ) T (12) R f = N i=1 ? i h i c<label>(13)</label></formula><p>where W ac and b ac are weight and bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Sentiment Classification</head><p>We concatenate R f with h cls and then fed the final vector into a linear layer, which is followed by a SoftMax classifier for prediction:</p><formula xml:id="formula_17">P = SoftMax(W p [h cls , R f ] + b p )<label>(14)</label></formula><p>where P is the predicted sentiment distribution, W p and b p are weight matrix and bias. The cross-entropy loss function is adopted for model training.</p><p>There are two reasons why we introduce h cls here. First, this can add a skip connection to BERT, shortening its loss back-propagation path to facilitate training. The second is for robustness. Possibly the syntax graphs are imperfect and the integrated knowledge contains noise. Hence h cls serves as a reference and makes the whole model more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We conduct experiments on three popular datasets for the ASC task: Lap14 and Res14 datasets are from SemEval 2014 task 4 <ref type="bibr" target="#b6">[7]</ref>, and Res15 dataset is from SemEval 2015 task 12 <ref type="bibr" target="#b46">[43]</ref>. The statistics of all datasets are presented in <ref type="table" target="#tab_1">Table I</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Setup</head><p>We adopt the BERT-base uncased version <ref type="bibr" target="#b19">[18]</ref>. We train our model using Adam optimizer <ref type="bibr" target="#b47">[44]</ref> with default configuration. The hyper-parameters are listed in <ref type="table" target="#tab_1">Table II</ref>. Accuracy (Acc) and Macro-F1 (F1) are adopted as evaluation metrics. As there is no official validation set, following previous works <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b48">[45]</ref>, we run our model three times with random initialization and report the average results on test sets, as shown in <ref type="table" target="#tab_1">Table  III</ref>. And to compare with the works reporting best results, we also report the best results on test sets, as shown in <ref type="table" target="#tab_1">Table IV</ref>. All computations are done on an NVIDIA Quadro RTX 6000 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compared Baselines</head><p>According to what kinds of external information are utilized, we divide the baselines into several group: 1) No external information is used:</p><p>? IAN <ref type="bibr" target="#b9">[9]</ref> separately encodes the aspect and context, then model their interactions using an interactive attention mechanism. 2) External corpus is used:</p><p>? PRET+MULT <ref type="bibr" target="#b25">[23]</ref> first pre-trains the model on documentlevel task, then trains the model on both document-level sentiment classification and ASC in the multi-task learning framework. ? TransCap <ref type="bibr" target="#b35">[32]</ref> utilizes a devised aspect-based capsule network to transfer knowledge from document-level task to aspect-level task. 3) Syntax Graph is used:</p><p>? ASGCN [12] employs a GCN to encode the syntax graph for capturing local syntactic information. ? BiGCN <ref type="bibr" target="#b48">[45]</ref> convolutes over hierarchical syntactic and lexical graphs to encode not only original syntactic information but also the corpus level word co-occurrence information. 4) BERT encoder is used:</p><p>? BERT-SPC <ref type="bibr" target="#b19">[18]</ref> takes the same input as our model and use h cls for sentiment classification. ? AEN-BERT <ref type="bibr" target="#b49">[46]</ref> adopts BERT encoder and uses the attentional encoder network to model the interactions between the aspect and context. 5) Both of syntax graph and BERT encoder are used:</p><p>? R-GAT+BERT <ref type="bibr" target="#b15">[15]</ref> use the relational graph attention network to aggregate the global relational information from all context word into the aspect node representation.   the comprehensive representation from different T-GCN layers. ? SAGAT <ref type="bibr" target="#b14">[14]</ref> utilizes graph attention network and BERT to fully obtain both syntax and semantic information. ? KGCapsAN-BERT <ref type="bibr" target="#b52">[49]</ref> utilizes multi-prior knowledge to guide the capsule attention process and use a GCN-based syntactic layer to integrate the syntactic knowledge. And we label all models with what kinds of external information they leverage, as shown in <ref type="table" target="#tab_1">Table III and Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Main Results</head><p>The performance comparison of all models on average scores is shown in <ref type="table" target="#tab_1">Table III</ref>, and the comparison on best scores is shown in <ref type="table" target="#tab_1">Table IV</ref>. We can observe that: Syntax graphs, external training corpus, and BERT can all improve ASC. Especially, simple BERT-SPC significantly outperforms all models that do not adopt BERT, even if some of them leverage syntax graph and external training corpus. This shows the power of pre-trained language models on ASC. And combining BERT and syntactic information can further improve results as sufficient semantics captured by BERT and the syntactic information conveyed by syntax graphs can cooperate to assist ASC. However, all baselines do not leverage aspect knowledge and only consider either local syntactic information or global relational information. As a result, their derived aspect representation lack some important clues of aspect and their captured syntactic information is insufficient, leading to their inferior performance compared to our KaGRMN-DSG model.</p><p>We obtain consistent improvements over baselines in terms of Acc and F1 on all datasets, achieving new state-of-the-art results. On average results, our KaGRMN-DSG overpasses previous best results by 1.92%, 1.46%, and 4.74% in terms of Macro-F1 on Lap14, Res14, and Res15 datasets respectively. On best results, KaGRMN-DSG overpasses previous best results by 3.10%, 1.49%, and 5.09% in terms of Macro-F1 on Lap14, Res14, and Res15 datasets respectively. The improvements are contributed by the superiorities of KaGR-MN, which effectively leverage beneficial aspect knowledge, and DSG-Net, which combines GCN and Relational MHA to capture sufficient syntactic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We empirically analyze KaGRMN-DSG and prove the necessity of every component by conducting an ablation study, whose results are shown in <ref type="table" target="#tab_5">Table V</ref>. In this section we answer the following research questions (RQs): Effect of Aspect Knowledge. To study the pure impact of aspect knowledge, we devise two variants: M 1 and M 2 . In M 1 , the original description is replaced with the aspect itself. In this case, there is no aspect knowledge available for KaGR-MN and its function becomes modeling the interactions between the aspect and context. Surprisingly, even without knowledge, M 2 can obtain promising results. We attribute this to the advanced architecture and effective functions of KaGR-MN, in which aspect and context are separately encoded and their interactions are effectively modeled by KaGR-MN. On the other hand, the performance degradation of M 0 convincingly demonstrates the pure improvements contributed by the aspect knowledge conveyed by aspect descriptions. In M 2 , DSG-Net, KI Gate, and A2C Att are all removed, so M 2 has a BERT+KaGR-MN architecture and the final aspect representation is used for prediction. M 2 consistently outperforms baselines, proving that KaGR-MN can derive a good enough aspect representation in which the clues for aspect sentiment reasoning are retained. Along time steps, recurrently leveraging aspect knowledge, KaGR-MN can capture more and more beneficial clues, semantics and dependencies then retain them in aspect representation and context memories. And effectively utilizing beneficial aspect knowledge is the key advantage of our method compared with previous works.</p><p>Effect of Syntactic Information. The results gap of M 3 and M 0 shows the improvement DSG-Net achieves by cooperating with the aspect knowledge. These results validate the advantages of combining both kinds of syntactic information to capture sufficient syntactic information. We then study the effects of Position-aware GCN and Relational MHA. We can observe that both M 4 and M 5 perform worse than M 0 , proving both the local syntactic information and global relational information should be captured for ASC. In previous works, only either one of them is considered, leading to insufficient syntactic information.</p><p>In contrast, our model marries them and lets them compensate for each other, sufficiently capturing syntactic clues.</p><p>Effect of Knowledge Integration Gate. Without KI Gate, M 6 obtains worse results than M 0 . This indicates that after DSG-Net, some aspect knowledge is further needed and KI Gate is efficient to re-enhance the final aspect representation with the needed knowledge.</p><p>Effect of Aspect-to-Context Attention. In M 7 , the final aspect representation is used for prediction. We can find that M 7 has limited performance degradation compared to M 0 . This proves that although previous modules can discover and extract clues for ASC, there are still important clues contained in non-aspect hidden states rather than final aspect representation. Hence it is necessary to employ A2C Att to aggregate the aspect-related semantics in all hidden states into the final representation.</p><p>Effect of A2D Att and Self MHA in KaGR-MN Cell. The significant performance decrease of M 8 shows that A2D Att is indispensable to dynamically summarize the specifically needed aspect knowledge from M D . Without Self MHA, the integrated knowledge in aspect representation can not be contextualized and context memories cannot be updated. As a result, M 9 performs much worse than M 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Investigation on Knowledge Gates</head><p>KaGRMN-DSG has two different knowledge gates (AdaKI and KI) for knowledge integrating. Here we empirically investigate these two knowledge gates by testing their four different settings. The results are shown in <ref type="table" target="#tab_1">Table VI</ref>. We can find that M 10 and M 12 have slight decreases in performances when respectively compared with M 0 and M 11 . This is because KI Gate can preserve the knowledge in r T k while AdaKI Gate may lose some knowledge when adapting to the semantic space of R a . M 11 and M 12 perform much worse than M 0 and M 10 . This is because the semantic space adaption of AdaKI Gate in KaGR-MN can maintain the semantics consistency of r t * a and M t?1 C , which is crucial for subsequent knowledge contextualizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Impact of Time Step Number T</head><p>We plot the performance trends of KaGRMN-DSG with increasing T on the three datasets, as presented in <ref type="figure" target="#fig_2">Fig. 3</ref>. We can observe that the performances show a trend of increases at first and then decreases. And the best result is obtained when T is 2 or 3 for Res15 and 4 for Lap14 and Res14. This shows that appropriately increasing T can gradually improve the results, which is consistent with our expectation. This can also prove the effectiveness of the recurrent manner of KaGR-MN. However, too large T leads to inferior performances,   which is also consistent with our expectation. One possible explanation is that too much knowledge integrated into the aspect representation and context memories will harm their original contextual information. Another is that too many recurrent steps will lead to overfitting on training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Case Study</head><p>We show some cases in <ref type="table" target="#tab_1">Table VII</ref>. Note that the only difference between KaGRMN-DSG (M 0 ) and M 1 is that the input D in M 1 is replaced with A. We can observe that M 0 can accurately predict the correct labels in all cases, while M 1 fails all cases although its overall performance is promising (as shown in <ref type="table" target="#tab_5">Table V)</ref> Without leveraging aspect knowledge, the aspect representation and semantics derived by M 1 are inadequate. As shown in <ref type="table" target="#tab_1">Table VIII</ref>, BERT cannot capture the exact meanings and properties of Mountain Lion OS and iTune, although it is one of the strongest language models. In Case 1, M 1 regards Mountain Lion OS as 'lion' which is 'dangerous'. Then considering 'not hard', M 1 is confused on P and O. In contrast, leveraging aspect knowledge, M 0 captures the exact meaning: an operating system. Then considering the aspect-related semantics ('not hard'), M 0 correctly predicts P. In Case 2, the aspect sentiment expression is a little obscure as there are no explicit sentiment trigger words (e.g. delicious, good, expensive). Even if M 1 captures aspect-related context semantics, it fails due to the lack of property information of iTune. Thanks to the integrated aspect knowledge, M 0 is aware that iTune is primarily used for media playing rather than selling products, thus correctly predicts N.</p><p>Looking into Case 3 and Case 4, we can find that due to the lack of aspect knowledge, M 1 is prone to be affected by some misleading sentiment trigger words: 'best' in case 3 and 'but' in case 4. The reason why M 0 wins M 1 is that M 0 can combine the aspect knowledge and the aspect-related semantics together to capture the correct clues for ASC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Computation Time Analysis</head><p>The comparison of time costing and avg F1 of BERT-SPC, BERT+T-GCN and our KaGRMN-DSG model is shown in <ref type="table" target="#tab_1">Table IX</ref>. We can find that although our model demands more training time and inference time than BERT-SPC, it overpasses BERT-SPC on avg F1 by a large margin (6.3%). As for BERT+T-GCN, which is the best-performing baseline, although it costs lightly less inference time than our KaGRMN-DSG, it costs much more time for training, and more importantly, its performance is significantly inferior to us. Additionally, since Local Syntactic Information Modeling and Global Relational Information Modeling both take the output of KaGRMN as input, they can be parallelized theoretically, so the training time and inference time of our KaGRMN-DSG model can be further reduced in practice. In a word, our model may cost more time for training and inference than some baseline models, but it is worthy considering the significant performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we point out the two challenges encountering existing ASC models and we therefore propose a novel KaGRMN-DSG model to end-to-end embed and leverage aspect knowledge, then capture sufficient syntactic information by marrying both kinds of syntactic information. In our model, the integrated beneficial aspect knowledge and sufficient syntactic information can effectively cooperate, yielding new state-ofthe-art results.</p><p>Future directions include exploring the visual knowledge of aspects, as well as designing deeper and more sufficient dual syntactic interaction to let the two kinds of syntactic information interact with each other in their respective modeling processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of KaGRMN-DSG. The internal architecture of KaGR-MN cell is shown inFig.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? DGEDT-BERT<ref type="bibr" target="#b13">[13]</ref> employs a dual-transformer network to model the interactions between the flat textual knowledge and dependency graph empowered knowledge. ? A-KVMN+BERT<ref type="bibr" target="#b50">[47]</ref> uses a key-value memory network to leverage not only word-word relations but also their dependency types. ? BERT+T-GCN<ref type="bibr" target="#b51">[48]</ref> leverages the dependency types in T-GCN and use an attentive layer ensemble to learn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Impact of the time step number T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DATASET</head><label>I</label><figDesc>STATISTICS OF THE THREE DATASETS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Positive</cell><cell cols="2">Neutral</cell><cell cols="2">Negative</cell></row><row><cell></cell><cell>Train</cell><cell>Test</cell><cell>Train</cell><cell>Test</cell><cell>Train</cell><cell>Test</cell></row><row><cell>Lap14</cell><cell>994</cell><cell>341</cell><cell>464</cell><cell>169</cell><cell>870</cell><cell>128</cell></row><row><cell>Res14</cell><cell>2164</cell><cell>728</cell><cell>637</cell><cell>196</cell><cell>807</cell><cell>196</cell></row><row><cell>Res15</cell><cell>912</cell><cell>326</cell><cell>36</cell><cell>34</cell><cell>256</cell><cell>182</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II SETTING</head><label>II</label><figDesc>OF HYPER-PARAMETERS.</figDesc><table><row><cell>Hyper-params</cell><cell>Lap14</cell><cell>Dataset Res14</cell><cell>Res15</cell></row><row><cell>learning rate</cell><cell cols="3">1 ? 10 ?5 5 ? 10 ?5 3 ? 10 ?5</cell></row><row><cell>batch size</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>dropout rate</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>de</cell><cell>768</cell><cell>768</cell><cell>768</cell></row><row><cell>ds</cell><cell>256</cell><cell>256</cell><cell>128</cell></row><row><cell>H s n</cell><cell>3</cell><cell>3</cell><cell>6</cell></row><row><cell>H d n</cell><cell>2</cell><cell>4</cell><cell>6</cell></row><row><cell>T</cell><cell>4</cell><cell>4</cell><cell>2</cell></row><row><cell>GCN layer number</cell><cell>2</cell><cell>2</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCES</head><label>III</label><figDesc>COMPARISONS OF AVERAGE RESULTS WITH RANDOM INITIALIZATION. K, B, T AND G DENOTE THE MODEL LEVERAGES ASPECT KNOWLEDGE, BERT, EXTRA T RAINING CORPUS AND SYNTAX GRAPH, RESPECTIVELY. BEST RESULTS ARE IN BOLD AND PREVIOUS SOTA RESULTS ARE UNDERLINED. * DENOTES THAT WE PRODUCE THE RESULTS USING THEIR ORIGINAL SOURCE CODES. ? INDICATES KAGRMN-DSG SIGNIFICANTLY OUTPERFORMS BASELINES UNDER T-TEST (p &lt; 0.01).</figDesc><table><row><cell>External Information</cell><cell>Model</cell><cell>Acc</cell><cell cols="2">Lap14</cell><cell>F1</cell><cell>Acc</cell><cell cols="2">Res14</cell><cell>F1</cell><cell>Acc</cell><cell>Res15</cell><cell>F1</cell></row><row><cell>? ? ?</cell><cell>IAN [9]</cell><cell>72.05</cell><cell></cell><cell></cell><cell>67.38</cell><cell>79.26</cell><cell></cell><cell></cell><cell>70.09</cell><cell>78.54</cell><cell>52.65</cell></row><row><cell>? T ?</cell><cell>PRET+MULT [23]</cell><cell>71.15</cell><cell></cell><cell></cell><cell>67.46</cell><cell>79.11</cell><cell></cell><cell></cell><cell>69.73</cell><cell>81.30</cell><cell>68.74</cell></row><row><cell>? T ?</cell><cell>TransCap [32]</cell><cell>73.51</cell><cell></cell><cell></cell><cell>69.81</cell><cell>79.55</cell><cell></cell><cell></cell><cell>71.41</cell><cell>-</cell><cell>-</cell></row><row><cell>? ? G</cell><cell>ASGCN [12]</cell><cell>75.55</cell><cell></cell><cell></cell><cell>71.05</cell><cell>80.77</cell><cell></cell><cell></cell><cell>72.02</cell><cell>79.89</cell><cell>61.89</cell></row><row><cell>? ? G</cell><cell>BiGCN [45]</cell><cell>74.59</cell><cell></cell><cell></cell><cell>71.84</cell><cell>81.97</cell><cell></cell><cell></cell><cell>73.48</cell><cell>81.16</cell><cell>64.79</cell></row><row><cell>? B ?</cell><cell>BERT-SPC  *  [18]</cell><cell>78.47</cell><cell></cell><cell></cell><cell>73.67</cell><cell>84.94</cell><cell></cell><cell></cell><cell>78.00</cell><cell>83.40</cell><cell>65.00</cell></row><row><cell>? B ?</cell><cell>AEN-BERT [46]</cell><cell>79.93</cell><cell></cell><cell></cell><cell>76.31</cell><cell>83.12</cell><cell></cell><cell></cell><cell>73.76</cell><cell>-</cell><cell>-</cell></row><row><cell>? B G</cell><cell>R-GAT+BERT  *  [15]</cell><cell>79.31</cell><cell></cell><cell></cell><cell>75.40</cell><cell>86.10</cell><cell></cell><cell></cell><cell>80.04</cell><cell>83.95</cell><cell>69.47</cell></row><row><cell>? B G</cell><cell>DGEDT-BERT [13]</cell><cell>79.8</cell><cell></cell><cell></cell><cell>75.6</cell><cell>86.3</cell><cell></cell><cell></cell><cell>80.0</cell><cell>84.0</cell><cell>71.0</cell></row><row><cell>? B G</cell><cell>A-KVMN+BERT  *  [47]</cell><cell>79.20</cell><cell></cell><cell></cell><cell>75.76</cell><cell>85.89</cell><cell></cell><cell></cell><cell>78.29</cell><cell>83.89</cell><cell>67.88</cell></row><row><cell>? B G</cell><cell>BERT+T-GCN  *  [48]</cell><cell>80.56</cell><cell></cell><cell></cell><cell>76.95</cell><cell>85.95</cell><cell></cell><cell></cell><cell>79.40</cell><cell>84.81</cell><cell>71.09</cell></row><row><cell>K B G</cell><cell>KaGRMN-DSG (Ours)</cell><cell cols="2">81.87  ?</cell><cell cols="2">78.43  ?</cell><cell cols="2">87.35  ?</cell><cell cols="2">81.21  ?</cell><cell cols="2">86.59  ?</cell><cell>74.46  ?</cell></row><row><cell></cell><cell>Our Improvements</cell><cell cols="2">1.62%</cell><cell cols="2">1.92%</cell><cell cols="2">1.22%</cell><cell cols="2">1.46%</cell><cell cols="2">2.10%</cell><cell>4.74%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCES</head><label>IV</label><figDesc>COMPARISONS OF BEST RESULTS. K, B, T AND G DENOTE THE MODEL LEVERAGES ASPECT KNOWLEDGE, BERT, EXTRA T RAINING CORPUS AND SYNTAX GRAPH, RESPECTIVELY. BEST RESULTS ARE IN BOLD AND PREVIOUS SOTA RESULTS ARE UNDERLINED. * DENOTES THAT WE PRODUCE THE RESULTS USING THEIR ORIGINAL SOURCE CODES.</figDesc><table><row><cell>External Information</cell><cell>Model</cell><cell>Acc</cell><cell cols="2">Lap14</cell><cell>F1</cell><cell>Acc</cell><cell cols="2">Res14</cell><cell>F1</cell><cell>Acc</cell><cell>Res15</cell><cell>F1</cell></row><row><cell>? B ?</cell><cell>BERT-SPC  *  [18]</cell><cell>78.84</cell><cell></cell><cell></cell><cell>73.95</cell><cell>85.80</cell><cell></cell><cell></cell><cell>78.48</cell><cell>83.76</cell><cell>68.33</cell></row><row><cell>? B G</cell><cell>SAGAT [14]</cell><cell>80.37</cell><cell></cell><cell></cell><cell>76.94</cell><cell>85.08</cell><cell></cell><cell></cell><cell>77.94</cell><cell>-</cell><cell>-</cell></row><row><cell>? B G</cell><cell>KGCapsAN-BERT [49]</cell><cell>79.47</cell><cell></cell><cell></cell><cell>76.61</cell><cell>85.36</cell><cell></cell><cell></cell><cell>79.00</cell><cell>-</cell><cell>-</cell></row><row><cell>? B G</cell><cell>R-GAT+BERT  *  [15]</cell><cell>79.46</cell><cell></cell><cell></cell><cell>75.75</cell><cell>86.61</cell><cell></cell><cell></cell><cell>80.78</cell><cell>84.13</cell><cell>71.12</cell></row><row><cell>? B G</cell><cell>A-KVMN+BERT [47]</cell><cell>79.78</cell><cell></cell><cell></cell><cell>76.14</cell><cell>85.98</cell><cell></cell><cell></cell><cell>77.94</cell><cell>84.14</cell><cell>68.49</cell></row><row><cell>? B G</cell><cell>BERT+T-GCN [48]</cell><cell>80.88</cell><cell></cell><cell></cell><cell>77.03</cell><cell>86.16</cell><cell></cell><cell></cell><cell>79.95</cell><cell>85.26</cell><cell>71.69</cell></row><row><cell>K B G</cell><cell>KaGRMN-DSG (Ours)</cell><cell>82.13</cell><cell></cell><cell></cell><cell>79.42</cell><cell>87.68</cell><cell></cell><cell></cell><cell>81.98</cell><cell>87.08</cell><cell>75.34</cell></row><row><cell></cell><cell>Our Improvements</cell><cell cols="2">1.55%</cell><cell cols="2">3.10%</cell><cell cols="2">1.24%</cell><cell cols="2">1.49%</cell><cell cols="2">2.13%</cell><cell>5.09%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF ABLATION STUDY.</figDesc><table><row><cell>Variants</cell><cell>Lap14 Acc</cell><cell>Res14 Acc</cell><cell>Res15 Acc</cell></row><row><cell>M 0 : KaGRMN-DSG (full model)</cell><cell>81.87</cell><cell>87.35</cell><cell>86.59</cell></row><row><cell>M 1 : w/o Aspect Knowledge (descriptions are replaced with aspects)</cell><cell>80.30 (? 1.57)</cell><cell>86.43 (? 0.92)</cell><cell>85.24 (? 1.35)</cell></row><row><cell>M 2 : only KaGRMN (w/o DSG-Net + KI Gate + A2C Att)</cell><cell>80.72 (? 1.15)</cell><cell>86.55 (? 0.80)</cell><cell>85.36 (? 1.23)</cell></row><row><cell>M 3 : w/o DSG-Net</cell><cell>80.56 (? 1.31)</cell><cell>86.43 (? 0.92)</cell><cell>85.56 (? 1.03)</cell></row><row><cell>M 4 : w/o Relational MHA</cell><cell>80.98 (? 0.89)</cell><cell>86.67 (? 0.68)</cell><cell>85.79 (? 0.8)</cell></row><row><cell>M 5 : w/o Position-aware GCN</cell><cell>81.03 (? 0.84)</cell><cell>86.76 (? 0.59)</cell><cell>85.67 (? 0.92)</cell></row><row><cell>M 6 : w/o KI Gate</cell><cell>81.09 (? 0.78)</cell><cell>87.11 (? 0.24)</cell><cell>85.79 (? 0.80)</cell></row><row><cell>M 7 : w/o A2C Att</cell><cell>81.50 (? 0.37)</cell><cell>87.00 (? 0.35)</cell><cell>86.41 (? 0.18)</cell></row><row><cell>M 8 : w/o A2D Att</cell><cell>80.93 (? 0.94)</cell><cell>86.70 (? 0.65)</cell><cell>85.61 (? 0.98)</cell></row><row><cell>M 9 : w/o Self MHA</cell><cell>80.36 (? 1.51)</cell><cell>86.46 (? 0.89)</cell><cell>85.24 (? 1.35)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>OF DIFFERENT KNOWLEDGE GATE SETTINGS.</figDesc><table><row><cell>Variants</cell><cell>Gate 1</cell><cell>Gate 2</cell><cell>Lap14 Acc</cell><cell>Res14 Acc</cell><cell>Res15 Acc</cell></row><row><cell>M 0</cell><cell>AdaKI</cell><cell>KI</cell><cell>81.87</cell><cell>87.35</cell><cell>86.59</cell></row><row><cell>M 10</cell><cell>AdaKI</cell><cell>AdaKI</cell><cell>81.50 (? 0.37)</cell><cell>87.05 (? 0.30)</cell><cell>85.98 (? 0.61)</cell></row><row><cell>M 11</cell><cell>KI</cell><cell>KI</cell><cell>81.09 (? 0.78)</cell><cell>86.73 (? 0.62)</cell><cell>85.36 (? 1.23)</cell></row><row><cell>M 12</cell><cell>KI</cell><cell>AdaKI</cell><cell>81.03 (? 0.84)</cell><cell>86.58 (? 0.77)</cell><cell>85.24 (? 1.35)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII CASES</head><label>VII</label><figDesc>DEMONSTRATION. [N, P, O] DENOTES PREDICTED SENTIMENT DISTRIBUTION: [NEGATIVE, POSITIVE, NEUTRAL]. The [Mountain Lion OS] A is not hard to figure out if you are familiar with Microsoft Windows. D: OS X Mountain Lion is ... Apple Inc.'s desktop and server operating system ... On start up it asks endless questions just so [iTune] A can sell you more of their products. D: iTunes is a media player, media library, Internet radio broadcaster, mobile device management utility ... While the [smoothies]A are a little big for me, the fresh juices are the best i have ever had! D: A smoothie is a drink made from pureed raw fruit and/or vegetables, typically using a blender ... All the various Greek and Cypriot dishes are excellent, but the [gyro] A is the reason to come -if you don't eat one your trip was wasted. D: A gyro or gyros is a Greek dish made from meat cooked on a ...</figDesc><table><row><cell cols="2">Case</cell><cell>[N, P, O]</cell></row><row><cell>1.</cell><cell cols="2">C: M0: [0.0, 0.999 , 0.001] M1: [0.01, 0.49 ? , 0.5]</cell></row><row><cell>2.</cell><cell cols="2">C: M0: [0.57 , 0.41, 0.02] M1: [0.03, 0.67 ? , 0.30]</cell></row><row><cell>3.</cell><cell cols="2">C: M0: [0.62 , 0.0, 0.38] M1: [0.02, 0.97 ? , 0.01]</cell></row><row><cell>4.</cell><cell cols="2">C: M0: [0.02, 0.98 , 0.0] M1: [0.88 ? , 0.11, 0.01]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII MISUNDERSTANDING</head><label>VIII</label><figDesc>FROM BERT PRESENTED BY SEMANTIC COSINE SIMILARITY (S). v IS THE AVERAGE OF ENTITY'S HIDDEN STATES. a i DENOTES THE A IN CASE i. TRAINING TIME AND INFERENCE TIME (PER SAMPLE) AS WELL AS THE AVG F1 ON THE THREE DATASETS.</figDesc><table><row><cell>Entity (e)</cell><cell>S(ve, va 1 )</cell><cell>Entity (e)</cell><cell>S(ve, va 2 )</cell></row><row><cell>lion</cell><cell>0.8516</cell><cell>media player</cell><cell>0.4720</cell></row><row><cell>mountain</cell><cell>0.7997</cell><cell>radio broadcaster</cell><cell>0.5887</cell></row><row><cell>operating system</cell><cell>0.6826</cell><cell>software</cell><cell>0.7051</cell></row><row><cell>dangerous animal</cell><cell>0.8272</cell><cell>utility</cell><cell>0.6982</cell></row><row><cell></cell><cell cols="2">TABLE IX</cell><cell></cell></row><row><cell>COMPARISON OF Models</cell><cell cols="3">Training Time? Inference Time? Avg F1?</cell></row><row><cell>BERT-SPC</cell><cell>0.007309s</cell><cell>0.002219s</cell><cell>73.59%</cell></row><row><cell>BERT+T-GCN</cell><cell>0.033835s</cell><cell>0.003350s</cell><cell>76.22%</cell></row><row><cell>KaGRMN-DSG</cell><cell>0.015333s</cell><cell>0.004208s</cell><cell>78.91%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://wiki.dbpedia.org/ 2 https://www.wikipedia.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use Glove word embedding<ref type="bibr" target="#b37">[34]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS This work was supported by Australian Research Council</head><p>Grant (DP180100106 and DP200101328). Ivor W. Tsang was also supported by A * STAR Centre for Frontier AI Research (CFAR).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1253</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive feature selection-based adaboost-knn with direct optimization for dynamic emotion recognition in human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning class-aligned and generalized domain-invariant representations for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="489" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional mkl based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/icdm/icdm2016.html#PoriaCCH16" />
		<editor>ICDM, F. Bonchi, J. Domingo-Ferrer, R. Baeza-Yates, Z.-H. Zhou, and X. Wu</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time speech emotion and sentiment recognition for interactive dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D16-1110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1042" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryland</forename><surname>Baltimore</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P14-2009" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/568</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/568" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<editor>C. Sierra, Ed. ijcai.org</editor>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-grained attention network for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3433" to="3442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive self-supervised attention learning for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1053" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment classification with aspect-specific graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1464" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4568" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.588" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntax-aware graph attention network for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.69" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="799" to="810" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3229" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspectlevel sentiment analysis via convolution over dependency tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1569" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5679" to="5688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denmark</forename><surname>Copenhagen</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1047" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Target-dependent Twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P11-1016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/S13-2053" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Texas</forename><surname>Austin</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D16-1058" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C16-1311" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
	<note>The COLING</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-2092" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="579" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parameterized convolutional neural networks for aspect level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Carley</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Australia</forename><surname>Melbourne</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1234" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Target-sensitive memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-1088" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="957" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; A</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gangemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hollink</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/esws/eswc2018.html#SchlichtkrullKB18" />
	</analytic>
	<monogr>
		<title level="m">ESWC, ser. Lecture Notes in Computer Science</title>
		<editor>A. Tordai, and M. Alam</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Durable relationship prediction and description using a large dynamic graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/www/www21.html#WangJS18" />
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1575" to="1600" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Syntax-aware aspect level sentiment classification with graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Carley</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1549" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5469" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transfer capsule network for aspect level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Earlier attention? aspect-aware LSTM for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/738</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/738" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
		<editor>S. Kraus, Ed. ijcai.org</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5313" to="5319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno>arxiv:1410.3916</idno>
		<ptr target="http://arxiv.org/abs/1410.3916" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Light gated recurrent units for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJCjUqxe" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6087" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dcr-net: A deep co-interactive relation network for joint dialog act recognition and sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8665" to="8672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.286" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Enhancing aspect-level sentiment analysis with word dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04" />
			<biblScope unit="page" from="3726" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2910" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Knowledge guided capsule attention network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/taslp/taslp28.html#ZhangLXLCY20" />
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2538" to="2551" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">He is currently a second year Ph.D student at Australian AI Institute, University of Technology Sydney (UTS)</title>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Bowen Xing received his B.E. degree and Master degree from Beijing Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>His research focuses on graph neural network, multi-task learning. sentiment analysis and dialog system</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tsang is an IEEE Fellow and the Director of A*STAR Centre for Frontier AI Research (CFAR)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ivor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">His research focuses on transfer learning, deep generative models, learning with weakly supervision, big data analytics for data with extremely high dimensions in features, samples and labels. His work is recognised internationally for its outstanding contributions to those fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Previously</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020, he was recognized as the AI 2000 AAAI/IJCAI Most Influential Scholar in Australia for his outstanding contributions to the field, between 2009 and 2019. His research on transfer learning was awarded the Best Student Paper Award at CVPR 2010 and the 2014 IEEE TMM Prize Paper Award. In addition, he received the IEEE TNN Outstanding</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>at University of Technology Sydney (UTS), and Research Director of the Australian Artificial Intelligence Institute (AAII</orgName>
		</respStmt>
	</monogr>
	<note>2019, his JMLR paper. Paper Award in 2007 for his innovative work on solving the inverse problem of non-linear representations. Recently, Prof Tsang was conferred the IEEE Fellow for his outstanding contributions to large-scale machine learning and transfer learning</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">He serves as a Senior Area Chair/Area Chair for NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mlj</forename><surname>Jmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jair</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieee</forename><surname>Tpami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieee</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tbd</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieee</forename><surname>Tetci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, AAAI and IJCAI, and the steering committee of ACML</title>
		<imprint/>
	</monogr>
	<note>Prof Tsang serves as the Editorial Board for the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
