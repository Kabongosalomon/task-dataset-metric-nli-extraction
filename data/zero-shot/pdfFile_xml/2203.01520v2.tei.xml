<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Open Challenge for Inductive Link Prediction on Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
							<email>mikhail.galkin@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<settlement>Mila</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
							<email>berrendorf@dbs.ifi.lmu.de</email>
							<affiliation key="aff1">
								<orgName type="institution">LMU Munich M?nchen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
							<email>cthoyt@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Harvard Medical School Boston</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Open Challenge for Inductive Link Prediction on Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACM Reference Format: Mikhail Galkin, Max Berrendorf, and Charles Tapley Hoyt. 2022. An Open Challenge for Inductive Link Prediction on Knowledge Graphs. In Pro-ceedings of TheWebConf Workshop on Graph Learning Benchmarks 2022 (WWW22). ACM, New York, NY, USA, 5 pages. https://doi.org/XXXXXXX. XXXXXXX</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Neural networks; Knowledge representation and reasoning KEYWORDS Knowledge graph</term>
					<term>dataset</term>
					<term>neural networks</term>
					<term>link prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An emerging trend in representation learning over knowledge graphs (KGs) moves beyond transductive link prediction tasks over a fixed set of known entities in favor of inductive tasks that imply training on one graph and performing inference over a new graph with unseen entities. In inductive setups, node features are often not available and training shallow entity embedding matrices is meaningless as they cannot be used at inference time with unseen entities. Despite the growing interest, there are not enough benchmarks for evaluating inductive representation learning methods. In this work, we introduce ILPC 2022, a novel open challenge on KG inductive link prediction.</p><p>To this end, we constructed two new datasets based on Wikidata with various sizes of training and inference graphs that are much larger than existing inductive benchmarks. We also provide two strong baselines leveraging recently proposed inductive methods. We hope this challenge helps to streamline community efforts in the inductive graph representation learning area. ILPC 2022 follows best practices on evaluation fairness and reproducibility, and is available at https://github.com/pykeen/ilpc2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The transductive link prediction (LP) task over knowledge graphs (KGs) has dominated the evaluation of knowledge graph embedding models (KGEMs) since their first appearance in 2011 <ref type="bibr" target="#b16">[17]</ref>. Because the same graph is used for training and inference in the transductive Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WWW22, <ref type="bibr">April 26, 2022</ref>  setting, a majority of KGEMs use the shallow embedding paradigm in which a learnable (or infrequently, a pre-computed) vector is assigned to each entity seen during training. Most large benchmarking studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref> focus solely on the transductive LP task following the shallow embedding paradigm.</p><p>In the inductive setting, a different graph can be used for inference that either contains a combination of seen and unseen entities (i.e., semi-inductive) or only unseen entities (i.e., fully inductive) and either the same or a subset of relations from the training graph <ref type="bibr" target="#b0">[1]</ref>. During evaluation, there are two scoring tasks between entities seen and unseen during training: seen ? unseen and unseen ? unseen. In this work, we focus on the fully inductive setting where node features are not given and links to be predicted fall into the unseen ? unseen category ( <ref type="figure" target="#fig_0">Figure 1</ref>). Because the presence of unseen entities in the inference graph is fundamentally incompatible with the shallow embedding paradigm, several non-shallow methods compatible with the inductive setting have been recently proposed <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>, many of which leverage recent advancements in graph neural networks and geometric deep learning <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this work, we outline the open Inductive Link Prediction Challenge (ILPC 2022) which aims to support community efforts to create more efficient and performant inductive link prediction models for KGs. With it, we introduce two novel inductive link prediction datasets of different sizes leveraging Wikidata <ref type="bibr" target="#b23">[24]</ref> along with two strong baselines employing the recently proposed NodePiece <ref type="bibr" target="#b11">[12]</ref> representation learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There are far fewer high quality inductive link prediction benchmarks than transductive link prediction benchmarks. For example, the Open Graph Benchmark <ref type="bibr" target="#b14">[15]</ref> contains two datasets for transductive link prediction, WikiKG2 and WikiKG90Mv2 <ref type="bibr" target="#b13">[14]</ref>, but none for inductive link prediction. Wikidata5M <ref type="bibr" target="#b24">[25]</ref> and inductive splits published in <ref type="bibr" target="#b9">[10]</ref> demonstrate encoding entities by applying a language model to their respective descriptions. They evaluate inductive LP performance only on the single triple level without an inference graph. Therefore, those tasks can be seen as a probe of textual encoders and richness of their features rather than evaluation of graph representation learning capabilities.</p><p>To the best of our knowledge, the only notable inductive LP benchmarks comprise inductive versions of three popular transductive LP datasets, FB15k-237 <ref type="bibr" target="#b21">[22]</ref>, WN18RR <ref type="bibr" target="#b10">[11]</ref>, and NELL-995 <ref type="bibr" target="#b7">[8]</ref> proposed in in <ref type="bibr" target="#b20">[21]</ref>. Each KG accommodates four splits of various training and inference graph sizes making overall 12 distinct datasets. Most of those datasets are rather small where the largest one has 27K training triples and 12K in the inference graph. While the modest sizes were originally motivated by the computational complexity of the proposed method, some aspects have been resolved in subsequent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>While our tasks draw inspiration from <ref type="bibr" target="#b20">[21]</ref>, the new ILPC 2022 datasets are different in several ways: 1) the graphs are excerpts from Wikidata, the largest publicly available KG actively maintained by a large community with numerous practical applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, hence, new results are more likely to be apprehended by a wider audience; 2) our largest graph is almost an order of magnitude larger than that of <ref type="bibr" target="#b20">[21]</ref> and its inference graph size is challenging for modern GNN methods with inductive capabilities; 3) we make sure training and inference graphs are connected components without dangling nodes; 4) our evaluation criteria assumes ranking over all nodes in the inference graph instead of 50 random samples as done in <ref type="bibr" target="#b20">[21]</ref>. We describe the dataset construction process and design decisions in more detail in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TASK DEFINITION</head><p>Let T(E, R) := {(?, , ) | ?, ? E, ? R} denote the set of possible triples between entities from E and relations from R. Let E ? denote the entities occurring in training and E ? the entities in the inference graph. Then, since we are in the fully inductive setting, we have a set of training triples T ? T(E ? , R) (comprising the training graph), and a set of inference, validation and test triples</p><formula xml:id="formula_0">T , T , T ? T(E ? , R)</formula><p>where T represents the inference graph. Note that sets of training and inference entities are disjoint, i.e., E ? ? E ? = ?. The inductive link prediction task is defined as training a model on T , running inference over a new graph T and predicting missing links in the inference graph.</p><p>We use the two-sided workflow for evaluation that comprises both the head prediction and tail prediction task, i.e., for each evaluation triple (?, , ) ? T , we consider the ranking tasks (?, , ?) and (?, , ), where we need to provide scores for each candidate entity ? E ? . We use realistic ranks, i.e., in case of ties, the assigned rank is the average of the best/smallest and the worst/largest rank in any ordering respecting the sort criterion <ref type="bibr" target="#b3">[4]</ref>. As single-figure aggregations of the individual ranks, we use the following metrics, all sharing that larger numbers indicate better performance, and with an optimal value of 1.</p><p>H@k. The Hits at (H@k) metric denotes the relative frequency of the correct entity's rank being at most , for different ? {1, 3, 5, 10, 100}. Its value range is [0, 1].</p><p>MRR. The mean reciprocal rank (MRR) is the inverse of the harmonic mean of the individual ranks. In contrast to H@k, which AMRI. The adjusted mean rank index (AMRI) <ref type="bibr" target="#b3">[4]</ref> is an affine transformation of the (arithmetic) mean rank, which is normalized for the chance effect: It is given as</p><formula xml:id="formula_1">= 1 ? ?1 E[ ]?1 ,</formula><p>where MR denotes the arithmetic mean of ranks, and E[ ] its expectation under random ordering. Thereby, it is implicitly normalized by the number of candidates, and thus allows comparison across cases with differing number of candidates, e.g., due to different number of entities, or filtered candidates. Its value range is [?1, 1], where 0 corresponds to a performance equal to the expected performance under random ordering of entities for the ranking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASET CONSTRUCTION</head><p>The design principles behind creating ILPC 2022 include that the dataset: i) should represent a real-world KG used in many practical downstream tasks; ii) should be challenging for modern inductive LP approaches and GNN architectures; iii) should be larger than existing benchmarks; iv) should allow for faster iteration and hypothesis testing; v) training and inference graphs should each be a connected component.</p><p>We chose CoDEx <ref type="bibr" target="#b18">[19]</ref>, a high-quality subset of Wikidata for transductive link prediction, as a source of triples then sampled two inductive datasets then using the following procedure: and test triples T from the inference graph T such that they only contain nodes from T .</p><p>Following this procedure, we sample two datasets, ILPC22-S (small) and ILPC22-L (large). For both datasets, the size of the inference graph is about 66% of the training graph in the number of entities and about 33% of the training graph in the number of triples. The sizes of the validation and test sets are about 10% of the inference graph. While the validation and test sets are openly available for participants comprising a public leaderboard, we also sample a hold-out test set of triples from the inference graph T for the final evaluation of model submissions. The hold-out test set is kept privately by the authors and is not publicly available in the challenge repository.</p><p>The smaller ILPC22-S is designed for faster hypothesis testing and model iteration and consists of 10K entities and 96 relations in the training graph with 6.6K entities in the inference graph. The larger ILPC22-L is a challenging inductive dataset with 46K training and 30K inference entities with 130 relations, where the size of the inference graph (77K triples) is comparable to the whole training graph of smaller ILPC22-S. The overall statistics of ILPC22-S and ILPC22-L are presented in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_2">Figure 2</ref>. We used the Zenodo integration with GitHub to assign a DOI to each release to maximize reproducibility. Users should report/cite the versioned DOI of the dataset they used for training along with their results, such as <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND BASELINES</head><p>We provide two strong baselines employing NodePiece <ref type="bibr" target="#b11">[12]</ref>, a recently proposed inductive graph representation learning method that learns a fixed-size vocabulary of tokens that remain invariant at training and inference time. This way, both seen and unseen nodes can be uniformly tokenized using the same basic vocabulary and their representations are obtained via encoding the resulting tokens set. With the fully inductive setup of ILPC 2022, the vocabulary consists of unique relation types, their inverses, and an auxiliary padding token making the vocabulary size |V | = 2 ? |R| + 1 constant   to the number of entities or total triples in graphs. For all baselines, NodePiece is configured to tokenize each entity with 5 unique outgoing relations and use a 2-layer MLP as a token encoder. The first baseline uses just the NodePiece tokenization with the MLP encoder to obtain entity representations to be sent to the scoring function. It is designed to be the faster method for hypothesis testing and model iteration achieving high recall at moderate computational costs. The second baseline adds a message passing GNN encoder on top of the obtained NodePiece representations to further enrich entity and relation representations. Here, we use a 2-layer CompGCN <ref type="bibr" target="#b22">[23]</ref> relational GNN architecture with the DistMult <ref type="bibr" target="#b25">[26]</ref> composition function. We denote the first plain NodePiece baseline as -GNN and the second GNN-enabled baseline as +GNN.</p><p>Both implemented baselines have the same internal dimension of 32 (we did not find any noticeable performance improvement when increasing the dimensionality) and use the same DistMult scoring function (decoder). All models were trained in the negative sampling setup with self-adversarial loss (NSSAL) <ref type="bibr" target="#b19">[20]</ref> using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> on a single RTX 8000 GPU. During training, models on ILPC22-S consumed at most 2 GB VRAM, and at most 3 GB on larger ILPC22-L. Other hyperparameters are listed in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Experiments were conducted using PyKEEN <ref type="bibr" target="#b2">[3]</ref> and reported in <ref type="table" target="#tab_4">Table 3</ref>. We observe that despite short training times, the plain NodePiece baselines (-GNN) already achieves relatively high recall in terms of H@100. We attribute that to the effect of relation-based entity tokenization, i.e., a sample of incident relations is discriminative enough for broad separation of possibly correct links which is captured by H@100. While the GNN-enabled baseline exhibits 4-10? performance improvements ILPC22-S, it does not demonstrate such high gains for ILPC22-L, suggesting that the large dataset still poses challenges for state of the art GNNs and methodological improvements are still needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Participation</head><p>We solicit submissions from both individual researchers and teams, the submission instructions and leaderboards are provided in the official repository https://github.com/pykeen/ilpc2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented ILPC 2022, a new open challenge on inductive link prediction on knowledge graphs. For this challenge, we sampled two new datasets from Wikidata larger than existing benchmarks. We make sure the target metrics are meaningful, interpretable, and follow the best reproducibility criteria. For starters, we provided two baselines leveraging recently proposed inductive models that are fast to train on one hand or achieve higher performance on the other hand. We invite more submissions for this challenge extending the baselines or proposing new inductive LP models where the authors could estimate their performance on the public leaderboard. The challenge is expected to run for 6 months and we plan to report the intermediate results of submitted models on the private test set at the GLB workshop at the WebConf 2022.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Transductive vs. inductive link prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>Join the training, validation, and test triples of the original transductive dataset into a single graph. (2) Sample a set of entities E ? for the inference graph. The remaining entities E ? comprise the training graph. (3) Sample subgraphs induced by E ? (training graph) and E ? (inference graph) from the total set of triples T . (4) Fix the set of relations R in the training graph and ensure the relations in the inference graph are a subset of R, otherwise remove the triple from the inference graph. (5) Remove disconnected nodes from both training and inference graphs to ensure the graphs are connected components. (6) Sample validation triples T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of relation type frequencies (without inverses) in training and inference graphs of ILPC22 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Virtual ? 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-XXXX-X/18/06. https://doi.org/XXXXXXX.XXXXXXX</figDesc><table><row><cell>Transductive link prediction</cell><cell cols="2">Inductive link prediction: new unseen inference graph</cell></row><row><cell>Links to predict</cell><cell></cell><cell>Links to predict</cell></row><row><cell>Training graph</cell><cell>Training graph</cell><cell>Inductive inference graph</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: Hyperparameters</cell><cell></cell></row><row><cell></cell><cell cols="2">ILPC22-S</cell><cell cols="2">ILPC22-L</cell></row><row><cell cols="5">Metric -GNN +GNN -GNN +GNN</cell></row><row><cell>MRR</cell><cell cols="4">0.0381 0.1326 0.0651 0.0705</cell></row><row><cell cols="3">H@100 0.4678 0.4705</cell><cell>0.287</cell><cell>0.374</cell></row><row><cell>H@10</cell><cell cols="4">0.0917 0.2509 0.1246 0.1458</cell></row><row><cell>H@5</cell><cell cols="4">0.0500 0.1899 0.0809 0.0990</cell></row><row><cell>H@3</cell><cell cols="4">0.0219 0.1396 0.0542 0.0730</cell></row><row><cell>H@1</cell><cell cols="4">0.007 0.0763 0.0373 0.0319</cell></row><row><cell>AMRI</cell><cell>0.666</cell><cell>0.730</cell><cell>0.646</cell><cell>0.682</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Baseline evaluation of two variations of the Node-Piece model on both datasets</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving Inductive Link Prediction Using Hyper-relational Facts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="74" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models under a Unified Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3124805</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2021.3124805" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-825.html" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the Ambiguity of Rank-Based Evaluation of Entity Alignment or Link Prediction Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Faerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06914</idno>
		<ptr target="http://arxiv.org/abs/2002.06914" />
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoregressive Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=5k8F6UU39V" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multilingual Autoregressive Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Plekhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.12528" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>In arXiv pre-print 2103.12528.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational Knowledge Graph Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Look Before You Hop&amp;#58; Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Christmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishiraj</forename><surname>Saha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyotsna</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
	<note>CIKM &apos;19</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive Entity Representations from Text via Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450141</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2021</title>
		<meeting>The Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xMJWUKJnFSw" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">2022. migalkin/ilpc2022: Initial prerelease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6213459</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6213459" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<title level="m">OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011-06-28" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkxSmlBFvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CoDEx: A Comprehensive Knowledge Graph Completion Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07810</idno>
		<ptr target="http://arxiv.org/abs/2009.07810" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komal</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-4007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Composition-based Multi-Relational Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Louis-Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
