<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revealing the Importance of Semantic Retrieval for Machine Reading at Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
							<email>yixin1@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
							<email>songhe17@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revealing the Importance of Semantic Retrieval for Machine Reading at Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine Reading at Scale (MRS) is a challenging task in which a system is given an input query and is asked to produce a precise output by "reading" information from a large knowledge base. The task has gained popularity with its natural combination of information retrieval (IR) and machine comprehension (MC). Advancements in representation learning have led to separated progress in both IR and MC; however, very few studies have examined the relationship and combined design of retrieval and comprehension at different levels of granularity, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective pipeline system with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting external textual knowledge for machine comprehensive systems has long been an important yet challenging problem. Success requires not only precise retrieval of the relevant information sparsely restored in a large knowledge source but also a deep understanding of both the selected knowledge and the input query to give the corresponding output. Initiated by <ref type="bibr" target="#b1">Chen et al. (2017)</ref>, the task was termed as Machine Reading at Scale (MRS), seeking to provide a challenging situation where machines are required to do both semantic retrieval and comprehension at different levels of granularity for the final downstream task.</p><p>Progress on MRS has been made by improving individual IR or comprehension sub-modules with recent advancements on representative learning <ref type="bibr" target="#b17">(Peters et al., 2018;</ref><ref type="bibr" target="#b18">Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref>. However, partially due to the lack of annotated data for intermediate retrieval in an MRS setting, the evaluations were done mainly on the final downstream task and with much less consideration on the intermediate retrieval performance. This led to the convention that upstream retrieval modules mostly focus on getting better coverage of the downstream information such that the upper-bound of the downstream score can be improved, rather than finding more exact information. This convention is misaligned with the nature of MRS where equal effort should be put in emphasizing the models' joint performance and optimizing the relationship between the semantic retrieval and the downstream comprehension subtasks.</p><p>Hence, to shed light on the importance of semantic retrieval for downstream comprehension tasks, we start by establishing a simple yet effective hierarchical pipeline system for MRS using Wikipedia as the external knowledge source. The system is composed of a term-based retrieval module, two neural modules for both paragraphlevel retrieval and sentence-level retrieval, and a neural downstream task module. We evaluated the system on two recent large-scale open do-main benchmarks for fact verification and multihop QA, namely FEVER  and HOTPOTQA <ref type="bibr" target="#b23">(Yang et al., 2018)</ref>, in which retrieval performance can also be evaluated accurately since intermediate annotations on evidences are provided. Our system achieves the start-ofthe-art results with 45.32% for answer EM and 25.14% joint EM on HOTPOTQA (8% absolute improvement on answer EM and doubling the joint EM over the previous best results) and with 67.26% on FEVER score (3% absolute improvement over previously published systems).</p><p>We then provide empirical studies to validate design decisions. Specifically, we prove the necessity of both paragraph-level retrieval and sentencelevel retrieval for maintaining good performance, and further illustrate that a better semantic retrieval module not only is beneficial to achieving high recall and keeping high upper bound for downstream task, but also plays an important role in shaping the downstream data distribution and providing more relevant and high-quality data for downstream sub-module training and inference. These mechanisms are vital for a good MRS system on both QA and fact verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine Reading at Scale First proposed and formalized in <ref type="bibr" target="#b1">Chen et al. (2017)</ref>, MRS has gained popularity with increasing amount of work on both dataset collection <ref type="bibr" target="#b11">(Joshi et al., 2017;</ref> and MRS model developments <ref type="bibr" target="#b21">(Wang et al., 2018;</ref><ref type="bibr" target="#b2">Clark and Gardner, 2017;</ref><ref type="bibr" target="#b8">Htut et al., 2018)</ref>. In some previous work , paragraph-level retrieval modules were mainly for improving the recall of required information, while in some other works <ref type="bibr" target="#b23">(Yang et al., 2018)</ref>, sentence-level retrieval modules were merely for solving the auxiliary sentence selection task. In our work, we focus on revealing the relationship between semantic retrieval at different granularity levels and the downstream comprehension task. To the best of our knowledge, we are the first to apply and optimize neural semantic retrieval at both paragraph and sentence levels for MRS. Automatic Fact Checking: Recent work  formalized the task of automatic fact checking from the viewpoint of machine learning and NLP. The release of FEVER  stimulates many recent developments <ref type="bibr" target="#b16">(Nie et al., 2019;</ref><ref type="bibr" target="#b24">Yoneda et al., 2018;</ref><ref type="bibr" target="#b7">Hanselowski et al., 2018)</ref> on data-driven neural networks for automatic fact checking. We consider the task also as MRS because they share almost the same setup except that the downstream task is verification or natural language inference (NLI) rather than QA. Information Retrieval Success in deep neural networks inspires their application to information retrieval (IR) tasks <ref type="bibr" target="#b9">(Huang et al., 2013;</ref><ref type="bibr" target="#b6">Guo et al., 2016;</ref><ref type="bibr" target="#b14">Mitra et al., 2017;</ref><ref type="bibr" target="#b3">Dehghani et al., 2017)</ref>. In typical IR settings, systems are required to retrieve and rank <ref type="bibr" target="#b15">(Nguyen et al., 2016)</ref> elements from a collection of documents based on their relevance to the query. This setting might be very different from the retrieval in MRS where systems are asked to select facts needed to answer a question or verify a statement. We refer the retrieval in MRS as Semantic Retrieval since it emphasizes on semantic understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In previous works, an MRS system can be complicated with different sub-components processing different retrieval and comprehension sub-tasks at different levels of granularity, and with some subcomponents intertwined. For interpretability considerations, we used a unified pipeline setup. The overview of the system is in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To be specific, we formulate the MRS system as a function that maps an input tuple (q, K) to an output tuple (?, S) where q indicates the input query, K is the textual KB,? is the output prediction, and S is selected supporting sentences from Wikipedia. Let E denotes a set of necessary evidences or facts selected from K for the prediction. For a QA task, q is the input question and? is the predicted answer. For a verification task, q is the input claim and? is the predicted truthfulness of the input claim. For all tasks, K is Wikipedia.</p><p>The system procedure is listed below: (1) Term-Based Retrieval: To begin with, we used a combination of the TF-IDF method and a rule-based keyword matching method 2 to narrow the scope from whole Wikipedia down to a set of related paragraphs; this is a standard procedure in MRS <ref type="bibr" target="#b1">(Chen et al., 2017;</ref><ref type="bibr" target="#b16">Nie et al., 2019)</ref>. The focus of this step is to efficiently select a candidate set P I that can cover the information as much as possible (P I ? K) while keeping the  size of the set acceptable enough for downstream processing.</p><p>(2) Paragraph-Level Neural Retrieval: After obtaining the initial set, we compare each paragraph in P I with the input query q using a neural model (which will be explained later in Sec 3.1). The outputs of the neural model are treated as the relatedness score between the input query and the paragraphs. The scores will be used to sort all the upstream paragraphs. Then, P I will be narrowed to a new set P N (P N ? P I ) by selecting top k p paragraphs having relatedness score higher than some threshold value h p (going out from the P-Level grey box in <ref type="figure" target="#fig_0">Fig. 1</ref>). k p and h p would be chosen by keeping a good balance between the recall and precision of the paragraph retrieval.</p><p>(3) Sentence-Level Neural Retrieval: Next, we select the evidence at the sentence-level by decomposing all the paragraphs in P N into sentences. Similarly, each sentence is compared with the query using a neural model (see details in Sec 3.1) and obtain a set of sentences S ? P N for the downstream task by choosing top k s sentences with output scores higher than some threshold h s (S-Level grey box in <ref type="figure" target="#fig_0">Fig. 1</ref>). During evaluation, S is often evaluated against some ground truth sentence set denoted as E.</p><p>(4) Downstream Modeling: At the final step, we simply applied task-specific neural models (e.g., QA and NLI) on the concatenation of all the sen-tences in S and the query, obtaining the final output?.</p><p>In some experiments, we modified the setup for certain analysis or ablation purposes which will be explained individually in Sec 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling and Training</head><p>Throughout all our experiments, we used BERT-Base <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> to provide the state-ofthe-art contextualized modeling of the input text. 3 Semantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss. To be specific, we fed the query and context into BERT as:</p><formula xml:id="formula_0">[CLS ] Query [SEP ] Context [SEP ]</formula><p>We applied an affine layer and sigmoid activation on the last layer output of the [CLS ] token which is a scalar value. The parameters were updated with the objective function:</p><formula xml:id="formula_1">J retri = ? i?T p/s pos log(p i ) ? i?T p/s neg log(1 ?p i )</formula><p>wherep i is the output of the model, T QA: We followed <ref type="bibr" target="#b4">Devlin et al. (2018)</ref> for QA span prediction modeling. To correctly handle yes-or-no questions in HOTPOTQA, we fed the two additional "yes" and "no" tokens between [CLS ] and the Query as:</p><formula xml:id="formula_2">[CLS ] yes no Query [SEP ] Context [SEP ]</formula><p>where the supervision was given to the second or the third token when the answer is "yes" or "no", such that they can compete with all other predicted spans. The parameters of the neural QA model were trained to maximize the log probabilities of the true start and end indexes as:</p><formula xml:id="formula_3">J qa = ? i log(? s i ) + log(? e i )</formula><p>where? s i and? e i are the predicted probability on the ground-truth start and end position for the ith example, respectively. It is worth noting that we used ground truth supporting sentences plus some other sentences sampled from upstream retrieved set as the context for training the QA module such that it will adapt to the upstream data distribution during inference.</p><p>Fact Verification: Following , we formulate downstream fact verification as the 3-way natural language inference (NLI) classification problem <ref type="bibr" target="#b13">(MacCartney and Manning, 2009;</ref><ref type="bibr" target="#b0">Bowman et al., 2015)</ref> and train the model with 3-way cross entropy loss. The input format is the same as that of semantic retrieval and the objective is J ver = ? i y i ? log(? i ), where? i ? R 3 denotes the model's output for the three verification labels, and y i is a one-hot embedding for the ground-truth label. For verifiable queries, we used ground truth evidential sentences plus some other sentences sampled from upstream retrieved set as new evidential context for NLI. For nonverifiable queries, we only used sentences sampled from upstream retrieved set as context because those queries are not associated with ground truth evidential sentences. This detail is important for the model to identify non-verifiable queries and will be explained more in Sec 6. Additional training details and hyper-parameter selections are in the Appendix (Sec. A; <ref type="table">Table 6</ref>).</p><p>It is worth noting that each sub-module in the system relies on its preceding sub-module to provide data both for training and inference. This means that there will be upstream data distribution misalignment if we trained the sub-module in isolation without considering the properties of its precedent upstream module. The problem is similar to the concept of internal covariate shift <ref type="bibr" target="#b10">(Ioffe and Szegedy, 2015)</ref>, where the distribution of each layer's inputs changes inside a neural network. Therefore, it makes sense to study this issue in a joint MRS setting rather than a typical supervised learning setting where training and test data tend to be fixed and modules being isolated. We release our code and the organized data both for reproducibility and providing an off-the-shelf testbed to facilitate future research on MRS.</p><p>4 Experimental Setup MRS requires a system not only to retrieve relevant content from textual KBs but also to poccess enough understanding ability to solve the downstream task. To understand the impact or importance of semantic retrieval on the downstream comprehension, we established a unified experimental setup that involves two different downstream tasks, i.e., multi-hop QA and fact verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks and Datasets</head><p>HOTPOTQA: This dataset is a recent large-scale QA dataset that brings in new features: (1) the questions require finding and reasoning over multiple documents; (2) the questions are diverse and not limited to pre-existing KBs; (3) it offers a new comparison question type <ref type="bibr" target="#b23">(Yang et al., 2018)</ref>. We experimented our system on HOTPOTQA in the fullwiki setting, where a system must find the answer to a question in the scope of the entire Wikipedia, an ideal MRS setup. The sizes of the train, dev and test split are 90,564, 7,405, and 7,405. More importantly, HOTPOTQA also provides human-annotated sentence-level supporting facts that are needed to answer each question. Those intermediate annotations enable evaluation on models' joint ability on both fact retrieval and answer span prediction, facilitating our direct analysis on the explainable predictions and its relations with the upstream retrieval. FEVER: The Fact Extraction and VERification dataset  is a recent dataset collected to facilitate the automatic fact checking. The work also proposes a benchmark task in which given an arbitrary input claim, candidate systems are asked to select evidential sentences from Wikipedia and label the claim as either SUPPORT, REFUTE, or NOT ENOUGH INFO, if the claim can be verified to be true, false, or non-verifiable, respectively, based on the evidence. The sizes of the train, dev and test split are <ref type="bibr">145,449, 19,998, and 9,998</ref>. Similar to HOT-POTQA, the dataset provides annotated sentencelevel facts needed for the verification. These intermediate annotations could provide an accurate evaluation on the results of semantic retrieval and thus suits well for the analysis on the effects of retrieval module on downstream verification.</p><p>As in <ref type="bibr" target="#b1">Chen et al. (2017)</ref>, we use Wikipedia as our unique knowledge base because it is a comprehensive and self-evolving information source often used to facilitate intelligent systems. Moreover, as Wikipedia is the source for both HOT-POTQA and FEVER, it helps standardize any further analysis of the effects of semantic retrieval on the two different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>Following ; <ref type="bibr" target="#b23">Yang et al. (2018)</ref>, we used annotated sentence-level facts to calculate the F1, Precision and Recall scores for evaluating sentence-level retrieval. Similarly, we labeled all the paragraphs that contain any ground truth fact as ground truth paragraphs and used the same three metrics for paragraph-level retrieval evaluation. For HOTPOTQA, following <ref type="bibr" target="#b23">Yang et al. (2018)</ref>, we used exact match (EM) and F1 metrics for QA span prediction evaluation, and used the joint EM and F1 to evaluate models' joint performance on both retrieval and QA. The joint EM and F1 are calculated as: P j = P a ? P s ; R j = R a ? R s ; F j = 2P j ?R j P j +R j ; EM j = EM a ? EM s , where P , R, and EM denote precision, recall and EM; the subscript a and s indicate that the scores are for answer span and supporting facts.</p><p>For the FEVER task, following , we used the Label Accuracy for evaluating downstream verification and the Fever Score   <ref type="table">Table 2</ref>: Performance of systems on FEVER. "F1" indicates the sentence-level evidence F1 score. "LA" indicates Label Acc. without considering the evidence prediction. "FS"=FEVER Score  for joint performance. Fever score will award one point for each example with the correct predicted label only if all ground truth facts were contained in the predicted facts set with at most 5 elements. We also used Oracle Score for the two retrieval modules. The scores were proposed in <ref type="bibr" target="#b16">Nie et al. (2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results on Benchmarks</head><p>We chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HOTPOTQA 4 .</p><p>As can be seen in <ref type="table" target="#tab_2">Table 1</ref>, with the proposed hierarchical system design, the whole pipeline sys-tem achieves new start-of-the-art on HOTPOTQA with large-margin improvements on all the metrics. More specifically, the biggest improvement comes from the EM for the supporting fact which in turn leads to doubling of the joint EM on previous best results. The scores for answer predictions are also higher than all previous best results with ?8 absolute points increase on EM and ?9 absolute points on F1. All the improvements are consistent between test and dev set evaluation.</p><p>Similarly for FEVER, we showed F1 for evidence, the Label Accuracy, and the FEVER Score (same as benchmark evaluation) for models in Table 2. Our system obtained substantially higher scores than all previously published results with a ?4 and ?3 points absolute improvement on Label Accuracy and FEVER Score. In particular, the system gains 74.62 on the evidence F1, 22 points greater that of the second system, demonstrating its ability on semantic retrieval.</p><p>Previous systems <ref type="bibr" target="#b5">(Ding et al., 2019;</ref><ref type="bibr" target="#b23">Yang et al., 2018)</ref> on HOTPOTQA treat supporting fact retrieval (sentence-level retrieval) just as an auxiliary task for providing extra model explainability. In <ref type="bibr" target="#b16">Nie et al. (2019)</ref>, although they used a similar three-stage system for FEVER, they only applied one neural retrieval module at sentence-level which potentially weaken its retrieval ability. Both of these previous best systems are different from our fully hierarchical pipeline approach. These observations lead to the assumption that the performance gain comes mainly from the hierarchical retrieval and its positive effects on downstream. Therefore, to validate the system design decisions in Sec 3 and reveal the importance of semantic retrieval towards downstream, we conducted a series of ablation and analysis experiments on all the modules. We started by examining the necessity of both paragraph and sentence retrieval and give insights on why both of them matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Ablations</head><p>Intuitively, both the paragraph-level and sentencelevel retrieval sub-module help speeding up the downstream processing. More importantly, since downstream modules were trained by sampled data from upstream modules, both of neural retrieval sub-modules also play an implicit but important role in controlling the immediate retrieval distribution i.e. the distribution of set P N and set S (as shown in <ref type="figure" target="#fig_0">Fig. 1)</ref>, and providing better infer-ence data and training data for downstream modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Studies</head><p>Setups: To reveal the importance of neural retrieval modules at both paragraph and sentence level for maintaining the performance of the overall system, we removed either of them and examine the consequences. Because the removal of a module in the pipeline might change the distribution of the input of the downstream modules, we re-trained all the downstream modules accordingly. To be specific, in the system without the paragraph-level neural retrieval module, we re-trained the sentence-level retrieval module with negative sentences directly sampled from the term-based retrieval set and then also re-trained the downstream QA or verification module. In the system without the sentence-level neural retrieval module, we re-train the downstream QA or verification module by sampling data from both ground truth set and retrieved set directly from the paragraph-level module. We tested the simplified systems on both FEVER and HOTPOTQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>: <ref type="table" target="#tab_4">Table 3</ref> and 4 shows the ablation results for the two neural retrieval modules at both paragraph and sentence level on HOTPOTQA and FEVER. To begin with, we can see that removing paragraph-level retrieval module significantly reduces the precision for sentence-level retrieval and the corresponding F1 on both tasks. More importantly, this loss of retrieval precision also led to substantial decreases for all the downstream scores on both QA and verification task in spite of their higher upper-bound and recall scores. This indicates that the negative effects on downstream module induced by the omission of paragraph-level retrieval can not be amended by the sentence-level retrieval module, and focusing semantic retrieval merely on improving the recall or the upper-bound of final score will risk jeopardizing the performance of the overall system.</p><p>Next, the removal of sentence-level retrieval module induces a ?2 point drop on EM and F1 score in the QA task, and a ?15 point drop on FEVER Score in the verification task. This suggests that rather than just enhance explainability for QA, the sentence-level retrieval module can also help pinpoint relevant information and reduce the noise in the evidence that might otherwise distract the downstream comprehension   <ref type="table">Table 4</ref>: Ablation over the paragraph-level and sentence-level neural retrieval sub-modules on FEVER. "LA"=Label Accuracy; "FS"=FEVER Score; "Orcl." is the oracle upperbound of FEVER Score assuming all downstream modules are perfect. "L-F1 (S/R/N)" means the classification f1 scores on the three verification labels: SUPPORT, REFUTE, and NOT ENOUGH INFO.</p><p>module. Another interesting finding is that without sentence-level retrieval module, the QA module suffered much less than the verification module; conversely, the removal of paragraph-level retrieval neural induces a 11 point drop on answer EM comparing to a ?9 point drop on Label Accuracy in the verification task. This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval. Finally, we also evaluate the F1 score on FEVER for each classification label and we observe a significant drop of F1 on NOT ENOUGH INFO category without retrieval module, meaning that semantic retrieval is vital for the downstream verification module's discriminative ability on NOT ENOUGH INFO label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sub-Module Change Analysis</head><p>To further study the effects of upstream semantic retrieval towards downstream tasks, we change training or inference data between intermediate layers and then examine how this modification will affect the downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Effects of Paragraph-level Retrieval</head><p>We fixed h p = 0 (the value achieving the best performance) and re-trained all the downstream parameters and track their performance as k p (the number of selected paragraph) being changed from 1 to 12. The increasing of k p means a potential higher coverage of the answer but more noise in the retrieved facts. <ref type="figure" target="#fig_3">Fig. 2</ref> shows the results.</p><p>As can be seen that the EM scores for supporting fact retrieval, answer prediction, and joint perfor- mance increase sharply when k p is changed from 1 to 2. This is consistent with the fact that at least two paragraphs are required to ask each question in HOTPOTQA. Then, after the peak, every score decrease as k p becomes larger except the recall of supporting fact which peaks when k p = 4. This indicates that even though the neural sentencelevel retrieval module poccesses a certain level of ability to select correct facts from noisier upstream information, the final QA module is more sensitive to upstream data and fails to maintain the overall system performance. Moreover, the reduction on answer EM and joint EM suggests that it might be risky to give too much information for downstream modules with a unit of a paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Effects of Sentence-level Retrieval</head><p>Similarly, to study the effects of neural sentencelevel retrieval module towards downstream QA and verification modules, we fixed k s to be 5 and set h s ranging from 0.1 to 0.9 with a 0.1 interval.  Then, we re-trained the downstream QA and verification modules with different h s value and experimented on both HOTPOTQA and FEVER. Question Answering: <ref type="figure" target="#fig_4">Fig. 3</ref> shows the trend of performance. Intuitively, the precision increase while the recall decrease as the system becomes more strict about the retrieved sentences. The EM score for supporting fact retrieval and joint performance reaches their highest value when h s = 0.5, a natural balancing point between precision and recall. More interestingly, the EM score for answer prediction peaks when h s = 0.2 and where the recall is higher than the precision. This misalignment between answer prediction performance and retrieval performance indicates that unlike the observation at paragraph-level, the downstream QA module is able to stand a certain amount of noise at sentence-level and benefit from a higher recall. Fact Verification: <ref type="figure" target="#fig_5">Fig. 4</ref> shows the trends for Label Accuracy, FEVER Score, and Evidence F1 by modifying upstream sentence-level threshold h s . We observed that the general trend is similar to    that of QA task where both the label accuracy and FEVER score peak at h s = 0.2 whereas the retrieval F1 peaks at h s = 0.5. Note that, although the downstream verification could take advantage of a higher recall, the module is more sensitive to sentence-level retrieval comparing to the QA module in HOTPOTQA. More detailed results are in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Answer Breakdown</head><p>We further sample 200 examples from HOT-POTQA and manually tag them according to several common answer types <ref type="bibr" target="#b23">(Yang et al., 2018)</ref>. The proportion of different answer types is shown in <ref type="figure" target="#fig_7">Figure 5</ref>. The performance of the system on each answer type is shown in <ref type="table" target="#tab_6">Table 5</ref>. The most frequent answer type is 'Person' (24%) and the least frequent answer type is 'Event' (2%). It is also interesting to note that the model performs the best in Yes/No questions as shown in <ref type="table" target="#tab_6">Table 5</ref>, reaching an accuracy of 70.6%.  <ref type="figure">Figure 6</ref>: An example with a distracting fact. P-Score and S-Score are the retrieval score at paragraph and sentence level respectively. The full pipeline was able to filter the distracting fact and give the correct answer. The wrong answer in the figure was produced by the system without paragraph-level retrieval module. <ref type="figure">Fig. 6</ref> shows an example that is correctly handled by the full pipeline system but not by the system without paragraph-level retrieval module. We can see that it is very difficult to filter the distracting sentence after sentence-level either by the sentence retrieval module or the QA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Examples</head><p>Above findings in both FEVER and HOT-POTQA bring us some important guidelines for MRS: (1) A paragraph-level retrieval module is imperative; (2) Downstream task module is able to undertake a certain amount of noise from sentence-level retrieval; (3) Cascade effects on downstream task might be caused by modification at paragraph-level retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a simple yet effective hierarchical pipeline system that achieves state-of-the-art results on two MRS tasks. Ablation studies demonstrate the importance of semantic retrieval at both paragraph and sentence levels in the MRS system. The work can give general guidelines on MRS modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting. <ref type="table">Table 6</ref>: Hyper-parameter selection for the full pipeline system. h and k are the retrieval filtering hyperparameters mentioned in the main paper. P-level and S-level indicate paragraph-level and sentence-level respectively. "{}" means values enumerated from a set. "[]" means values enumerated from a range with inter-val=0.1 "BS."=Batch Size "# E."=Number of Epochs</p><p>The hyper-parameters were chosen based on the performance of the system on the dev set. The hyper-parameters search space is shown in <ref type="table">Table 6</ref> and the learning rate was set to 10 ?5 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Term-Based Retrieval Details</head><p>FEVER We used the same key-word matching method in <ref type="bibr" target="#b16">Nie et al. (2019)</ref> to get a candidate set for each query. We also used TF-IDF <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> method to get top-5 related documents for each query. Then, the two sets were combined to get final term-based retrieval set for FEVER. The mean and standard deviation of the number of the retrieved paragraph in the merged set were 8.06 and 4.88.</p><p>HOTPOTQA We first used the same procedure on FEVER to get an initial candidate set for each query in HOTPOTQA. Because HOTPOTQA requires at least 2-hop reasoning for each query, we  then extract all the hyperlinked documents from the retrieved documents in the initial candidate set, rank them with TF-IDF <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> score and then select top-5 most related documents and add them to the candidate set. This gives the final term-based retrieval set for HOTPOTQA. The mean and standard deviation of the number of the retrieved paragraph for each query in HOTPOTQA were 39.43 and 16.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Results</head><p>? The results of sentence-level retrieval and downstream QA with different values of h s on HOTPOTQA are in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>? The results of sentence-level retrieval and downstream verification with different values of h s on FEVER are in <ref type="table" target="#tab_10">Table 8</ref>.</p><p>? The results of sentence-level retrieval and downstream QA with different values of k p on HOTPOTQA are in <ref type="table" target="#tab_11">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Examples and Case Study</head><p>We further provide examples, case study and error analysis for the full pipeline system. The examples are shown from <ref type="bibr">Tables 11,</ref><ref type="bibr">12,</ref><ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">15</ref>. The examples show high diversity on the semantic level and the error occurs often due to the system's failure of extracting precise (either wrong, surplus or insufficient) information from KB.   Ground Truth Facts: (D1NZ, 0) D1NZ is a production car drifting series in New Zealand.</p><p>(Drifting (motorsport), 0) Drifting is a driving technique where the driver intentionally oversteers...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Answer: Drifting</head><p>Predicted Facts: (D1NZ, 0) D1NZ is a production car drifting series in New Zealand.</p><p>(Drifting (motorsport), 0) Drifting is a driving technique where the driver intentionally oversteers... Predicted Answer: Drifting </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System Overview: blue dotted arrows indicate the inference flow and the red solid arrows indicate the training flow. Grey rounded rectangles are neural modules with different functionality. The two retrieval modules were trained with all positive examples from annotated ground truth set and negative examples sampled from the direct upstream modules. Thus, the distribution of negative examples is subjective to the quality of the upstream module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>is the negative set. As shown in Fig. 1, at sentence level, ground-truth sentences were served as positive examples while other sentences from upstream retrieved set were served as negative examples. Similarly at the paragraphlevel, paragraphs having any ground-truth sentence were used as positive examples and other paragraphs from the upstream term-based retrieval processes were used as negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and indicate the upperbound of final FEVER Score at one intermediate layer assuming all downstream modules are perfect. All scores are averaged over examples in the whole evaluation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The results of EM for supporting fact, answer prediction and joint score, and the results of supporting fact precision and recall with different values of k p at paragraph-level retrieval on HOTPOTQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The results of EM for supporting fact, answer prediction and joint score, and the results of supporting fact precision and recall with different values of h s at sentence-level retrieval on HOTPOTQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The results of Label Accuracy, FEVER Score, and Evidence F1 with different values of h s at sentence-level retrieval on FEVER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Proportion of answer types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Question:</head><label></label><figDesc>Wojtek Wolski played for what team based in the Miami metropolitan area? GT Answer: Florida Panthers GT Facts: [Florida Panthers,0]: The Florida Panthers are a professional ice hockey team based in the Miami metropolitan area. (P-Score : 0.99; S-Score : 0.98) [Wojtek Wolski,1]: In the NHL, he has played for the Colorado Avalanche, Phoenix Coyotes, New York Rangers, Florida Panthers, and the Washington Capitals. (P-Score : 0.98; S-Score : 0.95) Distracting Fact: [History of the Miami Dolphins,0]: The Miami Dolphins are a professional American football franchise based in the Miami metropolitan area. (P-Score : 0.56; S-Score : 0.97) Wrong Answer : The Miami Dolphins</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b5">Ding (2019)</ref> 37.6 49.4 23.1 58.5 12.2 35.3 whole pip. 46.5 58.8 39.9 71.5 26.6 49.2 Dev set</figDesc><table><row><cell>Method</cell><cell>Ans</cell><cell>Sup</cell><cell>Joint</cell></row><row><cell></cell><cell>EM F1</cell><cell>EM F1</cell><cell>EM F1</cell></row><row><cell cols="2">Yang (2018) 24.7 34.4</cell><cell>5.3 41.0</cell><cell>2.5 17.7</cell></row><row><cell cols="2">Yang (2018) 24.0 32.9</cell><cell>3.9 37.7</cell><cell>1.9 16.2</cell></row><row><cell>MUPPET</cell><cell cols="3">30.6 40.3 16.7 47.3 10.9 27.0</cell></row><row><cell cols="4">Ding (2019) 37.1 48.9 22.8 57.7 12.4 34.9</cell></row><row><cell>whole pip.</cell><cell cols="3">45.3 57.3 38.7 70.8 25.1 47.6</cell></row><row><cell>Test set</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results of systems on HOTPOTQA.</figDesc><table><row><cell>Model</cell><cell>F1</cell><cell>LA</cell><cell>FS</cell></row><row><cell>Hanselowski (2018)</cell><cell>-</cell><cell cols="2">68.49 64.74</cell></row><row><cell>Yoneda (2018)</cell><cell cols="3">35.84 69.66 65.41</cell></row><row><cell>Nie (2019)</cell><cell cols="3">51.37 69.64 66.15</cell></row><row><cell cols="4">Full system (single) 76.87 75.12 70.18</cell></row><row><cell>Dev set</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Hanselowski (2018) 37.33 65.22 61.32</cell></row><row><cell>Yoneda (2018)</cell><cell cols="3">35.21 67.44 62.34</cell></row><row><cell>Nie (2019)</cell><cell cols="3">52.81 68.16 64.23</cell></row><row><cell cols="4">Full system (single) 74.62 72.56 67.26</cell></row><row><cell>Test set</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>87.93 50.25 39.86 75.60 71.15 71.54 46.50 58.81 26.60 49.16   </figDesc><table><row><cell>Method</cell><cell cols="3">P-Level Retrieval</cell><cell></cell><cell cols="2">S-Level Retrieval</cell><cell></cell><cell cols="2">Answer</cell><cell cols="2">Joint</cell></row><row><cell></cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>EM</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell cols="2">Whole Pip. 35.17 Pip. w/o p-level 6.02</cell><cell cols="2">89.53 11.19</cell><cell>0.58</cell><cell cols="5">29.57 60.71 38.84 31.23 41.30</cell><cell>0.34</cell><cell>19.71</cell></row><row><cell cols="4">Pip. w/o s-level 35.17 87.92 50.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">44.77 56.71</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation over the paragraph-level and sentence-level neural retrieval sub-modules on HOTPOTQA. 48.84 91.23 63.62 88.92 71.29 83.38 76.87 70.18 75.01 81.7/75.7/67.1 Pip. w/o p-level 94.69 18.11 92.03 30.27 91.07 44.47 86.60 58.77 61.55 67.01 76.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">P-Level Retrieval</cell><cell></cell><cell></cell><cell cols="2">S-Level Retrieval</cell><cell></cell><cell></cell><cell cols="2">Verification</cell></row><row><cell></cell><cell>Orcl.</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>Orcl.</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>LA</cell><cell>FS</cell><cell>L-F1 (S/R/N)</cell></row><row><cell>Whole Pip.</cell><cell cols="11">94.15 5/72.7/40.8</cell></row><row><cell cols="5">Pip. w/o s-level 94.15 48.84 91.23 63.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">55.92 61.04 72.1/67.6/27.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="8">: System performance on different answer</cell></row><row><cell cols="5">types. "PN"= Proper Noun</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8%</cell><cell>24%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>8%</cell><cell></cell><cell cols="3">15%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>9%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>7%</cell><cell>12%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">70.58823529</cell><cell></cell></row><row><cell>50</cell><cell cols="4">56 45.16129032 50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">28</cell><cell>14 31</cell><cell>13 26</cell><cell cols="2">14 4 28.57142857 19 7 36.84210526 17 12</cell><cell>5</cell><cell>2</cell><cell>11 3</cell><cell>17 6</cell><cell>9 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>42.97 81.20 55.03 46.95 59.73 1.81 37.42 29.99 57.66 0.1 30.06 65.26 78.50 69.72 47.48 59.78 19.62 47.86 46.09 55.63 0.2 34.83 69.59 76.47 71.28 47.62 59.93 22.89 49.15 49.24 54.53 0.3 37.52 72.21 74.66 71.81 47.14 59.51 24.63 49.44 50.67 53.55 0.4 39.16 74.17 72.89 71.87 46.68 58.96 25.81 49.18 51.62 51.90 0.5 39.86 75.60 71.15 71.54 46.50 58.81 26.60 49.16 52.59 50.83 0.6 39.80 76.59 69.05 70.72 46.22 58.31 26.53 48.48 52.86 49.48 0.7 38.95 77.47 66.80 69.71 45.29 57.47 25.96 47.59 53.06 47.86 0.8 37.22 77.71 63.70 67.78 44.30 55.99 24.67 45.92 52.41 45.32 0.9 32.60 75.60 57.07 62.69 42.08 52.85 21.44 42.26 50.48 40.61</figDesc><table><row><cell>h p</cell><cell></cell><cell>S-Level Retrieval</cell><cell></cell><cell cols="2">Answer</cell><cell></cell><cell></cell><cell>Joint</cell></row><row><cell></cell><cell>EM</cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>Prec. Rec.</cell></row><row><cell>0</cell><cell>3.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Detailed Results of downstream sentence-level retrieval and question answering with different values of h s on HOTPOTQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results with different h s on FEVER. 39.86 75.60 71.15 71.54 46.56 58.74 26.6 49.09 52.54 50.79 3 10.53 59.54 74.13 63.92 43.63 55.51 7.09 40.61 38.52 49.20 4 6.55 50.60 74.46 57.98 40.42 51.99 4.22 34.57 30.84 45.98 5 4.89 45.27 73.60 53.76 39.02 50.36 2.97 32.14 26.92 43.89 6 3.70 42.22 71.84 51.04 37.35 48.41 2.36 28.66 24.37 41.63 7 3.13 40.22 70.35 49.15 36.91 47.70 2.05 27.49 23.10 40.60 8 2.88 38.77 69.28 47.83 36.28 46.99 1.88 26.58 22.13 39.85 9 2.57 37.67 68.46 46.81 35.71 46.30 1.68 25.77 21.32 38.87 10 2.31 36.74 67.68 45.94 35.07 45.74 1.50 25.05 20.56 38.21 11 2.09 35.97 67.04 45.21 34.96 45.56 1.39 24.65 20.18 37.60 12 1.89 35.37 66.60 44.67 34.09 44.74 1.22 23.99 19.57 36.98</figDesc><table><row><cell>k p</cell><cell></cell><cell>S-Level Retrieval</cell><cell></cell><cell cols="2">Answer</cell><cell></cell><cell></cell><cell>Joint</cell></row><row><cell></cell><cell>EM</cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>Prec. Rec.</cell></row><row><cell>1</cell><cell>0</cell><cell cols="4">81.28 40.64 53.16 26.77 35.76</cell><cell>0</cell><cell cols="2">21.54 33.41 17.32</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Detailed Results of downstream sentence-level retrieval and question answering with different values of k p on HOTPOTQA. D1NZ is a series based on what oversteering technique?</figDesc><table /><note>Question:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>HotpotQA correct prediction with sufficient evidence.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code/data made publicly available at: https:// github.com/easonnie/semanticRetrievalMRS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Details of term-based retrieval are in Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We used the pytorch BERT implementation in https://github.com/huggingface/ pytorch-pretrained-BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Results can also be found at the leaderboard websites for the two datasets: https://hotpotqa.github.io and https://competitions.codalab.org/ competitions/18814</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments and Yicheng Wang for useful comments. This work was supported by awards from Verisk, Google, Facebook, Salesforce, and Adobe (plus Amazon and Google GPU cloud credits). The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural ranking models with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ukp-athene: Multi-sentence textual entailment for claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Workshop on Fact Extraction and Verification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training a ranking function for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phu Mon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ranking paragraphs for improving answer recall in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miyoung</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<title level="m">Natural language inference. Citeseer</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunder-standingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated fact checking: Task formulations, methods and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLIN)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Christos Christodoulopoulos, and Arpit Mittal</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">R: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ucl machine reading group: Four factor framework for fact finding (hexaf)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Workshop on Fact Extraction and Verification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Question: The football manager who recruited David Beckham managed Manchester United during what timeframe?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">-96 Manchester United F.C. season,3) Instead , he had drafted in young players like Nicky Butt</title>
	</analytic>
	<monogr>
		<title level="m">Ground Truth Facts</title>
		<editor>Paul Ince , Mark Hughes and Andrei Kanchelskis...</editor>
		<meeting><address><addrLine>David Beckham, Paul Scholes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1995" to="96" />
		</imprint>
	</monogr>
	<note>Manchester United F.C. season, 2) Alex Ferguson had sold experienced players</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">who managed Manchester United between 1945 and</title>
		<editor>Predicted Facts: (Matt Busby,0) Sir Alexander Matthew Busby , CBE...</editor>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HotpotQA incorrect prediction with insufficient/wrong evidence. Question: Where did Ian Harland study prior to studying at the oldest college at the University of Cambridge?</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Harland was educated at The Dragon School in Oxford and Haileybury. (Ian Harland,1) He then went to university at Peterhouse, Cambridge, taking a law degree</title>
	</analytic>
	<monogr>
		<title level="m">Ground Truth Facts: (Ian Harland,0) From a clerical family</title>
		<meeting><address><addrLine>Peterhouse, Cambridge, 1</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>It is the oldest college of the university..</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ground Truth Answer: The Dragon School in Oxford</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
