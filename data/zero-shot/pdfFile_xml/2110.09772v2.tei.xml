<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work studies learning from a synergy process of 3D Morphable Models (3DMM) and 3D facial landmarks to predict complete 3D facial geometry, including 3D alignment, face orientation, and 3D face modeling. Our synergy process leverages a representation cycle for 3DMM parameters and 3D landmarks. 3D landmarks can be extracted and refined from face meshes built by 3DMM parameters. We next reverse the representation direction and show that predicting 3DMM parameters from sparse 3D landmarks improves the information flow. Together we create a synergy process that utilizes the relation between 3D landmarks and 3DMM parameters, and they collaboratively contribute to better performance. We extensively validate our contribution on full tasks of facial geometry prediction and show our superior and robust performance on these tasks for various scenarios. Particularly, we adopt only simple and widely-used network operations to attain fast and accurate facial geometry prediction. Codes and data:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial geometry prediction including 3D facial alignment, face orientation estimation, and 3D face modeling are fundamental tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b73">73]</ref> and have applications on face recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b60">60]</ref>, tracking <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, and compression <ref type="bibr" target="#b54">[54]</ref>. Recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref> predict facial geometry by estimating 3D Morphable Models (3DMM) parameters that include shape and expression variations. Yet, face orientation for these previous works is only a by-product without evaluation and discussion on relations with 3D landmarks and 3D face models. In contrast, we fully evaluate facial geometry, including 3D facial alignment, face orientation estimation, and 3D face modeling.</p><p>3D face meshes can be built from 3DMM parameters, and 3D landmarks can be extracted from vertices by querying associated indices. 3D landmarks are widely used to guide 3D facial geometry learning. Previous works <ref type="bibr">Figure 1</ref>. Results from our SynergyNet with monocular image inputs. Note that 3D landmarks can predict hidden face outlines in 3D rather than follow visible outlines on images. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref> only directly extract coarse landmarks from fitted 3D faces and compute supervised alignment losses with groundtruth landmarks. These works utilize a representation direction, from 1D parameters to 3D landmarks. However, though 3D landmarks are very sparse (a 68-point definition is commonly used), they compactly and efficiently describe facial outlines in 3D space. We think 3D landmarks can be further exploited to predict underlying 3D faces as supportive information. Hence, in addition to only going from 1D parameters to 3D landmarks, we propose a further step to reversely regress 3DMM parameters from 3D landmarks and establish a representation cycle. The advantage is that predicting 3D face using 3DMM from 2D images is naturally an ill-posed problem, but prediction from 3D landmarks can alleviate the intrinsic ill-posedness. To our knowledge, we are the first to study this reverse representation direction, from 3D landmarks to 3DMM parameters. Together we build a representation cycle as a synergy process that adopts collaborative relation between 3DMM parameters and 3D landmarks to improve the information flow and attain better performance.</p><p>We propose SynergyNet, a synergy process network that includes two stages. The first stage contains a backbone net-work to regress 3DMM parameters from images and construct 3D face meshes. After landmark extraction by querying associated indices, we propose a landmark refinement module that aggregates 3DMM semantics and incorporates them into point-based features to produce refined 3D landmarks. We closely validate how each information source contributes to 3D landmark refinement. From the representation perspective, the first stage goes from 1D parameters to 3D landmarks. Next, the second stage contains a landmark-to-3DMM module that predicts 3DMM parameters from 3D landmarks, which is a reverse representation direction compared with the first stage. We leverage this step to regress embedded facial geometry lying in sparse landmarks. The overall framework is in <ref type="figure">Fig. 2</ref>.</p><p>We will first review 3DMM as basics in Sec.3.1 used in the previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref>. 3DMM regression contains pose, shape, and expression parameter estimation from a monocular face image through a backbone network. 3D faces are constructed as foundation models by 3DMM, and 3D landmarks are extracted from face meshes. Next, in Sec.3.2, we introduce the proposed multi-attribute feature aggregation (MAFA), including landmark features, image features, and shape and expression of 3DMM semantics. MAFA is then used to produce finer landmark structures. The advantage is that refinement based on only coarse landmarks is hard because the information is unitary. Joining different attributes can refine and correct raw structures.</p><p>In Sec.3.3, we introduce the reverse direction to regress 3DMM parameters from 3D landmarks, based on the assumption that 3D landmarks contain rough facial geometry. The advantage is that regressing 3DMM parameters from 3D landmarks can avoid inherent ambiguity in conventional 3DMM-based methods that predict facial geometry only from images. A self-constraining loss for both 3DMM parameters regressed from images and from 3D landmarks is used: since two 3DMM parameters describe the same identity, they should be numerically consistent.</p><p>Especially, our SynergyNet contains only simple and widely-used network operations in the whole synergy process. We quantitatively analyze performance gains introduced by each adopted information and each regression target with extensive experiments. We evaluate our Synerg-yNet on all tasks of facial alignment, face orientation estimation, and 3D face modeling using the standard datasets for each task. Our SynergyNet attains superior performance than other related work. <ref type="figure">Fig.1</ref> demonstrates the ability of our SynergyNet.</p><p>In summary, we present the following contributions: 1. We propose SynergyNet to study a synergy process that leverages the collaborative relation between 3DMM parameters and 3D landmarks to learn better 3D facial geometry. This is the first study to include reverse representation direction, i.e., from 3D landmarks to 3DMM parameters. <ref type="figure">Figure 2</ref>. Framework of our SynergyNet. Backbone network learns to regress 3DMM parameters (?p,?s, and ?e) and reconstruct 3D face meshes from monocular face images. Multi-Attribute feature aggregation gathers underlying 3DMM semantics and the latent image code to refine landmarks further. The landmark-to-3DMM module regresses 3DMM from refined landmarks L r to reveal the embedded facial geometry in 3D landmarks. A self-constraining consistency is applied to 3DMM parameters regressed from different sources. This synergy process includes a forward representation direction, from 3DMM parameters to refined 3D landmarks, and a reverse direction, from 3D landmarks to regress 3DMM parameters, to attain better performance. The red and blue arrows after shape and expression (expr) decoders show the main areas of deformation that each 3DMM semantics controls.</p><p>2. We propose multi-attribute feature aggregation for landmark refinement using multiple information sources and closely analyze performance gain for each information.</p><p>3. We conduct extensive and detailed benchmarking on 3D facial geometry, including facial alignment, face orientation estimation, and 3D face modeling, to validate our superior performance on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Facial Alignment via 3D Face Modeling</head><p>3D facial alignment aims at predicting 3D landmarks on images. In contrast, 2D approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b62">62]</ref> usually regress direct landmark coordinates or heatmaps based on visible facial parts. If input faces are self-occluded due to large face poses, their methods either only estimate landmarks along visible face outlines rather than hidden outlines or produce much larger errors at invisible parts that make the results unreliable.</p><p>3D approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref> predict aligned 3D faces with images. This way, occluded landmarks can be registered. 3DDFA <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b71">71]</ref> adopts Basel Face Model (BFM) and use 3DMM fitting to reconstruct face meshes from monocular images. PRNet <ref type="bibr" target="#b16">[17]</ref> predicts 2D UVposition maps that encode 3D points and uses BFM mesh connectivity to build face models. Compared with 3DDFA, PRNet might have higher mesh deformation ability since its 3D points are not from 3DMM parameterization. However, it is harder to obtain a smooth and reliable mesh for PRNet. 2DASL <ref type="bibr" target="#b49">[49]</ref> based on 3DMM further adopts a differentiable renderer and a discriminator to produce high-quality 3D face models. 3DDFA-V2 <ref type="bibr" target="#b19">[20]</ref> based on 3DDFA further introduces a meta-joint optimization strategy and a short video synthesis to attain the current best result. 3D faces and 3D landmarks extracted by vertex indexing are outputs of these methods. However, their landmarks are raw without refinement. Our landmarks are refined with multiattribute feature aggregation, and we further adopt 3DMM from 3D landmarks as another information source.</p><p>Another line of work adopts self-supervision from images and targets at more realistic 3D face synthesis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref>. Their self-supervised factors usually rely on visible facial areas to collect visual cues for prediction and may not be robust to large-pose cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Face Orientation Estimation</head><p>Face orientation estimation has applications on humanrobot interaction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b52">52]</ref>. Euler angles (yaw, pitch, roll) are used to represent the orientation. Deep Head Pose <ref type="bibr" target="#b35">[35]</ref> uses networks to predict 2D landmarks and face orientation at the same time. HopeNet <ref type="bibr" target="#b40">[40]</ref> uses bin-based angle regression and QuatNet <ref type="bibr" target="#b21">[22]</ref> uses a multi-regression loss for head pose. FSA-Net <ref type="bibr" target="#b64">[64]</ref> constructs a fine-grained structure mapping for features aggregation. TriNet <ref type="bibr" target="#b6">[7]</ref> uses a vector-based representation for pose estimation. These works focus on face orientation as a standalone task. On the other hand, although 3DMM-based 3D alignment approaches estimate rotation matrices, previous works only focus on evaluation and discussion on landmarks and 3D faces <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref>. To gain an insight into full facial geometry, we benchmark both standalone orientation estimation methods and 3DMM-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method, illustrated in <ref type="figure">Fig. 2</ref>, aims at precise and accurate 3D facial alignment, face orientation estimation, and 3D face modeling by utilizing a synergy process of 3D landmarks and 3DMM parameters to guide 3D facial geometry learning better. The pipeline contains two stages. The first stage includes a preliminary 3DMM regression from images and a multi-attribute feature aggregation (MAFA) for landmark refinement. The second stage contains a landmark-to-3DMM regressor to reveal the embedded facial geometry in sparse landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Morphable Models (3DMM)</head><p>3DMM reconstructs face meshes using principal component analysis (PCA). Given a mean face M ? R 3Nv with N v 3D vertices, 3DMM deforms M into a target face mesh by predicting the shape and expression variations. U s ? R 3Nv?40 is the basis for shape variation manifold that represents different identities, U e ? R 3Nv?10 is the basis for expression variation manifold, and ? s ? R <ref type="bibr" target="#b40">40</ref> and ? e ? R 10 are the associated basis coefficients. The 3D face reconstruction can be formulated in Eq.1.</p><formula xml:id="formula_0">S f = M at(M + U s ? s + U e ? e ),<label>(1)</label></formula><p>where S f ? R 3?Nv represents a reconstructed frontal face model after the vector-to-matrix operation (M at). To align S f with input view, a 3x3 rotation matrix R ? SO(3), a translation vector t ? R 3 , and a scale ? are predicted to transform S f by Eq.2.</p><formula xml:id="formula_1">S v = ? RS f + t,<label>(2)</label></formula><p>where S v ? R 3?Nv aligns with input view. ? R and t are included as 3DMM parameters in most works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b70">70]</ref>, and thus we use ? p ? R 12 instead. We follow the current best work 3DDFA-V2 <ref type="bibr" target="#b19">[20]</ref> to predict 62-dim 3DMM parameters ? for pose, shape, and expression. We follow 3DDFA-V2 to adopt MobileNet-V2 as the backbone network to encode input images and use fullyconnected (FC) layers as decoders for predicting 3DMM parameters from the bottleneck image feature z. We separate the decoder into several heads by 3DMM semantics, which jointly predict the whole 62-dim parameters. The advantage of separate heads is that disentangling pose, shape, and expression controls secures better information flow. The illustration in <ref type="figure">Fig. 2</ref> shows the encoder-decoder structure. The decoding is formulated as ? m = Dec m (z), m ? {p, s, e}, showing pose, shape and expression. With groundtruth notation * hereafter, the supervised 3DMM regression loss is shown as follows.</p><formula xml:id="formula_2">L 3DMM = m ? m ? ? * m 2 .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From 3DMM to Refined 3D Landmarks</head><p>After regressing 3DMM parameters (? p ,? s ,? e ), 3D face mesh for the input face can be constructed by Eq.1 and be aligned with input face by Eq.2. We adopt popular BFM <ref type="bibr" target="#b37">[37]</ref>, which includes about 53K vertices, as the mean face M in Eq.1. Then, 3D landmarks L c ? R 3?N l are extracted by landmark indices. N l = 68 is used in 300W-LP <ref type="bibr" target="#b70">[70]</ref> as our training dataset.</p><p>Previous studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref> directly use extracted landmarks L c to compute the alignment loss for learning 3D facial geometry. However, these extracted landmarks are raw without refinement. Instead, we adopt a refinement module that aggregates multi-attribute features to produce finer landmark structures. Landmarks can be seen as a sequence of 3D points. Weight-sharing multi-layer perceptrons (MLPs) are commonly used for extracting features from structured points. PointNet-based frameworks <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b63">63]</ref> use an MLP-encoder to extract highdimensional embeddings. At the bottleneck, global point max-pooling is applied to obtain global point features. Then an MLP-decoder is used to regress per-point attributes. An MLP-based refinement module takes sparse landmarks L c as inputs and uses the MLP-encoder and MLP-decoder to produce finer landmarks.</p><p>Instead of using L c alone for the refinement, our refinement module adopts multi-attribute feature aggregation (MAFA), including input images and 3DMM semantics that provide information from different domains. For example, shape contains information of thinner/thicker faces, and expression contains information for eyebrow or mouth movements. Therefore, these pieces of information can help regress finer landmark structures. Specifically, our MAFA fuses information of the image, using its bottleneck features z after global average pooling and shape and expression 3DMM parameters. These features and parameters are global information without spatial dimensions. We first use FC layers for domain adaption. Later we concatenate them into a multi-attribute feature vector and then repeat this vector N l times to make multi-attribute features compatible with per-point features. We last append the repeated features to the low-level point features and feed them to an MLP-decoder to produce refined 3D landmarks. The overall design is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Skip connection is used from the coarse to refined landmarks to facilitate training.</p><p>We use groundtruth landmarks to guide the training. The alignment loss function is formulated as follows.</p><formula xml:id="formula_3">L lmk = n L smL1 (L r n ? L * n ), n ? [1, N l ],<label>(4)</label></formula><p>where N l is number of landmarks, * denotes groundtruth, and L smL1 is smooth L1 loss. So far, the operations of constructing 3D face meshes and landmark extraction and refinement transform 3DMM parameters to refined 3D landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">From Refined Landmarks to 3DMM</head><p>We next describe the reverse direction of representation that goes from refined landmarks to 3DMM parameters.</p><p>Previous works only consider 3DMM parameter regression from images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref>. However, facial landmarks are sparse keypoints lying at eyes, nose, mouth, and face outlines, which are principal areas that ? s and ? e control. We assume that approximate facial geometry is embedded in sparse landmarks. Thus, we further build a landmark-to-3DMM module to regress 3DMM parameters from the refined landmarks L r using the holistic landmark features. To our knowledge, we are the first to study this reverse representation direction, from landmarks to 3DMM parameters.</p><p>The landmark-to-3DMM module also contains an MLPencoder to extract high dimensional point features and use a global point max-pooling to obtain holistic landmark features. Later separate FC layers transform the holistic landmark features to 3DMM parameters to get?, including pose, shape, and expression. We refer? to landmark geometry, since this 3DMM geometry is regressed from landmarks. We adopt a supervised loss with groundtruth ? * for ? as follows.</p><formula xml:id="formula_4">L 3DMM lmk = m ? m ? ? * m 2 ,<label>(5)</label></formula><p>where m contains pose, shape, and expression. Furthermore, since? regressed from the landmarks and ? regressed from the face image describe the same identity, they should be numerically similar. We further add a novel self-supervision control as follows.</p><formula xml:id="formula_5">Lg = m ?m ??m 2 ,<label>(6)</label></formula><p>where m ? {p, s, e}. L g improves information flow that lets 3DMM regressed from images obtain support from landmark geometry. The advantage of self-supervision control (Eq.6) is that since images and sparse landmarks are different data representations (2D grids and 3D points) using different network architectures and operations, more descriptive and richer features can be extracted and aggregated under this multirepresentation strategy. Although conceptually sparse landmarks provide rough face outlines, our experiments show that this reverse representation direction further contributes to the performance gain and attains superior results than related work.</p><p>Overall, the total loss combination is shown as follows</p><formula xml:id="formula_6">L total = ?1L 3DMM + ?2L lmk + ?3L 3DMM lmk + ?4Lg,<label>(7)</label></formula><p>where ? terms are loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Representation Cycle</head><p>Our overall framework creates a cycle of representations. First, the image encoder and separate decoders regress 1D parameters from a face image input. Then, we construct 3D meshes from parameters and refine extracted 3D landmarks-this is the forward direction that switches representations from 1D parameters to 3D points. Next, the reverse representation direction adopts a landmark-to-3DMM module to switch representations from 3D points back to 1D parameters. Therefore it forms a representation cycle <ref type="figure" target="#fig_1">(Fig.  4)</ref>, and we minimize the consistency loss to facilitate the training. The forward and reverse representation direction between 3DMM parameters and refined 3D landmarks form a synergy process that collaboratively improves the learning of facial geometry. Landmarks are extracted and refined, and the refined landmarks and landmark geometry further supports better 3DMM parameter predictions using the selfsupervised consistency loss (Eq.6).</p><p>Compared with a simple baseline using only the forward representation, i.e., going from image to 3DMM and directly extracting 3D points from built meshes to compute alignment loss, our proposed landmark refinement (MAFA) and the reverse representation (landmark-to-3DMM module) only bring about 5% more time in average for a single feed-forward pass. This is because landmarks are sparse and compact, and weight-sharing MLPs are lightweight.</p><p>We choose simple and widely-used network operations to show that without special operations, landmarks and 3DMM parameters can still guide the 3D facial geometry learning better. Through the following studies and experiments, we closely validate each module we introduce to the plain 3DMM regression from images, including MAFA for landmark refinement and landmark-to-3DMM module. Network details are described in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Evaluation is conducted on the three focused tasks: facial alignment, face orientation estimation, and 3D face models. One of our main contributions is close studies that exhibit a detailed performance gain breakdown for each module we propose and each attribute we include in MAFA.</p><p>Procedure. We train on 300W-LP <ref type="bibr" target="#b70">[70]</ref>, which is a standard and widely-used training set on 3D face tasks. It collects in-the-wild face images and conducts face profiling <ref type="bibr" target="#b70">[70]</ref> for producing 3DMM parameters with BFM <ref type="bibr" target="#b37">[37]</ref> and FaceWarehouse texture <ref type="bibr" target="#b4">[5]</ref>. The dataset also performs out-of-plane face rotation for augmentation, attaining more than 60K face images and fitted 3D models.</p><p>During training, we use a learning rate of 0.08, batch size of 1024, and momentum of 0.9 of the SGD <ref type="bibr" target="#b72">[72]</ref> optimizer. We train our network with 80 epochs, and the learning rate decays to <ref type="bibr" target="#b0">1</ref> 10 and 1 100 of the initial after 48 and 64 epochs. We use random color jittering and random horizontal flip. We further adopt face-swapping augmentation that exchanges textures with others, overlays them on the original mesh, and last renders the 3D model onto the original image. This is to increase the appearance variety that creates novel texture-geometry pairs. We train on 4x GTX 1080Ti GPUs, and the training takes about 8 hours.</p><p>At test time, the refined landmarks L r , fitted 3D face S v with its orientation from ? p are the outputs for evaluation. The processing of landmark geometry is saved at test time since its information is auxiliary, and we leave the discussion and evaluation of landmark geometry in the supplementary. Our inference attains 2600fps inference speed on average for the 3D landmark prediction and about 2300fps for the dense 3D face prediction on a single GPU with the MobileNet backbone. The speed calculation includes inference time batching and communication overhead of data loading to GPU-memory. This satisfies the real-world applications for fast inference.</p><p>In addition to facial geometry, we also study texture synthesis in the supplementary for more realistic 3D faces.</p><p>Test sets for facial alignment. Facial alignment is standardly evaluated on AFLW2000-3D <ref type="bibr" target="#b70">[70]</ref>, which contains the first 2000 images of AFLW <ref type="bibr" target="#b34">[34]</ref> with a 68-point landmark annotation. Two landmark sets of AFLW2000-3D are present, original and reannotated by LS3D-W <ref type="bibr" target="#b3">[4]</ref>). The reannotated one carries better quality. We separately report performances on the two versions to fairly compare with related work. We also follow <ref type="bibr" target="#b19">[20]</ref> to evaluate on the full AFLW set, which contains 21K images with a 21-point landmark annotation. The two datasets are used for showing evaluation on different numbers of facial landmarks.</p><p>Test sets for 3D face modeling. We evaluate 3D face modeling on AFLW2000-3D <ref type="bibr" target="#b70">[70]</ref>, MICC Florence [2], 300VW <ref type="bibr" target="#b44">[44]</ref>, and Artistic-Faces <ref type="bibr" target="#b66">[66]</ref> for both quantitative and qualitative analysis. AFLW2000-3D contains 2000 fitted 3D faces. Florence contains high-resolution real face scans of 53 individuals. 300VW collects talks or interviews from the web, and Artistic-Faces gathers artistic style faces.</p><p>Test sets for face orientation estimation. Most previous 3DMM-based works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b70">70]</ref> focus on the facial alignment and 3D face modeling. To fully evaluate facial geometry, we introduce face orientation estimation for evaluation. AFLW2000-3D contains large-pose faces with orientation annotation that make it suitable for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Facial Alignment Evaluation</head><p>Metrics. Normalized mean error (NME) in Eq.8 for sparse facial landmarks with Euclidean distance is reported.</p><formula xml:id="formula_7">NME = 1 T T t=1 u t ? v t 2 B ,<label>(8)</label></formula><p>where u t and v t are landmark prediction/groundtruth that are both registered with face images, T is the number of samples, and B is bounding box size, square root of box areas, as the normalization term for each face. Ablation study. We show three different ablation studies in this section to thoroughly examine the contribution of each part in SynergyNet. The aim is to validate our introduced multi-attribute feature fusion and analyze how each attribute contributes to the final performance.</p><p>(1) We first conduct ablation studies of SynergyNet on AFLW2000-3D. Compared with conventional frameworks that only use a backbone network to regress 3DMM parameters from images, the proposed multi-attribute feature aggregation for landmark refinement and the landmark-to-3DMM module in the synergy process are examined. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b71">71]</ref>, we report NME under three yaw angle ranges. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The table shows that both landmark refinement and landmark geometry support stages contribute to the final performance for better facial landmark estimation. MAFA adopts the advantage of multi-attribute information fusion to refine landmarks. The landmark-to-3DMM reveals the geometric information embedded in the 3D landmarks and helps 3DMM prediction with the representation cycle. From both tables' third and fourth rows, directly predicting landmark geometry from raw landmarks without refinement only obtains limited performance gains. By contrast, based on finer landmarks, the reverse representation stage further improves the results. This validates our SynergyNet design that both MAFA and the landmark-to-3DMM are required to attain the best performance.</p><p>(2) We then study the performance contribution of each attribute at MAFA and each 3DMM regression target at the reverse representation direction stage. For the former, we experiment with different feature aggregation and fusion: only point feature, point+image feature, and all attributes in <ref type="figure" target="#fig_0">Fig. 3</ref> (point, image, and 3DMM semantics). For the latter, we examine the performance gain of each regression target at the landmark-to-3DMM, including pose, shape, and expression from 3D landmarks. Results are shown in <ref type="table">Table 2</ref>. Row 1-3 show that the gain of using image and 3DMM semantics mainly comes from small or medium pose ranges because images and the derived 3DMM parameters capture more descriptive features on frontal faces. Large poses <ref type="table">Table 2</ref>. Study on different attributes used at landmark refinement and different regression targets at landmark-to-3DMM (L?3D). AFLW2000-3D Original is adopted for facial alignment evaluation. The first three rows exploit different attributes for feature aggregation. Row 3 uses all attributes shown in <ref type="figure" target="#fig_0">Fig.3</ref>. Row 4 to 6 further study performance gains of different regression targets at the reverse representation direction.  may cause self-occlusion on images and make prediction unreliable. Row 4 to 6 exhibit effects of regressing pose only (Row 4), shape and expression (Row 5), and all (Row 6). The improvements mainly come from large-pose cases. The reason is that the reverse direction regresses parameters from 3D landmarks that provide features from the 3D space, which naturally avoids self-occlusion compared with 2D. Such a strategy benefits alignment for large poses.</p><p>(3) We further investigate the other two possible network designs of landmark-to-3DMM. Comparison 1 refines landmarks and regresses? at the same step, and thus? is regressed from L c in this setting. This is to study whether the reverse representation direction is benefited from refined landmarks L r . Comparison 2 further includes z, ? s , and ? e in the landmark-to-3DMM regression in <ref type="figure">Fig. 2</ref>, forming another multi-attribute feature aggregation to regress the?. This setting analyzes whether aggregation can also assist? prediction.</p><p>From <ref type="table" target="#tab_2">Table 3</ref>, results of Comparison 1 show that regress-  ing? from L c does not perform better than L r due to the finer and more accurate structure of L r . Results of Comparison 2 show that multi-attribute aggregation used at the landmark-to-3DMM does not bring better performance. We assume this is because the information has been joined at the landmark refinement phase.</p><p>Comparison to related work. We benchmark performance on the widely-used AFLW2000-3D. The two versions of annotations (original and reannotated) are used. To have a fair comparison, we show results on the two different sets separately and compare them with other reported performances. <ref type="table" target="#tab_3">Table 4</ref> shows the comparison on the original, and the reannotated version is in the supplementary. Our SynergyNet holds the best performance among all the related work on this standard dataset in <ref type="table" target="#tab_3">Table 4</ref>. From the breakdown, our performance gain mainly comes from medium and large pose cases. We find that prior works encounter performance bottlenecks since they only regress 3DMM from images. However, referring to <ref type="table" target="#tab_0">Table 1</ref>, MAFA has already shown the best performance compared with prior arts due to its ability to fuse multi-attribute features. Then the landmark-to-3DMM further shows lower errors to break through the performance bottleneck.</p><p>Following 3DDFA-V2 <ref type="bibr" target="#b19">[20]</ref>, we next use the AFLW full set for evaluation (21K testing images with 21-point landmarks). We show the comparison in <ref type="table" target="#tab_4">Table 5</ref>. Our work has the best performance, especially with a performance gap over others on large-pose cases. <ref type="table">Table 6</ref>. Ablative for face orientation estimation. The same modules are studied in <ref type="table" target="#tab_0">Table 1</ref>   <ref type="table">Table 7</ref>. Study on different attributes and different regression targets for face orientation estimation. The same structures are also studied in <ref type="table">Table 2</ref> for the alignment task. The first three rows study different attributes for aggregation. Based on the landmark refinement, Row 4 to 6 further study performance gains of different regression targets at the reverse representation stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Orientation Estimation Evaluation</head><p>Metrics and studies. Following the evaluation protocol in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b64">64]</ref>, we calculate the mean absolute error (MAE) of predicted Euler angles in degrees. Groundtruth angles for each face in AFLW2000-3D are used, except for 31 samples whose yaw angles are outside the range [-99 ? , 99 ? ]. We first study different combinations of the proposed modules in <ref type="table">Table 6</ref>. From the results, finer landmark structures from MAFA lead to better orientation estimation. The landmark-to-3DMM further contributes to better performance since pose parameter regression from 3D points leads to more robust poses than 2D images.</p><p>We further study information fusion of using different attributes at MAFA and examine different parameter regression targets at the landmark-to-3DMM in <ref type="table">Table 7</ref>. This analysis shows that more accurate orientation estimation is mainly benefited by pose regression at the reverse representation direction stage because Euler angles estimated from 3D representation are more robust than from 2D images. This breakdown also explains the performance gain of the landmark-to-3DMM in <ref type="table">Table 6</ref>.</p><p>Benchmark comparison. We collect works that focus only on face orientation estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b64">64]</ref> and 3DMM-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref>. The 3DMM-based works do not include evaluation of this task. To show the full benchmark list, we evaluate their methods using the released pretrained models. <ref type="table" target="#tab_7">Table 8</ref> shows that our method is the best and holds a performance gap over others. We display visual comparison in <ref type="figure" target="#fig_2">Fig. 5</ref> and more studies and discussion in the supplementary.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D Face Modeling Evaluation</head><p>Metrics and Comparison. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">49]</ref>, we first evaluate 3D face modeling on AFLW2000-3D. Two protocols are used. Protocol 1 suggested by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70]</ref> uses the iterative closet point (ICP) algorithm to register groundtruth 3D models and predicted models. NME of per-point error normalized by interocular distances is calculated. Protocol 2, suggested by <ref type="bibr" target="#b19">[20]</ref> and also called dense alignment, calculates the per-point error normalized by bounding box sizes with groundtruth models aligned with images. Since ICP is not used, pose estimation would <ref type="table">Table 9</ref>. 3D face modeling comparison on AFLW2000-3D. Refer to Sec. 4.3 for the protocol details.</p><p>Protocol-1 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70]</ref> 3DDFA <ref type="bibr" target="#b70">[70]</ref> DeFA <ref type="bibr" target="#b31">[31]</ref> PRNet <ref type="bibr" target="#b16">[17]</ref> 2DASL <ref type="bibr" target="#b49">[49]</ref> SynergyNet (our) NME 5.37 5.55 3.96 2.10 1.97 Protocol-2 <ref type="bibr" target="#b19">[20]</ref> 3DDFA <ref type="bibr" target="#b70">[70]</ref> DeFA <ref type="bibr" target="#b31">[31]</ref> 3DDFA-V2 <ref type="bibr" target="#b19">[20]</ref> SynergyNet (our) NME 6.56 6.04 4.18 4.06 affect the performance under this protocol, and the NME would be higher. We illustrate numerical comparison in <ref type="table">Table 9</ref>. The results show the ability of SynergyNet to recover 3D face models from monocular inputs and attain the best performance. In addition, we further exhibit visual comparison in <ref type="figure" target="#fig_3">Fig. 6</ref>. Our SynergyNet is capable of recovering 3D faces under rare and out-of-domain scenarios, such as heavily cropped or underwater cases. Next, we evaluate the performance of 3D face modeling on Florence [2] with real scanned 3D faces. We follow the protocol from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, which renders 3D face models on different views with pitch of -15, 20, and 25 degrees and yaw of -80, -40, 0, 40, and 80 degrees. The rendered images are used as the test inputs. After reconstruction, face models are cropped to 95mm from the nose tip, and ICP is performed to calculate point-to-plane root mean square error (RMSE) with cropped groundtruth. Numerical results are shown in <ref type="table" target="#tab_0">Table 10</ref>. An error curve that shows our robustness to yaw angle changes and a qualitative comparison on Florence are displayed in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work proposes a synergy process that utilizes the relation between 3D landmarks and 3DMM parameters, and they collaboratively contribute to better performance. We establish a representation cycle, including forward direction, from 3DMM to 3D landmarks, and reverse representation direction, from 3D landmarks to 3DMM. Specifically, We propose two modules, multi-attribute feature aggregation for landmark refinement and the landmark-to-3DMM module. Extensive experiments validate our network design, and we show a detailed performance breakdown for each included attribute and regression target. Our Synergy-Net only adopts simple network operations and attains superior performance, making it a fast, accurate, and easy-toimplement method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture, Hyper-Parameters, and Network Parameter Studies</head><p>Details of architecture. Based on <ref type="figure">Fig. 2</ref> in the main paper (pipeline graph of our SynergyNet), detailed network architecture is described here. Following <ref type="bibr" target="#b19">[20]</ref>, we use MobileNet-V2 as the backbone for 3DMM regression from images. The latent image features z after the global max-pooling is 1280-dim. The pose, shape, and expression decoders are fully-connected (FC) layers with input z and output 3DMM parameters of 12 (? p ), 40 (? s ), and 10 (? e ) dimensions for pose, shape, and expression. <ref type="figure">Fig. S1</ref> shows the network architecture of multi-attribute feature aggregation. Aggregation of the latent image features, ? s , and ? e and global point features forms a 2354dim feature vector. We repeat this vector and append it to the low-level point features to obtain multi-attribute point features, whose size is 68?2418. Later we use another MLP-block to obtain refined landmarks L r . <ref type="figure">Fig. S2</ref> illustrates the architecture of the landmark-to-3DMM module. With L r as the module input, this module reverses the representation direction and regresses 3DMM parameters?, also referred to as landmark geometry in the paper.</p><p>Loss weights. For weights of loss terms (Eq.7 in paper), we choose ? 1 = 0.02, ? 2 = 0.03, ? 3 = 0.02, and ? 4 = 0.001 for training.</p><p>Study on the backbone for 3DMM regression from images. We next conduct a study on the backbone choices for facial alignment using the AFLW full set. We select MobileNet-V2 <ref type="bibr" target="#b41">[41]</ref>, ResNet50 <ref type="bibr" target="#b20">[21]</ref>, ResNet101 <ref type="bibr" target="#b20">[21]</ref>, ResNeSt50 <ref type="bibr" target="#b68">[68]</ref>, and ResNeSt101 <ref type="bibr" target="#b68">[68]</ref> for comparison. From <ref type="table" target="#tab_9">Table S2</ref>, one could see that the 101-residual layer network is too deep, and thus the performance drops compared with the 50-residual layer network. ResNeSt, a splitattention variant of ResNet, can improve the performance for the 101-residual layer case, but the performance of ResNeSt50 is on par with ResNet50. We think this is because the split-attention scales better and remedies the undesirable effects of deeper networks, which are described in their work <ref type="bibr" target="#b68">[68]</ref>.</p><p>Study on the number of network parameters. We further conduct a study to verify the effectiveness of the introduced multi-attribute feature aggregation (MAFA) for landmark refinement and the landmark-to-3DMM modules. The following experiment shows that our performance gain comes from designing the two proposed modules in our synergy process, rather than simply using more network parameters.</p><p>By using MobileNet-V2 as the face image encoder backbone, network parameters of our SynergyNet amount to 3.8M (3.0M for the backbone) and 0.8M for the MAFA and landmark-to-3DMM modules). We build another baseline model, Image ? 3DMM (larger), that only contains 3DMM parameter regression from images using more network parameters. We add additional MLP layers with ReLU and BN, which amount to 0.8M parameters, after the image bottleneck feature z for regressing ? p , ? s , and ? e . Thus, this baseline model and our SynergyNet have approximately the same number of network parameters.</p><p>In <ref type="table" target="#tab_9">Table S2</ref>, we show experiments with the baseline model on facial alignment and face orientation estimation. More network parameter adoption for regressing 3DMM from images only leads to minor improvements. Especially for facial alignment, using extra 0.8M parameters of MLPs only gives 0.02 overall performance gain. The results validate the designed synergy process. Without the proposed modules, using more parameters only brings minor improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation on Reannotated AFLW2000-3D</head><p>Reannotation of AFLW2000-3D is provided in LS3D-W <ref type="bibr" target="#b3">[4]</ref>. Few works, Deng et al <ref type="bibr" target="#b12">[13]</ref>, DHM <ref type="bibr" target="#b46">[46]</ref> and MCG-Net <ref type="bibr" target="#b43">[43]</ref>, report their performance on the annotated version. To aggregate more results for this study, we also in- <ref type="figure">Figure S1</ref>. Detailed structure of MAFA. mlp <ref type="bibr" target="#b64">(64,</ref><ref type="bibr" target="#b64">64)</ref> means two MLP layers with output channel sizes 64 and 64. ReLU and batch normalization are used for each layer. The notations correspond to those in the main paper. (z is latent image feature, ?s and ?e are 3DMM shape and expression parameters regressed from images, L c and L r are 3D landmarks before and after the landmark refinement.) <ref type="table" target="#tab_9">Table S2</ref>. Comparison with the baseline that simply uses more network parameters. The first table shows results on AFLW2000-3D Original for facial alignment, and the second table shows results also on AFLW2000-3D for face orientation estimation. # of params means the number of network parameters. More network parameter use for the baseline 3DMM regression from images only results in a limited performance gain. The experiment validates that the performance gain of our SynergyNet is not simply from more parameter adoption.  <ref type="figure">Figure S2</ref>. Detailed structure of the landmark-to-3DMM module. The notations correspond to those in the main paper. L r is refined 3D landmarks, and? is regressed landmark geometry.</p><p>clude evaluation of 3DDFA <ref type="bibr" target="#b70">[70]</ref>, PRNet <ref type="bibr" target="#b16">[17]</ref>, and 3DDFA-V2 <ref type="bibr" target="#b19">[20]</ref> using their pretrained models. From <ref type="table" target="#tab_9">Table S2</ref>, nor-malized mean errors (NMEs) are generally lower than using the original annotation. This shows the higher quality of the reannotation. Among the methods for comparison, our result is the best and holds a performance gap over others. Compared with PRNet, the second-best method in the table, our improvements are derived from large pose cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion on Reverse Representation Direction</head><p>Why use sparse landmarks rather than full vertices? Landmark geometry? in Sec.3.3 of the main paper describes revealing facial geometry underlying in sparse 3D landmarks. In contrast to sparse landmarks (68 points in our work), mesh from BFM Face includes 53.5K vertices (45K if excluding the neck and ears). When surveying on point processing, research usually adopts only 1024 or 2048 points <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b63">63]</ref>. Much denser points are inefficient for point processing, and the accommodation is also limited by GPU memory. On the other hand, because facial alignment is considered as an upstream task for the downstream application such as face recognition <ref type="bibr" target="#b45">[45]</ref> or recent streaming video compression <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b54">54]</ref>, high efficiency is more desirable. Although a point-sampling strategy could be used for downsizing, 3D landmarks are very efficient and compact for expressing facial traits and outlines. Therefore, 3D landmarks are desirable for predicting facial geometry, and our focus of this work is to exert the synergy between facial landmarks and 3DMM parameters, which collaboratively contribute to better performance.</p><p>Advantage of the reverse representation direction. Compared with 2D images, 3D sparse landmarks describe facial traits and approximate face outlines. Although landmarks are sparse, the representation provides another view to learn facial geometry and complements with 3DMM regressed from images. For example, facial geometry for large pose cases is hard to estimate from the 2D due to selfocclusion. Further, the face orientation is defined in the 3D space; thus, it is more advantageous to estimate face orientation from 3D points, whose learning paradigm provides less ambiguity. From <ref type="table">Table 2</ref> and 7 in the main paper that study contributions for each regression target at the L ? 3D stage, MAFA+L ? 3D(all) improves the performance on facial alignment and face orientation estimation. The results show the ability of the reverse representation direction.</p><p>We also conduct evaluations using? as the output on facial alignment and face orientation estimation using AFLW2000-3D. The 3D landmarks reconstructed by? and the face orientation converted from its pose parameter? p attain an NME of 6.48 on the alignment and an MAE of 5.76 on the orientation estimation. The results are reasonable since the direct input to the landmark-to-3DMM module is L r , 68-point sparse landmarks that present only approximate facial traits and outlines. However, these numerical results are comparable with some methods in <ref type="table" target="#tab_3">Table 4</ref> and 8 of the main paper. The results validate our training strategy so that 3DMM estimation directly from sparse landmarks achieves on par performance with some studies for 3DMM <ref type="figure" target="#fig_0">Figure S3</ref>. Histogram and cumulative histogram of the offset term: L r ? L c 2 2 . These plots show the difference of the landmark set before and after refinement. regression from images, whose information lies on dense grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation on L r v.s. L c</head><p>From <ref type="table" target="#tab_0">Table 1</ref> and 6 in the paper (1 st row: without refinement and use L c for evaluation; 2 nd row: with refinement and use L r for evaluation), one can observe from these two tables that with the refinement branch, the performance is significantly improved. Specifically, <ref type="table" target="#tab_0">Table 1</ref>-facial alignment error: 3.88 (L c ) to 3.49 (L r ); <ref type="table">Table 6</ref>-face orientation error: 4.06 (L c ) to 3.65 (L r ).</p><p>We further show the histograms of offsets L r ? L c 2 2 in <ref type="figure" target="#fig_0">Fig.S3</ref>. We use AFLW2000-3D for evaluation. One can see that the difference of L r and L c peaks at 0.22 pixels for each landmark. This matches the purpose of refinement that by predicting each landmark more precisely, the total improvements can break through the performance bottleneck in the benchmark list (paper <ref type="table" target="#tab_3">Table 4</ref>). In addition, we plot the normalized mean error (NME) for random 200 people in <ref type="figure" target="#fig_1">Fig.S4</ref> (zoom in for the best view). One can find that the blue bars are higher overall, meaning the error using L c is higher than L r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. 3D Faces on Florence</head><p>Based on the Florence experiment in Section 4.3 and Table 10 of the main paper, we further show an error curve comparison in <ref type="figure" target="#fig_2">Fig. S5</ref> for 3D face modeling. Our method is robust to pose changes and attains a nearly flat error curve. Although the results of 2DASL for low and medium cases are close to ours, they are not robust for large pose cases.</p><p>We visualize the reconstructed meshes and compare with the current best-performing 3DMM-based method <ref type="figure" target="#fig_2">Figure S5</ref>. Error curve for 3D face modeling by yaw angle on the Florence dataset. The numbers at the top are mean RMSE over all testing data. Our method is rather robust to pose changes and attains the lowest overall point-to-plane RMSE. <ref type="figure" target="#fig_3">Figure S6</ref>. UV-texture GAN. The generator produces a UV map from a fixed template and an input image. The generated UV map combines structures of the template and the skin color of the image.</p><p>(3DDFA-V2 <ref type="bibr" target="#b19">[20]</ref>) and UV-position-based method (PRNet <ref type="bibr" target="#b16">[17]</ref>) in <ref type="figure">Fig. S9</ref>. We mark point-to-plane RMSEs beside each face model. Our reconstructed faces show narrower eye-to-side distances, higher cheeks, and pointed chins from the upper example. These features are consistent with the groundtruth model. On the other hand, 3DDFA-V2 shows wider faces, unapparent cheeks, and non-pointed chins; thus, their errors are higher. Besides, for a cropping range of 95mm from the nose tip, 3DDFA-V2 shows more forehead areas than the groundtruth model, which means the geometry prediction is inaccurate. PRNet is not 3DMMbased. Although PRNet has higher flexibility to predict pervertex deformation due to its non-parametric nature, it is also harder to estimate a precise 3D face via vertex regression on a UV-position map. From the lower example, our faces are wider and consistent with the groundtruth. In contrast, 3DDFA-V2 shows more elongated shapes for large pose cases. PRNet also shows skewed faces under large pose scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Texture Synthesis</head><p>Most previous works for 3D facial alignment via 3D face modeling mainly focus on the geometry [?, <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71]</ref>. To get more realistic 3D face models, here we also conduct a smaller study on texture synthesis based on our predicted 3D face models.</p><p>Similar to 3DMM fitting for 3D faces, as illustrated in the main paper Eq.(1), textures can also be synthesized by adding a mean texture term and a multiplication term of texture basis and parameters. For example, BFM Face contains texture parameters with a 199-dim texture basis. However, 3DMM texture fitting usually produces over-smooth textures that lack reality. (See examples in <ref type="figure">Fig. S8)</ref>.</p><p>Here we introduce a simple but effective UV-texture Generative Adversarial Network (UV-texture GAN) for texture synthesis. The model structure is illustrated in <ref type="figure" target="#fig_3">Fig. S6</ref>. UV mapping <ref type="bibr" target="#b18">[19]</ref> involves per-vertex color mappings from UV-texture maps. Each vertex is associated with its (u, v)coordinate for querying vertex color from the three-channel UV-texture maps.</p><p>The introduced UV-texture GAN adopts a pixel-to-pixel image translator that transforms unstructured in-the-wild images to a canonical UV space to generate the UV-texture maps from images. However, pixel-to-pixel style transfer <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b53">53]</ref> retains input image structures, such as salient object outlines, and produces a different style artifact. It is hard to map unconstrained face images onto the canonical UV space by direct pixel-to-pixel translators. To resolve the issue, we further feed a template UV map together with a face image as inputs to the generator <ref type="figure" target="#fig_3">(Fig. S6)</ref>. The template is projected from the mean texture of BFM Face. Further, we shortcut the template to the output for facilitating the training procedure, where the generator learns a mapping from the six-channel input to the residual UV space. We display the ability of the template in <ref type="figure">Fig. S7</ref>.</p><p>To form our training set, we collect about 2K in-the-wild frontal face images and warp the faces onto the UV space with the aids of facial landmarks. The generator and discriminator architectures are the same as pix2pix model <ref type="bibr" target="#b22">[23]</ref>. Least-square GAN (LSGAN) [?] is used as the loss for training. We train the network with 300 epochs. Adam is adopted as the optimizer with an initial learning rate of 0.0002. After 100 epochs, the learning rate starts to drop linearly to 0.</p><p>We show a comparison in <ref type="figure">Fig.S8</ref> for synthesized textures from the introduced UV-texture GAN and conventional 3DMM texture fitting. Results of UV-texture GAN are more realistic and are not over-smooth compared with textures from 3DMM fitting. Skin colors are more similar to images since the introduced UV-texture GAN combines hues from images and structures from the template to produce more realistic textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. More Qualitative Results</head><p>Here we further show more qualitative results on the 300VW dataset for talks or interview videos <ref type="bibr" target="#b44">[44]</ref> in <ref type="figure">Fig.  S10</ref>, S11, S12, S13 and Artistic Faces (AF) for different <ref type="figure">Figure S7</ref>. Effects of using the template in the UV-texture GAN. Without the template shown in <ref type="figure" target="#fig_3">Fig. S6</ref>, facial traits such as eyes and moth are blurry and inaccurate. <ref type="figure">Figure S8</ref>. Synthesized texture comparison. Textures synthesized by the introduced UV-texture GAN are more realistic than textures from 3DMM fitting. artistic style faces <ref type="bibr" target="#b66">[66]</ref> in <ref type="figure" target="#fig_1">Fig. S14</ref>, S15. <ref type="figure">Figure S9</ref>. Reconstructed face comparison by yaw angle on examples from the Florence dataset. Numbers beside the reconstructed models are their normalized point-to-plane RMSEs. Our results are robust to pose changes. For the upper example, 3DDFA-V2 shows wider faces, unapparent cheeks, non-pointed chins, and larger forehead areas with a cropping range of 95mm from the nose tip; therefore, their results hold higher errors. PRNet shows imprecise facial structures. In addition, their faces are twisted for large pose cases. For the lower example, the groundtruth face is wider, but 3DDFA-V2 shows more elongated faces, and PRNet predictions are unreliable. Our results are more similar to the groundturh shape. <ref type="figure">Figure S10</ref>. Results of 3D geometry prediction on 300VW from our method. Row 1-4: images, 3D landmarks, face orientation, 3D faces. <ref type="figure">Figure S11</ref>. (Continued) Results of 3D geometry prediction on 300VW from our method. Our result is robust to motion blur for the right-hand-side case.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Structure of multi-attribute landmark refinement. The input is L c from the foundation face model. The left MLPs extract global point features and fuse the global features with other attributes, including images features, shape, and expression parameters. The concatenation is appended to the low-level features to create multi-attribute point features, which are used to regress the refined landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of representation cycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison of facial alignment and orientation estimation. The case on the left is low-resolution, blurry, and thus challenging. The case on the right is of rare and extreme roll rotation. Our results show more robustness over 3DDFA-V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison of 3D face models. Our results are robust to rare and out-of-domain face examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>S4. NME of random 200 people: The lower the better. Zoom in for the best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S12 .</head><label>S12</label><figDesc>(Continued) Results of 3D geometry prediction on 300VW from our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S13 .</head><label>S13</label><figDesc>(Continued) Results of 3D geometry prediction on 300VW from our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S14 .</head><label>S14</label><figDesc>Results of 3D geometry prediction on Artistic Faces from our method. Row 1-5: images, 3D landmarks, face orientation, 3D faces, textures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S15 .</head><label>S15</label><figDesc>(Continued) Results of 3D geometry prediction on Artistic Faces from our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablative for facial alignment. The first table is for AFLW2000-3D using original groundtruth annotation. The second table is for the reannotated version. '-' means the module is not used, and the corresponding loss terms are not introduced. The first row setting without all the introduced modules contains only a simple baseline of a backbone network to regress 3DMM parameters from only images.</figDesc><table><row><cell>AFLW2000-3D</cell><cell>Multi-Attribute Feature</cell><cell>Landmark-to-</cell><cell>0 to 30</cell><cell>30 to 60</cell><cell>60 to 90</cell><cell>All</cell></row><row><cell>Original</cell><cell>Aggregation for Refinement</cell><cell>3DMM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>2.99</cell><cell>3.80</cell><cell>4.86</cell><cell>3.88</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>2.68</cell><cell>3.32</cell><cell>4.35</cell><cell>3.49</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>2.69</cell><cell>3.57</cell><cell>4.69</cell><cell>3.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.66</cell><cell>3.30</cell><cell>4.27</cell><cell>3.41</cell></row><row><cell>AFLW2000-3D</cell><cell>Multi-Attribute Feature</cell><cell>Landmark-to-</cell><cell>0 to 30</cell><cell>30 to 60</cell><cell>60 to 90</cell><cell>All</cell></row><row><cell>Reannotated</cell><cell>Aggregation for Refinement</cell><cell>3DMM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>2.34</cell><cell>2.99</cell><cell>4.27</cell><cell>3.20</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>2.24</cell><cell>2.67</cell><cell>3.76</cell><cell>2.89</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>2.23</cell><cell>2.69</cell><cell>3.90</cell><cell>2.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.16</cell><cell>2.61</cell><cell>3.66</cell><cell>2.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Settings</cell><cell>0 to 30</cell><cell>30 to 60</cell><cell>60 to 90</cell><cell>All</cell></row><row><cell>Comparison 1</cell><cell>2.63</cell><cell>3.35</cell><cell>4.50</cell><cell>3.49</cell></row><row><cell>Comparison 2</cell><cell>2.62</cell><cell>3.31</cell><cell>4.31</cell><cell>3.41</cell></row><row><cell>Adopted setting</cell><cell>2.66</cell><cell>3.30</cell><cell>4.27</cell><cell>3.41</cell></row></table><note>Landmark-to-3DMM network structure study. Fa- cial alignment on AFLW2000-3D Original is evaluated. Refer to Section 4.1 for the two comparison settings. The last row is the adopted setting introduced in Sec.3.3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Benchmark on AFLW2000-3D for facial alignment. The original annotation version is used. Our performance is the best with a gap over others on large poses.</figDesc><table><row><cell>AFLW2000-3D Original</cell><cell>0 to 30</cell><cell>30 to 60</cell><cell>60 to 90</cell><cell>All</cell></row><row><cell>ESR [6]</cell><cell>4.60</cell><cell>6.70</cell><cell>12.67</cell><cell>7.99</cell></row><row><cell>3DDFA [70]</cell><cell>3.43</cell><cell>4.24</cell><cell>7.17</cell><cell>4.94</cell></row><row><cell>Dense Corr [67]</cell><cell>3.62</cell><cell>6.06</cell><cell>9.56</cell><cell>6.41</cell></row><row><cell>3DSTN [3]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell></row><row><cell>3D-FAN [4]</cell><cell>3.16</cell><cell>3.53</cell><cell>4.60</cell><cell>3.76</cell></row><row><cell>3DDFA-PAMI [71]</cell><cell>2.84</cell><cell>3.57</cell><cell>4.96</cell><cell>3.79</cell></row><row><cell>PRNet [17]</cell><cell>2.75</cell><cell>3.51</cell><cell>4.61</cell><cell>3.62</cell></row><row><cell>2DASL [49]</cell><cell>2.75</cell><cell>3.46</cell><cell>4.45</cell><cell>3.55</cell></row><row><cell>3DDFA-V2 (MR) [20]</cell><cell>2.75</cell><cell>3.49</cell><cell>4.53</cell><cell>3.59</cell></row><row><cell>3DDFA-V2 (MRS) [20]</cell><cell>2.63</cell><cell>3.42</cell><cell>4.48</cell><cell>3.51</cell></row><row><cell>SynergyNet (our)</cell><cell>2.65</cell><cell>3.30</cell><cell>4.27</cell><cell>3.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Quantitative facial alignment comparison on AFLW with 21-point landmark definition.</figDesc><table><row><cell>AFLW</cell><cell>0 to 30</cell><cell>30 to 60</cell><cell>60 to 90</cell><cell>All</cell></row><row><cell>ESR [6]</cell><cell>5.66</cell><cell>7.12</cell><cell>11.94</cell><cell>8.24</cell></row><row><cell>3DDFA [70]</cell><cell>4.75</cell><cell>4.83</cell><cell>6.39</cell><cell>5.32</cell></row><row><cell>3D-FAN [4]</cell><cell>4.40</cell><cell>4.52</cell><cell>5.17</cell><cell>4.69</cell></row><row><cell>3DSTN [3]</cell><cell>3.55</cell><cell>3.92</cell><cell>5.21</cell><cell>4.23</cell></row><row><cell>3DDFA-PAMI [71]</cell><cell>4.11</cell><cell>4.38</cell><cell>5.16</cell><cell>4.55</cell></row><row><cell>PRNet [17]</cell><cell>4.19</cell><cell>4.69</cell><cell>5.45</cell><cell>4.77</cell></row><row><cell>3DDFA-V2 [20]</cell><cell>3.98</cell><cell>4.31</cell><cell>4.99</cell><cell>4.43</cell></row><row><cell>SynergyNet (our)</cell><cell>3.76</cell><cell>3.92</cell><cell>4.48</cell><cell>4.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>for facial alignment. MAE of Euler angles in degree is reported.</figDesc><table><row><cell>Multi-Attribute</cell><cell>Landmark-</cell><cell>Yaw</cell><cell>Pitch</cell><cell>Roll</cell><cell>Mean</cell></row><row><cell>Feature Aggregation</cell><cell>to-3DMM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>-</cell><cell>3.97</cell><cell>4.93</cell><cell>3.28</cell><cell>4.06</cell></row><row><cell></cell><cell>-</cell><cell>3.72</cell><cell>4.37</cell><cell>2.88</cell><cell>3.65</cell></row><row><cell>-</cell><cell></cell><cell>3.67</cell><cell>4.48</cell><cell>2.95</cell><cell>3.70</cell></row><row><cell></cell><cell></cell><cell>3.42</cell><cell>4.09</cell><cell>2.55</cell><cell>3.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Face orientation estimation benchmark comparison on ALFW2000-3D. PnP shows solving perspective-n-point problems using groundtruth landmarks. We do not include PRNet here since it does not infer face orientation directly and also obtains poses by PnP with predicted landmarks.</figDesc><table><row><cell>AFLW2000-3D</cell><cell>Yaw</cell><cell>Pitch</cell><cell>Roll</cell><cell>Mean</cell></row><row><cell>PnP-landmark</cell><cell>5.92</cell><cell>11.76</cell><cell>8.27</cell><cell>8.65</cell></row><row><cell>FAN-12 point [4]</cell><cell>6.36</cell><cell>12.30</cell><cell>8.71</cell><cell>9.12</cell></row><row><cell>HopeNet [40]</cell><cell>6.47</cell><cell>6.56</cell><cell>5.44</cell><cell>6.16</cell></row><row><cell>SSRNet-MD [65]</cell><cell>5.14</cell><cell>7.09</cell><cell>5.89</cell><cell>6.01</cell></row><row><cell>FSANet [64]</cell><cell>4.50</cell><cell>6.08</cell><cell>4.64</cell><cell>5.07</cell></row><row><cell>QuatNet [22]</cell><cell>3.97</cell><cell>5.62</cell><cell>3.92</cell><cell>4.15</cell></row><row><cell>TriNet [7]</cell><cell>4.20</cell><cell>5.77</cell><cell>4.04</cell><cell>3.97</cell></row><row><cell>RankPose [10]</cell><cell>2.99</cell><cell>4.75</cell><cell>3.25</cell><cell>3.66</cell></row><row><cell>3DDFA-TPAMI [71]</cell><cell>4.33</cell><cell>5.98</cell><cell>4.30</cell><cell>4.87</cell></row><row><cell>2DASL [49]</cell><cell>3.85</cell><cell>5.06</cell><cell>3.50</cell><cell>4.13</cell></row><row><cell>3DDFA-V2 [20]</cell><cell>4.06</cell><cell>5.26</cell><cell>3.48</cell><cell>4.27</cell></row><row><cell>SynergyNet (our)</cell><cell>3.42</cell><cell>4.09</cell><cell>2.55</cell><cell>3.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>3D face modeling comparison on Florence. Point-toplane RMSE is calculated for evaluation.</figDesc><table><row><cell>Florence</cell><cell>PRNet</cell><cell>2DASL</cell><cell>3DDFA-V2</cell><cell>SynergyNet</cell></row><row><cell></cell><cell>[17]</cell><cell>[49]</cell><cell>[20]</cell><cell>(our)</cell></row><row><cell>RMSE</cell><cell>2.25</cell><cell>2.05</cell><cell>2.04</cell><cell>1.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 .</head><label>S2</label><figDesc>Backbone study on the AFLW full set. We compare MobileNet<ref type="bibr" target="#b41">[41]</ref>, ResNet<ref type="bibr" target="#b20">[21]</ref>, and ResNeSt<ref type="bibr" target="#b68">[68]</ref>, a ResNet variant with split-attention.</figDesc><table><row><cell>Backbone</cell><cell cols="3">0 to 30 30 to 60 60 to 90</cell><cell>All</cell></row><row><cell>MobileNet</cell><cell>3.86</cell><cell>4.13</cell><cell>4.61</cell><cell>4.20</cell></row><row><cell>ResNet50</cell><cell>3.76</cell><cell>3.92</cell><cell>4.48</cell><cell>4.06</cell></row><row><cell>ResNeSt50</cell><cell>3.76</cell><cell>3.92</cell><cell>4.52</cell><cell>4.07</cell></row><row><cell>ResNet101</cell><cell>3.90</cell><cell>4.14</cell><cell>5.08</cell><cell>4.38</cell></row><row><cell>ResNeSt101</cell><cell>3.78</cell><cell>4.04</cell><cell>4.62</cell><cell>4.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S2 .</head><label>S2</label><figDesc>Comparison on AFLW2000-3D Reannotation. Our method has the best alignment result and holds a performance gap over others.</figDesc><table><row><cell>AFLW2000-3D</cell><cell cols="3">0 to 30 30 to 60 60 to 90</cell><cell>All</cell></row><row><cell>Reannotated</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DHM [46]</cell><cell>2.28</cell><cell>3.10</cell><cell>6.95</cell><cell>4.11</cell></row><row><cell>3DDFA [70]</cell><cell>2.84</cell><cell>3.52</cell><cell>5.15</cell><cell>3.83</cell></row><row><cell>PRNet [17]</cell><cell>2.35</cell><cell>2.78</cell><cell>4.22</cell><cell>3.11</cell></row><row><cell>MGCNet [43]</cell><cell>2.72</cell><cell>3.12</cell><cell>3.76</cell><cell>3.20</cell></row><row><cell>Deng et al [13]</cell><cell>2.56</cell><cell>3.11</cell><cell>4.45</cell><cell>3.37</cell></row><row><cell>3DDFA-V2 [20]</cell><cell>2.84</cell><cell>3.03</cell><cell>4.13</cell><cell>3.33</cell></row><row><cell>SynergyNet (our)</cell><cell>2.05</cell><cell>2.49</cell><cell>3.52</cell><cell>2.65</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We sincerely thank Jingjing Zheng, Jim Thomas, and Cheng-Hao Kuo for their detailed feedback on this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>We document this supplementary into the following sections. In Section B, we provide details of our network architectures and loss weights for training. We further present a study for network backbone choices and a study for showing that our performance gain is not simply from using more network parameters. In Section C, evaluation of facial alignment on AFLW2000-3D reannotation version is exhibited. In Section D, we present analysis and discussion on the reverse representation direction. In Section E, we dig into performance comparison using L r and L c . In Section F, an error curve and visual comparison of 3D face modeling on Florence are illustrated. In Section G, we describe texture synthesis using introduced UV-texture GAN and further compare with textures from 3DMM fitting. In Section H, we add more qualitative results from our face geometry prediction using the 300VW video dataset and Artistic Faces.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nvidia maxine cloud-ai video-streaming platform</title>
		<ptr target="https://developer.nvidia.com/maxine?ncid=so-yout-26905#cid=dl13_so-yout_en-us" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding, J-HGBU &apos;11</title>
		<meeting>the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding, J-HGBU &apos;11</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrasekhar</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression. International Journal of Computer Vision (IJCV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A vector-based representation to enhance head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep, landmark-free fame: Face alignment, modeling, and expression estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comprehensive performance evaluation of deformable face tracking &quot;in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Grigorios G Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rankpose: Learning generalised feature with rank supervision for head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangkit</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuojun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Masked face recognition challenge: The insightface track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The menpo benchmark for multi-pose 2d and 3d facial landmark localisation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="599" to="624" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teacher supervises students how to learn from partially labeled images for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning an animatable detailed 3d face model from in-thewild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolkart</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computer graphics: principles and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hughes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Addison-Wesley Professional</publisher>
			<biblScope unit="volume">12110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quatnet: Quaternion-based head pose estimation with multiregression loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face recognition based on facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniwat</forename><surname>Juhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pintavirooj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 10th Biomedical Engineering International Conference (BMEiCON)</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">From real-time attention assessment to &quot;with-me-ness&quot; in human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?verin</forename><surname>Lemaignan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Jacq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dillenbourg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A prior-less method for multi-face tracking in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust facial landmark tracking via cascade regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (PR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1619" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3317" to="3326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rethinking pseudo-lidar representation. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Annotated Facial Landmarks in the Wild: A Largescale, Real-world Database for Facial Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<meeting>First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep head pose: Gaze-direction estimation in multimodal video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robot reading human gaze: Why eye tracking is better than head tracking for human-robot collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oskar</forename><surname>Palinko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Rea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Sandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Sciutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5048" to="5054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to regress 3d face shape and expression from an image without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7763" to="7772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-supervised monocular 3d face reconstruction by occlusion-aware multiview geometry consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grigoris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How effective are landmarks and their geometry for face recognition? Computer vision and image understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Samal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep evolutionary 3d diffusion heat maps for large-pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fml: Face model learning from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10812" to="10822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2549" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d face reconstruction from a single image assisted by 2d face images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5163" to="5172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Extreme 3d face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3935" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human computer interaction with head pose, eye gaze and body gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="789" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-supervised 3d face reconstruction via conditional estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Occlusion pattern-based dictionary for robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Jiun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Occluded face recognition using low-rank regression with generalized gradient direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Jiun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (PR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Geometry-aware instance segmentation with disparity maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07802,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient multi-domain dictionary learning with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GlobalSIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mvf-net: Multi-view 3d face morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanzi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5661" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ssr-net: A compact soft stagewise regression network for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Husan</forename><surname>Hunag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Cheng</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The face of art: landmark detection and geometric style in portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning dense facial correspondences in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4723" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters (SPL)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3d total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2595" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3d face reconstruction, tracking, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="523" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
