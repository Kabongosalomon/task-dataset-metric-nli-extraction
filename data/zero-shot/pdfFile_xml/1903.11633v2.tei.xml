<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Laplace Landmark Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Snap Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Laplace Landmark Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Landmark localization in images and videos is a classic problem solved in various ways. Nowadays, with deep networks prevailing throughout machine learning, there are revamped interests in pushing facial landmark detectors to handle more challenging data. Most efforts use network objectives based on L 1 or L 2 norms, which have several disadvantages. First of all, the generated heatmaps translate to the locations of landmarks (i.e. confidence maps) from which predicted landmark locations (i.e. the means) get penalized without accounting for the spread: a highscatter corresponds to low confidence and vice-versa. For this, we introduce a LaplaceKL objective that penalizes for low confidence. Another issue is a dependency on labeled data, which are expensive to obtain and susceptible to error. To address both issues, we propose an adversarial training framework that leverages unlabeled data to improve model performance. Our method claims state-of-the-art on all of the 300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in the Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size: 1/8 the number of channels (i.e. 0.0398 MB) is comparable to the state-of-the-art in real-time on CPU. Thus, this work is of high practical value to real-life application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To localize landmarks is to find pixel locations in visual media corresponding to points of interest. In face alignment, these points correspond to face parts. For bodies and hands, landmarks correspond to projections of joints on to the camera plane <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>. Historically, landmark detection and shape analysis tasks date back decades: from Active Shape Models <ref type="bibr" target="#b3">[4]</ref> to Active Appearance Models <ref type="bibr" target="#b2">[3]</ref>, with the latter proposed to analyze and detect facial landmarks.</p><p>A need for more advanced models to handle increasingly tricky views has triggered revamped interest in facial landmark localization. Thus, came a wave of different types * The work was done while the first author was interning at Snap Inc.  <ref type="figure">Figure 1</ref>. Heatmaps generated by softargmax-based models (middle block) and the proposed LaplaceKL (right block), each with heatmaps on the input images (left) and a zoomed-in view of an eye region (right). These heatmaps are confidence scores (i.e. probabilities) that a pixel is a landmark. softargmax-based methods generate highly scattered mappings (low certainty), while the same network trained with our loss is concentrated (i.e. high certainty). We further validate the importance of minimizing scatter experimentally ( <ref type="table" target="#tab_2">Table 2</ref>). Best if viewed electronically. of deep neural architectures that pushed state-of-the-art on more challenging datasets. These modern-day networks are trained end-to-end on paired labeled data (d, s), where d is the image and s are the actual landmark coordinates. Many of these used encoder-decoder style networks to generate feature maps (i.e. heatmaps) to transform into pixel coordinates <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>. The network must be entirely differentiable to train end-to-end. Hence, the layer (or operation) for transforming the K heatmaps to pixel coordinates must be differentiable <ref type="bibr" target="#b14">[15]</ref>. Note that each of the K heatmaps corresponds to the coordinates of a landmark. Typically, the softargmax operation determines the location of a landmark as the expectation over the generated 2D heatmaps. Thus, metrics like or determine the distance between the actual arXiv:1903.11633v2 [cs.CV] 14 Aug 2019 and predicted coordinatess, i.e. e =s ? s.</p><p>There are two critical shortcomings of the methodology discussed above. (1) These losses only penalize for differences in mean values in coordinate space, and with no explicit penalty for the variance of heatmaps. Thus, the generated heatmaps are highly scattered: high variance means low confidence. <ref type="bibr" target="#b1">(2)</ref> This family of objectives is entirely dependent on paired training samples (i. <ref type="figure">e. (d, s)</ref>). However, obtaining high-quality data for this is expensive and challenging. Not only does each sample require several marks, but unintentional, and often unavoidable, labels are of pixellevel marks subject to human error (i.e. inaccurate and imprecise ground-truth labels). All the while, plenty of unlabeled face data are available for free.</p><p>In this paper, we propose a practical framework to satisfy the two shortcomings. Thus, our first contribution alleviates the first issue. For this, we introduce a new loss function that penalizes for the difference in distribution defined by location and scatter ( <ref type="figure">Fig. 1</ref>). Independently, we treat landmarks as random variables with Laplace(s, 1) distributions, from which the KL-divergence between the predicted and ground-truth distributions defines the loss. Hence, the goal is to match distributions, parameterized by both a mean and variance, to yield heatmaps of less scatter (i.e. higher confidence). We call this objective the LaplaceKL loss.</p><p>Our second contribution is an adversarial training framework for landmark localization. We propose this to tackle the problem of paired data requirements by leveraging unlabeled data for free. We treat our landmark detection network as a generator (G) of normalized heatmaps (i.e. probability maps) that pass to the discriminator (D) to learn to distinguish between the true and generated heatmaps. This allows for large amounts of unlabeled data to further boost the performance of our LaplaceKL-based models. In the end, D proves to improve the predictive power of the LaplaceKL-based model by injecting unlabeled data into the pipeline during training. As supported by experiments, the adversarial training framework complements the proposed LaplaceKL loss (i.e. an increase in unlabeled data results in a decrease in error). To demonstrate this, we first show the effectiveness of the proposed loss by claiming state-of-the-art without adversarial training to then further improve with more unlabeled data added during training! Furthermore, we reduced the size of the model by using <ref type="bibr" target="#b0">1</ref> 16 , 1 8 , 1 4 , and 1 2 the original number of convolution filters, with the smallest costing only 79 Kb on disk. We show an accuracy drop for models trained with the proposed LaplaceKL as far less than the others trained with a softargmax-based loss. So again, it is the case that more unlabeled training data results in less of a performance drop at reduced sizes. It is essential to highlight that variants of our model at or of larger size than 1/8 the original size compare well to the existing state-of-the-art. We claim that the pro-posed contributions are instrumental for landmark detection models used in real-time production, mobile devices, and other practical purposes.</p><p>Our contributions are three-fold: (1) A novel Laplace KL-divergence objective to train landmark localization models that are more certain about predictions; (2) An adversarial training framework that leverages large amounts of unlabeled data during training; <ref type="bibr" target="#b2">(3)</ref> Experiments that show our model outperforms recent works in face landmark detection, along with ablation studies that, most notably, reveal our model compares well to state-of-the-art at 1/8 its original size (i.e. &lt;160 Kb) and in real-time (i.e. &gt;20 fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review relevant works on landmark localization and generative adversarial network (GAN).</p><p>Landmark localization has been of interest to researchers for decades. At first, most methods were based on Active Shape Models <ref type="bibr" target="#b3">[4]</ref> and Active Appearance Models <ref type="bibr" target="#b2">[3]</ref>. Then, Cascaded Regression Methods (CRMs) were introduced, which operate sequentially; starting with the average shape, then incrementally shifting the shape closer to the target shape. CRMs offer high speed and accuracy (i.e. &gt;1,000 fps on CPU <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>).</p><p>More recently, deep-learning-based approaches have prevailed in the community due to end-to-end learning and improved accuracy. Initial works mimicked the iterative nature of cascaded methods using recurrent convolutional neural networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Besides, several have been several methods for dense landmark localization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref> and 3D face alignment <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref> proposed: all of which are fully-supervised and, thus, require labels for each image.</p><p>Nowadays, there is an increasing interest in semisupervised methods for landmark localization. Recent work used a sequential multitasking method which was capable of injecting labels of two types into the training pipeline, with one type constituting the annotated landmarks and the other type consisting of facial expressions (or handgestures) <ref type="bibr" target="#b14">[15]</ref>. The authors argued that the latter label type was more easily obtainable, and showed the benefits of using both types of annotations by claiming state-ofthe-art on several tasks. Additionally, they explore other semi-supervised techniques (e.g. equivariance loss). In <ref type="bibr" target="#b7">[8]</ref>, a supervision-by-registration method was proposed, which significantly utilized unlabeled videos for training a landmark detector. The fundamental assumption was that the neighboring frames of the detected landmarks should be consistent with the optical flow computed between the frames. This approach demonstrated a more stable detector for videos, and improved accuracy on public benchmarks.</p><p>Landmark localization data resources have significantly evolved as well, with the 68-point mark-up scheme of the MultiPIE dataset <ref type="bibr" target="#b10">[11]</ref> widely adopted. Despite the initial </p><formula xml:id="formula_0">K q k = " &gt; A A A C A H i c b Z D L S s N A F I Z P v N Z 6 i 7 p w o Y v B I r g q i Q i 6 L L g R d F H B X q A N Z T K d t E N n k j A z E U r I x l d x 4 0 I R t z 6 G O 9 / G S Z q F t v 4 w 8 P G f c 5 h z f j / m T G n H + b a W l l d W 1 9 Y r G 9 X N r e 2 d X X t v v 6 2 i R B L a I h G P Z N f H i n I W 0 p Z m m t N u L C k W P q c d f 3 K d 1 z u P V C o W h Q 9 6 G l N P 4 F H I A k a w N t b A P u w L r M c E 8 / Q u G x Q s R X p r 2 K 4 5 d a c Q W g S 3 h B q U a g 7 s r / 4 w I o m g o S Y c K 9 V z n V h 7 K Z a a E U 6 z a j 9 R N M Z k g k e 0 Z z D E g i o v L Q 7 I 0 K l x h i i I p H m h R o X 7 e y L F Q q m p 8 E 1 n v q K a r + X m f 7 V e o o M r L 2 V h n G g a k t l H Q c K R j l C e B h o y S Y n m U w O Y S G Z 2 R W S M J S b a Z F Y 1 I b j z J y 9 C + 7 z u O n X 3 / q L W O C 7 j q M A R n M A Z u H A J D b i B J r S A Q A b P 8 A p v 1 p P 1 Y r 1 b H 7 P W J a u c O Y A / s j 5 / A E r p l r g = &lt; / l a t e x i t &gt; Unlabelled (d u )</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s g h y Y u 5 d 7 x h 0 y E 2 L n g 6 D g n h 1 q       excitement for MultiPIE throughout the landmark localization community <ref type="bibr" target="#b47">[48]</ref>, it is now considered one of the easy datasets captured entirely in a controlled lab setting. A more challenging dataset, Annotated Facial Landmarks in the Wild (AFLW) <ref type="bibr" target="#b19">[20]</ref>, was then released with up to 21 facial landmarks per face (i.e. occluded or "invisible" landmarks were not marked). Finally, came the 300W dataset made-up of face images from the internet, labeled with the same 68-point mark-up scheme as MultiPIE, and promoted as a data challenge <ref type="bibr" target="#b26">[27]</ref>. Currently, 300W is among the most widely used benchmarks for facial landmark localization. In addition to 2D datasets, the community created several datasets annotated with 3D keypoints <ref type="bibr" target="#b0">[1]</ref>.</p><formula xml:id="formula_1">x A = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f U V f i Z r A I d V M S E X R Z c O O y g n 1 A m 5 b J Z N I O n U z C z E Q p o f + h g g t F 3 P o v 7 v w b J 2 0 X 2 n p g 4 H D O v d w z x 0 8 4 U 9 p x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 Z + 8 f N F W c S k I b J O a x b P t Y U c 4 E b W i m O W 0 n k u L I 5 7 T l j 6 5 z v 3 V P p W K x u N P j h H o R H g g W M o K 1 k X q V b o T 1 0 A + z Y N J L z / p 2 2 a k 6 U 6 B l 4 s 5 J G e a o 9 + 2 v b h C T N K J C E 4 6 V 6 r h O o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S E B 7 R j q M A R V V 4 2 T T 1 B p 0 Y J U B h L 8 4 R G U / X 3 R o Y j p c a R b y b z k G r R y 8 X / v E 6 q w y s v Y y J J N R V k d i h M O d I x y i t A A Z O U a D 4 2 B B P J T F Z E h l h i o k 1 R J V O C u /</formula><formula xml:id="formula_2">A = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f U V f i Z r A I d V M S E X R Z c O O y g n 1 A m 5 b J Z N I O n U z C z E Q p o f + h g g t F 3 P o v 7 v w b J 2 0 X 2 n p g 4 H D O v d w z x 0 8 4 U 9 p x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 Z + 8 f N F W c S k I b J O a x b P t Y U c 4 E b W i m O W 0 n k u L I 5 7 T l j 6 5 z v 3 V P p W K x u N P j h H o R H g g W M o K 1 k X q V b o T 1 0 A + z Y N J L z / p 2 2 a k 6 U 6 B l 4 s 5 J G e a o 9 + 2 v b h C T N K J C E 4 6 V 6 r h O o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S E B 7 R j q M A R V V 4 2 T T 1 B p 0 Y J U B h L 8 4 R G U / X 3 R o Y j p c a R b y b z k G r R y 8 X / v E 6 q w y s v Y y J J N R V k d i h M O d I x y i t A A Z O U a D 4 2 B B P J T F Z E h l h i o k 1 R J V O C u /</formula><formula xml:id="formula_3">A = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f U V f i Z r A I d V M S E X R Z c O O y g n 1 A m 5 b J Z N I O n U z C z E Q p o f + h g g t F 3 P o v 7 v w b J 2 0 X 2 n p g 4 H D O v d w z x 0 8 4 U 9 p x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 Z + 8 f N F W c S k I b J O a x b P t Y U c 4 E b W i m O W 0 n k u L I 5 7 T l j 6 5 z v 3 V P p W K x u N P j h H o R H g g W M o K 1 k X q V b o T 1 0 A + z Y N J L z / p 2 2 a k 6 U 6 B l 4 s 5 J G e a o 9 + 2 v b h C T N K J C E 4 6 V 6 r h O o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S E B 7 R j q M A R V V 4 2 T T 1 B p 0 Y J U B h L 8 4 R G U / X 3 R o Y j p c a R b y b z k G r R y 8 X / v E 6 q w y s v Y y J J N R V k d i h M O d I x y i t A A Z O U a D 4 2 B B P J T F Z E h l h i o k 1 R J V O C u /</formula><formula xml:id="formula_4">A = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f U V f i Z r A I d V M S E X R Z c O O y g n 1 A m 5 b J Z N I O n U z C z E Q p o f + h g g t F 3 P o v 7 v w b J 2 0 X 2 n p g 4 H D O v d w z x 0 8 4 U 9 p x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 Z + 8 f N F W c S k I b J O a x b P t Y U c 4 E b W i m O W 0 n k u L I 5 7 T l j 6 5 z v 3 V P p W K x u N P j h H o R H g g W M o K 1 k X q V b o T 1 0 A + z Y N J L z / p 2 2 a k 6 U 6 B l 4 s 5 J G e a o 9 + 2 v b h C T N K J C E 4 6 V 6 r h O o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S E B 7 R j q M A R V V 4 2 T T 1 B p 0 Y J U B h L 8 4 R G U / X 3 R o Y j p c a R b y b z k G r R y 8 X / v E 6 q w y s v Y y J J N R V k d i h M O d I x y i t A A Z O U a D 4 2 B B P J T F Z E h l h i o k 1 R J V O C u /</formula><formula xml:id="formula_5">= " &gt; A A A C A X i c b Z D L S s N A F I Y n 9 V b r L e p G 0 M V g E V y V R A R d F t y 4 c F H B X q A N Y T K Z t E N n J m F m U i i h b n w V N y 4 U c e t b u P N t n K R Z a O s P A x / / O Y c 5 5 w 8 S R p V 2 n G + r s r K 6 t r 5 R 3 a x t b e / s 7 t n 7 B x 0 V p x K T N o 5 Z L H s B U o R R Q d q a a k Z 6 i S S I B 4 x 0 g / F N X u 9 O i F Q 0 F g 9 6 m h C P o 6 G g E c V I G 8 u 3 j w Y c 6 R F G L L u b + Q V L n q F w M v P t u t N w C s F l c E u o g 1 I t 3 / 4 a h D F O O R E a M 6 R U 3 3 U S 7 W V I a o o Z m d U G q S I J w m M 0 J H 2 D A n G i v K y 4 Y A b P j B P C K J b m C Q 0 L 9 / d E h r h S U x 6 Y z n x H t V j L z f 9 q / V R H 1 1 5 G R Z J q I v D 8 o y h l U M c w j w O G V B K s 2 d Q A w p K a X S E e I Y m w N q H V T A j u 4 s n L 0 L l o u E 7 D v b + s N 0 / K O K r g G J y C c + C C K 9 A E t 6 A F 2 g C D R / A M X s G b 9 W S 9 W O / W x 7</formula><formula xml:id="formula_6">= " &gt; A A A C A X i c b Z D L S s N A F I Y n 9 V b r L e p G 0 M V g E V y V R A R d F t y 4 c F H B X q A N Y T K Z t E N n J m F m U i i h b n w V N y 4 U c e t b u P N t n K R Z a O s P A x / / O Y c 5 5 w 8 S R p V 2 n G + r s r K 6 t r 5 R 3 a x t b e / s 7 t n 7 B x 0 V p x K T N o 5 Z L H s B U o R R Q d q a a k Z 6 i S S I B 4 x 0 g / F N X u 9 O i F Q 0 F g 9 6 m h C P o 6 G g E c V I G 8 u 3 j w Y c 6 R F G L L u b + Q V L n q F w M v P t u t N w C s F l c E u o g 1 I t 3 / 4 a h D F O O R E a M 6 R U 3 3 U S 7 W V I a o o Z m d U G q S I J w m M 0 J H 2 D A n G i v K y 4 Y A b P j B P C K J b m C Q 0 L 9 / d E h r h S U x 6 Y z n x H t V j L z f 9 q / V R H 1 1 5 G R Z J q I v D 8 o y h l U M c w j w O G V B K s 2 d Q A w p K a X S E e I Y m w N q H V T A j u 4 s n L 0 L l o u E 7 D v b + s N 0 / K O K r g G J y C c + C C K 9 A E t 6 A F 2 g C D R / A M X s G b 9 W S 9 W O / W x 7</formula><formula xml:id="formula_7">= " &gt; A A A C A X i c b Z D L S s N A F I Y n 9 V b r L e p G 0 M V g E V y V R A R d F t y 4 c F H B X q A N Y T K Z t E N n J m F m U i i h b n w V N</formula><p>GANs were recently introduced <ref type="bibr" target="#b9">[10]</ref>, quickly becoming popular in research and practice. GANs have been used to generate images <ref type="bibr" target="#b24">[25]</ref> and videos <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>, and to do image manipulation <ref type="bibr" target="#b8">[9]</ref>, text-to-image <ref type="bibr" target="#b41">[42]</ref>, image-to-image <ref type="bibr" target="#b44">[45]</ref>, video-to-video <ref type="bibr" target="#b35">[36]</ref> translation and re-targeting <ref type="bibr" target="#b29">[30]</ref>.</p><p>An exciting feature of GANs are the ability to transfer visual media across different domains. Thus, various semi-supervised and domain-adaptation tasks adopted GANs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>. Many have leveraged synthetic data to improve model performance on real data. For example, a GAN transferred images of human eyes from the real domain to bootstrap training data <ref type="bibr" target="#b28">[29]</ref>. Other researchers used them to synthetically generate photo-realistic images of outdoor scenes, which also aided in bettering performance in image segmentation <ref type="bibr" target="#b12">[13]</ref>. Sometimes, labeling images captured in a controlled setting is manageable (i.e. versus an uncontrolled setting). For instance, 2D body pose annotations were available in-the-wild, while 3D annotations mostly were for images captured in a lab setting. Therefore, images with 3D annotations were used in adversarial training to predict 3D human body poses as seen in-the-wild <ref type="bibr" target="#b40">[41]</ref>. <ref type="bibr" target="#b5">[6]</ref> formulated one-shot recognition as a problem data imbalance and augmented additional samples in the form of synthetically generated face embeddings.</p><p>Our work differs from these others in several ways. Firstly, a majority, if not all, used a training objective that only accounts for the location of landmarks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, i.e. no consideration for variance (i.e. confidence). Thus, landmarks distributions have been assumed to be describable with a single parameter (i.e. a mean). Networks trained this way yield an uncertainty about the prediction, while still providing a reasonable location estimate. To mitigate this, we explicitly parametrize the distribution of landmarks using location and scale. For this, we propose a KLdivergence based loss to train the network end-to-end. Secondly, previous works used GANs for domain adaptation in some fashion. In this work, we do not perform any adaptation between domains as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>, nor do we use any additional training labels as in <ref type="bibr" target="#b14">[15]</ref>. Specifically, we have D do the quality assessment on the predicted heatmaps for a given image. The resulting gradients are used to improve the ability of the generator to detect landmarks. We show that both contributions improve accuracy when used separately. Then, the two contributions are combined to further boost state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our training framework utilizes both labeled and unlabeled data during training. Shown in <ref type="figure" target="#fig_7">Fig. 2</ref> are the highlevel graphical depiction of cases where labels are available (blue arrows) and unavailable (red arrows). Notice the framework has two branches, supervised (Eq. 3) and unsupervised (Eq. 7), where only the supervised (blue arrow)uses labels to train. Next, are details for both branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully Supervised Branch</head><p>We define the joint distribution of the image d ? R h?w?3 and landmarks s ? R K?2 as p(d, s), where K is the total number of landmarks. The form of the distribution p(d, s) is unknown; however, joint samples are available when labels are present (i. <ref type="figure">e. (d, s) ? p(d, s)</ref>). During training, we aim to learn a conditional distribution q ? (s|d) modeled by a neural network with parameters ?. Landmarks are then detected done by samplings ? q ? (s|d). We now omit parameters ? from notation for cleaner expressions. The parameter values are learned by maximizing the likelihood that the process described by the model did indeed produce the data that was observed, i.e. trained by minimizing the following loss function w.r.t. its parameters:</p><formula xml:id="formula_8">L(?) = E (d,s)?p(d,s) s ? s 2 .<label>(1)</label></formula><p>Alternatively, it is possible to train a neural network to predict normalized probability maps(i.e. heatmaps):h ? q(h|d), where h ? R K?h?w and each h k ? R h?w represents a normalized probability map for landmark k, where k = 1..K. To get the pixel locations, one could perform the argmax operation over the heatmaps by setting s = argmax(h)). However, this operation is not differentiable and, therefore, unable to be trained end-to-end. A differentiable variant of argmax (i.e. softargmax <ref type="bibr" target="#b1">[2]</ref>) was recently used to localize landmarks <ref type="bibr" target="#b14">[15]</ref>. For the 1D case, the softargmax operation is expressed</p><formula xml:id="formula_9">softargmax(?h) = x softmax(?h x ) ? x = x e ?hx j e ?hj ? x = x p(x) ? x = E h [x],<label>(2)</label></formula><p>where h x is the predicted probability mass at location x, j e ?hj is the normalization factor, and ? is the temperature factor controlling the predicted distribution <ref type="bibr" target="#b1">[2]</ref>. We denote coordinate in boldface (i.e. x = (x 1 , x 2 )), and write 2D softargmax operation ass = E h [x] with L SAM = L(?).</p><p>Essentially, the softargmax operation is the expectation of the pixel coordinate over the selected dimension. Hence, the softargmax-based loss assumes the underlying distribution is describable by just its mean (i.e. location), regardless of how sure a prediction, the objective then is to match mean values. To avoid cases in which the trained model is uncertain about the predicted mean, while still yielding a low error, we parametrize the distribution using {?, ?}, where ? is the mean or the location and ? is the variance or the scale, respectfully, for the selected distribution.</p><p>We want the model to be certain about the predictions (i.e. a small variance or scale). We consider two parametric distributionsGaussian(?, ?) and Laplace(?, b) with</p><formula xml:id="formula_10">Data: {(d l i , s l i )} i=1,...,n , {(d u i )} i=1,...,m ? D , ? G ? initialize network parameters while t ? T do (D l t , S l t ) ? sample mini-batch from labeled data (D u t ) ? sample mini-batch from unlabeled data H fake ? G(D u t ) H real ? ?(S l t ) L adv ? log D([D l t , H real ]) + log(1 ? D([D u t , H fake ]) L G ? compute loss using Eq. 2 or Eq. 3 // update model parameters ? D + ? ? ?? ? D L adv ? G + ? ? ?? ? G (L G ? ?L adv ) end</formula><p>Algorithm 1: Training the proposed model. Denoting the true conditional distribution of the landmarks as p(s|d) we define the objective as follows:</p><formula xml:id="formula_11">? 2 = E h [(x ? E h [x]) 2 ] and b = E h [|x ? E h [x]|].</formula><formula xml:id="formula_12">L KL = E (d,s)?p(d,s) D KL (q(s|d)||p(s|d)) , (3)</formula><p>where D KL is the KL-divergence. We assumed a true distribution for the case of Gaussian (i.e. Gaussian(?, 1), where ? is the ground-truth locations of the landmarks). For the case with Laplace, we sought Laplace(?, 1). KLdivergence conveniently has a closed-form solution for this family of exponential distributions <ref type="bibr" target="#b13">[14]</ref>. Alternatively, sampling yields an approximation. The blue arrow in <ref type="figure" target="#fig_7">Fig. 2</ref> represent the labeled branch of the framework.</p><p>Statistically speaking, given two estimators with different variances, we would prefer one that has a smaller variance (see <ref type="bibr" target="#b6">[7]</ref> for an analysis of the bias-variance trade-off). A lower variance implies higher confidence in the prediction. To this end, we found an objective measuring distance between distributions is accurate and robust. The neural network must satisfy an extra constraint on variance and, thus, yields predictions of higher certainty. See higher confident heatmaps in <ref type="figure" target="#fig_9">Fig. 1 and Fig. 3</ref>. The experimental evaluation further validates this ( <ref type="table" target="#tab_2">Table 2 and Table 3</ref>). Also, <ref type="figure" target="#fig_13">Fig. 5</ref> shows sample results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised Branch</head><p>The previous section discusses several objectives to train the neural network with the available paired or fully labeled data (i.e. (d l , s l )). We denote data samples with the superscript l to distinguish them from unpaired or unlabeled data (d u ). In general, it is difficult for a human to label many images with landmarks. Hence, unlabeled data are abundant and easier to obtain, which calls for capitalizing on this abundant data to improve training. In order to do so, we adapt the adversarial learning framework for landmark localization. We treat our landmarks predicting network as a generator (G), G = q(h|d). discriminator (D) takes the form <ref type="figure">D([d, h]</ref>), where [?, ?] is a tensor concatenation operation . We define the real samples for D as {d l , h = ?(s l )}, where ?(?) generates the true heatmaps given the locations of the ground-truth landmarks. Fake samples are given by {d u ,h ? q(h|d u )}. With this notation, and we define the min-max objective for landmark localization as:</p><formula xml:id="formula_13">min G max D L adv (D, G),<label>(4)</label></formula><p>where L adv (D, G) writes as:</p><formula xml:id="formula_14">E (d l ,s l )?p(d,s) log D([d l , ?(s l )]) + E (d u )?p(d) log(1 ? D([d u , G(d u ))]) .<label>(5)</label></formula><p>In this setting, provided an input image, the goal of D is to learn to decipher between the real and fake heatmaps from appearance. Thus, the goal of G is to produce fake heatmaps that closely resemble the real. Within this framework, D intends to provide additional guidance for G by learning from both labeled and unlabeled data. The objective in Eq. 4 is solved using alternating updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>We fused the softargmax-based and adversarial losses as</p><formula xml:id="formula_15">min G max D ? ? L adv (G, D) + L SAM (G) ,<label>(6)</label></formula><p>with the KL-divergence version of the objective defined as:</p><formula xml:id="formula_16">min G max D ? ? L adv (G, D) + L KL (G) ,<label>(7)</label></formula><p>with the weight for the adversarial loss ? = 0.001. This training objective includes both labeled and unlabeled data in the formulation. In the experiments, we show that this combination significantly improves the accuracy of our approach. We also argue that the softargmax-based version cannot fully utilize the unlabeled data since the predicted heatmaps differ too much from the real heatmaps. See Algorithm 1 for the training procedure for T steps of the proposed model. We show the unlabeled branch of the framework is shown graphically in red arrows <ref type="figure" target="#fig_7">(Fig. 2)</ref>. <ref type="table">Table 1</ref>. Architecture of the generator (G). Layers listed with the size and number of filters (i.e. h ? w ? n). DROP, MAX, and UP stand for dropout (probability 0.2), max-pooling (stride 2), and bilinear upsampling (2x). Note the skip connections about the bottleneck: coarse-to-fine, connecting encoder (i.e. EID) to the decoder (i.e. DID) by concatenating feature channels before fusion via fully-connected layers. Thus, all but the 2 topmost layers had feature dimensions and the number of feature maps preserved (i.e. layers that transformed feature maps to K heatmaps). A stride of 1 and padded such to produce output sizes listed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation</head><p>We follow the ReCombinator network (RCN) initially proposed in <ref type="bibr" target="#b15">[16]</ref>. Specifically, we use a 4-branch RCN as our base model, with input images and output heatmaps of size 80?80. Convolutional layers of the encoder consist of 64 channels, while the convolutional layers of the decoder output 64 channels out of the 128 channels at its input (i.e. 64 channels from the previous layer concatenated with the 64 channels skipped over the bottleneck via branching). We applied Leaky-ReLU, with a negative slope of 0.2, on all but the last convolution layer. See <ref type="table">Table 1</ref> for details on the generator architecture. Drop-out followed this, after all but the first and last activation. We use Adam optimizer with a learning rate of 0.001 and weight decay of 10 ?5 . In all cases, networks were trained from scratch, using no data augmentation nor any other 'training tricks. <ref type="bibr">'</ref> D was a 4-layered PatchGAN <ref type="bibr" target="#b16">[17]</ref>. Before each convolution layer Gaussian noise (? = 0.2) was added <ref type="bibr" target="#b33">[34]</ref>, and then batch-normalization (all but the top and bottom layers) and Leaky-ReLU with a negative slope of 0.2 (all but the top layer). The original RGB image was stacked on top of the K heatmaps from G and fed as the input of D <ref type="figure" target="#fig_7">(Fig. 2)</ref>. Thus, D takes in (K + 3) channels. We set ? = 1 for 2. Pytorch was used to implement the entire framework. An important note to make is that models optimized with Laplace distribution consistently outperformed the Gaussian-based. For instance, our LaplaceKL baseline had a Normalized Mean Square Error (NMSE) of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluated the proposed on two widely used benchmark datasets for face alignment. No data augmentation techniques used when training our models nor was the learning rate dropped: this leaves no ambiguity into whether or not the improved performance came from training tricks or the learning component itself. All results for the proposed were from models trained for 200 epochs.</p><p>We next discuss the metric used to evaluate performance, NMSE, with differences between datasets in the normalization factor. Then, the experimental settings, results, and analysis for each dataset are covered separately. Finally, ablation studies show characterizations of critical hyperparameters and, furthermore, the robustness of the proposed LaplaceKL+D(70K) with a comparable performance with just 1/8 the number of feature channels and &gt;20 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metric</head><p>Per convention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>, NMSE, a normalized average of euclidean distances, was used. Mathematically speaking:</p><formula xml:id="formula_17">NMSE = K k=1 s k ?s k 2 K ? d ,<label>(8)</label></formula><p>where the number of visible landmarks set as K, k = {1, 2, ..., K} are the indices of the visible landmark, the nor-malization factor d depends on the face size, and s k ? R 2 ands k ? R 2 are the ground-truth and predicted coordinates, respectfully. The face size d ensured that the NMSE scores across faces of different size were fairly weighted. Following predecessors, NMSE was used to evaluate both datasets, except with different points referenced to calculate d. The following subsections provide details for finding d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">300W + MegaFace</head><p>The 300W dataset is amongst the most popular datasets for face alignment. It has 68 visible landmarks (i.e. K = 68) for 3,837 images (i.e. 3,148 training and 689 test). We followed the protocol of the 300W challenge <ref type="bibr" target="#b26">[27]</ref> and evaluated using NMSE (Eq. 8), where d is set as the inter-ocular distance (i.e. distance between outer corners of the eyes). Per convention, we evaluated different subsets of 300W (i.e. common and challenge, which together form full).</p><p>We compared the performance of the proposed objective trained in a semi-supervised fashion. During training, 300W dataset made-up the labeled data (i.e. real), and a random selection from MegaFace provided the unlabeled data (i.e. fake) <ref type="bibr" target="#b21">[22]</ref>. MTCNN 1 was used to detect five landmarks (i.e. eye pupils, corners of the mouth, and middle of nose and chin) <ref type="bibr" target="#b42">[43]</ref>, which allowed for similar face crops from either dataset. Specifically, we extended the square hull that enclosed the five landmarks by 2? the radii in each direction. In other words, the smallest bounding box spanning the 5 points (i.e. the outermost points lied on the parameter), and then transformed from rectangles-to-squares with sides of length 2? max(height, width). Note that the midpoint of the original rectangle was held constant to avoid shift translations (i.e. rounded up a pixel if the radius was even and extended in all directions).</p><p>The LaplaceKL+D(70K) model obtained state-of-theart on 300W, yielding the lowest error on 300W (Table 2 (300W columns)).</p><p>LaplaceKL+D(N ) and softargmax+D(N ) denote the models trained with unlabeled data, where N representing the number of unlabeled images added from MegaFace.</p><p>First, notice that LaplaceKL trained without unlabeled data still achieved state-of-the-art. The LaplaceKL-based models then showed relative improvements with more unlabeled data added. The softargmax-based models cannot fully take advantage of the unlabeled data without minimizing for variance (i.e. generates heatmaps of less confidence and, thus, more spread). Our LaplaceKL, on the other hand, penalizes for spread (i.e. scale), making the job of D more challenging. As such, LaplaceKL-based models benefit from increasing amounts of unlabeled data.</p><p>Also, notice the largest gap between the baseline models <ref type="bibr" target="#b7">[8]</ref> and our best LaplaceKL+D(70K) model on the different sets of 300W. Adding more unlabeled helps more (i.e. LaplaceKL vs. LaplaceKL+D(70K) improvement is about 2.53%). However, it is essential to use samples not covered in the labeled set. To demonstrate this, we set the real and fake sets to 300W (i.e. d l = d u in the second term of Eq. 7). NMSE results for this experiment are listed as follows: LaplaceKL+D(300W) 4.06 (baseline-4.01) and softargmax+D(300W) 4.26 (baseline-4.24). As hypothesized, all the information from the labeled set had already been extracted in the supervised branch, leaving no benefit of using the same set in the unsupervised branch. Therefore, more unlabeled data yields more hard negatives to train with, which improves the accuracy of the rarely seen samples ( Additionally, the adversarial framework further boosted our 300W baseline was further boosted by (i.e. more unlabeled data yields a lower NMSE). Specifically, we demonstrated this by pushing state-of-the-art of the proposed on 300W from a NMSE of 4.01 to 3.91 (i.e. no unlabeled data to 70K unlabeled pairs, respectfully). There were boosts at each step size of full (i.e. larger N ? NMSE).</p><p>We randomly selected unlabeled samples for LaplaceKL+D(70K) and softargmax+D(70K) to visualize predicted heatmaps <ref type="figure" target="#fig_9">(Fig. 3)</ref>. In each case, the heatmaps produced by the softargmax-based models spread wider, explaining the worsened quantitative scores (Table 2). The models trained with the proposed contributions tend to yield higher probable pixel location (i.e. a more concentrated predicted heatmaps). For most images, the heatmaps generated by models trained with the LaplaceKL loss have distributions for landmarks that were more confident and properly distributed: our LaplaceKL+D(70K) yielded heatmaps that vary 1.02 pixels from the mean, while softargmax+D(70K) has a variation of 2.59. Learning the landmark distributions with our LaplaceKL loss is conceptually and theoretically intuitive ( <ref type="figure">Fig. 1)</ref>. Moreover, it is experimentally proven ( <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The AFLW dataset</head><p>We evaluated the LaplaceKL loss on the AFLW dataset <ref type="bibr" target="#b19">[20]</ref>. AFLW contains 24,386 faces with up to 21 landmarks annotations and 3D head pose labels. Following <ref type="bibr" target="#b14">[15]</ref>, 20,000 faces were used for training with the other 4,386 for testing. We ignored the two landmarks for the left and right earlobes, leaving up to 19 landmarks per face <ref type="bibr" target="#b7">[8]</ref>.</p><p>Since faces of AFLW have such variety head poses, most faces have landmarks out of view (i.e. missing). Thus, most samples were not annotated with the complete 19 landmarks, meaning that it does not allow for a constant sized tensor (i.e. real heatmaps) for the adversarial training. Therefore, we compared the softargmax and KL-based objectives with existing state-of-the-art. The face size d for the NMSE was the square root of the bounding box hull <ref type="bibr" target="#b0">[1]</ref>.</p><p>Our LaplaceKL-based model scored results comparable to existing state-of-the-art (i.e. RCN+ (L+ELT) <ref type="bibr" target="#b14">[15]</ref>) on the larger, more challenging AFLW dataset while outperforming all others. It is essential to highlight here that <ref type="bibr" target="#b14">[15]</ref> puts great emphasis on data augmentation, while we do not apply any. Also, since landmarks are missing in some samples (i.e. no common reference points exist across all samples), we were unable to prepare faces for our semi-supervised component-a subject for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>The error is next measured as a function of model size <ref type="table">(Table 3)</ref>, along with different? values (Eq. 2) and scales b used to parameterize the Laplacian <ref type="figure" target="#fig_11">(Fig. 4)</ref>. The latter <ref type="table">Table 3</ref>. NMSE on 300W (full set) for networks trained with fewer channels in each convolutional layer by 1/16, 1/8, 1/4, 1/2, and unmodified in size (i.e. the original) listed from left-to-right. We measured performance with a 2.8GHz Intel Core i7 CPU.   characterizes the baseline and supports the values used for these hyper-parameters, while the former reveals a critical characteristic for the practicality of the proposed.</p><p>Specifically, we decreased the model size by reducing the number of channels at each convolutional layer by factors of 2. The softargmax-based model worsened by about 47% and 79% in NMSE at an and the channel count, respectfully (i.e. 4.25 ? 6.86 and 9.79). LaplaceKL, on the other hand, decreased by about 24% with an 8 th and 59% with a 16 th the number of channels (i.e. 4.01 ? 5.09 and 7.38, respectfully). Our model trained with unlabeled data (i.e. LaplaceKL+D(70K)) dropped just about 21% and 57% at factors of 8 and 16, respectfully (i.e. 3.91 ? 4.85 and 7.01). In the end, LaplaceKL+D(70K) proved best with reduced sizes: with &lt;0.040M parameters, it still compares to previous state-of-the-art <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref>, which is a clear advantage. For instance, SDM <ref type="bibr" target="#b38">[39]</ref>, requires 1.693M parameters (25.17MB) for 7.52 in NMSE (300W full). 2 Yet our smallest and next-to-smallest get 7.01 and 4.85 with only 0.174M (0.076 MB) and 0.340M (0.166 MB) parameters.</p><p>The processing speed also boosts with fewer channels (i.e. to train and at inference). For instance, the model reduced by a factor of 16 processes 26.51 frames per second (fps) on a CPU of Macbook Pro (i.e. 2.8GHz Intel Core i7), with the original running at 4.92 fps. Our best LaplaceKLbased model proved robust to size reduction, obtaining 4.85 NMSE at 21.38 fps when reduced by 1/8.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, s l ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n L v P q y W P e + B / B 4 m r j Y O T r L m 3 y 1 I = " &gt; A A A C B X i c b V D L S g M x F L 1 T X 7 W + R l 2 J L o J F q C B l R g R d F t y 4 r G A f 0 N a S y W T a Y C Y z J B m h D N 2 4 8 R v 0 C 9 y 4 U M S t / + D O v z H T V t D W A x f O P e e G 3 H u 8 m D O l H e f L y s 3 N L y w u 5 Z c L K 6 t r 6 x v 2 5 l Z d R Y k k t E Y i H s m m h x X l T N C a Z p r T Z i w p D j 1 O G 9 7 N e e Y 3 b q l U L B J X e h D T T o h 7 g g W M Y G 2 k r r 1 X a o d Y 9 7 0 g 9 Y f X / A j 9 d M p 0 h 1 2 7 6 J S d E d A s c S e k C B N U u / Z n 2 4 9 I E l K h C c d K t V w n 1 p 0 U S 8 0 I p 8 N C O 1 E 0 x u Q G 9 2 j L U I F D q j r p 6 I o h O j C K j 4 J I m h I a j d T f L 1 I c K j U I P T O Z L a m m v U z 8 z 2 s l O j j r p E z E i a a C j D 8 K E o 5 0 h L J I k M 8 k J Z o P D M F E M r M r I n 0 s M d E m u I I J w Z 0 + e Z b U j 8 u u U 3 Y v T 4 q V n Y c M j 5 C H X d i H E r h w C h W 4 g C r U g M A d P M E L v F r 3 1 r P 1 Z r 2 P k 8 t Z k w i 3 4 Q + s j 2 8 i 5 p v F &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n L v P q y W P e + B / B 4 m r j Y O T r L m 3 y 1 I = " &gt; A A A C B X i c b V D L S g M x F L 1 T X 7 W + R l 2 J L o J F q C B l R g R d F t y 4 r G A f 0 N a S y W T a Y C Y z J B m h D N 2 4 8 R v 0 C 9 y 4 U M S t / + D O v z H T V t D W A x f O P e e G 3 H u 8 m D O l H e f L y s 3 N L y w u 5 Z c L K 6 t r 6 x v 2 5 l Z d R Y k k t E Y i H s m m h x X l T N C a Z p r T Z i w p D j 1 O G 9 7 N e e Y 3 b q l U L B J X e h D T T o h 7 g g W M Y G 2 k r r 1 X a o d Y 9 7 0 g 9 Y f X / A j 9 d M p 0 h 1 2 7 6 J S d E d A s c S e k C B N U u / Z n 2 4 9 I E l K h C c d K t V w n 1 p 0 U S 8 0 I p 8 N C O 1 E 0 x u Q G 9 2 j L U I F D q j r p 6 I o h O j C K j 4 J I m h I a j d T f L 1 I c K j U I P T O Z L a m m v U z 8 z 2 s l O j j r p E z E i a a C j D 8 K E o 5 0 h L J I k M 8 k J Z o P D M F E M r M r I n 0 s M d E m u I I J w Z 0 + e Z b U j 8 u u U 3 Y v T 4 q V n Y c M j 5 C H X d i H E r h w C h W 4 g C r U g M A d P M E L v F r 3 1 r P 1 Z r 2 P k 8 t Z k w i 3 4 Q + s j 2 8 i 5 p v F &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n L v P q y W P e + B / B 4 m r j Y O T r L m 3 y 1I = " &gt; A A A C B X i c b V D L S g M x F L 1 T X 7 W + R l 2 J L o J F q C B l R g R d F t y 4 r G A f 0 N a S y W T a Y C Y z J B m h D N 2 4 8 R v 0 C 9 y 4 U M S t / + D O v z H T V t D W A x f O P e e G 3H u 8 m D O l H e f L y s 3 N L y w u 5 Z c L K 6 t r 6 x v 2 5 l Z d R Y k k t E Y i H s m m h x X l T N C a Z p r T Z i w p D j 1 O G 9 7 N e e Y 3 b q l U L B J X e h D T T o h 7 g g W M Y G 2 k r r 1 X a o d Y 9 7 0 g 9 Y f X / A j 9 d M p 0 h 1 2 7 6 J S d E d A s c S e k C B N U u / Z n 2 4 9 I E l K h C c d K t V w n 1 p 0 U S 8 0 I p 8 N C O 1 E 0 x u Q G 9 2 j L U I F D q j r p 6 I o h O j C K j 4 J I m h I a j d T f L 1 I c K j U I P T O Z L a m m v U z 8 z 2 s l O j j r p E z E i a a C j D 8 K E o 5 0 h L J I k M 8 k J Z o P D M F E M r M r I n 0 s M d E m u I I J w Z 0 + e Z b U j 8 u u U 3 Y v T 4 q V n Y c M j 5 C H X d i H E r h w C h W 4 g C r U g M A d P M E L v F r 3 1 r P 1 Z r 2 P k 8 t Z k w i 3 4 Q + s j 2 8 i 5 p v F &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n L v P q y W P e + B / B 4 m r j Y O T r L m 3 y 1 I = " &gt; A A A C B X i c b V D L S g M x F L 1 T X 7 W + R l 2 J L o J F q C B l R g R d F t y 4 r G A f 0 N a S y W T a Y C Y z J B m h D N 2 4 8 R v 0 C 9 y 4 U M S t / + D O v z H T V t D W A x f O P e e G 3 H u 8 m D O l H e f L y s 3 N L y w u 5 Z c L K 6 t r 6 x v 2 5 l Z d R Y k k t E Y i H s m m h x X l T N C a Z p r T Z i w p D j 1 O G 9 7 N e e Y 3 b q l U L B J X e h D T T o h 7 g g W M Y G 2 k r r 1 X a o d Y 9 7 0 g 9 Y f X / A j 9 d M p 0 h 1 2 7 6 J S d E d A s c S e k C B N U u / Z n 2 4 9 I E l K h C c d K t V w n 1 p 0 U S 8 0 I p 8 N C O 1 E 0 x u Q G 9 2 j L U I F D q j r p 6 I o h O j C K j 4 J I m h I a j d T f L 1 I c K j U I P T O Z L a m m v U z 8 z 2 s l O j j r p E z E i a a C j D 8 K E o 5 0 h L J I k M 8 k J Z o P D M F E M r M r I n 0 s M d E m u I I J w Z 0 + e Z b U j 8 u u U 3 Y v T 4 q V n Y c M j 5 C H X d i H E r h w C h W 4 g C r U g M A d P M E L v F r 3 1 r P 1 Z r 2 P k 8 t Z k w i 3 4 Q + s j 2 8 i 5 p v F &lt; / l a t e x i t &gt; Generated heat-maps L KL &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D t K l t d B J S N r j j g k k 6 / B p 7 I y + K q k = " &gt; A A A C A H i c b Z D L S s N A F I Z P v N Z 6 i 7 p w o Y v B I r g q i Q i 6 L L g R d F H B X q A N Z T K d t E N n k j A z E U r I x l d x 4 0 I R t z 6 G O 9 / G S Z q F t v 4 w 8 P G f c 5 h z f j / m T G n H + b a W l l d W 1 9 Y r G 9 X N r e 2 d X X t v v 6 2 i R B L a I h G P Z N f H i n I W 0 p Z m m t N u L C k W P q c d f 3 K d 1 z u P V C o W h Q 9 6 G l N P 4 F H I A k a w N t b A P u w L r M c E 8 / Q u G x Q s R X p r 2 K 4 5 d a c Q W g S 3 h B q U a g 7 s r / 4 w I o m g o S Y c K 9 V z n V h 7 K Z a a E U 6 z a j 9 R N M Z k g k e 0 Z z D E g i o v L Q 7 I 0 K l x h i i I p H m h R o X 7 e y L F Q q m p 8 E 1 n v q K a r + X m f 7 V e o o M r L 2 V h n G g a k t l H Q c K R j l C e B h o y S Y n m U w O Y S G Z 2 R W S M J S b a Z F Y 1 I b j z J y 9 C + 7 z u O n X 3 / q L W O C 7 j q M A R n M A Z u H A J D b i B J r S A Q A b P 8 A p v 1 p P 1 Y r 1 b H 7 P W J a u c O Y A / s j 5 / A E r p l r g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D t K l t d B J S N r j j g k k 6 / B p 7 I y + K q k = " &gt; A A A C A H i c b Z D L S s N A F I Z P v N Z 6 i 7 p w o Y v B I r g q i Q i 6 L L g R d F H B X q A N Z T K d t E N n k j A z E U r I x l d x 4 0 I R t z 6 G O 9 / G S Z q F t v 4 w 8 P G f c 5 h z f j / m T G n H + b a W l l d W 1 9 Y r G 9 X N r e 2 d X X t v v 6 2 i R B L a I h G P Z N f H i n I W 0 p Z m m t N u L C k W P q c d f 3 K d 1 z u P V C o W h Q 9 6 G l N P 4 F H I A k a w N t b A P u w L r M c E 8 / Q u G x Q s R X p r 2 K 4 5 d a c Q W g S 3 h B q U a g 7 s r / 4 w I o m g o S Y c K 9 V z n V h 7 K Z a a E U 6 z a j 9 R N M Z k g k e 0 Z z D E g i o v L Q 7 I 0 K l x h i i I p H m h R o X 7 e y L F Q q m p 8 E 1 n v q K a r + X m f 7 V e o o M r L 2 V h n G g a k t l H Q c K R j l C e B h o y S Y n m U w O Y S G Z 2 R W S M J S b a Z F Y 1 I b j z J y 9 C + 7 z u O n X 3 / q L W O C 7 j q M A R n M A Z u H A J D b i B J r S A Q A b P 8 A p v 1 p P 1 Y r 1 b H 7 P W J a u c O Y A / s j 5 / A E r p l r g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D t K l t d B J S N r j j g k k 6 / B p 7 I y + K q k = " &gt; A A A C A H i c b Z D L S s N A F I Z P v N Z 6 i 7 p w o Y v B I r g q i Q i 6 L L g R d F H B X q A N Z T K d t E N n k j A z E U r I x l d x 4 0 I R t z 6 G O 9 / G S Z q F t v 4 w 8 P G f c 5 h z f j / m T G n H + b a W l l d W 1 9 Y r G 9 X N r e 2 d X X t v v 6 2 i R B L a I h G P Z N f H i n I W 0 p Z m m t N u L C k W P q c d f 3 K d 1 z u P V C o W h Q 9 6 G l N P 4 F H I A k a w N t b A P u w L r M c E 8 / Q u G x Q s R X p r 2 K 4 5 d a c Q W g S 3 h B q U a g 7 s r / 4 w I o m g o S Y c K 9 V z n V h 7 K Z a a E U 6 z a j 9 R N M Z k g k e 0 Z z D E g i o v L Q 7 I 0 K l x h i i I p H m h R o X 7 e y L F Q q m p 8 E 1 n v q K a r + X m f 7 V e o o M r L 2 V h n G g a k t l H Q c K R j l C e B h o y S Y n m U w O Y S G Z 2 R W S M J S b a Z F Y 1 I b j z J y 9 C + 7 z u O n X 3 / q L W O C 7 j q M A R n M A Z u H A J D b i B J r S A Q A b P 8 A p v 1 p P 1 Y r 1 b H 7 P W J a u c O Y A / s j 5 / A E r p l r g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D t K l t d B J S N r j j g k k 6 / B p 7 I y +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>j l Z d I 8 r 7 p O 1 b 2 9 K N e O n n I 8 Q x G O 4 Q Q q 4 M I l 1 O A G 6 t A A A h I e 4 R X e r A f r x X q 3 P m b N F a x 5 h Y f w B 9 b n D z i / l c I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s g h y Y u 5 d 7 x h 0 y E 2 L n g 6 D g n h 1 q x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>j l Z d I 8 r 7 p O 1 b 2 9 K N e O n n I 8 Q x G O 4 Q Q q 4 M I l 1 O A G 6 t A A A h I e 4 R X e r A f r x X q 3 P m b N F a x 5 h Y f w B 9 b n D z i / l c I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s g h y Y u 5 d 7 x h 0 y E 2 L n g 6 D g n h 1 q x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>j l Z d I 8 r 7 p O 1 b 2 9 K N e O n n I 8 Q x G O 4 Q Q q 4 M I l 1 O A G 6 t A A A h I e 4 R X e r A f r x X q 3 P m b N F a x 5 h Y f w B 9 b n D z i / l c I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s g h y Y u 5 d 7 x h 0 y E 2 L n g 6 D g n h 1 q x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>j l Z d I 8 r 7 p O 1 b 2 9 K N e O n n I 8 Q x G O 4 Q Q q 4 M I l 1 O A G 6 t A A A h I e 4 R X e r A f r x X q 3 P m b N F a x 5 h Y f w B 9 b n D z i / l c I = &lt; / l a t e x i t &gt; L adv &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 e L m q H L 8 M + R b M C M e T F h r 3 e d r T 5 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>y 1 Y p U z h + C P r M 8 f c m + X Z g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 e L m q H L 8 M + R b M C M e T F h r 3 e d r T 5 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>y 1 Y p U z h + C P r M 8 f c m + X Z g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 e L m q H L 8 M + R b M C M e T F h r 3 e d r T 5 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .</head><label>2</label><figDesc>y 4 U c e t b u P N t n K R Z a O s P A x / / O Y c 5 5 w 8 S R p V 2 n G + r s r K 6 t r 5 R 3 a x t b e / s 7 t n 7 B x 0 V p x K T N o 5 Z L H s B U o R R Q d q a a k Z 6 i S S I B 4 x 0 g / F N X u 9 O i F Q 0 F g 9 6 m h C P o 6 G g E c V I G 8 u 3 j w Y c 6 R F G L L u b + Q V L n q F w M v P t u t N w C s F l c E u o g 1 I t 3 / 4 a h D F O O R E a M 6 R U 3 3 U S 7 W V I a o o Z m d U G q S I J w m M 0 J H 2 D A n G i v K y 4 Y A b P j B P C K J b m C Q 0 L 9 / d E h r h S U x 6 Y z n x H t V j L z f 9 q / V R H 1 1 5 G R Z J q I v D 8 o y h l U M c w j w O G V B K s 2 d Q A w p K a X S E e I Y m w N q H V T A j u 4 s n L 0 L l o u E 7 D v b + s N 0 / K O K r g G J y C c + C C K 9 A E t 6 A F 2 g C D R / A M X s G b 9 W S 9 W O / W x 7 y 1 Y p U z h + C P r M 8 f c m + X Z g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 e L m q H L 8 M + R b M C M e T F h r 3 e d r T 5 8 = " &gt; A A A C A X i c b Z D L S s N A F I Y n 9 V b r L e p G 0 M V g E V y V R A R d F t y 4 c F H B X q A N Y T K Z t E N n J m F m U i i h b n w V N y 4 U c e t b u P N t n K R Z a O s P A x / / O Y c 5 5 w 8 S R p V 2 n G + r s r K 6 t r 5 R 3 a x t b e / s 7 t n 7 B x 0 V p x K T N o 5 Z L H s B U o R R Q d q a a k Z 6 i S S I B 4 x 0 g / F N X u 9 O i F Q 0 F g 9 6 m h C P o 6 G g E c V I G 8 u 3 j w Y c 6 R F G L L u b + Q V L n q F w M v P t u t N w C s F l c E u o g 1 I t 3 / 4 a h D F O O R E a M 6 R U 3 3 U S 7 W V I a o o Z m d U G q S I J w m M 0 J H 2 D A n G i v K y 4 Y A b P j B P C K J b m C Q 0 L 9 / d E h r h S U x 6 Y z n x H t V j L z f 9 q / V R H 1 1 5 G R Z J q I v D 8 o y h l U M c w j w O G V B K s 2 d Q A w p K a X S E e I Y m w N q H V T A j u 4 s n L 0 L l o u E 7 D v b + s N 0 / K O K r g G J y C c + C C K 9 A E t 6 A F 2 g C D R / A M X s G b 9 W S 9 W O / W x 7 y 1 Y p U z h + C P r M 8 f c m + X Z g = = &lt; / l a t e x i t &gt; l ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r 6 2 q N I T r q P c R c x p Y k / T T k 8 w S 6 A = " &gt; A A A B / X i c b V D L S s N A F L 3 x W e s r P j b i J l i E u i m J C L o s u H F Z w T 6 g i W U y n b R D Z z J h Z i L U U P w O X b l x o Y h b / 8 O d f + O k 7 U J b D w w c z r m X e + a E C a N K u + 6 3 t b C 4 t L y y W l g r r m 9 s b m 3 b O 7 s N J V K J S R 0 L J m Q r R I o w G p O 6 p p q R V i I J 4 i E j z X B w m f v N O y I V F f G N H i Y k 4 K g X 0 4 h i p I 3 U s f d 9 w U k P l X 2 O d D + M M j W 6 Z S c d u + R W 3 D G c e e J N S Q m m q H X s L 7 8 r c M p J r D F D S r U 9 N 9 F B h q S m m J F R 0 U 8 V S R A e o B 5 p G x o j T l S Q j d O P n G O j d J 1 I S P N i 7 Y z V 3 x s Z 4 k o N e W g m 8 5 B q 1 s v F / 7 x 2 q q O L I K N x k m o S 4 8 m h K G W O F k 5 e h d O l k m D N h o Y g L K n J 6 u A + k g h r U 1 j R l O D N f n m e N E 4 r n l v x r s 9 K 1 Y P H H E 9 Q g E M 4 g j J 4 c A 5 V u I I a 1 A H D P T z D K 7 x Z D 9 a L 9 W 5 9 T J p b s K Y V 7 s E f W J 8 / Y V G Y m g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r 6 2 q N I T r q P c R c x p Y k / T T k 8 w S 6 A = " &gt; A A A B / X i c b V D L S s N A F L 3 x W e s r P j b i J l i E u i m J C L o s u H F Z w T 6 g i W U y n b R D Z z J h Z i L U U P w O X b l x o Y h b / 8 O d f + O k 7 U J b D w w c z r m X e + a E C a N K u + 6 3 t b C 4 t L y y W l g r r m 9 s b m 3 b O 7 s N J V K J S R 0 L J m Q r R I o w G p O 6 p p q R V i I J 4 i E j z X B w m f v N O y I V F f G N H i Y k 4 K g X 0 4 h i p I 3 U s f d 9 w U k P l X 2 O d D + M M j W 6 Z S c d u + R W 3 D G c e e J N S Q m m q H X s L 7 8 r c M p J r D F D S r U 9 N 9 F B h q S m m J F R 0 U 8 V S R A e o B 5 p G x o j T l S Q j d O P n G O j d J 1 I S P N i 7 Y z V 3 x s Z 4 k o N e W g m 8 5 B q 1 s v F / 7 x 2 q q O L I K N x k m o S 4 8 m h K G W O F k 5 e h d O l k m D N h o Y g L K n J 6 u A + k g h r U 1 j R l O D N f n m e N E 4 r n l v x r s 9 K 1 Y P H H E 9 Q g E M 4 g j J 4 c A 5 V u I I a 1 A H D P T z D K 7 x Z D 9 a L 9 W 5 9 T J p b s K Y V 7 s E f W J 8 / Y V G Y m g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r 6 2 q N I T r q P c R c x p Y k / T T k 8 w S 6 A = " &gt; A A A B / X i c b V D L S s N A F L 3 x W e s r P j b i J l i E u i m J C L o s u H F Z w T 6 g i W U y n b R D Z z J h Z i L U U P w O X b l x o Y h b / 8 O d f + O k 7 U J b D w w c z r m X e + a E C a N K u + 6 3 t b C 4 t L y y W l g r r m 9 s b m 3 b O 7 s N J V K J S R 0 L J m Q r R I o w G p O 6 p p q R V i I J 4 i E j z X B w m f v N O y I V F f G N H i Y k 4 K g X 0 4 h i p I 3 U s f d 9 w U k P l X 2 O d D + M M j W 6 Z S c d u + R W 3 D G c e e J N S Q m m q H X s L 7 8 r c M p J r D F D S r U 9 N 9 F B h q S m m J F R 0 U 8 V S R A e o B 5 p G x o j T l S Q j d O P n G O j d J 1 I S P N i 7 Y z V 3 x s Z 4 k o N e W g m 8 5 B q 1 s v F / 7 x 2 q q O L I K N x k m o S 4 8 m h K G W O F k 5 e h d O l k m D N h o Y g L K n J 6 u A + k g h r U 1 j R l O D N f n m e N E 4 r n l v x r s 9 K 1 Y P H H E 9 Q g E M 4 g j J 4 c A 5 V u I I a 1 A H D P T z D K 7 x Z D 9 a L 9 W 5 9 T J p b s K Y V 7 s E f W J 8 / Y V G Y m g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r 6 2 q N I T r q P c R c x p Y k / T T k 8 w S 6 A = " &gt; A A A B / X i c b V D L S s N A F L 3 x W e s r P j b i J l i E u i m J C L o s u H F Z w T 6 g i W U y n b R D Z z J h Z i L U U P w O X b l x o Y h b / 8 O d f + O k 7 U J b D w w c z r m X e + a E C a N K u + 6 3 t b C 4 t L y y W l g r r m 9 s b m 3 b O 7 s N J V K J S R 0 L J m Q r R I o w G p O 6 p p q R V i I J 4 i E j z X B w m f v N O y I V F f G N H i Y k 4 K g X 0 4 h i p I 3 U s f d 9 w U k P l X 2 O d D + M M j W 6 Z S c d u + R W 3 D G c e e J N S Q m m q H X s L 7 8 r c M p J r D F D S r U 9 N 9 F B h q S m m J F R 0 U 8 V S R A e o B 5 p G x o j T l S Q j d O P n G O j d J 1 I S P N i 7 Y z V 3 x s Z 4 k o N e W g m 8 5 B q 1 s v F / 7 x 2 q q O L I K N x k m o S 4 8 m h K G W O F k 5 e h d O l k m D N h o Y g L K n J 6 u A + k g h r U 1 j R l O D N f n m e N E 4 r n l v x r s 9 K 1 Y P H H E 9 Q g E M 4 g j J 4 c A 5 V u I I a 1 A H D P T z D K 7 x Z D 9 a L 9 W 5 9 T J p b s K Y V 7 s E f W J 8 / Y V G Y m g = = &lt; / l a t e x i t &gt; The proposed semi-supervised framework for landmarks localization. The labeled and unlabeled branched are marked with blue and red arrows, respectfully. Given an input image, G produces K heatmaps, one for each landmark. Labels are used to generate real heatmaps as ?(s l ). G produces fake samples from the unlabeled data. Source images are concatenated on heatmaps and passed to D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>We define a function ? (h) to compute the scale (or variance) of the predicted heatmapsh using the location, where the locations are now the expectation of being a landmark in the heatmap space. Thus, ? (h) = p(x)||x ?s|| ? ? , wher? s = E h [x], ? = 1 for Laplacian, and ? = 2 for Gaussian. Thus,s and ? (h)) are used to parameterize a Laplace (or Gaussian) distribution for the predicted landmarks q(h|d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Random samples (300W). Heatmaps predicted by our LaplaceKL+D(70K) (middle, i.e. L-KL+D(70K)) and softargmax+D(70K) (right, i.e. SAM+D(70K)) alongside face images with ground-truth sketched on the face (left). For this, colors were set by value for the K heatmaps generated for each landmark (i.e. range of [0, 1] as shown in color bar), and then were superimposed on the original face. Note that the KL-divergence loss yields predictions of much greater confidence and, hence, produced separated landmarks when visualized heatmap space. In other words, the proposed has minimal spread about the mean, as opposed to the softargmax-based model with heatmaps with individual landmarks smudged together. Best viewed electronically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Results of ablation study on LaplaceKL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2</head><label></label><figDesc>https://github.com/tntrung/sdm_face_alignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 .</head><label>5</label><figDesc>Random samples of landmarks predicted using LaplaceKL (white), with the ground truth drawn as line segments (red). Notice the predicted points tend to overlap with the groundtruth. Best viewed in color. Zoom-in for greater detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>NMSE on AFLW and 300W normalized by the square root of BB area and interocular distance, respectfully.</figDesc><table><row><cell></cell><cell>AFLW</cell><cell cols="3">300W Common Challenge Full</cell></row><row><cell>SDM [39]</cell><cell>5.43</cell><cell>5.57</cell><cell>15.40</cell><cell>7.52</cell></row><row><cell>LBF [26]</cell><cell>4.25</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>MDM [32]</cell><cell>-</cell><cell>4.83</cell><cell>10.14</cell><cell>5.88</cell></row><row><cell>TCDCN [44]</cell><cell>-</cell><cell>4.80</cell><cell>8.60</cell><cell>5.54</cell></row><row><cell>CFSS [46]</cell><cell>3.92</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>CFSS [21]</cell><cell>2.17</cell><cell>4.36</cell><cell>7.56</cell><cell>4.99</cell></row><row><cell>RCSR [38]</cell><cell>-</cell><cell>4.01</cell><cell>8.58</cell><cell>4.90</cell></row><row><cell>RCN+ (L+ELT) [15]</cell><cell>1.59</cell><cell>4.20</cell><cell>7.78</cell><cell>4.90</cell></row><row><cell>CPM + SBR [8]</cell><cell>2.14</cell><cell>3.28</cell><cell>7.58</cell><cell>4.10</cell></row><row><cell>Softargmax</cell><cell>2.26</cell><cell>3.48</cell><cell>7.39</cell><cell>4.25</cell></row><row><cell>Softargmax+D(10K)</cell><cell>-</cell><cell>3.34</cell><cell>7.90</cell><cell>4.23</cell></row><row><cell>Softargmax+D(30K)</cell><cell>-</cell><cell>3.41</cell><cell>7.99</cell><cell>4.31</cell></row><row><cell>Softargmax+D(50K)</cell><cell>-</cell><cell>3.41</cell><cell>8.06</cell><cell>4.32</cell></row><row><cell>Softargmax+D(70K)</cell><cell>-</cell><cell>3.34</cell><cell>8.17</cell><cell>4.29</cell></row><row><cell>LaplaceKL</cell><cell>1.97</cell><cell>3.28</cell><cell>7.01</cell><cell>4.01</cell></row><row><cell>LaplaceKL+D(10K)</cell><cell>-</cell><cell>3.26</cell><cell>6.96</cell><cell>3.99</cell></row><row><cell>LaplaceKL+D(30K)</cell><cell>-</cell><cell>3.29</cell><cell>6.74</cell><cell>3.96</cell></row><row><cell>LaplaceKL+D(50K)</cell><cell>-</cell><cell>3.26</cell><cell>6.71</cell><cell>3.94</cell></row><row><cell>LaplaceKL+D(70K)</cell><cell>-</cell><cell>3.19</cell><cell>6.87</cell><cell>3.91</cell></row><row><cell cols="5">4.01 on 300W, while Gaussian-based got 4.71. Thus, the</cell></row><row><cell cols="5">sharper,"peakier" Laplace distribution proved to be more</cell></row><row><cell cols="5">numerically stable under current network configuration, as</cell></row><row><cell cols="5">Gaussian required a learning rate a magnitude smaller to</cell></row><row><cell cols="4">avoid vanishing gradients. Indeed, we used Laplace.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 (</head><label>2</label><figDesc>300W challenge set)). Our best model was ?2.7% better than<ref type="bibr" target="#b7">[8]</ref> on easier samples (i.e. common), ?4.7% better on average (i.e. full), and, moreover, ?9.8% better on the more difficult (i.e. challenge), ?4.7% better on average (full), and, moreover, ?9.8% better on the more difficult (challenge). These results further highlight the advantages of training with the proposed LaplaceKL loss, along with the adversarial training framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Number of parameters, millions 0.0174 0.0389 0.1281 0.4781 1.8724</figDesc><table><row><cell>Softargmax</cell><cell>9.79</cell><cell>6.86</cell><cell>4.83</cell><cell>4.35</cell><cell>4.25</cell></row><row><cell>Softargmax+D(70K)</cell><cell>9.02</cell><cell>6.84</cell><cell>4.85</cell><cell>4.38</cell><cell>4.29</cell></row><row><cell>LaplaceKL</cell><cell>7.38</cell><cell>5.09</cell><cell>4.39</cell><cell>4.04</cell><cell>4.01</cell></row><row><cell>LaplaceKL+D(70K)</cell><cell>7.01</cell><cell>4.85</cell><cell>4.30</cell><cell>3.98</cell><cell>3.91</cell></row><row><cell>Storage (MB)</cell><cell>0.076</cell><cell>0.162</cell><cell>0.507</cell><cell>1.919</cell><cell>7.496</cell></row><row><cell>Speed (fps)</cell><cell>26.51</cell><cell>21.38</cell><cell>16.77</cell><cell>11.92</cell><cell>4.92</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/davidsandberg/facenet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We demonstrated the benefits of the proposed LaplaceKL loss and leveraging unlabeled data in an adversarial training framework.</p><p>Hypothetically and empirically, we showed the importance of penalizing a landmark predictor's uncertainty. Thus, training with the proposed objective yields predictions of higher confidence, outperforming previous state-of-the-art methods. We also revealed the benefits of adding unlabeled training data to boost performance via adversarial training. In the end, our model performs state-of-the-art on all three splits of the renown 300W (i.e. common, challenge, and full), and second-to-best on the AFLW benchmark. Also, we demonstrate the robustness of the proposed by significantly reducing the number of parameters. Specifically, with 1/8 the number of channels (i.e. &lt;170Kb on disk), the proposed still yields an accuracy comparable to the previous state-of-the-art in real-time (i.e. 21.38 fps). Thus, the contributions of the proposed framework are instrumental for models intended for use in real-world production.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gradient descent optimization of smoothed information retrieval metrics. Information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timothy F Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="681" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active shape modelssmart snakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-shot face recognition via generative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified bias-variance decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d guided fine-grained face manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Riza Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Stefanos Zafeiriou, and Iasonas Kokkinos</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?szl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshop</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Red-net: A recurrent encoder-decoder network for video-based face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stphane</forename><surname>Lathuilire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jamie Shotton, and Deva Ramanan. Depth-based hand pose estimation: data, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1868" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Viewpoint-consistent 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?szl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ev-action: Electromyography-vision multi-modal action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taotao</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12602</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Video-tovideo synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent convolutional face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent convolutional shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
