<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">APPROACHES TOWARD PHYSICAL AND GENERAL VIDEO ANOMALY DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Kart</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">APPROACHES TOWARD PHYSICAL AND GENERAL VIDEO ANOMALY DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Anomaly Detection, Video Anomaly Detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, many works have addressed the problem of finding never-seen-before anomalies in videos. Yet, most work has been focused on detecting anomalous frames in surveillance videos taken from security cameras. Meanwhile, the task of anomaly detection (AD) in videos exhibiting anomalous mechanical behavior, has been mostly overlooked. Anomaly detection in such videos is both of academic and practical interest, as they may enable automatic detection of malfunctions in many manufacturing, maintenance, and real-life settings. To assess the potential of the different approaches to detect such anomalies, we evaluate two simple baseline approaches: (i) Temporal-pooled image AD techniques. (ii) Density estimation of videos represented with features pretrained for video-classification.</p><p>Development of such methods calls for new benchmarks to allow evaluation of different possible approaches. We introduce the Physical Anomalous Trajectory or Motion (PHANTOM) dataset 1 , which contains six different video classes. Each class consists of normal and anomalous videos. The classes differ in the presented phenomena, the normal class variability, and the kind of anomalies in the videos. We also suggest an even harder benchmark where anomalous activities should be spotted on highly variable scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Detecting never-seen-before novelties, or anomalies, is a key ability humans use to raise awareness of new dangers and opportunities. Examples of such include spotting new behaviors in natural systems, detecting security threats, or spotting equipment malfunctions. While anomaly detection (AD) methods aimed at images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> usually address a large variety of data domains, video AD has been mostly focused on surveillance videos. In such videos <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, typically taken from security cameras, normal walking behavior is usually defined as the normal class, and other behaviors (riding a bike, crowd gathering) are defined as anomalies. In recent years, many new techniques have been suggested, pushing forward performance on such benchmarks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Nevertheless, other kinds of video anomalies remain mostly out of the scope of contemporary research.</p><p>In this work we explore different kinds of anomalies, examining different types of methods to address them. Our work addresses physical abnormalities in the motion or trajectory of an object that have been largely overlooked and presents a new benchmarks for video anomaly detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Video Anomaly Detection As most work on video anomaly detection has focused on surveillance videos, various methods have been tried to tackle this problem. Deep learning methods have been able to make significant progress toward solving this problem, using autoencoders <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, generative models <ref type="bibr" target="#b9">[10]</ref>, or prediction <ref type="bibr" target="#b10">[11]</ref>. More recent methods have been able to outperform using pretraining and self-supervised learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Image Anomaly Detection Image anomaly detection is a fastgrowing field, often using similar techniques to those used in video AD. Deep learning models have been able to outperform classical methods using auto-encoders and later RotNet-type self-supervised methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>. Lately, contrastive learning methods have been used to further improve performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. A promising line of work suggests to detect anomalies using pretrained features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1]</ref>. Pretrained features robustly outperform self-supervised methods, especially on small datasets or when dealing with subtle anomalies. We use similar approaches in this paper.</p><p>Existing Datasets Challenging datasets and benchmarks have been key in advancing the field of computer vision and machine learning. In image anomaly detection, most work has been focused on utilizing standard image classification datasets, while specialized benchmarks focusing on surveillance have been suggested for video anomaly detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. So far, video AD has so far been heavily reliant on these datasets. On one hand, this ensured that this rapidly-developing field continued to stay relevant to the evaluated task. On the other hand, this may have limited the generality of the methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATED METHODS</head><p>In our work, we use feature extractors from pretrained image and video networks to represent the videos. As the train set contains normal-only videos, we assume that this set makes up a single highdensity region in the feature space. With this assumption, for new unseen videos we elect to use the k-nearest neighbors (kNN) distance from the train set as the anomaly score. This simple baseline approach of feature extraction combined with kNN outperforms all previous state-of-the-art methods on small datasets <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image features-based AD</head><p>In the first approach, we work with networks pretrained for singleimage classification. As these networks have been shown to be very useful on image anomaly detection, the first approach we try is to adapt them to represent entire videos. The train set Vtrain = V1, . . . , Vn is comprised of normal videos only. Each video is separated into frames Vi = vi,1, . . . , vi,t i . The frames are evenly sampled and a feature extractor F is used to extract frame features.</p><formula xml:id="formula_0">F (vi,j) = fi,j<label>(1)</label></formula><p>In order to return a single feature vector for each video, we use maximum or average temporal pooling. We refer to each of the above as a time series operation T , taking the vector representation of each frame fi,1, . . . , fi,t i and returning a single vector representation for the entire video fi.</p><formula xml:id="formula_1">F (Vi) = T (fi,1, . . . , fi,t i ) = fi<label>(2)</label></formula><p>We now have a set of embeddings representing the train set, Ftrain = fi, . . . , fn. Given a new video sample Vy in the test set, we score its abnormality by extracting its features fy and then by computing its kNN distance from Ftrain.</p><formula xml:id="formula_2">d(Vy) = 1 k f ?N k (fy ) f ? fy 2 (3)</formula><p>Here, N k (fy) represents the embeddings of the k-nearest neighbors to fy in Ftrain. After obtaining this distance, we determine whether the video is normal or not by verifying that the distance is greater than some threshold. This approach can be viewed as a simplified version of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. As official implementations of these methods were not available during this study, we are unable to provide a comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video features-based AD</head><p>Aiming not only to capture the semantics of each image, but also the dynamics of the video, we also utilize pretrained video representations. We use networks pretrained for classification on large video datasets, as such networks are likely to represent a video's content and dynamics in a meaningful way to humans.</p><p>Here too, the train set Vtrain = V1, . . . , Vn is normal. This time, the feature vector can be extracted directly from the video.</p><formula xml:id="formula_3">F (Vi) = fi<label>(4)</label></formula><p>We use feature extractors to obtain the train set embeddings Ftrain = f1, . . . , fn. Given a new test video Vy, we proceed as discussed above to compute the kNN distance from Ftrain and use it as the anomaly score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Surveillance method</head><p>To compare our methods to the rapidly developing field of algorithms for AD in surveillance videos, we choose to run the Memoryguided Normality for Anomaly Detection (MNAD <ref type="bibr" target="#b6">[7]</ref>) method on our dataset. MNAD is a state-of-the-art method for video AD for which we were able to find an official implementation. This method returns a score for each video frame. To adapt it such that it returns a single score per video, we take the average of the frame scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classname</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subclass</head><p>ViT w. avg pooling TimeSformer MNAD </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Our experimental results detail when our method performs well and establish a baseline for future work. We compare our method to MNAD on our dataset as well as on Something-Something-V2 <ref type="bibr" target="#b20">[21]</ref>. Lastly, we run our method on the UCSD Pedestrian 1 and 2 <ref type="bibr" target="#b4">[5]</ref> datasets commonly used in anomaly detection for surveillance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Anomalous Trajectory or Motion (PHANTOM) Dataset</head><p>To evaluate the presented approaches, we created the Physical Anomalous Trajectory or Motion (PHANTOM) dataset consisting of six classes featuring everyday objects or physical setups, and showing nine different kinds of anomalies. We designed our classes to evaluate detection of various modes of video abnormalities that are generally excluded in video AD settings. The train and test sets of each class contain approximately 30 videos of varying lengths. The train set contains only normal videos, while the test set is evenly balanced between normal and anomalous videos. The classes were designed to be of varying difficulties and to feature different types of anomalies. For example, the window class was filmed in multiple lighting scenarios to increase variance.</p><p>The normal videos include motion that follows an expected trajectory (pendulum, keyboard) or an expected movement (window). The sushi class features procedural motion, while candle and magnets feature more subtle motion that only appears locally. The anomalous videos can feature an interference of the regular motion (window, candle, magnets), an added or removed step in the usual procedure (sushi), motion that follows a different trajectory (pendulum, keyboard), or contains a different object (pendulum). The pendulum and magnets classes contain more than one type of anomaly. An overview of these classes is displayed in Tab.2.</p><p>General Activity Dataset To examine a harder, even more general video AD setting, we work with the Something-Something-V2 (SSv2) dataset. It features 174 classes of various activities such as Throwing something in the air and catching it where something is not limited to a single object type. We adapt this dataset for use for general AD as described in Sec. 4.3 and it can be thought of as suitable crossover between physical and surveillance videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Physical AD on PHANTOM dataset</head><p>To determine the ability of our proposed method at finding anomalies in videos, we test it on our dataset. We explore the different modes Tearing Closing <ref type="figure">Fig. 2</ref>. Examples of frames from two of the ten video types in the SSv2 classes that we used. For each class, two frames are shown to demonstrate the high class variability.</p><p>of the method and uncover the strengths and weaknesses of each setting. We measure our results using the area under the curve of the receiver operating characteristic (ROCAUC) as is common in previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Image Features: Here we run the videos in our dataset framewise through different pretrained image networks. We choose a Residual Network (ResNet <ref type="bibr" target="#b21">[22]</ref>) and the Vision Transformer (ViT <ref type="bibr" target="#b22">[23]</ref>) as previously they have been tested extensively on numerous image datasets and have been shown to achieve excellent transfer learning results. Each network comes pretrained on ImageNet and ImageNet-21K. In order to summarize the frame features into a single feature vector we use temporal pooling. After obtaining the video features, we give each video an anomaly score. The results of the best performing image-based method on the PHANTOM dataset appear in Tab.2.</p><p>Video Features: Video networks return a single feature vector per video, therefore it is not necessary to pool the frame-level features. We use the following pretrained video networks: TimeSformer <ref type="bibr" target="#b23">[24]</ref> pretrained on Kinetics-600 <ref type="bibr" target="#b24">[25]</ref> and SSv2, and the Big-Little-Video-Net Temporal Aggregation Module (bLVNet-TAM <ref type="bibr" target="#b25">[26]</ref>) architecture pretrained on SSv2. A summary of the best-performing method is given in Tab.2.</p><p>Surveillance Method: Lastly, we must compare the performance of the image and video features to a method meant for surveillance footage. For this, We choose MNAD and train it on the normal </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to MNAD on SSv2</head><p>Next, we choose to contrast our simple method of feature extraction and kNN to MNAD on the SSv2 dataset. We choose 10 classes and from among these classes, we determine the largest subclass and label it normal. We run this subclass against the videos in the largest subclass of the other 9 classes which are labeled anomalous. This evaluation is similar to that of the PHANTOM dataset. The names of the chosen classes and subclasses appear in Tab.3 together with the results.This task proved challenging for all methods and the results show that while our method works well on our PHANTOM dataset, it is not well-adapted for general videos. This highlights general and physical videos as different modalities which may call for different AD solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison on surveillance dataset</head><p>In order to examine the performance of our proposed benchmarks against surveillance videos, we test it on other types of data. To this end, we select the UCSD Pedestrian 1 and 2 datasets. These datasets contain surveillance footage of two pedestrian scenes and are commonly used for video AD evaluation. Surveillance AD methods differ from ours in that each frame is given an anomaly score. To overcome this, we adapt our procedure in the following way. We break the video into overlapping windows of constant length such that each frame appears in several windows. Thus, each video in the train set contributes several feature vectors equal to the number of overlapping windows in that video. Given an unseen video, we again divide the video into windows and compute the window features. We compute the kNN distance between each window and the windows in the train set. As each frame can appear in several windows, the anomaly score that is given is the average of the distances computed for the windows in which that frame ap-pears. A summary of the results appears in Tab.5 and are discussed in Sec. 5. We find that the better performing method on physical and general benchmarks under-performs on surveillance videos. This emphasizes the different nature of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>Video features vs. image features: Video features often outperformed the image features with the exception of the features obtained by ViT pretrained on ImageNet-1K. Still, we note that among the classes that favored the image features were those that either had subtle differences in the motion of the normal and anomalous classes (candle, magnets) or had similar motion but feature an entirely different object (pendulum). Conversely, the classes that favored the video features were those that feature an interruption in the motion (window) or motion that follows a different trajectory (keyboard, sushi). Intuitively, this is in line with what one may expect as the image networks are pretrained to classify objects of different classes while the video networks are pretrained to classify different types of motion. This may serve as a general guideline in order to determine when one technique may outperform the other.</p><p>The need for a general video AD method: While our method works well at finding dynamics-related anomalies like those seen in our dataset, the results in Tab.5 show that our pretrained methods are not well-adapted to surveillance datasets. On the other hand, while MNAD attains state-of-the-art results on UCSD, it is not able to solve our dataset as easily. When running these methods on the SSv2 dataset, Tab.3 shows that they both struggle with this task. The videos in the train and test sets that we chose contain high variability both in the video background and the object in question. For example, the Tearing paper subclass is filmed in numerous scenes with the paper being either blank or with words printed on it, and being of different sizes and colors. This variability made solving these classes difficult for both our method and MNAD. These results stress the difference between these video AD settings and the need to develop a technique that will be able to solve general video AD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In our work, we focus on detecting anomalies in physical and general videos. We introduce new such benchmarks and evaluate baseline approaches on them. We find that the simple pretrained approach that struggles to outperform on surveillance data outperforms on our own suggested benchmarks. Taken together, our work highlights physical and general video anomaly detection as new tasks that call for the development of new approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of frames from each of the six video types in the PHANTOM dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Anomaly Detection Performance on the SSv2 dataset (ROCAUC)</figDesc><table><row><cell>Tearing something into two pieces</cell><cell>paper</cell><cell>0.70</cell><cell>0.51</cell><cell>0.36</cell></row><row><cell>Putting something on a surface</cell><cell>pen/pencil</cell><cell>0.63</cell><cell>0.70</cell><cell>0.61</cell></row><row><cell>Rolling something on a flat surface</cell><cell>pen/pencil</cell><cell>0.54</cell><cell>0.51</cell><cell>0.56</cell></row><row><cell>Something falling like a feather or paper</cell><cell>paper</cell><cell>0.65</cell><cell>0.52</cell><cell>0.58</cell></row><row><cell>Unfolding something</cell><cell>paper</cell><cell>0.64</cell><cell>0.51</cell><cell>0.44</cell></row><row><cell cols="2">Taking one of many similar things on the table pen/pencil</cell><cell>0.55</cell><cell>0.61</cell><cell>0.54</cell></row><row><cell>Closing something</cell><cell>box</cell><cell>0.42</cell><cell>0.45</cell><cell>0.58</cell></row><row><cell>Showing that something is empty</cell><cell>cup</cell><cell>0.51</cell><cell>0.46</cell><cell>0.48</cell></row><row><cell>Plugging something into something</cell><cell>cable</cell><cell>0.54</cell><cell>0.51</cell><cell>0.58</cell></row><row><cell>Pushing something so that it falls off the table</cell><cell>pen/pencil</cell><cell>0.60</cell><cell>0.45</cell><cell>0.47</cell></row><row><cell>Average</cell><cell></cell><cell>0.58</cell><cell>0.52</cell><cell>0.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Performance of Transformer vs. CNN-based architectures on PHANTOM classes (ROCAUC). Comparison to other methods on UCSD Ped1 and Ped2 datasets classes for 30, 60 and 120 epochs. We obtain the best performing run with 60 epochs. The results are shown in Tab.2.</figDesc><table><row><cell cols="2">classname</cell><cell cols="4">ResNet w. avg pooling ViT w. avg pooling bLVNet-TAM TimeSformer</cell></row><row><cell>candle</cell><cell></cell><cell>0.71</cell><cell>0.76</cell><cell>0.48</cell><cell>0.58</cell></row><row><cell cols="2">keyboard</cell><cell>0.88</cell><cell>0.94</cell><cell>0.99</cell><cell>0.95</cell></row><row><cell cols="2">pendulum: motion</cell><cell>0.62</cell><cell>0.87</cell><cell>0.95</cell><cell>1.00</cell></row><row><cell cols="2">pendulum: object</cell><cell>1.00</cell><cell>1.00</cell><cell>0.92</cell><cell>0.87</cell></row><row><cell>sushi</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell cols="2">window</cell><cell>0.84</cell><cell>0.86</cell><cell>1.00</cell><cell>0.93</cell></row><row><cell cols="2">average</cell><cell>0.84</cell><cell>0.91</cell><cell>0.89</cell><cell>0.89</cell></row><row><cell>Method</cell><cell cols="2">UCSD Ped1 UCSD Ped2</cell><cell></cell><cell></cell></row><row><cell>Kim[27]</cell><cell>59.0</cell><cell>69.3</cell><cell></cell><cell></cell></row><row><cell>Liu[28]</cell><cell>83.1</cell><cell>95.4</cell><cell></cell><cell></cell></row><row><cell>Ionescu[8]</cell><cell>-</cell><cell>97.8</cell><cell></cell><cell></cell></row><row><cell>Pang[12]</cell><cell>71.7</cell><cell>83.2</cell><cell></cell><cell></cell></row><row><cell>MNAD[7]</cell><cell>-</cell><cell>97.0</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>52.6</cell><cell>75.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work was partly supported by the Federmann Cyber Security</head><p>Research Center in conjunction with the Israel National Cyber Directorate.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Panda: Adapting pretrained features for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using pretraining can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
		<title level="m">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Anomaly detection in crowded scenes</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anomaly detection in video via self-supervised and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Barbalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bman: bidirectional multi-scale aggregation networks for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-trained deep ordinal regression for end-to-end video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Xiao Bai</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep nearest neighbor anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mean-shifted contrastive loss for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03844</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A day on campusan anomaly detection dataset for events in a single camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantini</forename><surname>Pranav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhenggang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-timescale trajectory prediction for abnormal human activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royston</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajbabu</forename><surname>Velmurugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasis</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Not only look, but also listen: Learning multimodal violence detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continual learning for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ano-graph: Learning normal scene contextual graphs to detect video anomalies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10502</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00869</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
