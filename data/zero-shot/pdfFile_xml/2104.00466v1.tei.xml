<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Calibration for Long-Tailed Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chinese University of Hong Kong SmartMore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chinese University of Hong Kong SmartMore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chinese University of Hong Kong SmartMore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chinese University of Hong Kong SmartMore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Calibration for Long-Tailed Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code: https://github.com/Jia-Research-Lab/MiSLAS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks may perform poorly when training datasets are heavily class-imbalanced. Recently, twostage methods decouple representation learning and classifier learning to improve performance. But there is still the vital issue of miscalibration. To address it, we design two methods to improve calibration and performance in such scenarios. Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-confidence for classes and improve classifier learning. For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework. Our proposed methods set new records on multiple popular long-tailed recognition benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With numerous available large-scale and high-quality datasets, such as ImageNet <ref type="bibr" target="#b26">[27]</ref>, COCO <ref type="bibr" target="#b18">[19]</ref>, and Places <ref type="bibr" target="#b39">[40]</ref>, deep convolutional neural networks (CNNs) have made notable breakthrough in various computer vision tasks, such as image recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, object detection <ref type="bibr" target="#b25">[26]</ref>, and semantic segmentation <ref type="bibr" target="#b5">[6]</ref>. These datasets are usually artificially balanced with respect to the number of instances for each object/class. However, in many realworld applications, data may follow unexpected long-tailed distributions, where the numbers of instances for different classes are seriously imbalanced. When training CNNs on these long-tailed datasets, the performance notably degrades. To address this terrible issue, a number of methods were proposed for long-tailed recognition.</p><p>Recently, many two-stage approaches have achieved significant improvement comparing with one-stage methods. Deferred re-sampling (DRS, <ref type="bibr" target="#b3">[4]</ref>) and deferred reweighting (DRW, <ref type="bibr" target="#b3">[4]</ref>) first train CNNs in a normal way in Stage-1. DRS tunes CNNs on datasets with class-balanced resampling while DRW tunes CNNs by assigning different weights to classes in Stage-2. Zhou et al. <ref type="bibr" target="#b38">[39]</ref> proposed bilateral branch network (BBN) in one stage to simulate the process of DRS by dynamically combining instancebalanced sampler and the reverse-balanced sampler. Kang et al. <ref type="bibr" target="#b14">[15]</ref> proposed two-stage decoupling models, classifier re-training (cRT) and learnable weight scaling (LWS), to further boost performance, where decoupling models freeze the backbone and just train the classifier with class-balanced resampling in Stage-2.</p><p>Confidence calibration <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref> is to predict probability by estimating representative of true correctness likelihood. It is important for recognition models in many applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. Expected calibration error (ECE) is widely used in measuring calibration of the network. To compute ECE, all N predictions are first grouped into B interval bins of equal size. ECE is defined as:</p><formula xml:id="formula_0">ECE = B b=1 |S b | N acc(S b ) ? conf(S b ) ? 100%,</formula><p>where S b is the set of samples whose prediction scores fall into Bin-b. acc(?) and conf(?) are the accuracy and predicted confidence of S b , respectively. Our study shows, because of the imbalanced composition ratio of each class, networks trained on long-tailed datasets are more miscalibrated and over-confident. We draw the reliability diagrams with 15 bins in <ref type="figure" target="#fig_0">Fig. 1</ref>, which compares the plain cross-entropy (CE) model trained on the original CIFAR-100 dataset, the plain CE model, cRT, and LWS trained on CIFAR-100-LT with imbalanced factor (IF) 100. It is noticeable that networks trained on long-tailed datasets usually have higher ECEs. The two-stage models of cRT and LWS suffer from over-confidence as well. Moreover, Figs. 9 and 10 (the first two plots) in Appendix C depict that this phenomenon also commonly exists on other long-tailed datasets, such as CIFAR-10-LT and ImageNet-LT.</p><p>Another issue is that two-stage decoupling ignores the dataset bias or domain shift <ref type="bibr" target="#b24">[25]</ref> in the two stages. In details, two-stage models are first trained on the instancedbalanced dataset D I in Stage-1. Then, models are trained on the class-balanced dataset D C in Stage-2. Obviously, P DI (x, y) = P DC (x, y) and distributions of the dataset by different sampling ways are inconsistent. Motivated by transfer learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>, we focus on the batch normalization <ref type="bibr" target="#b11">[12]</ref> layer to deal with the dataset bias problem.</p><p>In this work, we propose a Mixup Shifted Label-Aware Smoothing model (MiSLAS) to effectively solve above issues. Our key contributions are as follows.</p><p>? We discover that models trained on long-tailed datasets are much more miscalibrated and over-confident than those trained on balanced data. Two-stage models suffer from this problem as well.</p><p>? We find that mixup can remedy over-confidence and have a positive effect on representation learning but a negative or negligible effect on classifier learning. To further enhance classifier learning and calibration, we propose label-aware smoothing to handle different degrees of overconfidence for classes.</p><p>? It is the first attempt to note the dataset bias or domain shift in two-stage resampling methods for long-tailed recognition. To deal with it in the decoupling framework, we propose shift learning on the batch normalization layer, which can greatly improve performance.</p><p>? We extensively validate our MiSLAS on multiple longtailed recognition benchmark datasets -experimental results manifest the effectiveness. Our method yields new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Re-sampling and re-weighting. There are two groups of re-sampling strategies: over-sampling the tail-class images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and under-sampling the head-class images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref>. Over-sampling is regularly useful on large datasets and may suffer from heavy over-fitting to tail classes especially on small datasets. For under-sampling, it discards a large portion of data, which inevitably causes degradation of the generalization ability of deep models. Reweighting <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref> is another prominent strategy. It assigns different weights for classes and even instances. The vanilla re-weighting method gives class weights in reverse proportion to the number of samples of classes. However, with large-scale data, re-weighting makes deep models difficult to optimize during training. Cui et al. <ref type="bibr" target="#b6">[7]</ref> relieved the problem using the effective numbers to calculate the class weights. Another line of work is to adaptively re-weight each instance. For example, focal loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> assigned smaller weights for well-classified samples.</p><p>Confidence calibration and regularization. Calibrated confidence is significant for classification models in many applications. Calibration of modern neural networks is first discussed in <ref type="bibr" target="#b8">[9]</ref>. The authors discovered that model capacity, normalization, and regularization have strong effect on network calibration. mixup <ref type="bibr" target="#b36">[37]</ref> is a regularization technique to train with interpolation of input and labels. mixup inspires follow-up of manifold mixup <ref type="bibr" target="#b31">[32]</ref>, Cut-Mix <ref type="bibr" target="#b35">[36]</ref>, and Remix <ref type="bibr" target="#b4">[5]</ref> that have shown significant improvement. Thulasidasan et al. <ref type="bibr" target="#b29">[30]</ref> found that CNNs trained with mixup are better calibrated. Label smoothing <ref type="bibr" target="#b28">[29]</ref> is another regularization technique that encourages the model to be less over-confident. Unlike cross-entropy that computes loss upon the ground truth labels, label smoothing computes loss upon a soft version of labels. It relieves over-fitting and increases calibration and reliability <ref type="bibr" target="#b22">[23]</ref>.</p><p>Two-stage methods. Cao et al. <ref type="bibr" target="#b3">[4]</ref> proposed deferred reweighting (DRW) and deferred re-sampling (DRS), working better than conventional one-stage methods. Its stage-2, starting from better features, adjusts the decision boundary and locally tunes features. Recently, Kang et al. <ref type="bibr" target="#b14">[15]</ref> and Zhou et al. <ref type="bibr" target="#b38">[39]</ref> concluded that although class re-balance matters for jointly training representation and classifier, instancebalanced sampling gives more general representations.</p><p>Based on this observation, Kang et al. <ref type="bibr" target="#b14">[15]</ref> achieved state-of-the-art results by decomposing representation and classifier learning. It first trains the deep models with <ref type="table">Table 1</ref>: Top-1 accuracy (%) and ECE (%) of the plain cross-entropy (CE) model, and decoupling models of cRT (left) and LWS (right), for ResNet families trained on the ImageNet-LT dataset. We vary the augmentation strategies with ( ), or without ( ) mixup ? = 0.2, on both of the stages. instance-balanced sampling, and then fine-tunes the classifier with class-balanced sampling with parameters of representation learning fixed. Similarly, Zhou et al. <ref type="bibr" target="#b38">[39]</ref> integrated mixup training into the proposed cumulative learning strategy. It bridges the representation learning and classifier re-balancing. The cumulative learning strategy requires dual samplers of instance-balanced and reversed instancebalanced sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Study of mixup Strategy</head><p>For the two-stage learning framework, Kang et al. <ref type="bibr" target="#b14">[15]</ref> and Zhou et al. <ref type="bibr" target="#b38">[39]</ref> found that instance-balanced sampling gives the most general representation among all for longtailed recognition. Besides, Thulasidasan et al. <ref type="bibr" target="#b29">[30]</ref> show that networks trained with mixup are better calibrated. Based on these findings, when using instance-balanced sampling, we explore the effect of mixup in the two-stage decoupling framework for higher representation generalization and overconfidence reduction.</p><p>We train a plain cross-entropy model, and two two-stage models of cRT and LWS, on ImageNet-LT for 180 epochs in Stage-1 and finetune them for 10 epochs in Stage-2, respectively. We vary the training setup (with/without mixup ? = 0.2) for both stages. Top-1 accuracy of these variants is listed in <ref type="table">Table 1</ref>. It reveals the following. (i) When applying mixup, improvement of CE can be ignored. But the performance is greatly enhanced for both cRT and LWS.</p><p>(ii) Applying additional mixup in Stage-2 yields no obvious improvement or even damages performance. The reason is that mixup encourages representation learning and is yet with adverse or negligible effect on classifier learning.</p><p>Besides, we draw the final classifier weight norms of these variants in <ref type="figure" target="#fig_1">Fig. 2</ref>. We show the L 2 norms of the weight vectors for all classes, as well as the training data distribution sorted in a descending manner concerning the number of instances. We observe that when applying mixup (in orange), the weight norms of the tail classes tends to be large and the weight norms of the head classes decrease. It means mixup may be more friendly to tail classes.</p><p>We also list ECEs of the above models in <ref type="table">Table 1</ref>. When adding mixup in just Stage-1, both cRT and LWS models can consistently obtain better top-1 accuracy and lower ECEs for different backbones (Row-4 and Row-6). Due to the unsatisfied top-1 accuracy enhancement and unstable ECE decline of mixup for classifier learning (by adding mixup in Stage-2), we propose a label-aware smoothing to further improve both calibration and classifier learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label-aware Smoothing</head><p>In this subsection, we analyze and deal with the two issues of over-confidence and limited improvement by classifier learning. Suppose weight of the classifier is W ? R M ?K , where M is the number of features and K is the number of classes. The cross-entropy encourages the whole network to be over-confident on the head classes. The cross-entropy loss after the softmax activation is l(y,</p><formula xml:id="formula_1">p) = ? log(p y ) = ?w y x + log( exp(w i x)), where y ? {1, 2, ..., K} is the label. x ? R M</formula><p>is the feature vector send to classifier and w i is the i-th column vector of W . The optimal solution is w * y x = inf, while other w i x, i = y are small enough. Because the head classes contain much more training examples, the network makes the weight norm w of the head classes larger to approach the optimal solution. It results in predicted probabilities mainly near 1.0 (see <ref type="figure" target="#fig_2">Fig. 3</ref>, the upper half in light blue). Another fact is that distributions of predicted probability are related to instance numbers. Unlike balanced recognition, applying different strategies for these classes is necessary for solving the long-tailed problem.</p><p>Here, we propose label-aware smoothing to solve the over-confidence in cross-entropy and varying distributions of predicted probability issues. It is expressed as</p><formula xml:id="formula_2">l(q, p) = ? K i=1 q i log p i , q i = 1 ? y = 1 ? f (N y ), i = y, y K?1 = f (Ny) K?1 , otherwise,<label>(1)</label></formula><p>where y is a small label smoothing factor for Class-y, relating to its class number N y . Now the optimal solution becomes (proof presented in Appendix E)</p><formula xml:id="formula_3">w * i x = log (K?1)(1? y ) y + c, i = y, c, otherwise,<label>(2)</label></formula><p>where c is an arbitrary real number. Compared with the optimal solution in cross-entropy, the label-aware smoothing encourages a finite output, more general and remedying overfit. We suppose the labels of the long-tailed dataset are assigned in a descending order concerning the number of instances, i.e., N 1 ? N 2 ? ... ? N K . Because the head classes contain more diverse examples, the predicted probabilities are more promising than those of tail classes. Thus, we require the classes with larger instance numbers to be penalized with stronger label smoothing factors -that is, the related function f (N y ) should be negatively correlated to N y . We define three types of related function f (N y ) as</p><p>? Concave form:</p><formula xml:id="formula_4">f (N y ) = K + ( 1 ? K ) sin ?(N y ? N K ) 2(N 1 ? N K ) ; (3.a)</formula><p>? Linear form:</p><formula xml:id="formula_5">f (N y ) = K + ( 1 ? K ) N y ? N K N 1 ? N K ; (3.b) ? Convex form: f (N y ) = 1 +( 1 ? K ) sin 3? 2 + ?(N y ? N K ) 2(N 1 ? N K ) , (3.c)</formula><p>where 1 and K are two hyperparameters. Illustration of these functions is shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. If we set 1 ? K ,</p><formula xml:id="formula_6">1 ? 2 ? ... ? K is obtained.</formula><p>For large instance number N y for Class-y, label-aware smoothing allocates a strong smoothing factor. It lowers the fitting probability to relieve over-confidence because the head and medium classes are more likely to be over-confident than the tail classes (see <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>As the form of label-aware smoothing is more complicated than cross-entropy, we propose a generalized classifier learning framework to fit it. Here we give a quick review about cRT and LWS. cRT learns a classifier weight, which contains KM learnable parameters, while LWS is restricted to learning the weight scaling vector s ? R K with only K learnable parameters.</p><p>In contrast, cRT has more learnable parameters and more powerful representation ability. LWS tends to obtain better validation losses and performance on large-scale datasets (refer to the experiment part in <ref type="bibr" target="#b14">[15]</ref>). So LWS has a better generalization property. To combine the advantages of cRT and LWS, we design the classifier framework in Stage-2 as</p><formula xml:id="formula_7">z = diag(s) (rW + ?W ) x.<label>(4)</label></formula><p>In Eq. <ref type="formula" target="#formula_7">(4)</ref>, we fix the original classifier weight W in Stage-2.</p><p>If we make the learnable scaling vector s fixed, set s = 1 and retention factor r = 0, and just learn the new classifier weight ?W ? R M ?K , Eq. (4) degrades to cRT. Because LWS fixes the original classifier weights W and only learns the scaling s, Eq. (4) degrades to LWS if we set r = 1 and ?W = 0. In most cases, LWS achieves better results on large-scale datasets. Thus, we let s learnable and set r = 1. We also make ?W learnable to improve the representation ability and optimize ?W by a different learning rate. ?W can be viewed as shift transformation on W . It changes the direction of weight vector w in W , which LWS does not similarly achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Shift Learning on Batch Normalization</head><p>In the two-stage training framework, models are first trained with instance-balanced sampling in Stage-1 and then trained with class-balanced sampling in Stage-2. Since the framework involves two samplers, or two datasets -instancebalanced dataset D I and class-balanced dataset D C -we regard this two-stage training framework as a variant of transfer learning. If we view the two-stage decoupling training framework from the transfer learning perspective, fixing the backbone part and just tuning the classifier in Stage-2 are clearly unreasonable, especially for the batch normalization (BN) layers.</p><p>Specifically, we suppose the input to network is x i , the input feature of some BN layer is g(x i ), and the mini-batch size is m. The mean and running variance of Channel-j for these two stages are</p><formula xml:id="formula_8">x i ? P DI (x, y), ? (j) I = 1 m m i=1 g(x i ) (j) , ? 2 I (j) = 1 m m i=1 g(x i ) (j) ? ? (j) I 2 ,<label>(5)</label></formula><formula xml:id="formula_9">x i ? P DC (x, y), ? (j) C = 1 m m i=1 g(x i ) (j) , ? 2 C (j) = 1 m m i=1 g(x i ) (j) ? ? (j) C 2 .<label>(6)</label></formula><p>Due to different sampling strategies, the composition ratios of head, medium, and tail classes are also different, which lead to P DI (x, y) = P DC (x, y). By Eqs. <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_9">(6)</ref>, there exist biases in ? and ? under two sampling strategies, i.e., ? I = ? C and ? 2 I = ? 2 C . Thus, it is infeasible for the decoupling framework that BN shares mean and variance across datasets with two sampling strategies. Motivated by AdaBN <ref type="bibr" target="#b16">[17]</ref> and TransNorm <ref type="bibr" target="#b32">[33]</ref>, we update the running mean ? and variance ? and yet fix the learnable linear transformation parameters ? and ? for better normalization in Stage-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Setup</head><p>Our experimental setup including the implementation details and evaluation protocol mainly follows <ref type="bibr" target="#b3">[4]</ref> for CIFAR-10-LT and CIFAR-100-LT, and <ref type="bibr" target="#b14">[15]</ref> for ImageNet-LT, Places-LT, and iNuturalist 2018. Please see Appendix A for more details of training and hyperparameter setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets Explanation</head><p>CIFAR-10 and CIFAR-100. CIFAR-10 and CIFAR-100 both have 60,000 images, 50,000 for training and 10,000 for validation with 10 categories and 100 categories. For fair comparison, we use the long-tailed versions of CIFAR datasets with the same setting as those used in <ref type="bibr" target="#b3">[4]</ref>. It is by controlling the degrees of data imbalance with an imbalanced factor ? = Nmax Nmin , where N max and N min are the numbers of training samples for the most and the least frequent classes. Following Cao et al. <ref type="bibr" target="#b3">[4]</ref> and Zhou et al. <ref type="bibr" target="#b38">[39]</ref>, we conduct experiments with IF 100, 50, and 10.</p><p>ImageNet-LT and Places-LT. ImageNet-LT and Places-LT were proposed by Liu et al. <ref type="bibr" target="#b19">[20]</ref>. ImageNet-LT is a long-tailed version of the large-scale object classification dataset ImageNet <ref type="bibr" target="#b26">[27]</ref> by sampling a subset following the Pareto distribution with power value ? = 6. It contains 115.8K images from 1,000 categories, with class cardinality ranging from 5 to 1,280. Places-LT is a long-tailed version of the large-scale scene classification dataset Places <ref type="bibr" target="#b39">[40]</ref>. It consists of 184.5K images from 365 categories with class cardinality ranging from 5 to 4,980.</p><p>iNaturalist 2018. iNaturalist 2018 <ref type="bibr" target="#b30">[31]</ref> is a classification dataset, which is on a large scale and suffers from extremely imbalanced label distribution. It is composed of 437.5K images from 8,142 categories. In addition, on iNaturalist 2018 dataset, we also face the fine-grained problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation Details</head><p>For all experiments, we use the SGD optimizer with momentum 0.9 to optimize networks. For CIFAR-LT, we mainly follow Cao et al. <ref type="bibr" target="#b3">[4]</ref>. We train all MiSLAS models with the ResNet-32 backbone on one GPU and use the multistep  <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>learning rate schedule, which decreases the learning rate by 0.1 at the 160 th and 180 th epochs in Stage-1. For ImageNet-LT, Places-LT, and iNaturalist 2018, we mainly follow Kang et al. <ref type="bibr" target="#b14">[15]</ref> and use the cosine learning rate schedule <ref type="bibr" target="#b20">[21]</ref> to train all MiSLAS models with the ResNet-10, 50, 101, and 152 backbones on four GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Calibration performance. Here we show the reliability diagrams with 15 bins of our methods on CIFAR-100-LT with IF 100 in <ref type="figure" target="#fig_3">Fig. 4</ref>. Comparing with <ref type="figure" target="#fig_0">Fig. 1</ref>, both mixup and label-aware smoothing can not only largely enhance the network calibration (even lower ECEs than those on balanced datasets) but also greatly improve the performance for long-tailed recognition. The similar trends can also be noticed on CIFAR-10-LT, ImageNet-LT, and Places-LT (see <ref type="table">Table 1</ref> and figures in Appendix C for detail), which proves the power of the proposed method on calibration.</p><p>All experimental results show that the training networks on imbalanced datasets lead to severe over-confidence. Since the conventional mixup and label-smoothing both contain the operation of softening the ground truth labels, it may suggest that training with hard labels is likely to be another contributing factor leading to network over-confidence.</p><p>Comparing re-weighting with label-aware smoothing.</p><p>Here we compare the proposed label-aware smoothing (LAS) with the re-weighting methods. The main difference is on label transformation. In particular, label-aware smoothing changes the hard label to the soft version based on label distribution (see the otherwise case of Eq. (1): q i = f (Ny) K?1 , i = y). While re-weighting methods do not contain such critical transformation and just set the values to zero by q i = 0, i = y.</p><p>Further, due to the transformation of labels, the optimal solution of w * i x in LAS becomes Eq. (2). In contrast, the optimal solution of re-weighting is the same as that of crossentropy w * i x = inf, which cannot properly change the  predicted distribution and leads to over-confidence. Based on our experimental results in <ref type="table" target="#tab_1">Table 2</ref>, using the re-weighting method in Stage-2 degrades performance and calibration compared with the case of LAS.</p><p>How 1 and K affect label-aware smoothing? In our label-aware smoothing, there are two hyperparameters in Eqs. (3.a), (3.b), and (3.c). They are 1 and K , which control penalty of classes. In a recognition system, if the predicted probability of Class-y is larger than 0.5, the classifier would classify the input to Class-y. Thus, to make it reasonable, we limit 0 ? K ? 1 ? 0.5.</p><p>Here we conduct experiments by varying 1 and K both from 0.0 to 0.5 on CIFAR-10-LT with IF 100. We plot the performance matrix upon 1 and K in <ref type="figure" target="#fig_5">Fig. 5</ref> for all possible variants. It shows that the classification accuracy is further improved by 3.3% comparing with conventional cross-entropy ( 1 = 0 and K = 0, green square) when we pick 1 = 0.3, and K = 0.0 (orange square) for labelaware smoothing. Consistent improvement 0.9% is yielded on CIFAR-100-LT with IF 100 when picking 1 = 0.4 and K = 0.1 for label-aware smoothing. How f (?) affects label-aware smoothing? As discussed in Sec. 3.2, the related function f (?) may play a significant role for the final model performance. We draw illustration of Eqs.        above, we set 1 = 0.4 and 100 = 0.1 here. After tuning for 10 epochs in Stage-2, accuracy of the concave model is the best. We also exploit other forms, e.g., exponential form of f (?), in Appendix B. The gain of changing form is quite limited compared with varying 1 and K .</p><p>How label-aware smoothing affects prediction distribution? To visualize the change in predicted probability distributions, we train two LWS models, one with cross-entropy and the other with label-aware smoothing on CIFAR-100-LT with IF 100. The cross-entropy-based distributions of the head, medium, and tail classes are shown in the upper part of <ref type="figure" target="#fig_2">Fig. 3</ref> in light blue. The label-aware smoothing-based distributions are in the bottom half in deep blue. We observe that the over-confidence of head and medium classes is much reduced, and the whole distribution of the tail classes slightly moves right when using label-aware smoothing. These empirical results are consistent with our analysis in Sec. 3.2.</p><p>Further analysis of shift learning. In this part, we conduct experiments to show the effectiveness and suitability of shift learning on BN. We train the LWS model on CIFAR-100-LT with IF 100. After 10-epoch finetuning in Stage-2, the model trained with BN shifting achieves accuracy 45.3%, 1.1% higher than that without BN shifting. We also visualize the change in BN. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, there exist biases in ? and ? 2 between datasets using different sampling strategies.</p><p>Due to different composition ratios of the head, medium and tail classes, the statistic mean ? and variance ? 2 vary. We also notice intriguing phenomena in <ref type="figure" target="#fig_7">Fig. 7</ref>: (i) the change in variance ? 2 is larger than that on mean ?. (ii) Change of ? and ? 2 in the deep BN layers is much smaller than that in the shallow BN layers.</p><p>Summary. Overall, <ref type="table" target="#tab_5">Table 3</ref> shows the ablation investigation on the effects of mixup (adding mixup in Stage-1, MU), shift learning on batch normalization (SL), and label-aware smoothing (LAS). We note each proposed module can not only improves accuracy (top of   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>To verify the effectivity, we compare the proposed method against previous one-stage methods of Range Loss <ref type="bibr" target="#b37">[38]</ref>, LDAM Loss <ref type="bibr" target="#b3">[4]</ref>, FSLwF <ref type="bibr" target="#b7">[8]</ref>, and OLTR <ref type="bibr" target="#b19">[20]</ref>, and against previous two-stage methods, including DRS-like, DRWlike <ref type="bibr" target="#b3">[4]</ref>, LFME <ref type="bibr" target="#b34">[35]</ref>, cRT, and LWS <ref type="bibr" target="#b14">[15]</ref>. For fair comparison, we add mixup on the LWS and cRT models. Remix <ref type="bibr" target="#b4">[5]</ref> is a recently proposed augmentation method for long-tail recognition. Because BBN <ref type="bibr" target="#b38">[39]</ref> has double samplers and is trained in a mixup-like manner, we directly compare our method with it.</p><p>Experimental results on CIFAR-LT. We conduct extensive experiments on CIFAR-10-LT and CIFAR-100-LT with IF 100, 50, and 10, using the same setting as previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref>. The results are summarized in <ref type="table" target="#tab_7">Table 4</ref>. Compared with previous methods, our MiSLAS outperforms all previous methods by consistently large margins both in top-1 accuracy and ECE. Moreover, the superiority holds for all imbalanced factors, i.e., 100, 50, and 10, on both CIFAR-10-LT and CIFAR-100-LT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results on large-scale datasets.</head><p>We further verify the effectiveness of our method on three largescale imbalanced datasets, i.e., ImageNet-LT, iNaturalist 2018, and Places-LT. <ref type="table" target="#tab_8">Table 5</ref> lists experimental results on ImageNet-LT (left), iNaturalist 2018 (center), and Places-LT (right). Notably, our MiSLAS outperforms other approaches and sets a new state-of-the-art with better accuracy and confidence calibration on almost all three large-scale long-tailed benchmark datasets. More results about the split class accuracy and different backbones on these three datasets are listed in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have discovered that models trained on long-tailed datasets are more miscalibrated and overconfident than those trained on balanced datasets. We accordingly propose two solutions of using mixup and designing label-aware smoothing to handle different degrees of overconfidence for classes. We note the dataset bias (or domain shift) in two-stage resampling methods for long-tailed recognition. To reduce dataset bias in the decoupling framework, we propose shift learning on the batch normalization layer, which further improves the performance. Extensive quantitative and qualitative experiments on various benchmarks show that our MiSLAS achieves decent performance for both top-1 recognition accuracy and confidence calibration, and makes a new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving Calibration for Long-Tailed Recognition (Supplementary Material)</head><p>A. Experiment Setup Following Liu et al. <ref type="bibr" target="#b19">[20]</ref> and Kang et al. <ref type="bibr" target="#b14">[15]</ref>, we report the commonly used top-1 accuracy over all classes on the balanced test/validation datasets, denoted as All. We further report accuracy on three splits of classes: Head-Many (more than 100 images), Medium (20 to 100 images), and Tail-Few (less than 20 images). The detailed setting of hyperparameters and training for all datasets used in our paper are listed in <ref type="table" target="#tab_9">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Common  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exponential Form of the Related Function f (?)</head><p>As discussed in Secs. 3.2 and 4.2, the form of the related function f (?) may play an important role for final model performance. We draw the illustration of Eqs. (3.a), (3.b), and (3.c) at the left of <ref type="figure">Fig. 8</ref>. For the CIFAR-100-LT dataset with imbalanced factor 100, K = 100, N 1 = 500, and N 100 = 5. Based on the ablation study results of 1 and K mentioned in Sec. 4.2, we set 1 = 0.4 and 100 = 0.1 here. After fintuning for 10 epochs in Stage-2, the accuracy of the concave model is the best. We also design an exponential related function, which is written as</p><formula xml:id="formula_10">y = f (N y ) = K + ( 1 ? K ) N y ? N K N 1 ? N K p , y = 1, 2, ..., K,<label>(7)</label></formula><p>where p is a hyperparameter to control the shape of the related function. For example, we get the concave related function when setting p &lt; 1 and convex function otherwise. Illustration of Eq. <ref type="formula" target="#formula_10">(7)</ref> is given on the right of <ref type="figure">Fig. 8</ref> Smoothing Factor p = 1/4 46.67% p = 1/2 46.82% p = 1 46.89% p = 2 46.81% p = 4 46.77%   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Eq. (2), the Optimal Solution of LAS</head><p>In this section, we prove the optimal solutions of cross-entropy, the re-weighting method, and LAS. Furthermore, the comparison among above three methods will also be discussed.</p><p>The general loss function form of these three methods for K classes can be written as</p><formula xml:id="formula_11">l = ? K i=1 q i log p i , p i = softmax(w i x), s.t., K i p i = 1,<label>(8)</label></formula><p>where p, w, and x are the predicted probability, the weight parameter of the last fully-connected layer, and the input of the last fully-connected layer, respectively. When the target label q is defined as</p><formula xml:id="formula_12">q i = 1, i = y, 0, i = y,</formula><p>where y is the original ground truth label. Eq. (8) becomes the commonly used cross-entropy loss function. Similarly, when the target label q is defined as q i = w i , i = y, and w i &gt; 0, 0, i = y, Eq. (8) becomes the re-weighting loss function. Moreover, when the target label q is</p><formula xml:id="formula_13">q i = 1 ? y = 1 ? f (N y ), i = y, y K?1 = f (Ny) K?1 , i = y,<label>(9)</label></formula><p>Eq. (8) becomes the proposed LAS method. To get the optimal solution of Eq. (8), we define its Lagrange multiplier form as</p><formula xml:id="formula_14">L = l + ? K i p i ? 1 = ? K i=1 q i log p i + ? K i p i ? 1 ,<label>(10)</label></formula><p>where ? is the Lagrange multiplier. The first order conditions of Eq. (10) w.r.t. ? and p can be written as</p><formula xml:id="formula_15">?L ?? = K i=1 p i ? 1 = 0, ?L ?p i = ? q i p i + ? = 0.<label>(11)</label></formula><p>According to Eq. (11), we get p i = qi K j=1 qj . Then, in the case of cross-entropy and re-weighting loss function, we get p i = 1, i = y and p i = 0, i = y. Noting that</p><formula xml:id="formula_16">p i = softmax(w i x) = exp(w i x) K j=1 exp(w j x)</formula><p>, the optimal solutions of w i x for both cross-entropy and re-weighting loss functions are the same, that is, w * i x = inf. This means that both cross-entropy and re-weighting loss functions make the weight vector of the right class w i , i = y large enough while the others w j , j = y sufficiently small. As a result, they cannot change the predicted distribution and relieve over-confidence effectively. In contrast, in our LAS, according to Eqs. (9) and (11), we get </p><p>where c ? R can be an arbitrary real number. Overall, comparing with the infinite optimal solution in cross-entropy and re-weighting method, LAS encourages a finite output, which leads to a more general result, properly refines the predicted distributions of the head, medium, and tailed classes, and remedies over-confidence effectively.</p><p>F. More Results about the Effect of mixup on cRT and LWS  As mentioned in Sec. 3.1 and <ref type="figure" target="#fig_1">Fig. 2</ref>, we observe that when applying mixup (orange line), the weight norms of the tail classes tend to be larger and the weight norms of the head classes are decreased, which means mixup may be more friendly to the tail classes. Here, we show more evidences that mixup reduces dominance of the head classes. In <ref type="figure" target="#fig_0">Figs. 13 and 14</ref>, norm of these variants are trained on Places-LT and iNaturalist 2018, respectively. The results are similar and consistent with those trained on ImageNet-LT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Reliability diagrams of ResNet-32. From top left to bottom right: the plain model trained on the original balanced CIFAR-100 dataset, the plain model, cRT, and LWS trained on CIFAR-100-LT with IF 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Classifier weight norms for the ImageNet-LT validation set where classes are sorted by descending values of N j , where N j denotes the number of training sample for Class-j. Left: weight norms of cRT with or without mixup. Right: weight norms of LWS with or without mixup. Light shade: true norm. Dark lines: smooth version. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Violin plot of predicted probability distributions for different parts of the classes, head (100+ images per class), medium (20-100 images per class), and tail (less than 20 images per class) on CIFAR-100-LT with IF 100. The upper half part in light blue denotes "LWS + cross-entropy". The bottom half part in deep blue represents "LWS + label-aware smoothing".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Reliability diagrams of ResNet-32 trained on CIFAR-100-LT with IF 100. From left to right: cRT with mixup, LWS with mixup, LWS with mixup and shifted BN, and MiSLAS (complying with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(3.a), (3.b), and (3.c) in Fig. 6. For CIFAR-100-LT with IF 100, we set K = 100, N 1 = 500, and N 100 = 5. Based on the ablation study results of 1 and K mentioned0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Ablation study of two hyperparameters 1 and K in label-aware smoothing. Heat map visualization on CIFAR-10-LT with IF 100 (left) and on CIFAR-100-LT with IF 100 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Function illustration and the test performance of Eqs. (3.a), (3.b), and (3.c). Concave form achieves the best result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>, Acc. 45.3% BN w/o shift, Acc. 44.2% Visualization of the changes in the running mean ? and variance ? 2 . The ResNet-32 based model is trained on CIFAR-100-LT with IF 100. Left: ? and ? 2 in the first BN of ResNet-32, which contains 16 channels. Right: ? and ? 2 in the last BN of ResNet-32, which contains 64 channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :Figure 10 :Figure 11 :Figure 12 :</head><label>89101112</label><figDesc>Function illustration and accuracy of Eqs. (3.a), (3.b), and (3.c) (left) and Eq. (7) (right). Reliability diagrams on CIFAR10 with 15 bins. From left to right: plain ResNet-32 model trained on the original CIFAR-10 dataset, plain model, cRT, LWS, and MiSLAS trained on long-tailed CIFAR-10 with imbalanced factor 100. Reliability diagrams on ImageNet with 15 bins. From left to right: plain ResNet-50 model trained on the original ImageNet dataset, plain model, cRT, LWS, and MiSLAS trained on ImageNet-LT. Reliability diagrams of ResNet-152 trained on Places-LT with 15 bins. From left to right: cRT, LWS, cRT with mixup, LWS with mixup, and MiSLAS. Reliability diagrams of ResNet-50 trained on iNaturalist 2018 with 15 bins. From left to right: cRT, LWS (under-confidence), cRT with mixup, LWS with mixup (under-confidence), and MiSLAS (under-confidence).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Classifier weight norms for the Places-LT evaluation set (365 classes in total) when classes are sorted by descending values of N j , where N j denotes the number of training sample for Class-j. Left: weight norms of cRT with/without mixup. Right: weight norms of LWS with/without mixup. Light shade: true norm. Dark lines: smooth version. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Classifier weight norms for the iNaturalist 2018 validation set (8,142 classes in total) when classes are sorted by descending values of N j , where N j denotes the number of training sample for Class-j. Left: weight norms of cRT with or without mixup. Right: weight norms of LWS with or without mixup. Light shade: true norm. Dark lines: smooth version. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison in terms of test accuracy (%) / ECE (%) of label-aware smoothing (LAS) with re-weighting, class- balanced cross-entropy (CB-CE, [7]) in Stage-2. Both mod- els are based on ResNet-32 and trained on CIFAR-100-LT with IF 100, 50, and 10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.17 46.73 46.89 46.93 46.89 46.87 46.13 46.62 46.83 47.04 46.94 46.11 46.60 46.76 46.96</figDesc><table><row><cell></cell><cell>47.0</cell></row><row><cell></cell><cell>46.8</cell></row><row><cell></cell><cell>46.6</cell></row><row><cell>46.08 46.45 46.85</cell><cell>46.4</cell></row><row><cell>46.12 46.50</cell><cell></cell></row><row><cell>46.05</cell><cell>46.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Ablation study for all proposed</cell></row><row><cell>modules on CIFAR-100-LT. Top: accu-</cell></row><row><cell>racy (%). Bottom: ECE (%). MU: apply-</cell></row><row><cell>ing mixup only in Stage-1. SL: shift learn-</cell></row><row><cell>ing on BN. LAS: label-aware smoothing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 )</head><label>3</label><figDesc>, but also greatly relieves over-confidence (bottom ofTable 3) on CIFAR-100-LT for all commonly-used imbalanced factors, i.e., 100, 50, and 10. They firmly manifest the effectiveness.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10-LT</cell><cell></cell><cell></cell><cell>CIFAR-100-LT</cell><cell></cell></row><row><cell>Method</cell><cell>100</cell><cell>50</cell><cell>10</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>CE</cell><cell>70.4</cell><cell>74.8</cell><cell>86.4</cell><cell>38.4</cell><cell>43.9</cell><cell>55.8</cell></row><row><cell>mixup [37]</cell><cell>73.1</cell><cell>77.8</cell><cell>87.1</cell><cell>39.6</cell><cell>45.0</cell><cell>58.2</cell></row><row><cell>LDAM+DRW [4]</cell><cell>77.1</cell><cell>81.1</cell><cell>88.4</cell><cell>42.1</cell><cell>46.7</cell><cell>58.8</cell></row><row><cell>BBN(include mixup) [39]</cell><cell>79.9</cell><cell>82.2</cell><cell>88.4</cell><cell>42.6</cell><cell>47.1</cell><cell>59.2</cell></row><row><cell>Remix+DRW(300 epochs) [5]</cell><cell>79.8</cell><cell>-</cell><cell>89.1</cell><cell>46.8</cell><cell>-</cell><cell>61.3</cell></row><row><cell>cRT+mixup</cell><cell>79.1 / 10.6</cell><cell>84.2 / 6.89</cell><cell>89.8 / 3.92</cell><cell>45.1 / 13.8</cell><cell>50.9 / 10.8</cell><cell>62.1 / 6.83</cell></row><row><cell>LWS+mixup</cell><cell>76.3 / 15.6</cell><cell>82.6 / 11.0</cell><cell>89.6 / 5.41</cell><cell>44.2 / 22.5</cell><cell>50.7 / 19.2</cell><cell>62.3 / 13.4</cell></row><row><cell>MiSLAS</cell><cell>82.1 / 3.70</cell><cell>85.7 / 2.17</cell><cell>90.0 / 1.20</cell><cell>47.0 / 4.83</cell><cell>52.3 / 2.25</cell><cell>63.2 / 1.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">Top-1 accuracy (%) / ECE (%) for ResNet-32 based models trained on CIFAR-10-LT and CIFAR-100-LT.</cell></row><row><cell>Method</cell><cell>ResNet-50</cell><cell>Method</cell><cell>ResNet-50</cell><cell>Method</cell><cell>ResNet-152</cell></row><row><cell>CE</cell><cell>44.6</cell><cell>CB-Focal [7]</cell><cell>61.1</cell><cell>Range Loss [38]</cell><cell>35.1</cell></row><row><cell>CE+DRW [4]</cell><cell>48.5</cell><cell>LDAM+DRW [4]</cell><cell>68.0</cell><cell>FSLwF [8]</cell><cell>34.9</cell></row><row><cell cols="2">Focal+DRW [18] 47.9</cell><cell cols="2">BBN(include mixup) [39] 69.6</cell><cell>OLTR [20]</cell><cell>35.9</cell></row><row><cell cols="2">LDAM+DRW [4] 48.8</cell><cell>Remix+DRW [5]</cell><cell>70.5</cell><cell cols="2">OLTR+LFME [35] 36.2</cell></row><row><cell>CRT+mixup</cell><cell>51.7 / 5.62</cell><cell>cRT+mixup</cell><cell>70.2 / 1.79</cell><cell>cRT+mixup</cell><cell>38.3 / 12.4</cell></row><row><cell>LWS+mixup</cell><cell>52.0 / 2.23</cell><cell cols="2">LWS+mixup(under-conf.) 70.9 / 9.41</cell><cell>LWS+mixup</cell><cell>39.7 / 11.7</cell></row><row><cell>MiSLAS</cell><cell>52.7 / 1.83</cell><cell>MiSLAS(under-conf.)</cell><cell>71.6 / 7.67</cell><cell>MiSLAS</cell><cell>40.4 / 3.59</cell></row><row><cell cols="2">(a) ImageNet-LT</cell><cell cols="2">(b) iNaturalist 2018</cell><cell cols="2">(c) Places-LT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Top-1 accuracy (%) / ECE (%) on ImageNet-LT (left), iNaturalist 2018 (center) and Places-LT (right).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Detailed experiment setting on five benchmark datasets. LR: initial learning rate, BS: batch size, WD: weight decay, LRS: learning rate schedule, and ?W : learning rate ratio of ?W .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>. Comparing accuracy of all variants, the influence of the related function form is quite limited for the final performance (0.3% increase). Because the concave related function Eq. (3.a) achieves the best performance, we choose it as the default setting of the related function f (?) for other experiments.</figDesc><table><row><cell>Smoothing Factor</cell><cell>1</cell><cell></cell><cell></cell><cell>46.89% convex 46.89% linear 47.04% concave</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>K</cell><cell>N K</cell><cell>Instance Number</cell><cell>N 1</cell><cell>K</cell><cell>N K</cell><cell>Instance Number</cell><cell>N 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comprehensive accuracy results on iNaturalist 2018 with ResNet-50 and training 200 epochs.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>Many</cell><cell>Medium</cell><cell>Few</cell><cell>All</cell></row><row><cell></cell><cell>Lifted Loss</cell><cell>41.1</cell><cell>35.4</cell><cell>24.0</cell><cell>35.2</cell></row><row><cell></cell><cell>Focal Loss</cell><cell>41.1</cell><cell>34.8</cell><cell>22.4</cell><cell>34.6</cell></row><row><cell></cell><cell>Range Loss</cell><cell>41.1</cell><cell>35.4</cell><cell>23.2</cell><cell>35.1</cell></row><row><cell></cell><cell>FSLwF</cell><cell>43.9</cell><cell>29.9</cell><cell>29.5</cell><cell>34.9</cell></row><row><cell></cell><cell>OLTR</cell><cell>44.7</cell><cell>37.0</cell><cell>25.3</cell><cell>35.9</cell></row><row><cell>ResNet-152</cell><cell>OLTR+LFME cRT</cell><cell>39.3 42.0</cell><cell>39.6 37.6</cell><cell>24.2 24.9</cell><cell>36.2 36.7</cell></row><row><cell></cell><cell>? -normalized</cell><cell>37.8</cell><cell>40.7</cell><cell>31.8</cell><cell>37.9</cell></row><row><cell></cell><cell>LWS</cell><cell>40.6</cell><cell>39.1</cell><cell>28.6</cell><cell>37.6</cell></row><row><cell></cell><cell>cRT+mixup</cell><cell>44.1</cell><cell>38.5</cell><cell>27.1</cell><cell>38.1</cell></row><row><cell></cell><cell>LWS+mixup</cell><cell>41.7</cell><cell>41.3</cell><cell>33.1</cell><cell>39.7</cell></row><row><cell></cell><cell>MiSLAS</cell><cell>39.6</cell><cell>43.3</cell><cell>36.1</cell><cell>40.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Detailed accuracy results on Places-LT, starting from an ImageNet pre-trained ResNet-152.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><forename type="middle">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Remix: Rebalanced mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ping</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="449" />
		</imprint>
	</monogr>
	<note>Intelligent data analysis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Calibrating predictive model estimates to support personalized medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Osl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucila</forename><surname>Ohno-Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ima-geNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Calibrating deep neural networks using focal loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jishnu</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amartya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="15288" to="15299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">When does label smoothing help? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting good probabilities with supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13888" to="13899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1953" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Table 7: Comprehensive accuracy results on ImageNet-LT with different backbone networks</title>
		<imprint/>
	</monogr>
	<note>ResNet-50. ResNet-101</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
