<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONTRASTIVE UNSUPERVISED LEARNING FOR SPEECH EMOTION RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Levy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Rozgic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Papayiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago ? Amazon Alexa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONTRASTIVE UNSUPERVISED LEARNING FOR SPEECH EMOTION RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech emotion recognition</term>
					<term>Contrastive predictive coding</term>
					<term>Unsupervised pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech emotion recognition (SER) is a key technology to enable more natural human-machine communication. However, SER has long suffered from a lack of public large-scale labeled datasets. To circumvent this problem, we investigate how unsupervised representation learning on unlabeled datasets can benefit SER. We show that the contrastive predictive coding (CPC) method can learn salient representations from unlabeled datasets, which improves emotion recognition performance. In our experiments, this method achieved state-of-the-art concordance correlation coefficient (CCC) performance for all emotion primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on the MSP-Podcast dataset, our method obtained considerable performance improvements compared to baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech emotion recognition (SER) aims at discerning the emotional state of a speaker, thus enabling more human-like interactions between human and machines. An agent can understand the command of a human better if it is able to interpret the emotional state of the speaker as well. Moreover, a digital assistant can prove to be a human-like companion when equipped with the capability of recognizing emotions. These fascinating applications provide key motivations underpinning the fast growing research interest in this area <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> Despite the substantial interest from both academia and industry, SER has not found many real-world applications. One possible reason is the unsatisfactory performance of existing systems. The difficulty is caused by, and contributes to, relatively small public data sets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> in this domain. The lack of large scale emotion annotated data hinders the application of deep learning methods, from which many other speechrelated tasks (e.g automatic speech recognition <ref type="bibr" target="#b4">[5]</ref>) have benefited greatly.</p><p>In order to circumvent the data sparsity issue of SER, we investigate the use of unsupervised pre-training. Unsupervised pre-training techniques have received increased attention over the last few years. The research interest in this direction is well-motivated: while deep-learning (DL) based methods achieve state-of-the-art results across multiple domains, these methods tend to be data-intensive. Training a large and deep neutral network usually requires very large labeled datasets. The cost of data labeling has since become a major obstacle for applying DL techniques to real-world applications, and SER is no exception. Motivated by recent developments in unsupervised representation learning, we leverage an unsupervised pre-training approach for SER.</p><p>The proposed method shows great performance improvement on two widely used public benchmarks. The improvements on recognizing valence (positivity/negativity of the tone of voice) are particularly encouraging, as valence is known to be very hard to predict from speech data alone, see e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Furthermore, our analysis implies, even without explicit supervision in training, emotion clusters emerge in the embedding space of the pre-trained model, confirming the suitability of unsupervised pre-training for SER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Recent studies on unsupervised representation learning have achieved great success in natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and computer vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. While leveraging unsupervised learning for SER has been investigated relatively little, previous attempts using autoencoders have been successful <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. More recently, it has been shown that learning to predict future information in a time series is a useful pre-training mechanism <ref type="bibr" target="#b13">[14]</ref>.</p><p>Unsupervised methods based on contrastive learning have established strong and feasible baselines in many domains, recently. For instance, contrastive predictive coding (CPC) <ref type="bibr" target="#b10">[11]</ref> is able to extract useful representations from sequential data and achieves competitive performance on various tasks, including phone and speaker classification in speech. Our work relies on the use of a CPC network for learning acoustic representations from large unlabeled speech datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BACKGROUND</head><p>The primary goal of this study is to learn representations that encode emotional attributes shared across frames of speech audios without supervision. We start by reviewing relevant concepts in emotion representation, then we give a brief review of the contrastive predictive coding (CPC) method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Emotion representation</head><p>In general, there are two widely used approaches to represent emotion: by emotion categories (happiness, sadness, anger, etc.) or by dimensional emotion metrics (aka emotion primitives) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>. Albeit intuitive, the categories-based representation may miss the subtleties of emotion "strength", e.g. annoyance versus rage. The dimensional emotion metrics often include activation (aka arousal, very calm versus very active) , valence (level of positivity or negativity) and dominance (very weak versus very strong). In this work, we mainly focus on predicting dimensional emotion metrics from speech. Since emotion representation is an active research topic, we refer the interested readers to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive predictive coding</head><p>As the name suggests, CPC falls into the contrastive learning paradigm: positive example and negative examples are constructed, and the loss function encourages separation of positive from negative examples. We give a detailed description of CPC below.</p><p>For an audio sequence X = (x 1 , x 2 , ..., x n ), CPC uses a nonlinear encoder f to project observation x t ? R Dx to its latent representation z t = f (x t ), where z t ? R Dz . Then an autoregressive model g is adopted to aggregate t consecutive latent representations from the past into a contextual repre-</p><formula xml:id="formula_0">sentation c t = g(z ?t ), where c t ? R Dc .</formula><p>Since c t summarizes the past, it should be able to infer the latent representation z t+k of future observations x t+k from c t , for a small k. For this purpose, a prediction function h k for a specific k takes the context representation as the input to predict the future representation:</p><formula xml:id="formula_1">z t+k = h k (c t ) = h k (g(z ?t )).<label>(1)</label></formula><p>To form a contrastive learning problem, some negative samples (i.e. other observation x) are drawn, either from the same sequence or other sequences, and their latent representations (z) are computed. Assuming N ?1 negatives are randomly sampled for each context representation, then positive and negatives form a set of N samples that contains only one positive and N ? 1 negatives. To guide feature learning, the CPC method proposes to discriminate the positive from negatives, which boils down to an N-way classification problem. CPC uses the infoNCE loss function: for an audio segment and a time step t, the infoNCE loss is defined as</p><formula xml:id="formula_2">L = ? k m=1 log exp(? t+m z t+m )/? exp(? t+m z t+m )/? + N ?1 i=1 exp(? t+m z i )/? ,<label>(2)</label></formula><p>where ? is a scaling factor (a.k.a temperature) to control the concentration-level of the feature distribution, k is the upperbound on time extrapolation. Notice that the summation over i assumes that the randomly drawn negative samples are labeled as {1, ...N ? 1}, and these are different for each z t+m . In addition, the loss function considers all the future time extrapolation up to k. Clearly, the loss <ref type="formula" target="#formula_2">(2)</ref> is additive across different audio segments and time steps, hence in training, the loss <ref type="formula" target="#formula_2">(2)</ref> is usually computed for batches of audio segments and all possible time steps in these segments, to utilize the mini-batch-based Adam <ref type="bibr" target="#b16">[17]</ref> optimizer.</p><p>Optimizing <ref type="formula" target="#formula_2">(2)</ref> results in larger inner product between a latent representation and its predicted counterpart, than any of the negatives -mismatched latent representation and predictions. Theoretical justification for the optimization objective function <ref type="formula" target="#formula_2">(2)</ref> can be found in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED METHOD</head><p>The proposed method consists of two stages: pre-training a "feature extractor" model with CPC on a large un-labeled dataset, and training an emotion recognizer with features learned in the first stage. In this section, we introduce the emotion recognizer and training loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Attention-based emotion recognizer</head><p>The output of CPC is a sequence of encoded vectors C = {c 1 , c 2 , ..., c L }, C ? R L?Dc . To predict primitive emotions for a certain speech utterance, an utterance-level embedding is desired. Since certain parts of an utterance are often more emotionally salient than others, we adopt a self-attention mechanism to focus on these periods for utilizing relevant features. Specifically, a structured self-attention <ref type="bibr" target="#b18">[19]</ref> layer aggregates information from the output of CPC and produces a fixed-length vector u as the representation of the speech utterance.</p><p>Given C as input of the emotion recognizer, we follow <ref type="bibr" target="#b18">[19]</ref> to compute the scaled dot-product attention representation H as</p><formula xml:id="formula_3">H = softmax CW Q (CW K ) / D attn CW V<label>(3)</label></formula><p>where W Q , W K , and W V are trainable parameters, and all have shape D c ? D attn . The subscripts Q, K, V stand for query, key, and value, as defined in <ref type="bibr" target="#b18">[19]</ref>.</p><p>In order to learn an embedding from multiple aspects, we use a multi-headed mechanism to process the input multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed to get the final embedding U ? R Du .</p><formula xml:id="formula_4">H j = softmax W j Q C(W j K C) / D attn W j V C (4) U = Concat(H 1 , H 2 , ..., H n )W O<label>(5)</label></formula><p>where W O ? R nDattn?Du is another trainable weight matrix, and U ? R L?Du is the sequence representation after the multi-headed attention layer. Following the multi-headed attention layer, we compute the mean and standard deviation along the time dimension, and concatenate them as the sequence representation</p><formula xml:id="formula_5">u = [ mean (U ); std (U )]<label>(6)</label></formula><p>Subsequently, two dense layers with ReLU activation are used. We apply a dropout after these two dense layers with a small dropout probability. The final output layer is a dense layer with hidden units of the number of emotion attributes (e.g. three dimensions corresponding to activation, valence and dominance respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss function</head><p>Following <ref type="bibr" target="#b19">[20]</ref>, we build a loss function based on the concordance correlation coefficient (CCC, <ref type="bibr" target="#b20">[21]</ref>). For two random variables X and Y , the CCC is defined as</p><formula xml:id="formula_6">CCC(X, Y ) = ? 2? X ? Y ? 2 X + ? 2 Y + (? X ? ? Y ) 2 ,<label>(7)</label></formula><p>where ? = ? XY ? X ? Y is the Pearson correlation coefficient, and ? and ? are the mean and standard deviation, respectively. As can be seen from <ref type="formula" target="#formula_6">(7)</ref>, CCC measures alignment of two random variables. In our setting, model predictions and data labels assume the role of X and Y in <ref type="bibr" target="#b6">(7)</ref>.</p><p>Since the emotion recognizer predicts at the same time activation, valence and dominance, we use a loss function that combines CCC act , CCC val , CCC dom values for activation, valence, and dominance, respectively</p><formula xml:id="formula_7">L = 1 ? ?CCC act ? ?CCC val ? ?CCC dom<label>(8)</label></formula><p>We set the trade-off parameters ? = ? = ? = 1/3 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SPEECH CORPORA</head><p>For unsupervised pre-training, we train the CPC model on LibriSpeech dataset <ref type="bibr" target="#b21">[22]</ref>, which is a large scale corpus originally created for automatic speech recognition (ASR). It contains 1000 hours of English audiobook reading speech, sampled at 16kHz. In our experiment, due to computational limitations, we use an official subset "train-clean-100" containing 100 hours of clean speech for unsupervised pre-training. In this subset, 126 male and 125 female speaker were assigned to the training set. For each speaker, the amount of speech was limited to 25 minutes to avoid imbalances in per-speaker duration.</p><p>To evaluate the empirical emotion recognition performance, we perform experiments on the widely used MSP-Podcast dataset <ref type="bibr" target="#b3">[4]</ref> and IEMOCAP dataset <ref type="bibr" target="#b2">[3]</ref>. MSP-Podcast is a database of spontaneous emotional speech. In our work, we used version 1.6 of the corpus, which contains 50,362 utterances amounting to 84 hours of audio recordings. Each utterance contains a single speaker with duration between 2.75s and 11s. We follow the official partition of the dataset, which has 34,280, 5,958, and 10,124 utterances in the training, validation and test sets, respectively. The dataset provides scores for activation, valence and dominance, as well as categorical emotion labels.</p><p>IEMOCAP is a widely used corpus in SER research. It has audio-visual recordings from five male and five female actors. The actors were instructed to either improvise or act out certain specific emotions. The dataset contains 5,531 utterances grouped into 5 sessions, which amount to about 12 hours of audio. Similar to MSP-Podcast, this dataset provides categorical and dimensional emotion labels. In this work, we focus on predicting the dimensional emotion metrics from the speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Setups</head><p>Our experiments investigate four different setups: a). supervised only (Sup): As a simple baseline, an emotion recognizer was trained and tested on 40-dimensional log filterbank energies (LFBE) features of IEMOCAP and MSP-Podcast, respectively. LFBE features have been tested in a wide variety of applications. b). joint CPC + supervised (jointCPC): JointCPC trained CPC model and emotion recognizer in an end-to-end manner, where the CPC model aims to learn features from the raw audios directly, while the Sup setup uses hand-crafted features for the supervised task. We included this baseline to test whether it is possible to learn better features when the feature extraction part is aware of the downstream task. c). miniCPC: Compared with jointCPC, miniCPC trains the CPC model and the emotion recognizer in two separate stages on the same datasets. In this setup, we can verify whether CPC model can learn universal representations that can facilitate various downstream tasks. d). CPC pre-train + supervised (preCPC): We first pretrained a CPC model with a 100-hour subset of the Lib-riSpeech dataset. Then an attention-based emotion recognizer will be trained on features that were extracted from the learned CPC model with MSP-Podcast and IEMOCAP, re-spectively. Since the training corpus for CPC is much larger than the labeled datasets, we can test whether introducing a large out-of-domain dataset for unsupervised pretraining is useful.</p><p>For the CPC model used in the above settings, we use a four layer CNN with strides <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>, filter-sizes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref> and 128 hidden units with ReLU activations to encode the 16KHz audio waveform inputs. A unidirectional gated recurrent unit (GRU) network with 256 hidden dimensions is used as the autoregressive model. For each output of GRU, we predict 12 timesteps in the future using 50 negative samples, sampled from the same sequence, in each prediction. We train the CPC model with fixed length utterances of 10s duration. Longer utterances are cut at 10s, and shorter ones were padded by repeating themselves.</p><p>For the emotion recognizer, an 8-head attention layer with 512 dimensional hidden states is used. The outputs of attention layer have the same dimension of the inputs. The two fully-connected layers have 128 hidden units. The drop out probability is set to 0.2 for the dropout layers.</p><p>Our model was implemented in PyTorch and all methods were conducted on 8 GPUs each with a minibatch size of 8 examples for CPC pretraining. We use Adam optimizer with a weight decay of 0.00001 and a learning rate of 0.0002. We used 50 epochs for training and saved the model that perform best on validation set for testing.</p><p>To evaluate the IEMOCAP dataset, we configured 5-fold cross-validation to evaluate the model. All experiments were run five times to produce the means and standard deviations. <ref type="table" target="#tab_0">Table 1</ref> and 2 present the performance in terms of CCC for activation, valence and dominance on the IEMOCAP and MSP-Podcast corpora, respectively. As shown in these tables, on both datasets preCPC consistently outperforms other setups. preCPC achieves higher CCC values for all metrics than Sup, which implies that the representations learned by CPC are superior to hand-crafted features for speech emotion recognition task. Surprisingly, even pre-training the CPC model on a small dataset, miniCPC still performs better than jointCPC on both datasets. We hypothesize that this is because unsupervised pre-training learns universal representations that are less specialized towards solving a certain task. Hence, it produces representations with better generalization which might facilitate various downstream tasks. However, for the jointCPC method, a trade-off has to be made between emotion prediction capability and representation learning. Also notice that, preCPC outperforms miniCPC by a large margin. This confirms our intuition that exposing the model to more diverse acoustic conditions and speaker variations is beneficial for learning robust features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head><p>We also plot the representations extracted by CPC from IEMOCAP to examine how suitable these representations are   for emotion. For visualization purposes, we used the categorical emotion labels when making the figure. As can be seen from <ref type="figure" target="#fig_0">Figure 1</ref>, the CPC model representation is capable of separating sadness from anger to a good extent, even though it is trained without emotion labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>Our experiment results demonstrated that CPC can learn useful features from unlabeled speech corpora that benefit emotion recognition. We have also observed significant performance improvement on widely used public benchmarks under various experiments setups, compared to baseline methods. Further, we also present a visualization that confirms the discriminative nature, with respect to emotion classes, of the CPC-learned representations. So far we mainly conducted experiments on LibriSpeech for pre-training. In the future, it would be interesting to investigate the impact of other corpora for pre-training. In particular, corpora that have more varied and expressive emotions might yield representations that are even more relevant for SER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Visualization of the learned representations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CCC scores (mean/std) on the IEMOCAP dataset Sup .664 ? .007 .638 ? .017 .718 ? .004 .635 ? .009 jointCPC .562 ? .012 .549 ? .032 .642 ? .013 .491 ? .016 miniCPC .660 ? .005 .673 ? .028 .702 ? .009 .606 ? .019 preCPC .731 ? .003 .752 ? .014 .752 ? .009 .691 ? .009</figDesc><table><row><cell>Methods</cell><cell>CCC avg</cell><cell>CCC act</cell><cell>CCC val</cell><cell>CCC dom</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>CCC scores (mean/std) on the MSP-Podcast dataset Sup .458 ? .005 .596 ? .007 .266 ? .004 .501 ? .013 jointCPC .491 ? .008 .628 ? .006 .280 ? .006 .568 ? .007 miniCPC .549 ? .006 .688 ? .009 .345 ? .005 .615 ? .011 preCPC .571 ? .004 .706 ? .006 .377 ? .008 .639 ? .012</figDesc><table><row><cell>Methods</cell><cell>CCC avg</cell><cell>CCC act</cell><cell>CCC val</cell><cell>CCC dom</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Brueckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5200" to="5204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="90" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IEMOCAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Lotfian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="471" to="483" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AUTOMATIC SPEECH RECOGNITION</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting moods from pictures and sounds: Towards truly personalized TV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpreting ambiguous emotional expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning approach to feature analysis for automatic speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Sefik Emre Eskimez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heinzelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5099" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introducing shared-hidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4818" to="4822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with future observation prediction for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13806</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing the emotional states that are expressed in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randolph R Cornelius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The ordinal nature of emotions: An emerging approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="2196" to="2202" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A concordance correlation coefficient to evaluate reproducibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Kuei</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
