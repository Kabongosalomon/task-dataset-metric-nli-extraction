<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Streaming Perception using Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Nambi</surname></persName>
							<email>akshayn@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Singh</surname></persName>
							<email>t-adsingh@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Yvs</surname></persName>
							<email>t-harishyvs@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanuja</forename><surname>Ganu</surname></persName>
							<email>taganu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Streaming Perception using Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Executing computer vision models on streaming visual data, or streaming perception is an emerging problem, with applications in self-driving, embodied agents, and augmented/virtual reality. The development of such systems is largely governed by the accuracy and latency of the processing pipeline. While past work has proposed numerous approximate execution frameworks, their decision functions solely focus on optimizing latency, accuracy, or energy, etc. This results in sub-optimum decisions, affecting the overall system performance. We argue that the streaming perception systems should holistically maximize the overall system performance (i.e., considering both accuracy and latency simultaneously). To this end, we describe a new approach based on deep reinforcement learning to learn these tradeoffs at runtime for streaming perception. This tradeoff optimization is formulated as a novel deep contextual bandit problem and we design a new reward function that holistically integrates latency and accuracy into a single metric. We show that our agent can learn a competitive policy across multiple decision dimensions, which outperforms state-of-the-art policies on public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasing number of scenarios have started relying on executing computer vision tasks, viz., classification or detection or segmentation on streaming visual data (or streaming perception) <ref type="bibr">[4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20]</ref>. For practical applications, such as self-driving vehicles or augmented and virtual reality (AR/VR), it is critical for the processing pipeline to be both fast and accurate, thus maximizing overall streaming performance, both accuracy and latency.</p><p>Past works has explored numerous latency sensitive approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref> and approximate execution frameworks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> that either adheres to strict latency requirements, or resource constraints, or an accuracy target, resulting in sub-optimum decisions. These approaches have three critical drawbacks for streaming perception tasks: (D1) The cost associated to figure out the different tradeoffs will explode combinatorially as the number of choices increase. For example, in streaming detection if the decision function has to choose the appropriate input resolution {360, 480, 560, 640, 720}, number of proposals {100, 300, 500, 1000}, tracker resolution {360, 480, 560, 640, 720} and stride {3, 5, 10, 15, 30}, then the design space will have over 500 combinations, which is non-trivial to select (See Section 3). (D2) The decision function do not consider content characteristics of the streaming data at run time. Content-aware approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b37">38]</ref> adapt the configurations or switch between models based on the content characteristics, e.g., complex frames are passed to expensive deeper/wider models. However, classifying a frame as "simple" or "complex" is challenging and identifying the right metrics to extract the characteristics is non-trivial (See Section 4.1). (D3) The decision function is not linked to overall accuracy and latency performance of the system. Current execution pipelines do not jointly optimize accuracy and latency for online real-time tasks. Streaming perception is significantly more challenging than offline perception <ref type="bibr" target="#b19">[20]</ref>, necessitating design of new reward functions that accommodates both accuracy and latency (See Section 4.2). In this paper, we propose a new approach based on deep reinforcement learning (RL) to learn various system tradeoffs at runtime for streaming perception. Our approach consists of three major components: First, we derive various informative metrics from images, which can be utilized to guide runtime tradeoff decisions; Second, our RL agent uses these metrics to decide among various tradeoff options, say, the input resolution or the model to use, such that it jointly optimizes both the accuracy and latency using a new reward function; and Third, the execution policy learned by our RL agent is both dynamic, i.e., changes configuration on the fly and the agent is trained online, i.e., considers accuracy and latency trade-offs in the online real-time setting (See <ref type="figure" target="#fig_0">Figure 1</ref>). We formulate this as a novel deep contextual bandit problem and our rationale for using contextual bandits stems from the fact that they are simple to train, effective and can easily fit within the latency budget of streaming perception tasks. Through extensive analysis we show that our approach learns an optimal "sweet spot" in the accuracy versus latency trade-off that uniquely maximizes streaming performance. We empirically show that the policy selected by our RL approach has superior overall performance than state-of-the-art approaches. Further, our RL approach is complementary to the current line of work in computer vision literature, where it can support better backbone CNN's, faster end-to-end models to further improve performance.</p><p>Contributions. Our main contributions are: <ref type="bibr" target="#b0">(1)</ref> We introduce a novel approach for learning various system tradeoffs to jointly optimize accuracy and latency. <ref type="bibr" target="#b1">(2)</ref> We design a new reward function that holistically integrates latency and accuracy into a single metric for streaming perception. <ref type="bibr" target="#b2">(3)</ref> We present a principled way to train our RL agent in an online real-time setting, which can be applied to other streaming tasks. <ref type="bibr">(4)</ref> We conduct extensive analysis to show that selecting runtime tradeoffs is non-trivial and the policy selected by our approach significantly improves streaming performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Latency Sensitive Perception. Two major lines of work have emerged in improving the execution latency. The first approach try to reduce the runtime of the backbone CNN's <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, coupled with techniques like quantization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> and pruning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>. The other approach focuses on designing end-to-end architectures and algorithms that are inherently faster <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref>. Further, accuracy and latency trade-off curves have been previously studied for different algorithms and configurations <ref type="bibr" target="#b16">[17]</ref>. However, evaluating accuracy and latency in tandem is difficult. Li et al. <ref type="bibr" target="#b19">[20]</ref> propose a new evaluation metric, sAP <ref type="bibr" target="#b1">[2]</ref>, to evaluate accuracy and latency together by evaluating the result while enforcing a real-time constraint on them. Mao et al. <ref type="bibr" target="#b25">[26]</ref> also propose a detection delay metric, however, their evaluation setup is offline and not generalizable.</p><p>Approximate Execution Frameworks. Many approximate execution frameworks have been designed to execute perception pipelines. They describe runtime decisions such as changing input resolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>, switching between models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>, changing model configurations (like number of object proposals) <ref type="bibr" target="#b39">[40]</ref>, selecting the right subset of frames to execute expensive models vs cheap trackers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">4]</ref>, offloading expensive model calls to a cloud resource <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>. However, the decision function is not linked to overall accuracy and latency performance, and neither is influenced by the content characteristics in the frame. Instead, they either minimize energy <ref type="bibr">[4]</ref>, contention from other running processes <ref type="bibr" target="#b39">[40]</ref>, or optimize solely the available latency budget <ref type="bibr" target="#b12">[13]</ref>. This results in choosing sub-optimal decisions at the expense of the trade-off being optimized, affecting overall performance.</p><p>Reinforcement Learning for Streaming Applications Intelligent planning systems that are reactive <ref type="bibr">[12]</ref> and flexible <ref type="bibr" target="#b14">[15]</ref> in bounded settings have been extensively studied, along with classical approaches to plan resource utilization and scheduling <ref type="bibr" target="#b5">[6]</ref>. One of the key challenges for existing approximate/flexible execution frameworks, has been the difficulty in designing reward functions that optimize both accuracy and latency together. RL models are known to be notoriously difficult to train without appropriately designed reward functions and reward shaping is often employed <ref type="bibr" target="#b27">[28]</ref>. Even then concerns of "reward hacking" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> remain, which results in models learning undesirable behaviours and safety concerns.  Runtime tradeoffs have many aspects of considerations -the task domain, tradeoff characteristics and the hardware the perception system runs on. Here, we'll briefly discuss these aspects. <ref type="table" target="#tab_0">Table 1</ref> shows the tradeoff between different scales (input resolution) and number of proposals for Faster R-CNN model on ImageNet-VID <ref type="bibr" target="#b32">[33]</ref> and Argoverse-HD <ref type="bibr" target="#b7">[8]</ref> datasets. The sweet spot between accuracy and latency for ImageNet-VID appears to be at 100 proposals, however, for scale it is not clearly evident as scale=360 results in 37% improvement in latency, but with 1.6 point reduction in AP when compared to scale=480. Further, the same tradeoffs for the Argoverse-HD dataset, have a sweet spot at 300 proposals and scale=640. This is likely due to the difference in the domain, as ImageNet-VID has fewer number of objects in average compared to Argoverse-HD. Thus, identifying the optimal configurations at runtime is non-trivial. <ref type="figure">Figure 2</ref> shows that there exist numerous tradeoff selections between (detector scale, tracker scale, tracking stride), which can satisfy an average latency budget of 33 ms (represented by the blue vertical line). One could pick any configuration in this cluster (marked by the red circle), and a sub-optimal choice can easily result in atleast 3 point AP reduction when compared with the best configuration. Furthermore, the choice of the model itself is another tradeoff, as different models exhibit different kinds of errors <ref type="bibr" target="#b6">[7]</ref> which can be a factor of choice in different application domains (See <ref type="table" target="#tab_1">Table 2</ref>). For example, at scale 320 YoloV3 is 1.5X faster, but with 10.8AP reduction when compared with FCOS.</p><p>To further aggravate this problem, the underlying hardware can also impact the optimal configuration. For instance, in our experiments on Argoverse-HD using a P40 GPU, the best scale turns out to be 600 instead of 900 as reported in <ref type="bibr" target="#b19">[20]</ref>, which was benchmarked with a 1080Ti GPU. Appendix A presents extensive results for various runtime tradeoffs across datasets, models and configurations.</p><p>The design space for these tradeoffs explodes combinatorially and is difficult to understand the impact of one tradeoff versus another and non-trivial to select the optimum configuration. This motivates the need for an agent that adaptively learns these tradeoffs at runtime with minimal compute overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In this section, we first present the metrics derived from our streaming frames, their utility, and the overheads this computation introduces. Then, we describe our reward function that is used to train the RL agent, and how it balances both accuracy and latency considerations. Finally, we describe the details of our RL agent along with the training and testing procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extracting Frame-level Metrics</head><p>Our goal is to derive various metrics from streaming data, which can be utilized to guide runtime tradeoff decisions. Thus, the key criterion while designing these metrics are -(1) The metrics should be informative in nature and should reflect the content characteristics of the image. (2) The latency overhead of obtaining these metrics should be low, otherwise the cost of obtaining them would counter any benefits of making runtime tradeoffs.</p><p>Key Idea. We posit the development of "tiny" regressors and classifiers which are trained to predict informative metrics in a supervised fashion. The metrics themselves capture some aspect of the runtime tradeoff, say changing the input resolution, or switching from one model to another. These regressors/classifiers are composed of a few layers operating on the output features of the backbone CNN's used by the models. This reduces the overheads of running these models drastically and easily fits in our latency budget. In this paper, we instantiate our problem statement to focus on runtime tradeoffs in video object detection and present metrics to that end.</p><p>1. Switchability. The complexity of the DNN models can vary depending on how deep or wide these models are, resulting in different accuracy-latency tradeoffs. The switchability metric aims to capture the differences between the different models that we can switch between during execution. To obtain this metric, we execute all the supported models F on image I to obtain detections {x f1 , x f2 ..x f k }, where x fi is the output obtained from model f i . We then compute the standard deviation of the mean IOU across all the detectors in F on I and assign the switchability category to either, low, medium, or high. For example, a low switchability category indicates that switching is not particularly useful, where both expensive/inexpensive models have similar performance on this image. Whereas, a high switchability category indicates switching the model will result in better accuracy, as some of the models in F have superior performance than others. Finally, we train a classifier in a supervised manner, which takes the output features of the backbone CNN as input (i.e., features from the last convolutional layer) and predicts the switchability category class.</p><p>2. Adaptive Scale. We use the scale metric from <ref type="bibr" target="#b10">[11]</ref>, which attempts to predict the optimum scale (input resolution) an image should be executed at, such that both accuracy and latency are improved. First, we determine which scale is better for a given image by comparing foreground box predictions at different scales (e.g., in our case s={720,600,480,360,240}). The scale at which the common foreground objects have the minimum loss is considered the optimal scale <ref type="bibr" target="#b10">[11]</ref>. Now that we have scale labels for each image, we train a regressor in a supervised manner to predict the "optimal" scale.</p><p>3. Scene aggregates. Apart from the above, we also compute few frame-level aggregates as described next: (i) Confidence Aggregates: We compute the mean and the standard deviation of confidence value associated with the predictions. The intuition here is that if the average confidence of the prediction is low for a scene, then it might necessitate the execution of a more "expensive" configuration. (ii) Category Aggregates: We compute number of instances for each category in the predictions, i.e., we provide a C * 1 vector with counts of objects of each category. This would help capture the kind of objects present in a given scene, say a rare object category might require additional computational resources to be localized better. (iii) Object Size Aggregates: We also compute the areas of each detection and bucket them as small, medium and large detections (same as COCO thresholds <ref type="bibr" target="#b23">[24]</ref>), and return the aggregate vector. This is useful as small objects are considered more difficult to detect than larger ones. (iv) Crop Extents: We divide the image into patches, and mark patches that don't overlap with any predicted detection, and compute the convex hull of these patches. These crop extents indicate which regions can be cropped out, improving latency with minimal loss of accuracy. Please see Appendix B for more details on these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Designing the Reward Function for Adaptive Streaming Perception</head><p>Existing execution frameworks either tries to reduce FLOPS of backbone networks or runtime of the models, mostly in an offline setup on archived data. However, for streaming perception, such approaches fail to capture the fact that when the algorithm/model finishes processing, the surrounding world has already changed. In this work, we design an online reward function, which optimizes both accuracy and latency requirements. Intuitively, our function compares the model output with a ground truth stream of the actual world state in an online fashion to generate the rewards.</p><p>The data stream is modeled as a set of sensor observations, ground truth world states and timestamps, denoted by</p><formula xml:id="formula_0">{(x i , y i , t i )} T i=1 .</formula><p>Let f be the streaming algorithm, and our agent is following the policy ? to configure the algorithm. Till time t, the algorithm f is provided with observations and timestamps, viz., {(x i , t i )|t i &lt; t}. The algorithm f generates an output prediction at any time, say {(? j , s j )} N j=1 , where s j is the timestamp when a particular prediction? j is produced and? j = f ? (x j ) with j indexing over the N outputs generated by f over the entire stream. Note, this output stream is not synchronised with the input stream and is not operating necessarily on every observed input. Further, the real time constraint is enforced by comparing the most recent output (?(t)) at time t i to the ground truth y i , ?(t) = argmax j s j &lt; t. This is equivalent to applying a zero-order hold for the algorithm's outputs to produce continuous predictions of world states. This is akin to the general setting described in <ref type="bibr" target="#b19">[20]</ref>, and is applicable for generic streaming scenarios. Now, let's consider that, our agent takes an action a 1 at t a1 to change configuration of algorithm f and the next action a 2 is taken at t a2 , now we define the reward function that our agent receives for <ref type="figure" target="#fig_9">Figure 3</ref>: (a) and (b) shows the distribution of R obtained by policy ? and ? f ixed resp., for sequences A and B. In (c), R f ixed_adv removes the intrinsic reward bias of the two sequences. taking action a 1 as (where L is an arbitrary single frame loss),</p><formula xml:id="formula_1">R(ta 1 , ta 2 ) = L({y k ,? ?(t k ) } ta 2 k=ta 1 )<label>(1)</label></formula><p>Intuitively, the reward for action a 1 is the accuracy obtained for a stream sequence segment until a new action a 2 is taken, while the zero-order hold ensures that only the results satisfying the latency constraints are accounted for. Thus, our reward function R ensures that the reward received by the agent considers both real time constraints and the accuracy of the algorithm f . It should be noted that R is a generalization of L streaming <ref type="bibr" target="#b19">[20]</ref> (instantiated as sAP metric <ref type="bibr" target="#b1">[2]</ref>), where R(0, T ) = L streaming . However, sAP is not a good reward function, as R(0, T ) would indicate the cumulative benefit of all the runtime decisions taken in a sequence, such a sparse reward design would make training the agent difficult and sample-inefficient. Our general construction of the reward function R does not assume any properties of the algorithm f or any properties of L(y,?) and it could be applied to various streaming perception tasks such as object detection or segmentation.</p><p>Different sequences and scenes may exhibit different levels of accuracy depending on inherent scene characteristics. In some cases, for intrinsically difficult sequences the agent might select the best configuration and yet receive lower reward, and in some other sequences selecting a sub-optimal configuration may receive a higher reward. For example, a video sequence with one large object that can be easily detected would have much higher average rewards than another sequence with many overlapping and far-away objects. This inconsistency in reward distributions can lead to undesirable behaviour <ref type="bibr" target="#b2">[3]</ref> in the agent. To normalize for this factor, we consider a fixed policy ? f ixed , i.e., a fixed configuration of algorithm f , and devise an alternative reward (R f ixed_adv ) as follows,</p><formula xml:id="formula_2">R f ixed_adv (ta 1 , ta 2 ) = ?(R(ta 1 , ta 2 ) ? R? f ixed (ta 1 , ta 2 ))<label>(2)</label></formula><p>where R ? f ixed is the reward obtained by executing algorithm f following the fixed policy and ? is an appropriate scale factor. Practically, algorithm f employing fixed policy ? f ixed is first executed on our training set once, the outputs and the timestamps are stored and then we load them for each sequence while training our agent to generate the reward, i.e., R f ixed_adv . As we can see from <ref type="figure" target="#fig_9">Figure 3</ref>(c), the new reward function normalizes the differences between two sequences. Subtracting the reward obtained from a fixed policy eliminates the intrinsic reward bias of the original reward function in a sequence, and provides us the advantage of our policy over the fixed policy. In Section 5, we show that one can select any fixed policy configuration (? f ixed ) of algorithm f and the policy learned is resilient to variations of the selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scheduling with Runtime Trade-Offs</head><p>Temporal aliasing, i.e., the mismatch between the output and the input streams, reduces performance and selecting which frames to process or ignore is very critical to achieve good streaming performance. It is shown that idle-free scheduling (i.e., processing the next available frame immediately when the resource is "free") is outperformed by a shrinking tail scheduling <ref type="bibr" target="#b19">[20]</ref>, where we wait for the next frame if there is a reduction in mismatch. However, this relies on the assumption that the run time r of the algorithm f is constant. This is reasonable as runtime distributions are peaky and unimodal <ref type="bibr" target="#b19">[20]</ref>.</p><p>In our scenario the algorithm's configuration changes at runtime, and hence previous assumption does not hold true. Recent work <ref type="bibr" target="#b19">[20]</ref> propose that the frame selection (or scheduling) problem can instead be modeled as a decision policy under a Markov Decision Process (MDP). We can compute the expected future cumulative mismatch for a policy under stochastic runtimes r. However, our decision space of configurations is discrete, and thus, the runtime of every configuration of the algorithm can be assumed to be constant. Furthermore, we do not change the algorithm configuration often due to temporal consistency (typically, every 80-120 frames). Thus, a fragment of the sequence where a particular configuration is used can be considered as a scenario where shrinking tail policy using that configuration's runtime will still hold. Thus, we modify the shrinking tail policy to incorporate this assumption (See Appendix C). However, a limitation of this approach is that it works only when the decision space considered is discrete and not continuous. Unpack K length sequence {(z 1 , a 1 , t 1 ) ? ? ? (z K , a K , t K )} from stream replay buffer S; for n = 1, ? ? ? , K do r n = R(t n , t n+1 ) ; Store (z n , a n , r n ) in agent's replay buffer B; ? = update(?) ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update Policy</head><p>Function update(?: Policy): // Optimize the Q function</p><p>Sample N tuples {(z 1 , a 1 , r 1 ), ? ? ? , (z N , a N , r N } uniformly from buffer B; for n = 1, ? ? ? , N do // Set the targets y n = r n // Calculate the loss for the batch</p><formula xml:id="formula_3">L = 1 N N n=1 1 |D| d?a n [y n ? Q ? (z n , d)] 2</formula><p>Use optimizer to optimize ? to minimize L;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Exploring and Exploiting Trade-Offs Using Deep RL</head><p>Till now, we described the metrics obtained from a frame, the reward function used to train our agent, and the scheduling considerations to choose which frames to process. We shall now describe how to train and test our Deep RL agent to make configuration decisions in a streaming scenario.</p><p>We formulate our problem as a novel deep contextual bandit problem, where a sequence of independent trials are conducted and in every trial the agent chooses an action from the set of possible actions along with the side-information provided, formally known as context. After every trial the agent receives a reward value as feedback for the chosen action and updates its policy accordingly. Thus, for our M -dimensional action space and m ? discrete sub-actions for each dimension ?, using the conventional discrete-action algorithms, a total of M ?=1 m ? possible actions need to be considered. Efficiently exploring such large action spaces is difficult, rendering the application of discrete-action reinforcement learning algorithms intractable. Thus, our agent independently considers the decision for each sub-action, while operating on a shared representation for action-value predictions, similar to action branching architectures <ref type="bibr" target="#b35">[36]</ref>, thus, significantly reducing the number of action-values to be predicted from M ?=1 m ? to M ?=1 m ? . We consider the metrics obtained from a frame as z, where z = M etrics(x) and x is the frame, and M etrics is the function that generates these metrics. z is considered as the side-information or context provided to the agent. Since our context space is continuous, we employ a deep neural network (specifically, a multi-layer perceptron) for predicting the action values Q ? (z, a i ) for each sub-action a i along with given context z, and ? is the set of learnable parameters. To learn these parameters, we employ a loss function that takes into account all the decision points. The loss L is calculated by sampling batch of {context, action and reward} tuples, i.e., (z, a, r) from the agent's replay buffer</p><formula xml:id="formula_4">B, L = E (z,a,r)?B 1 |D| ai?a [r ? Q ? (z, a i )] 2 .</formula><p>To perform the exploration-exploitation search during training, we employ instances of two popular approaches (1) Upper Confidence Bound (UCB) and (2) Epsilon Greedy <ref type="bibr" target="#b33">[34]</ref>.</p><p>1. UCB. This approach is inspired by UCB based exploration utilizing deep neural networks for action value estimation (like NeuralUCB <ref type="bibr" target="#b41">[42]</ref>). Action is chosen using the following equation, <ref type="figure">d)</ref> is the predicted action value of the action d at step ? (Note: step is the number of trials; different from time t) when the context is z ? . N ? (d) denotes the number of times that action d has been selected prior to step ? , and the parameter c &gt; 0 determines the confidence level.</p><formula xml:id="formula_5">a ? = argmax d?Di Q ? (z ? , d) + c ln(? ) N? (d) ?D i ? D where Q ? (z ? ,</formula><p>2. Epsilon-Greedy. Our second approach is epsilon greedy, where the agent will randomly select an action with probability . Otherwise, it will select the action with the maximum action-value. Initially, we set as 1, and decay it using a factor till it reaches a minimum threshold. The advantage of the epsilon greedy approach is that the exploration does not depend upon the magnitude of reward. While testing our learned policies, we greedily act upon the actions which provide the maximum expected reward given an input context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practical Challenges.</head><p>A key practical challenge while training the agent is to factor in average latency for generating the context from the observations, computing the agent's action during training, and the agent's training procedure as these would disrupt and change the system performance. While training, we randomize the agent's call to take an action with probability p. When action is to be taken, the agent receives z (context generated from the corresponding input observation x) and emits action a. We then store the tuple (z, a, t) in the stream replay buffer S (See Algorithm 1). Calculating the reward and updating the policy on the fly is computationally expensive, instead merely storing these tuples adds negligible latency. Once the policy ? has been applied on a sequence, we then re-construct the rewards from the stored tuples in the buffer S and store (z, a, r) tuples in agent replay buffer B. We then update the Q ? function by sampling the tuples from B. Finally, we compute the loss L and update the learnable parameters ?. Algorithm 2 presents training and policy update details. Link to our training and evaluation code will be updated shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>In this section, we instantiate our RL approach on optimizing runtime tradoffs for video object detection and compare with state of the art approaches.</p><p>Datasets. We perform our experiments on the Argoverse-HD dataset <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>, which contains diverse urban outdoor driving scenes from two US cities and ImageNet-VID dataset <ref type="bibr" target="#b32">[33]</ref>, which is a diverse dataset with 30 object categories.</p><p>Performance Measure. We evaluate using sAP <ref type="bibr" target="#b19">[20]</ref> measure (using their codebase <ref type="bibr" target="#b1">[2]</ref>), i.e., AP in the online real-time setting, as it simultaneously evaluates both latency and average precision of the detections. This is referred as AP in prior work <ref type="bibr" target="#b19">[20]</ref> and for simplicity we follow that convention.</p><p>Implementation Details. We use the mmdetection <ref type="bibr" target="#b8">[9]</ref> library to train and finetune our models, viz., (i) Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> with Feature Pyramid Networks <ref type="bibr" target="#b21">[22]</ref> (FPN) and ResNet-50 backbone, (ii) FCOS <ref type="bibr" target="#b36">[37]</ref> with ResNet-50 backbone and (iii) YOLOv3 <ref type="bibr" target="#b30">[31]</ref> with DarkNet-53 backbone. Further, while employing multiscale training on scales between [720, 320], we follow the 1x schedule of mmdetection and default parameters for all of them to maintain consistency. We finetune our models from a base model trained on COCO <ref type="bibr" target="#b23">[24]</ref> for the above datasets using the following protocol -for ImageNet-VID, we use a mix of images from ImageNet-DET and VID in 2:1 ratio as described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> and for Argoverse-HD we consider every 5 th frame from the provided training sequences as our training set. Our agent is trained on the training set of these datasets by considering all the video sequences from this set itself. All our evaluation is performed on the validation sets of ImageNet-VID and Argoverse-HD, as ground truth annotations for test set are not available. Our evaluation set has no overlapping sequences from training set, and during evaluation, our agent does not perform any exploration, and greedily considers the action with the maximum predicted action values. Unless stated otherwise, all our experiments are performed on Microsoft Azure's ND-series machines <ref type="bibr" target="#b0">[1]</ref>, having Intel Xeon E5-2690 v4 and Tesla P40's, specifically, ND24s with 4 P40 GPUs for training models and ND6 with 1 P40 GPU for evaluation.</p><p>Context Generation. We extract context (informative metrics) from a frame using "tiny" regressors and classifiers (see Section 4.1). To train these, we follow a CNN architecture similar to the one proposed in <ref type="bibr" target="#b10">[11]</ref>, except that we employ weight sharing to operate on outputs corresponding to the levels in deep features obtained from FPN. See Appendix D for performance of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL training.</head><p>For training our RL model, we employ a 4 layer MLP architecture implemented in Pytorch <ref type="bibr" target="#b28">[29]</ref>, with the hidden layer size of 256, input size being equal to the size of our context vector (i.e., 22 for Argoverse and 44 for ImageNet-VID) and output size being the sum of each action dimension, i.e. i |D i |. Depending on the exploration/exploitation strategy employed, the hyperparameter for UCB algorithm c is set at 1.5, while for epsilon-greedy algorithm, we set initial  <ref type="bibr" target="#b10">[11]</ref> (trained offline in a supervised manner) and without any scheduling (i.e. running the detector as soon as GPU is free). 4 Dynamic-Online policy (Ours): As described, the learned execution policy by our RL agent is both dynamic in nature, i.e., changes configuration on the fly and the agent is trained online, i.e., considers accuracy and latency trade-offs in the real-time setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In this section, we discuss our results on the Argoverse-HD dataset and refer to Appendix E for the results on ImageNet VID dataset. <ref type="table" target="#tab_3">Table 3</ref> shows the quantitative comparison results of our approach in comparison with SOTA (for detection and proposal configurations with Faster R-CNN). We can see that our approach obtains the best average AP scores when compared with all the other approaches.</p><p>1. Static Policy vs. Ours. We firstly compare with the meta-detector Streamer <ref type="bibr" target="#b19">[20]</ref>, which combines detector and forecasting for a detector at a given scale and we show the results with the best configurations they have provided. We can see that our dynamic-online policy with reward R 1 and R 2 (rows <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9)</ref> outperforms static policies with different scales (rows 1,2) by 3.1AP . This shows that our policy is able to switch based on the image content, utilizing a lower latency configuration wherever possible to improve accuracy. Note that our policy performs significantly better across nearly all AP thresholds and subsets, viz., AP 50 , AP 75 denote AP with IoU (Intersection over Union) thresholds at 0.5 and 0.75 respectively, and AP L , AP M and AP S denote AP for large, medium and small objects respectively.</p><p>2. Static Expert Policy vs. Ours. As described earlier, it is difficult to select the best possible configuration even for a expert user as the design space explodes combinatorially, and it's computationally expensive to brute force through all the possible combinations. However, we do tune Streamer's configuration by handpicking and selecting the proposals as 300 and 500, as they are known to be sweet-spots based on extensive empirical experiments (See <ref type="table" target="#tab_0">Table 1</ref>). In contrast, our policy can adpatively pick the best configurations at runtime to optimize both latency and accuracy and outperforms this static expert policy. <ref type="figure" target="#fig_2">Figure 4</ref> shows the configurations picked by our policy on the test data from Argoverse-HD, which optimizes both accuracy and latency.</p><p>3. Dynamic-Offline vs. Ours. AdaScale <ref type="bibr" target="#b10">[11]</ref> adaptively selects the input scale based on the image content. Our approach significantly improves the overall performance by 7.9AP when compared with dynamic offline policy of AdaScale (rows 5,6). This is mainly because, the dynamic-offline policies are trained offline with a focus on achieving minimum resolution (and thus minimizing single frame latency), and thus not optimizing both accuracy and latency together.     4. Our policy performance using Reward R 1 and R 2 . R 1 (see Equation 1) balances both accuracy and latency constraints for the action taken by our agent. Reward R 2 (see Equation 2) normalizes the reward with the help of a arbitrary fixed policy. We can see that our policy with R 2 performs better than R 1 , as the intrinsic sequence bias in the generated rewards is removed (See <ref type="figure" target="#fig_9">Figure 3(c)</ref>).</p><p>Resilience of our approach. <ref type="table" target="#tab_4">Table 4</ref> shows that our agent is resilient to the choice of the initial fixed policy ? f ixed used in our reward function R 2 . Our agent is able to learn a competitive policy even with the choice of a demonstrably bad fixed policy, (360, 1000), which has a very low input resolution and the maximum number of proposals. Similarly, from <ref type="table" target="#tab_5">Table 5</ref> our agent is resilient to the choice of action space as changing scale space from s 1 to s 2 yielded competitive overall performance. This means that as long as the action spaces are defined reasonably well, our agent will learn a near-optimal execution policy.</p><p>Impact of adding a tracker. <ref type="table" target="#tab_6">Table 6</ref> shows the impact of adding a tracker (i.e., Tracktor <ref type="bibr" target="#b4">[5]</ref>) to static policy and ours. The introduction of tracker (supports configuration of tracker scale and stride, i.e. number of times it's executed versus the detector) increases the design space to 500 configurations and is not feasible to select the best static policy. Our agent is able to learn a competitive policy that outperforms the known static policy by 1.6 AP. It should be noted that even if we execute every combination for just 1 epoch on training set and evaluate to decide on the best configuration, that would take us 500 epochs to execute, whereas our approach learned this policy in just 5 epochs.</p><p>Impact of model switching. Similarly, from <ref type="table" target="#tab_7">Table 7</ref>, we can see that the RL agent is able to learn the near-optimal configuration (model, scale, proposals), and switch to the better performing model on the fly and is able to improve upon the individual models by figuring out a reasonable configuration. Appendix F presents extensive results for Argoverse-HD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Takeaways and discussion</head><p>To sum up, our learned agent is able to outperform a variety of execution policies as it learns good execution policies for a variety of tradeoffs, while simultaneously accounting for both accuracy and latency. Moreoever, our reward functions are robust and resilient in nature, and can handle changes in the defined action space and arbitrary fixed policies to learn good execution policies.</p><p>Extending our approach to heterogeneous perception systems with many models and algorithms running in conjunction would be extremely useful, wherein the combinatorial explosion of configurations would be intractable for manual optimization. Furthermore, our approach can be extended to jointly optimize/consider other parameters (like energy efficiency etc.) and the generality of our approach suggests that a wide variety of streaming perception applications can be benefited from this. However, due to safety considerations of certain downstream applications, it's not advisable to use this framework in scenarios where strong performance guarantees are required. Finally, our approach is data driven, and the performance may be affected by the biases in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present a deep reinforcement learning based approach to optimize run-time tradeoff of streaming perception systems in a dynamic and online fashion. Our approach is conceptually simple, elegant and picks the right decision points, despite combinatorial explosion in design choices, while simultaneously optimizing both accuracy and latency in online streaming perception systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Additional Details: Runtime Tradeoffs</head><p>In this section, we present additional results wrt runtime tradeoffs as discussed in Section 3. <ref type="figure" target="#fig_3">Figure 5</ref> shows that there exist numerous tradeoff selections between (detector scale, tracker scale, tracking stride), which can satisfy an average latency budget of 33 ms (represented by the blue vertical line).</p><p>Here we use FCOS <ref type="bibr" target="#b36">[37]</ref> detector and Tracktor <ref type="bibr" target="#b4">[5]</ref> tracker on Argoverse-HD dataset. Model decisions are motivated by the analysis that different models exhibit different error characteristics, along with the variations in their accuracy-latency tradeoffs. We use TIDE <ref type="bibr" target="#b6">[7]</ref>, a toolbox that helps distinguishes between six error types in object detection (Cls: classification error; Loc: localization error; Both: both cls and loc error; Dupe: duplicate predictions error; Bkg: background error; Miss: missed detections error) to help us in our analysis. For every error type, the error difference is independently calculated (i.e. computes the ?-mAP, or change in mAP, if an error type was "fixed" by an oracle), and thus provides us relative breakdowns that can be compared. <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref> show the error analysis for two popular datasets, COCO <ref type="bibr" target="#b23">[24]</ref> and ImageNet-VID <ref type="bibr" target="#b32">[33]</ref>. The error breakdowns vary drastically between two datasets. As we note, YOLOv3 has a higher miss rate than the two other two models on both the datasets. Whereas, Faster R-CNN has a higher background error ratio compared to the other two models, which is concurrent to earlier observations <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details: Frame-level Metrics</head><p>We now present additional results for extracting frame-level metrics described in Section 4.1.</p><p>Switchability. <ref type="figure">Figures 8, 9</ref>, and 10 shows object detection results using three detectors, viz., frcnn, fcos and yolo for a frame in ImageNet-VID dataset. Based on our analysis, the sample frame in <ref type="figure">Figure 8</ref> and 9 has low standard deviation score indicating switching between models will not improve performance. This can be clearly seen in the figure, where all three detectors have similar detections for these frames.</p><p>However, sample frame in <ref type="figure" target="#fig_0">Figure 10</ref> has high standard deviation score indicating switching the model will result in better accuracy. We can see that the detections by the cheaper model (yolo) has much sparser detections as compared to expensive detectors such as fcos and faster rcnn.  <ref type="figure" target="#fig_0">Figure 11</ref> shows the detections obtained using Faster rcnn model on a sample frame at different input resolution/scale. We can clearly see that as the scale reduces the number of detections also reduce and on lower resolution mostly the foreground objects are detected. <ref type="figure" target="#fig_0">Figure 12</ref> shows similar behavior for fcos detector with different scales. However, we can also see the differences in the model performance between fcos and faster rcnn for each scale. For example, fcos at lower  Scene Aggregates. We now present additional details on how scene aggregates help to provide additional context. Specifically, for crop extents we divide the image into patches, and mark patches that don't overlap with any predicted detection, and compute the convex hull of these patches as shown in <ref type="figure" target="#fig_0">Figure 13</ref>. The blue marked patches indicates the overlap with the objects and black boundary indicates the patches outside this can be cropped as it does not have any overlap with the object. <ref type="table" target="#tab_8">Table 8</ref> shows the performance of crop extents on Argoverse-HD dataset. We can see that with crop extent there is significant reduction in latency with minimal loss in accuracy. This holds true for different tracker scales as shown in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Details: Modified Shrinking Tail Scheduling</head><p>In section 4.3, we presented the challenges in using the current shrinking tail scheduling algorithm for our streaming perception task. Further, to overcome these challenges we present a modified shrinking tail algorithm as described in Algorithm 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional details: Performance of context generators</head><p>Switchability. As we saw from the tradeoff graphs presented earlier, models exhibit different accuracy/latency tradeoffs and also exhibit different error variations on different datasets. We therefore try to model a switchability metric which gives us a sense of the incentive to switch from one model to another.</p><p>Let D denote the set of object detectors, and n be the total number of object detectors under consideration. For a given image I, we compute mean intersection over union (mIoU) score from obtained detections from all detectors in D. We compute the standard deviation(SD) of mIoU across all the detectors in D on I. Let the standard deviation metric of IoU follow a distribution ? with mean mu and standard deviation ?. We bucket the SD value for an image into three categories: (i) Low: SD ? [0, ? ? ? * ?] (ii) Medium: SD ? (? ? ? * ?, ? + ? * ?] (iii) High: SD &gt; ? + ? * ?.</p><p>We set ? as 0.5.</p><p>We then train a classifier utilizing the backbone features from the associated object detector. The architecture of the model is analogous to the one defined in <ref type="bibr" target="#b10">[11]</ref>, except that we utilize soft-max outputs, and a Cross Entropy Loss to train this model. We present the results of the classification in <ref type="table" target="#tab_9">Table 9</ref> and <ref type="table" target="#tab_0">Table 10</ref> for Argoverse-HD and ImageNet-VID datasets, respectively. We directly use the probability vector emitted as an input feature to our agent.</p><p>Overhead. <ref type="table" target="#tab_0">Table 11</ref> shows that the overhead of our metrics computation for a single frame (presented numbers are averaged over 1000 distinct frames) at different scales is around 25-35%. The majority of the overhead contribution is from the adaptive scale regressor and switchability classifier. However, it should be noted that metrics are computed only when the agent is executed (i.e., once every 30 detection calls), hence, the amortized overhead per frame is merely around 1% during runtime.</p><p>Adaptive Scale. We utilize the (1&amp;3) kernel combination <ref type="bibr" target="#b10">[11]</ref>, i.e. 1 * 1 and 3 * 3 filters applied on input features in parallel with global pooling. Moreoever, our network shares weights operating on every feature level (FPN outputs features for each pyramid level) if necessary. Our training scales for ImageNet-VID dataset and Argoverse dataset S train1 = {600, 480, 360, 240} and S train2 = {720, 640, 480, 360, 240} respectively. We train the regressor using the protocol specified in <ref type="bibr" target="#b10">[11]</ref>, except that we use an Adam optimizer with lr = 0.0001. We maintain the same min and max scale ranges during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results: ImageNet-VID dataset</head><p>We consider the Static Policy of s = 600 as it's the baseline configuration proposed by AdaScale <ref type="bibr" target="#b10">[11]</ref> for comparison. We employ Streamer <ref type="bibr" target="#b19">[20]</ref> to compensate for the streaming constraints using  For execution policies involving our RL agent, we employ scale action space as s 3 = {600, 480, 420, 360, 300, 240} (this is similar to the scales the scale regressor <ref type="bibr" target="#b10">[11]</ref> is trained at, except for the addition of 420 and 300) and keep the proposal action space same as np 1 = {100, 300, 500, 1000} and m = {yolov3, f rcnn, f cos}. As we observe in <ref type="table" target="#tab_0">Table 12</ref>, similar trends emerge as with Argoverse-HD dataset. We can see that AdaScale doesn't outperform the baseline in an online real-time setup, due to it's tendency to minimize single frame latency (Row 2 vs Row 1). Our approach is able to pick competitive policies that outperform both static and dynamic-offline policy with an increase of 3 AP. It should also be noted that the stated online accuracy (AP 50 = 62.0) is very close to the accuracy measured in offline mode (AP 50 (offline) = 63.8). Furthermore, <ref type="figure" target="#fig_0">Figure 14</ref> shows the scale and proposals our RL agent picks during evaluation. <ref type="figure" target="#fig_0">Figure 14</ref>: Configurations picked by our agent in evaluation on ImageNet-VID. y ? axis represents proposals and x ? axis represents scales. <ref type="table" target="#tab_0">Table 13</ref> shows that our agent is able to learn a near-optimal policy to determine when to switch between models on the fly maximize both accuracy and latency. Our policy performs comparably with respect to the other policies that uses a fixed model. <ref type="figure" target="#fig_0">Figure 15</ref> shows the configurations picked by our agent for different models and scales.</p><p>F Additional Results: Argoverse-HD dataset <ref type="figure" target="#fig_0">Figure 16</ref> shows the configurations selected by our agent during training on Argoverse-HD dataset for scale and proposals. We can see that in <ref type="figure" target="#fig_0">Figure 16</ref>(a) for the first approx. 1000 timesteps the agent explores all the possible proposals equally and then starts selecting the one which maximizes both latency and accuracy. We can see that proposals {300, 500} are the configurations that are selected most. Similarly, <ref type="figure" target="#fig_0">Figure 16</ref>(b) shows the exploration for different input scale. We can see that on    this dataset, the agent identifies scale {640, 560} to be the optimal ones that maximize latency and accuracy. <ref type="figure" target="#fig_0">Figure 17</ref> shows the configurations selected by our agent when we consider model, proposals and scale tradeoffs. <ref type="figure" target="#fig_0">Figure 17(a)</ref> shows that the agent has learnt the policy to select Faster-RCNN more frequently than FCOS and Yolo based on the runtime tradeoffs. However, in <ref type="figure" target="#fig_0">Figure 17</ref>(b) we can see that agent selects all proposals almost equally across timesteps. These plots show that our agent efficiently explores the action space and selects the best configurations to maximize both accuracy and latency. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Consider the decision set D = {D 1 , D 2 ..D n } (Say, dimensionality is M = |D|) where each decision dimension D i = {d 1 , d 2 ..d k } corresponds to a configuration parameter of the algorithm f discretized as d k . Thus, an action a is an M -dimensional vector corresponding to each decision dimension being a discrete sub-action with a fixed number of choices. For example, if we consider 2 decision dimensions, D = {D scale , D model }, the potential configurations for each dimension would be D scale = {720, 600, 480, 360, 240}, D model = {yolo, f cos, f rcnn}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Configurations picked by our agent in evaluation. x ? axis represents proposals and y? axis represents scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Tradeoffs points (tracker scale (Tracktor<ref type="bibr" target="#b4">[5]</ref>), detector scale (FCOS<ref type="bibr" target="#b36">[37]</ref>), stride) on Argoverse-HD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Breakdown of Detection Errors on the COCO dataset using TIDE<ref type="bibr" target="#b6">[7]</ref> Breakdown of Detection Errors on the ImageNet-VID dataset using TIDE<ref type="bibr" target="#b6">[7]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Frame with low standard deviation score for switchability. (L-R: Faster R-CNN, FCOS, YOLO) Frame with low standard deviation score for switchability. (L-R: Faster R-CNN, FCOS, YOLO) Adaptive Scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Frame with high standard deviation score for switchability. (L-R: Faster R-CNN, FCOS, YOLO) resolution, i.e., 240 could not even detect the foreground objects. Thus showing the tradeoffs that exist between models and input scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Object detections at different input scale -Faster R-CNN Object detections at different input scale -FCOS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Crop Extent: As a tracker can't add new objects, it doesn't need to extract features from whole frame. A "reasonable" crop should help decrease latency without a drop in accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 3 :</head><label>3</label><figDesc>Modified Shrinking-tail scheduling 1: Given finishing time s and algorithm runtimes for different configurations r in the unit of frames (assuming r[i] &gt; 1 ? i) and current algorithm configuration c, this policy returns whether the algorithm should wait for the next frame 2: Define tail function ? (t) = t ? t 3: return [? (s + r[c]) &lt; ? (s)] (Iverson bracket)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :Figure 16 :Figure 17 :</head><label>151617</label><figDesc>Configurations picked by our agent in evaluation on ImageNet-VID while switching. y ? axis represents model and x ? axis represents scales. Configuration selected by our agent during training on Agroverse-HD dataset. y ? axis represents the number of times a configuration is selected. (a) Selection of different number of proposals (b) Selection of different input scale. Configuration selected by our agent during training on Agroverse-HD dataset. y ? axis represents the number of time a configuration is selected. (a) Selection of different models (b) Selection of different proposals (c) Selection of different input scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Faster R-CNN: Tradeoff between scale and proposals (50 random sequences of ImageNet-VID and Argoverse-HD)</figDesc><table><row><cell></cell><cell></cell><cell>ImageNet-VID</cell><cell></cell><cell></cell><cell></cell><cell>Argoverse-HD</cell></row><row><cell></cell><cell></cell><cell>Proposals</cell><cell></cell><cell></cell><cell></cell><cell>Proposals</cell></row><row><cell cols="2">Scale 100</cell><cell>300</cell><cell>500</cell><cell cols="2">Scale 100</cell><cell>300</cell><cell>500</cell></row><row><cell>600</cell><cell cols="3">54.9 ms 41.1 mAP 41.1 mAP 41.2 mAP 56.8 ms 59.5 ms</cell><cell>720</cell><cell cols="2">68.5 ms 23.8 mAP 24.6 mAP 24.8 mAP 71.1 ms 74.2 ms</cell></row><row><cell>480</cell><cell cols="3">41.1 ms 42.2 mAP 42.3 mAP 42.3 mAP 43.0 ms 45.6 ms</cell><cell>640</cell><cell cols="2">53.5 ms 22.5 mAP 23.5 mAP 23.4 mAP 55.1 ms 58.2 ms</cell></row><row><cell>360</cell><cell cols="3">30.0 ms 40.6 mAP 40.7 mAP 40.7 mAP 31.7 ms 34.0 ms</cell><cell>480</cell><cell cols="2">38.7 ms 18.1 mAP 18.6 mAP 18.7 mAP 40.1 ms 42.8 ms</cell></row><row><cell>240</cell><cell cols="3">25.4 ms 31.0 mAP 31.0 mAP 31.1 mAP 27.4 ms 30.0 ms</cell><cell>360</cell><cell cols="2">25.1 ms 12.3 mAP 12.6 mAP 12.7 mAP 27.8 ms 28.5 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Tradeoff between models and scale for 50 random seq. of ImageNet-VID</figDesc><table><row><cell cols="2">Scale YoloV3 FCOS</cell></row><row><cell>480</cell><cell>41.7 ms 49.3 ms 31.8 AP 49.1 AP</cell></row><row><cell>384</cell><cell>28.2 ms 36.2 ms 34.9 AP 47.3 AP</cell></row><row><cell>320</cell><cell>22.9 ms 34.0 ms 35.1 AP 45.9 AP</cell></row></table><note>(tracker scale (Tracktor [5]), de- tector scale (Faster R-CNN [32]), stride, number of proposals) on Argoverse-HD dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Training the RL agentFunction train_agent(?: Policy, S: Stream Replay Buffer):</figDesc><table><row><cell>Algorithm 1: Obtaining Observations From</cell><cell>Algorithm 2:</cell></row><row><cell>Streaming Perception System</cell><cell></cell></row><row><cell>Initialize policy ?</cell><cell></cell></row><row><cell>Set change system configuration probability p</cell><cell></cell></row><row><cell>? = 0</cell><cell></cell></row><row><cell>for e = 0, 1, ? ? ? , E ? 1 do</cell><cell></cell></row><row><cell>Reset simulator;</cell><cell></cell></row><row><cell>for every sequence do</cell><cell></cell></row><row><cell>Initialize empty stream replay buffer S;</cell><cell></cell></row><row><cell>i = 1;</cell><cell></cell></row><row><cell>while streaming do</cell><cell></cell></row><row><cell>// System executes on streaming data</cell><cell></cell></row><row><cell>...</cell><cell></cell></row><row><cell>value = Draw from Bernoulli(p) ;</cell><cell></cell></row><row><cell>if value == true then</cell><cell></cell></row><row><cell>Observe xi from the stream;</cell><cell></cell></row><row><cell>z? = M etrics(xi);</cell><cell></cell></row><row><cell>a? = ?(z? ) ;</cell><cell></cell></row><row><cell>Change system config. using a? ;</cell><cell></cell></row><row><cell>ta ? = time.now();</cell><cell></cell></row><row><cell>Store (z? , a? , ta ? ) in stream replay buffer</cell><cell></cell></row><row><cell>S ;</cell><cell></cell></row><row><cell>? = ? + 1;</cell><cell></cell></row><row><cell>i = i + 1;</cell><cell></cell></row><row><cell>? = train_agent(?, S) ;</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Our dynamic online policy outperforms all the SOTA approaches on Argoverse-HD dataset. decay rate as 0.999 and minimum-as 0.15 and train for 10 epochs. We train our models using both reward functions defined in Equations 1 and 2, which we'll refer to as R 1 and R 2 , respectively.</figDesc><table><row><cell>Approach</cell></row></table><note>5.1 Comparison with State-of-the-Art (SOTA) We compare our approach with a variety of state of the art polices, viz., two static (offline policies) and one dynamic (offline policy). 1 Static Policy: We use state-of-the-art meta-detector Streamer [20] as our static policy. Here the policy uses a fixed detector and input scale, and then schedules the detector along with forecasting to compensate for the latency. 2 Static Expert Policy: This policy similar to static, except the configurations are modified from Streamer to the best possible ones based on an expert user's input. 3 Dynamic-Offline Policy: This policy dynamically changes the input using AdaScale</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Resilience to various fixed policiesApproach AP Ours (s=s 1 , np=np 1 , R=R 2 , ? f ixed = (640, 300)) 21.3 Ours (s=s 1 , np=np 1 , R=R 2 , ? f ixed = (480, 300)) 21.3 Ours (s=s 1 , np=np 1 , R=R 2 , ? f ixed = (360, 1000)) 21.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Resilience to choice of action spaces Approach AP Ours (s=s 2 , np=np 1 , R=R 1 , strategy= -greedy) 18.2 Ours (s=s 2 , np=np 1 , R=R 1 , strategy=ucb) 20.0 Ours (s=s 2 , np=np 1 , R=R 2 , strategy= -greedy) 20.4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance with Tracking</figDesc><table><row><cell>Approach</cell><cell>AP</cell></row><row><cell cols="2">Static Policy (s=900, ts=600, k=5) 17.8</cell></row><row><cell cols="2">Static Policy (s=600, ts=600, k=5) 19.0</cell></row><row><cell cols="2">Ours (s=s 1 ,ts, k, np=np1, R=R 2 ) 19.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance with Model switching Ours (m=m, s=s 1 , np=np 1 , R=R 2 ) 20.7</figDesc><table><row><cell>Approach</cell><cell>AP</cell></row><row><cell>Streamer (m=f cos, s=600)</cell><cell>16.7</cell></row><row><cell>Streamer (m=yolov3, s=600)</cell><cell>20.2</cell></row><row><cell>Streamer (m=f rcnn, s=600)</cell><cell>20.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell>Effect of crop extent on Argoverse-HD</cell></row><row><cell cols="2">(Det Scale, Tracker Scale) W/O Crop Extent Crop Extent</cell><cell>Improvement</cell></row><row><cell>(720, 600)</cell><cell cols="2">42.94ms, 20.4 AP 33.60ms, 20.3 AP 21.7%, -0.5%</cell></row><row><cell>(720, 480)</cell><cell cols="2">31.01ms, 17.7 AP 28.00ms, 17.5 AP 12.5%, -1.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>We also compare by augmenting Streamer with AdaScale<ref type="bibr" target="#b10">[11]</ref> (Row 2, dynamicoffline) wherein the image scale is dynamically changed using the scale metric they have proposed, and to compensate for the real-time scheduling issues that would arise by changing scale dynamically, we employ the Modified Shrinking Tail Scheduling policy.</figDesc><table><row><cell cols="4">Switchability Classification Perf.: Argoverse-HD</cell></row><row><cell></cell><cell cols="3">Precision Recall F1-Score</cell></row><row><cell>Low</cell><cell>0.81</cell><cell>0.87</cell><cell>0.84</cell></row><row><cell>Medium</cell><cell>0.54</cell><cell>0.56</cell><cell>0.55</cell></row><row><cell>High</cell><cell>0.71</cell><cell>0.45</cell><cell>0.55</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell>0.73</cell></row><row><cell cols="4">forecasting and shrinking tail scheduling (same as Argoverse-HD), for a fair comparison (Row 1,</cell></row><row><cell>Static policy).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Switchability Classification Perf.: ImageNet-VID</figDesc><table><row><cell></cell><cell cols="3">Precision Recall F1-Score</cell></row><row><cell>Low</cell><cell>0.72</cell><cell>0.80</cell><cell>0.76</cell></row><row><cell>Medium</cell><cell>0.50</cell><cell>0.42</cell><cell>0.46</cell></row><row><cell>High</cell><cell>0.62</cell><cell>0.60</cell><cell>0.61</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell>0.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Overhead of Context Generation Scale DefaultAdaScale AdaScale + Scene Agg. Scale + Scene Agg. + Switch.</figDesc><table><row><cell>900</cell><cell cols="2">104.1 ms 19ms</cell><cell>19.6ms</cell><cell>39.2ms</cell></row><row><cell>600</cell><cell>55.7ms</cell><cell>9.5ms</cell><cell>9.7ms</cell><cell>18.5ms</cell></row><row><cell>300</cell><cell>1.3ms</cell><cell>32.2ms</cell><cell>3.5ms</cell><cell>7.3ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Our dynamic online policy outperforms all the SOTA approaches on ImageNet-VID dataset.</figDesc><table /><note>Approach AP AP 50 AP 75 AP S AP M AP L</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Performance with Model switching on ImageNet-VID Approach AP Streamer (m=f cos, s=600) 29.0 Streamer (m=f rcnn, s=600) 34.4 Ours (m=m, s=s 3 , R=R 2 ) 33.6</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Preprint.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://docs.microsoft.com/en-us/azure/virtual-machines/nd-series" />
		<title level="m">Microsoft Azure Virtual Machines: ND-Series</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">sAP -Code for Towards streaming perception</title>
		<ptr target="https://github.com/mtli/sAP" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Man?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frugal following: Power thrifty object detection and tracking for mobile augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kittipat</forename><surname>Apicharttrisorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xukan</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 17th ACM Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deliberation scheduling for problem solving in time-constrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Boddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas L Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tide: A general toolbox for identifying object detection errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08115</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glimpse: Continuous, real-time object recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><surname>Yu-Han Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenin</forename><surname>Ravindranath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramvir</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 13th ACM Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adascale: Towards real-time video object detection using adaptive scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ting-Wu Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reactive reasoning and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">L</forename><surname>Georgeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="677" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mcdnn: An approximation-based execution framework for deep stream processing under resource constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeop</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthai</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Wolman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>the 14th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computation and action under bounded resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integerarithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">T-cnn: Tubelets with convolutional neural networks for object detection from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards streaming perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengtian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2178" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A delay metric for video object detection: What average precision fails to tell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Understanding learned reward functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05862</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daishi</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepdecision: A mobile deep learning framework for edge video analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolianz</forename><surname>Xukan Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2018-IEEE Conference on Computer Communications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action branching architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Kormushev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subrata</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganga</forename><surname>Meghanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Bagchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02068</idno>
		<title level="m">Approxnet: Content and contention aware video analytics system for the edge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Videochef: efficient approximation for streaming video processing pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subrata</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Misailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Bagchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ApproxDet: content and contention-aware approximate object detection for mobiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subrata</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somali</forename><surname>Chaterji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Bagchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 18th Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural contextual bandits with ucb-based exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<title level="m">Probabilistic two-stage detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Streamer + AdaScale [11] + Modified Scheduling (Dynamic-Offline Policy)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Policy (Ours) (s = s 3 , np=np 1 , R = R 2 , ? f ixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dynamic-Online</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
