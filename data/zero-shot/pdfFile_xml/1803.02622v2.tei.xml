<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation in RGBD Images for Robotic Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Welschehold</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Dornhege</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
						</author>
						<title level="a" type="main">3D Human Pose Estimation in RGBD Images for Robotic Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Perception and understanding of the surrounding environment is vital for many robotics tasks. Tasks involving interaction with humans heavily rely on prediction of the human location and its articulation in space. These applications involve, e.g., gesture control, hand-over maneuvers, and learning from demonstration.</p><p>On the quest of bringing service robots to mass market and into common households, one of the major milestones is their instructability: consumers should be able to teach their personal robots their own custom tasks. Teaching should be intuitive and not require expert knowledge or programming skills. Ideally, the robot should learn from observing its human teacher demonstrating the task at hand. Hence it needs to be able to follow the human motion. Especially the hands play a key role as they are our main tool of interaction with the environment.</p><p>Estimation of human pose is challenging due to variation in appearance, strong articulation and heavy occlusions by themselves or objects. Recent approaches present robust pose estimators in 2D, but for robotic applications full 3D pose estimation in real world units is indispensable. In this paper, we bridge this gap by lifting 2D predictions into 3D while incorporating information from a depth map. This lifting via a depth map is non-trivial for multiple reasons, for instance, occlusion of the person by an object leads to misleading depths, see <ref type="figure">Fig. 6</ref>.</p><p>We present a learning based approach that predicts full 3D human pose and hand normals from RGBD input. It outperforms existing baseline methods and we show feasibility of teaching a robot tasks by demonstration.</p><p>The approach first predicts human pose in 2D given the color image. A deep network takes the 2D pose and the depth map as input and derives the full 3D pose from this information. Building on the predicted hand locations we *Indicates equal contribution. All authors are with the Department of Computer Science at the University of Freiburg, 79110 Freiburg, Germany. This work was supported by the Baden-Wurttemberg Stiftung as part of the projects ROTAH and RatTrack. <ref type="figure">Fig. 1</ref>: Given a color image and depth map, our system detects keypoints in 3D and predicts the normal vectors of the hands if visible. Predictions of that system enable us to teach a robot tasks by demonstration.</p><p>additionally infer the hand palm normals from the cropped color image. Based on this pose estimation system, we demonstrate the feasibility of our action learning from human demonstration approach without the use of artificial markers on the person. We reproduce the demonstrated actions on our robot in real world experiments. An implementation of our approach and a summarizing video are available online. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The vast majority of publications in the field of human pose estimation deal with the problem of inferring keypoints in 2D given a color image <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, which is linked to the availability of large scale datasets <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Due to the large datasets, networks for keypoint localization in 2D have reached impressive performance, which we integrate into our approach.</p><p>Recent techniques learn a prior for human pose that allows prediction of the most likely 3D pose given a single color image <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Predictions of most monocular approaches live in a scale and translation normalized frame, which makes them impracticable for many robotic applications. Approaches that can recover full 3D from RGB alone <ref type="bibr" target="#b12">[13]</ref> use assumptions to resolve the depth ambiguity. Our approach does not need any assumptions to predict poses in world coordinates.</p><p>All approaches that provide predictions in real world units are based on active depth sensing equipment. Most prominent is the Microsoft Kinect v1 sensor. Shotton et al. <ref type="bibr" target="#b17">[18]</ref> describes a discriminative method that is based on random forest classifiers and yields a body part segmentation. This work was followed by numerous approaches that propose using random tree walks <ref type="bibr" target="#b23">[24]</ref>, a viewpoint invariant representation <ref type="bibr" target="#b5">[6]</ref>  predictions <ref type="bibr" target="#b13">[14]</ref>. In contrast to the mentioned techniques, we incorporate depth and color in a joint approach. So far little research went into approaches that incorporate both modalities <ref type="bibr" target="#b2">[3]</ref>. We propose a deep learning based approach to combine color and depth. Our approach leverages the discriminative power of keypoint detectors trained on large scale databases for color images and complements them with information from the depth map for lifting to real world 3D coordinates.</p><p>In the field of learning from demonstration, Calinon et al. <ref type="bibr" target="#b3">[4]</ref> use markers to track human hand trajectories for action learning. M?hlig et al. <ref type="bibr" target="#b14">[15]</ref> use an articulated model of the human body to track teacher actions. Although being able to imitate the human manipulation motions, grasp poses on the objects are either pre-programmed or assumed as given. Mao et al. <ref type="bibr" target="#b9">[10]</ref> use a marker-less hand tracking method to teach manipulation tasks. Unlike our work, they assume that the human demonstrations are suitable for robot execution without further adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>In this work we aim to estimate 3D human poses and the hand normal vectors from RGBD input. This procedure is summarized in <ref type="figure">Fig. 2</ref>. Subsequently, we extract human motion trajectories from demonstrations and transfer them to the robot with regard to its kinematics and grasping capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human Pose Estimation</head><p>We aim for estimating the human body keypoints w = ( w 1 , . . . , w J ) ? R 3?J for J keypoints in real world coordinates relative to the Kinect sensor given color image I ? R N ?M ?3 , depth map D ? R N ?M and their calibration. Additionally we predict the hand normal vectors n ? R 3?2 for both hands of the person. Without loss of generality we define the coordinate system, our predictions live in, to be identical with the color sensors frame.</p><p>For the Kinect, the color and depth sensors are located in close proximity, but still the frames resemble two distinct cameras. Our approach needs to collocate information of the two frames. Therefore we transform the depth map into the color frame using the camera calibration. As a result, our approach operates on the warped depth map D ? R N ?M . Due to occlusions, differences in resolution and noise, the resulting depth map D is sparse, but for better visualization a linear interpolation of D is shown in <ref type="figure">Fig. 2</ref>.</p><p>1) Color Keypoint Detector: The keypoint detector is applied to the color image I, which yields score maps s 2D ? R N ?M ?J encoding the likelihood of a specific human keypoint being present. The maxima of the score maps s 2D correspond to the predicted keypoint locations p = ( p 0 , . . . p J ) ? R 2?J in the image plane. Thanks to many datasets with annotated color frames for human pose estimation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b1">[2]</ref>, robust detectors are available. We use the Open Pose Library <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> with fixed weights in this work.</p><p>2) VoxelPoseNet: Given the warped depth map D a voxel occupancy grid V ? R K?K?K is calculated with K = 64. For this purpose the depth map D is transformed into a point cloud and we calculate an 3D coordinate w r , which is the center of V. We calculate w r as back projection of the predicted 2D 'neck' keypoint p r using the median depth d r extracted from the neighborhood of p r in D:</p><formula xml:id="formula_0">w r = d r ? K ?1 ? p r .<label>(1)</label></formula><p>Where K denotes the intrinsic calibration matrix camera and p r is in homogeneous coordinates. We pick the value d r from the depth map taking into account the closest 3 neighboring valid depth values around p r . We calculate V by setting elements to 1, when there is at least one point of the point cloud lying in the interval represented and zero otherwise. We chose the resolution of the voxel grid to be approximately 3 cm.</p><p>VoxelPoseNet gets V and a volume of tiled score maps s 2D as input and processes them with a series of 3D convolutions. We propose to tile s 2D along the z-axis, which is equivalent to an orthographic projection approximation. VoxelPoseNet estimates score volumes s 3D ? R K?K?K?J , which resemble keypoint likelihoods the same way as its 2D counterpart</p><formula xml:id="formula_1">w VPN = arg max x,y,z (s 3D ).<label>(2)</label></formula><p>We use the following heuristic to assemble our final prediction: On the one hand w VPN is predicted by VoxelPoseNet.</p><p>On the other hand we take the z-component of w VPN and the predicted 2D keypoints p 2D to calculate another set of world coordinates w projected . For these coordinates the accuracy in x-and y-direction is not limited by the choice of K anymore. We chose our final prediction w from w projected and w VPN based on the 2D networks prediction confidence, which is the score of s 2D at p. <ref type="figure">Fig. 2</ref> shows the network architecture used for Voxel-PoseNet, which is a encoder decoder architecture inspired by the U-net <ref type="bibr" target="#b15">[16]</ref> that uses dense blocks <ref type="bibr" target="#b6">[7]</ref> in the encoder. While decoding to the full resolution score map, we incorporate multiple intermediate losses denoted by s i 3D , which are discussed in section section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hand Normal Estimation</head><p>The approach presented in section section III-A yields locations for the human hands, which are used to crop the input image centered around the predicted hand keypoint. For HandNormalNet we adopt our previous work on hand pose estimation <ref type="bibr" target="#b24">[25]</ref>. We exploit that the network from <ref type="bibr" target="#b24">[25]</ref> estimates the relative transformation between the depicted hand pose and a canonical frame, which gives us the normal vector. We use that network without further retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network training</head><p>We train VoxelPoseNet using a sum of squared L 2 losses:</p><formula xml:id="formula_2">L = i s gt 3D ? s i, pred 3D 2 2<label>(3)</label></formula><p>with a batch size of 2. Datasets used for training are discussed in section section IV. The networks are implemented in Tensorflow <ref type="bibr" target="#b0">[1]</ref> and we use the ADAM solver <ref type="bibr" target="#b7">[8]</ref>. We train for 40000 iterations with an initial learning rate of 10 ?4 , which drops by the factor 0.1 every 10000 iterations. Ground truth score volumes s gt 3D are calculated from the ground truth keypoint location within the voxel V. A Gaussian function is placed at the ground truth location and normalized such that its maximum is equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Action learning</head><p>With the ability to record the human motion trajectories, action learning requires them to be transferred to the robot. Due to its deviating kinematics and grasping capabilities the robot cannot directly reproduce the human motions. For the necessary adaption and the action model generation we use the learning-from-demonstration approach presented in our previous work <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Here, the robot motion is designed to follow the teacher's demonstrations as closely as possible, while deviating as much as necessary to fulfill constraints posed by its geometry. We pose it as a graph optimization problem, in which trajectories of the manipulated object and the teacher's hand and torso serve as input. We account for the robot's grasping skills and kinematics as well as occlusions in the observations and collisions with the environment. We assume that the grasp on the object is fixed during manipulation and all trajectories are smooth in the sense that consecutive poses should be near each other. These constraints are addressed via the graphs edges. During optimization the teacher's demonstrations are adapted towards trajectories that are feasible for robot execution. For details on the graph structure and the implementation we refer to Welschehold et al. <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS</head><p>Currently there are no datasets for the Kinect v2 that provide high-quality skeleton annotation of the person. Due to its long presence, most publicly available sets are recorded with the Kinect v1. These datasets are not suited for our scenario, because of major technical differences between the two models. More recently published datasets, such as Shahroudy et al. <ref type="bibr" target="#b16">[17]</ref>, transitioned to the new model but used the Kinect SDK's prediction as pseudo ground truth. Using those datasets is prohibitive for exceeding the Kinect SDK's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi View Kinect Dataset (MKV)</head><p>Therefore, for training of our neural network we recorded a new dataset, which comprises 5 actors, 3 locations, and up to 4 viewpoints. There are 2 female and 3 male actors and the locations resemble different indoor setups. Some examples are depicted in <ref type="figure" target="#fig_0">Fig. 3</ref>. The poses include various upright and sitting poses as well as walking sequences. Short sequences were recorded simultaneously by multiple calibrated Kinect v2 devices with a frame rate of 10 Hz, while recording the skeletal predictions of the Kinect SDK. In a post processing step we applied state-of-the-art Human Keypoint Detectors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> and used standard triangulation techniques to lift the 2D predictions into 3D. This results in a dataset with 22406 samples. Each sample comprises of color image, depth map, infrared image, the SDK prediction and a ground truth skeleton annotation we get through triangulation. The skeleton annotations comprises of 18 keypoints that follow the Coco definitions <ref type="bibr" target="#b8">[9]</ref>. We apply data augmentation techniques and split the set into an evaluation set of 3546 samples (MVK-e) and a training set with 18860 (MVK-t). We divide the two sets by actors and assign both female actors into the evaluation set, which also leaves one location unique to this set. Additionally this dataset contains annotated hand normals for a small subset of the samples. The annotations stem from detected and lifted hand keypoints, which were used to calculate the hand normal ground truth. Because detection accuracy was much lower and bad samples were discarded afterwards this dataset is much smaller and provides a total of 129 annotated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Captury Dataset</head><p>Due to the limited number of cameras in the MKV setup and the necessity to avoid occluding too many cameras views at the same time, we are limited in the amount of possible object interaction of the actors. Therefore we present a second dataset that was recorded using a commercial marker-less motion capture system called Captury 2 . It uses 12 cameras to track the actor with 120 Hz and we calibrated a Kinect v2 device with respect to the Captury. The skeleton tracking provides 23 keypoints, from which we use 13 for comparison. We recorded three actors, which performed simple actions like pointing, walking, sitting and interacting with objects like a ball, chair or umbrella. One actor of this setting was already recorded for the MKV dataset and therefore constitutes the set used for training. Two previously unseen actors were recorded and form the evaluation set. There are 1535 samples for training (CAP-t) and 1505 samples for evaluation (CAP-e). The definition of human keypoints between the two datasets is compatible, except for the "head" keypoint, which misses a suitable counterpart in the MKV dataset. This keypoint is excluded from evaluation to avoid systematic error in the comparison. <ref type="table" target="#tab_2">Table I</ref> shows that the proposed PoseNet3D already reaches good results on the evaluation split of both datasets when trained only on MKV-t. Training a network only on CAP-t leads to inferior performance, which is due to starkly limited variation in the training split of the Captury dataset, which only contains a single actor and scene. Training jointly on both sets performs roughly on par with training exclusively on MKV-t. Therefore we use MKV-t as default training set for our networks and evaluate on CAP-e for following experiments. Furthermore, we confirm generalization of our MKV-t trained approach on the InOutDoor Dataset <ref type="bibr" target="#b11">[12]</ref>. Because the dataset does not contain pose annotations we present qualitative results in the supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS -POSE ESTIMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets for training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison to literature</head><p>In <ref type="table" target="#tab_2">Table II</ref> we compare our approach with common baseline methods. The first baseline is the Skeleton Tracker integrated in Microsofts Software Development Kit 3 (Kinect SDK). We show that its performance heavily drops on the   more challenging subset and therefore argue that it is unsuitable for many robotics applications. Furthermore, <ref type="figure" target="#fig_2">Fig. 5</ref> shows that the Kinect SDK is unable to predict keypoints farther away than a certain distance. The qualitative examples in <ref type="figure">Fig. 6</ref> reveal that the SDK is led astray by objects and is unable to distinguish if a person is facing towards or away from the camera, which expresses itself in mixing up left and right side. The baseline named Naive Lifting uses the same Keypoint detector for color images as our proposed approach and simply picks the corresponding depth value from the depth map. It chooses the depth value as median value of the 3 closest neighbors. The approach shows reasonable performance, but is prone to pick bad depth values from the noisy depth map. Also any kind of occlusion results into an error, which is seen in <ref type="figure">Fig. 6</ref>.</p><p>Tome et al. <ref type="bibr" target="#b19">[20]</ref> predicts scale and translation normalized poses. So in order to compare the results to the other approaches we provide the algorithm with ground truth scale and translation. For every prediction we seek scale and translation in order to minimize the reconstruction error between ground truth and prediction. <ref type="table" target="#tab_2">Table II</ref> shows that the approach reaches competitive results, but performs worst in our comparison, which is reasonable given the lack of depth information. In <ref type="figure" target="#fig_1">Fig. 4</ref> the approach stays far behind, which partly lies in the fact that the approach misses to provide predictions in 8.7% of the frames of CAP-e, which compares to 12.4% for Kinect SDK and 0% for Naive Lifting and our approach.</p><p>VoxelPoseNet outperforms its baseline methods, because it exploits both modalities. On the one hand, color information helps to disambiguate left and right side, which is infeasible from depth alone. On the other hand, the depth map provides valuable information to exactly infer the 3D keypoint. Furthermore, the network learns a prior about possible body part configurations, which makes it possible to infer 3D locations even for completely occluded keypoints (see <ref type="figure">Fig. 6</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. HandNormalNet</head><p>We use the annotated samples of MKV to evaluate the accuracy of the normal estimation we achieve with the adopted network from <ref type="bibr" target="#b24">[25]</ref>. For the 129 samples we get an average angular error of 60.3 degree, which is sufficient for the task learning application as is shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS -ACTION LEARNING</head><p>We evaluate our recently proposed graph-based approach <ref type="bibr" target="#b22">[23]</ref> for learning a mobile manipulation task from human demonstrations on data acquired with the approach for 3D human pose estimation presented in this work. We evaluate the methods on the same four tasks as in our previous work <ref type="bibr" target="#b22">[23]</ref>: one task of opening and moving through a room door and three tasks of opening small furniture pieces. The tasks will be referred to as room door, swivel door, drawer, and sliding door. Each consists of three parts. First a specific part of the object is grasped, i.e., a handle or a knob, then the object is manipulated according to its geometry, and lastly released. The demonstrations were recorded with a Kinect v2 at 10 Hz. As we need to track both, the manipulated object and the human teacher, the actions were recorded from a perspective that show the human from the side or back making pose estimation challenging. For an example of the setup see <ref type="figure" target="#fig_3">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adapting Human Demonstrations to Robot Requirements</head><p>First we evaluate adaption of the recorded demonstrations towards the robot capabilities. Specifically we compare the optimization for all aforementioned tasks for two different teacher pose estimation methods. The first relies on detecting markers attached to the teachers hand and torso and was conducted for our previous work <ref type="bibr" target="#b22">[23]</ref>. The second follows the approach presented in this work. In <ref type="table" target="#tab_2">Table III</ref> we summarize the numerical evaluation for both recording methods. The table shows that the offset between a valid robot grasp and the demonstrated grasp pose is higher for the 3D human pose estimation than for the estimation with markers for all tasks. The highest difference occurs for the room door task, because the hand is occluded in many frames resulting in fewer predictions. Nevertheless our graph-based optimization is still able to shift the human hand trajectory to reproduce the intended grasp, see <ref type="figure">Fig. 7</ref>. This is reflected in higher distances, both Euclidean and angular, between gripper and recorded hand poses after the optimization. Next we compare the standard deviation on the transformations between the object and the gripper, respectively the object and hand in the manipulation segment. These transformations correspond to the robot and human grasps. We see that for both the translational and the rotational part we have comparable values for the two pose estimation methods. This indicates that, although not being as accurate as using markers, we still have a high robustness in the pose estimation, meaning that the error is systematic and the relative measurements are consistent with little deviation. After the optimization we obtain low standard deviations for both the human and the robot grasp, which corresponds, as desired, to a fixed grasp during manipulation. On the one hand the results show that our graph optimization approach is able and stringently necessary to adapt noisy human teacher demonstrations to robot friendly trajectories. On the other hand they also demonstrate that our approach for pose estimation without markers is sufficiently accurate for action learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Imitation by the Robot</head><p>In a follow-up experiment we used the adapted demonstrations from our pose estimation approach shown in <ref type="table" target="#tab_2">Table III</ref> to learn action models that our PR2 robot can use to imitate the demonstrated actions in real world settings. These timedriven models are learned as in our previous work <ref type="bibr" target="#b22">[23]</ref> using mixtures of Gaussians <ref type="bibr" target="#b3">[4]</ref>. We learn combined action models for robot gripper and base in Cartesian space. The models are used to generate trajectories for the robot in the frame of the manipulated object. With the learned models we reproduced each action five times. For opening the swivel door we had one failure due to localization problems during the grasping. For the drawer and the room door all trials of grasping and manipulating were successful. The sliding door was always grasped successfully but due to the small door knob and the Kinect SDK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naive Lifting</head><p>Tome et al. <ref type="bibr" target="#b19">[20]</ref> Scene Others Proposed <ref type="figure">Fig. 6</ref>: Typical failure cases of the algorithms evaluated for samples from CAP-e. The first row shows the scene and the other two rows depict the ground truth skeleton in dashed blue and the prediction in solid green and red. Green color indicates the persons right side.</p><p>Predictions of our proposed approach are shown in the last row, whereas the middle row shows predictions by other algorithms. The first two columns correspond to predictions of the Kinect SDK, the next two are by the Naive Lifting approach and the last two by the approach presented by Tome et al. <ref type="bibr" target="#b19">[20]</ref>. Typical failures for the SDK are caused by objects and or people that face away from the camera. Naive Lifting fails when any sort of keypoint occlusion is present.   <ref type="bibr" target="#b22">[23]</ref>. There the human pose estimation was obtained using markers. The lower half shows the results of the experiments carried out with the human pose estimation presented in this work. The total number of recorded poses refers to the length after interpolating missing ones. The shown distance between gripper and grasp poses is a mean over the endpoints of the reaching segments of the demonstrations. For the distance between gripper and hand as well as the collisions and the kinematic feasibility all pose tuples are considered. Kinematic feasibility expresses the lookup in the inverse reachability map. For the relation between object and robot gripper respectively human hand a mean over all poses in the manipulation segments is calculated. Since gripper poses are initialized with the measured hand poses no meaningful distance before optimization can be given. For the three furniture operating tasks no collisions with the map are considered. <ref type="figure">Fig. 7</ref>: Adaption of the recorded human teacher trajectory to the robot grasping capabilities for grasping the handle of the drawer (left) and the swivel door (right). The gripper poses (magenta dots) are shifted towards the handle of the drawer, respectively the door, leading to a successful robot grasp. By just imitating the human hand motion (orange dots) the grasps would fail. tension resulting from the combined gripper and base motion, the knob was accidentally released during the manipulation process. We ran five successful trials of opening the sliding door by keeping the robot base steady. A visualization of the teaching process and the robot reproducing the action demonstration can be seen in <ref type="figure" target="#fig_3">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>We propose a CNN based system that jointly uses color and depth information in order to predict 3D human pose in real world units. This allows us to exceed the performance of existing methods. Our work introduces two RGBD datasets, which can be used for future approaches. We show, how our approach for 3D human pose estimation is applied in a task learning application that allows non-expert users to teach tasks to service robots. This is demonstrated in realworld experiments that enable our PR2 robot to reproduce human-demonstrated tasks without any markers on the human teacher.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Examples from the MKV dataset with ground truth skeleton overlayed. The two leftmost ones are samples from the training set and the other two show the evaluation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Performance of different algorithms on CAP-e measured as percentage of correct keypoints (PCK) on the more challenging subset of non-frontal poses and object interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Percentage of correct keypoints (PCK) over their distance to the camera. Most approaches are only mildly affected by the keypoint distance to the camera, but the Kinect SDK can only provide predictions in a limited range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 :</head><label>8</label><figDesc>On the left image the teacher demonstrates the task of opening the swivel door. Superimposed on the image we see the recorded trajectories for hand (orange), torso (green) and manipulated object (blue) which serve as the input for the action learning. The right image shows the robot reproducing the action using a model learned from the teacher demonstration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Performance measured as area under the curve (AUC) for different training sets of VoxelPoseNet. CAP-t does not generalize to MKV-e, whereas MKV-t provides sufficient variation to generalize to CAP-e. Training jointly on CAP-t and MKV-t doesn't improve results much anymore.</figDesc><table><row><cell></cell><cell>Captury full</cell><cell>Captury subset</cell><cell>Multi Kinect</cell></row><row><cell>Kinect SDK</cell><cell>13.5</cell><cell>16.4</cell><cell>8.9</cell></row><row><cell>Naive Lifting</cell><cell>14.7</cell><cell>15.2</cell><cell>8.8</cell></row><row><cell>Tome et al. [20]</cell><cell>22.7</cell><cell>21.9</cell><cell>15.1</cell></row><row><cell>Proposed</cell><cell>11.2</cell><cell>11.6</cell><cell>6.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Average mean end point error per keypoint of the predicted 3D pose for different approaches in cm. For the Captury dataset we additionally report results on the subset of non-frontal scenes and with object interaction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>After Opt. Before Opt. After Opt. Before Opt. After Opt. Before Opt. After Opt.</figDesc><table><row><cell></cell><cell cols="2">Room Door</cell><cell cols="2">Swivel Door</cell><cell cols="2">Drawer</cell><cell cols="2">Sliding Door</cell></row><row><cell></cell><cell cols="6">Before Opt. Human Pose Estimation with AR-Marker</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">10 demos, 1529 poses</cell><cell cols="2">4 demos, 419 poses</cell><cell cols="2">6 demos, 656 poses</cell><cell cols="2">10 demos, 1482 poses</cell></row><row><cell>Euclidean distance gripper-grasp</cell><cell>2.82 cm</cell><cell>0.55 cm</cell><cell>2.36 cm</cell><cell>0.49 cm</cell><cell>6.33 cm</cell><cell>0.37 cm</cell><cell>3.23 cm</cell><cell>0.60 cm</cell></row><row><cell>Angular distance gripper-grasp</cell><cell>18.3 ?</cell><cell>8.0 ?</cell><cell>5.3 ?</cell><cell>0.7 ?</cell><cell>5.4 ?</cell><cell>1.6 ?</cell><cell>6.5 ?</cell><cell>0.5 ?</cell></row><row><cell>Euclidean distance gripper-hand</cell><cell>?</cell><cell>2.2 cm</cell><cell>?</cell><cell>2.68 cm</cell><cell>?</cell><cell>5.54 cm</cell><cell>?</cell><cell>3.13 cm</cell></row><row><cell>Angular distance gripper-hand</cell><cell>?</cell><cell>13.5 ?</cell><cell>?</cell><cell>3.0 ?</cell><cell>?</cell><cell>2.8 ?</cell><cell>?</cell><cell>5.8 ?</cell></row><row><cell>Std dev on gripper-object trans.</cell><cell>1.7 cm</cell><cell>0.53 cm</cell><cell>2.35 cm</cell><cell>0.21 cm</cell><cell>2.66 cm</cell><cell>0.18 cm</cell><cell>0.51 cm</cell><cell>0.12 cm</cell></row><row><cell>Std dev on gripper-object rot.</cell><cell>20.5 ?</cell><cell>2.4 ?</cell><cell>19.3 ?</cell><cell>1.6 ?</cell><cell>0.88 ?</cell><cell>0.21 ?</cell><cell>3.4 ?</cell><cell>0.34 ?</cell></row><row><cell>Std dev on hand-object trans.</cell><cell>1.7 cm</cell><cell>0.5 cm</cell><cell>2.35 cm</cell><cell>0.16 cm</cell><cell>2.66 cm</cell><cell>0.28 cm</cell><cell>0.51 cm</cell><cell>0.16 cm</cell></row><row><cell>Std dev on hand-object rot.</cell><cell>20.5 ?</cell><cell>4.6 ?</cell><cell>19.3 ?</cell><cell>0.9 ?</cell><cell>0.88 ?</cell><cell>0.3 ?</cell><cell>3.4 ?</cell><cell>0.6 ?</cell></row><row><cell>Map collision free poses</cell><cell>89.2 %</cell><cell>99.74 %</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Kinematically achievable</cell><cell>69.8 %</cell><cell>96.86 %</cell><cell>85.9 %</cell><cell>99.52 %</cell><cell>87.3 %</cell><cell>100 %</cell><cell>63.2 %</cell><cell>99.93 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">3D Human Pose Estimation from RGBD</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">10 demos, 1215 poses</cell><cell cols="2">5 demos, 330 poses</cell><cell cols="2">5 demos, 370 poses</cell><cell cols="2">5 demos, 451 poses</cell></row><row><cell>Euclidean distance gripper-grasp</cell><cell>31.17 cm</cell><cell>0.40 cm</cell><cell>9.77 cm</cell><cell>0.53 cm</cell><cell>16.18 cm</cell><cell>0.32 cm</cell><cell>5.64 cm</cell><cell>0.19 cm</cell></row><row><cell>Angular distance gripper-grasp</cell><cell>130.27 ?</cell><cell>0.24 ?</cell><cell>102.9 ?</cell><cell>0.4 ?</cell><cell>108.89 ?</cell><cell>0.06 ?</cell><cell>149.30 ?</cell><cell>0.07 ?</cell></row><row><cell>Euclidean distance gripper-hand</cell><cell>?</cell><cell>32.78 cm</cell><cell>?</cell><cell>14.19 cm</cell><cell>?</cell><cell>24.18 cm</cell><cell>?</cell><cell>19.26 cm</cell></row><row><cell>Angular distance gripper-hand</cell><cell>?</cell><cell>63.94 ?</cell><cell>?</cell><cell>92.87 ?</cell><cell>?</cell><cell>103.39 ?</cell><cell>?</cell><cell>121.69 ?</cell></row><row><cell>Std dev on gripper-object trans.</cell><cell>17.40 cm</cell><cell>0.25 cm</cell><cell>1.75 cm</cell><cell>0.15 cm</cell><cell>1.08 cm</cell><cell>0.12 cm</cell><cell>1.03 cm</cell><cell>0.10 cm</cell></row><row><cell>Std dev on gripper-object rot.</cell><cell>34.31 ?</cell><cell>0.14 ?</cell><cell>23.39 ?</cell><cell>0.76 ?</cell><cell>14.28 ?</cell><cell>0.06 ?</cell><cell>15.86 ?</cell><cell>0.03 ?</cell></row><row><cell>Std dev on hand-object trans.</cell><cell>17.40 cm</cell><cell>8.01 cm</cell><cell>1.75 cm</cell><cell>1.18 cm</cell><cell>1.08 cm</cell><cell>0.83 cm</cell><cell>1.03 cm</cell><cell>0.73 cm</cell></row><row><cell>Std dev on hand-object rot.</cell><cell>34.31 ?</cell><cell>0.50 ?</cell><cell>23.39 ?</cell><cell>0.89 ?</cell><cell>14.28 ?</cell><cell>0.18 ?</cell><cell>15.86 ?</cell><cell>0.27 ?</cell></row><row><cell>Map collision free poses</cell><cell>89.14 %</cell><cell>99.51 %</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Kinematically achievable</cell><cell>38.10 %</cell><cell>96.05 %</cell><cell>80.0 %</cell><cell>97.27 %</cell><cell>60.81 %</cell><cell>98.92 %</cell><cell>63.64 %</cell><cell>95.12 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Results for the optimization for all four trained tasks. The upper half of the table summarizes the results from experiments conducted in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.thecaptury.com 3 https://www.microsoft.com/en-us/download/details.aspx?id=44561</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An adaptable system for rgb-d based human body detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baksheev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">De</forename><surname>Laet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of visual communication and image representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="52" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical dynamical systems for skills acquisition in humanoids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Tsagarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Humanoid Robots (Humanoids)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards Viewpoint Invariant 3d Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="160" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision (ECCV)</title>
		<meeting>of the Europ. Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning hand movements from markerless demonstrations for humanoid tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Baras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Humanoid Robots (Humanoids)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Choosing smartly: Adaptive multimodal fusion for object detection in changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vnect: real-time 3d human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>36:44:1-44:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Holistic Planimetric prediction to Local Volumetric prediction for 3d Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04758</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive imitation learning of object movement skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gienger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lifting from the Deep: Convolutional 3d Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning manipulation actions from human demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Welschehold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dornhege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning mobile manipulation actions from human demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Welschehold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dornhege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Random tree walk toward instantaneous 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I. Dong</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
