<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Abaskohi</surname></persName>
							<email>amir.abaskohi@ut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Rasouli</surname></persName>
							<email>arash.rasouli@ut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanin</forename><surname>Zeraati</surname></persName>
							<email>t.zeraati@ut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Bahrak</surname></persName>
							<email>bahrak@ut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse someone. It is commonly used on social media. The metaphorical and creative nature of sarcasm presents a significant difficulty for sentiment analysis systems based on affective computing. The methodology and results of our team, UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in this paper. We put different models, and data augmentation approaches to the test and report on which one works best. The tests begin with traditional machine learning models and progress to transformer-based and attention-based models. We employed data augmentation based on data mutation and data generation. Using RoBERTa and mutation-based data augmentation, our best approach achieved an F1sarcastic of 0.38 in the competition's evaluation phase. After the competition, we fixed our model's flaws and achieved an F1-sarcastic of 0.414.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Billions of internet users use social networks not only to stay in touch with friends, meet new people, and share user-generated content but also to express their opinions on a wide range of topics using a variety of methods such as posting comments, videos, photos, etc. with specific groups of people <ref type="bibr" target="#b26">(Tungthamthiti et al., 2016)</ref>. In these platforms, users could submit information on whatever topic they wanted, with no restrictions on the sort of content they may share. The lack of constraints and individuals' anonymity on these networks led to humorous sarcastic texts.</p><p>Because sarcasm indicates sentiment, detecting sarcasm in a text is critical for anticipating the text's accurate sentiment, making sarcasm detection a valuable tool with multiple applications in * equal contribution domains such as security, health, services, product evaluations, and sales. Sarcasm detection is an essential aspect of creative language comprehension <ref type="bibr" target="#b29">(Veale et al., 2019)</ref> and online opinion mining <ref type="bibr" target="#b11">(Kannangara, 2018)</ref>. Even for humans, identifying sarcasm is difficult due to heavily contextualized expressions <ref type="bibr" target="#b30">(Walker et al., 2012)</ref>. There are few labeled data resources for sarcasm detection. Any available texts that can be collected (for example, Tweets) contain many issues, such as an evolving dictionary of slang words and abbreviations, requiring many hours of human annotation to prepare the data for any potential use. Furthermore, the nature of sarcasm identification adds to the task's difficulty, as sarcasm may be considered relative and varies significantly across people, depending on a variety of criteria such as the context, area, time, and events surrounding the statement.</p><p>In an attempt to solve this issue, we participated in SemEval-2022 shared task 6 <ref type="bibr" target="#b1">(Abu Farha et al., 2022)</ref>, which aims to recognize whether a tweet is sarcastic or not. Our contributions are as follows:</p><p>1. We experiment with simple machine learning models like Support Vector Machine (SVM) and various word encodings.</p><p>2. To discover the optimum data preprocessing method, we tested the effect of various data preprocessing. mutation-based data augmentation, our top result gets an F1-sarcastic of 0.38. However, we obtain better outcomes, with a 0.414 F1-sarcastic after fixing the problems of our proposed method. The rest of this paper is organized as follows. In Section 2, we discuss the related work, Section 3 introduces the dataset. In Sections 4 and 5, we present our methodology and results, respectively. Finally Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We give a quick review of previous works on sarcasm detection in this part, followed by works on data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sarcasm Detection on Twitter</head><p>Sarcasm detection has been represented as a binary classification issue, with most tweets labeled with specific hashtags (e.g., #sarcasm, #sarcastic) being considered sarcastic. Many techniques in various languages have been proposed using this framework.</p><p>In <ref type="bibr" target="#b3">(Davidov et al., 2010)</ref>, Semi-supervised sarcasm detection experiments were done using a Twitter dataset (5.9 million tweets) and 66,000 Amazon product evaluations. On the product review dataset, they acquired an F-measure of 0.83. On the Twitter dataset, they obtained an F-measure of 0.55 using 5-fold cross-validation on their k-Nearest Neighbor (kNN) like classifier.</p><p>(Gonz?lez-Ib?nez et al., 2011) used 900 messages from Twitter sorted into three groups (sarcastic, positive sentiment, and negative sentiment). To find sarcastic tweets, they utilized the hashtags #sarcasm and #sarcastic. SVM with Sequential Minimum Optimization (SMO) and logistic regression were employed as classifiers. The best accuracy for the sarcastic class was 0.65. <ref type="bibr" target="#b22">(Reyes et al., 2012)</ref> presented elements to capture ambiguity, polarity, unexpectedness, and emotive situations in figurative language. F1-sarcastic of 0.65 was the best result in categorizing irony and general tweets.</p><p>The representativeness and significance of conceptual elements have been investigated in <ref type="bibr" target="#b23">(Reyes et al., 2013)</ref>. Punctuation marks, emoticons, quotations, capitalized words, lexicon-based features, character n-grams, skip-grams, and polarity skipgrams are all examples of these characteristics. Each of the four categories (irony, comedy, education, and politics) in their corpus has 10,000 tweets.</p><p>Using the Naive Bayes and decision trees algorithms, they evaluated two distributional scenarios: balanced distribution and unbalanced distribution (25% ironic tweets and 75% tweets from the three non-ironic categories). The decision trees classified the balanced distribution with an F1-sarcastic of 0.72 and the unbalanced distribution with an F1-sarcastic of 0.53.</p><p>One sort of sarcasm identified by <ref type="bibr" target="#b24">(Riloff et al., 2013)</ref> is the difference between a good mood and a bad scenario. Using a bootstrapping approach, the authors gathered collections of positive sentiment phrases and negative circumstance words from sarcastic tweets. They suggested a method for classifying tweets as sarcastic if they contain a positive predictive close to a negative context phrase. They used a SVM classifier using unigrams and bigrams as features to evaluate a human-annotated dataset of 3000 tweets (23% sarcastic), getting an F1-sarcastic of 0.48. The F1-sarcastic of the hybrid strategy, which combined the findings of the SVM classifier with their baseline method, was 0.51. <ref type="bibr" target="#b14">(Lukin and Walker, 2017)</ref> used bootstrapping, syntactic patterns, and a high precision classifier to classify sarcasm and nastiness in online chats. On their snark dataset, they got an F1-sarcastic of 0.57.</p><p>In <ref type="bibr" target="#b18">(Oprea and Magdy, 2019)</ref>, LSTM, Att-LSTM, CNN, SIARN, MIARN, 3CNN, and Dense-LSTM models were used to assess the task dataset that was introduced in (Oprea and Magdy, 2019), which is an unbalanced dataset and labeled by the tweets' writers. Using Multi-Dimension Intra-Attention (MIARN) <ref type="bibr" target="#b25">(Tay et al., 2018)</ref> Network, they could get an F-score of 0.364.</p><p>In <ref type="bibr" target="#b8">(Guo et al., 2021)</ref>, the Latent Optimized Adversarial Neural Transfer (LOANT) model was suggested as a novel latent-optimized adversarial neural transfer model for cross-domain sarcasm detection. LOANT surpasses classical adversarial neural transfer, multitask learning, and meta-learning baselines using stochastic gradient descent (SGD) with a one-step look-ahead and sets a new state-ofthe-art F-score of 0.4101 on the iSarcasm dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Natural Language Processing(NLP) encompasses a wide range of tasks, from text categorization to question answering, but no matter what you do, the quantity of data you have to train your model has a significant influence on the model's performance.</p><p>Using the data you already have, data augmentation techniques are used to produce extra, synthetic data. Augmentation techniques are widely used in computer vision applications, but they may also be used in natural language processing.</p><p>In the instance of Twitter, (Van Hee et al., 2018) and <ref type="bibr" target="#b10">(Ili? et al., 2018)</ref> found that adding more data from the same domain did not improve the performance for recognizing sarcasm and irony. Although their result is not general for all sarcasm detection tasks and the result of data augmentation depends on the data and augmentation method.</p><p>(Lee et al., 2020)'s idea is to make a new datapoint out of the context sequence [c1, c2" cn] and label it "NOT SARCASM." The sequence could not be identified as "SARCASM" without the answer [r1]. They believe that the newly created negative samples will aid the model in focusing on the link between the response [r1] and its contexts [c1, c2, cn]. They also create positive samples using back-translation procedures <ref type="bibr" target="#b2">(Berard et al., 2019;</ref><ref type="bibr" target="#b32">Zheng et al., 2019)</ref> in French, Spanish, and Dutch to balance out the quantity of negative examples.</p><p>In <ref type="bibr">(Feng et al., 2020)</ref> different data augmentation methods were tested on Yelp Reviews dataset(Yel, 2014) for GPT-2 generative model <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>. They used "Random Insertion, Deletion, &amp; Swap", "Semantic Text Exchange (STE)", "Synthetic Noise", and "Keyword Replacement". They showed in some case data augmentation could help them to reach better performance.</p><p>This paper is the first to look at generative-based and mutation-based data augmentation strategies in sarcasm detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We mostly used the iSarcasm <ref type="bibr" target="#b18">(Oprea and Magdy, 2019)</ref> dataset in this study. In specific experiments, we integrated the primary dataset with various secondary datasets, including the Sarcasm Headlines Dataset <ref type="bibr" target="#b16">(Misra and Arora, 2019)</ref> and Sentiment140 dataset <ref type="bibr" target="#b6">(Go et al., 2009)</ref> to increase the quantity of data and compensate for the lack of sarcastic data. For each dataset, the details are further discussed. It is worthy to mention that all of the supplementary datasets we included had a negative impact on our model's performance. We believe this was the result of a different data gathering method. Because to the differing labeling process and domain, the distribution diverged from that of iSarcasm. As a result, the following sections are solely depen-dent on the iSarcasm dataset, with no other datasets being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Task Dataset: iSarcasm</head><p>According to <ref type="bibr" target="#b18">(Oprea and Magdy, 2019)</ref>, the sarcasm labeling using hashtags to build datasets captures just the sarcasm that the annotators were able to detect, leaving out the intended sarcasm. When the author intends for the content to be sarcastic, it is called intended sarcasm. The iSarcasm dataset includes 4484 tweets: 3707 non-sarcastic and 777 sarcastic. Because some tweets had been erased, we only had access to 3469 tweets for the job. The unbalanced dataset and the scarcity of sarcastic data were two of the most significant issues we encountered. <ref type="table" target="#tab_1">Table 1</ref> displays some of the dataset's annotated remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sarcasm Headlines Dataset</head><p>Sarcasm Headlines Dataset <ref type="bibr" target="#b16">(Misra and Arora, 2019;</ref><ref type="bibr" target="#b17">Misra and Grover, 2021)</ref> was gathered from two news websites. It is beneficial since it overcomes the constraints of Twitter datasets due to noise. As the second edition of this dataset includes more data and a greater variety of data than the first version, we chose the second version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentiment140 Dataset</head><p>We needed to compensate for the limited data to train our model successfully. As a result, we chose the sentiment140 dataset <ref type="bibr" target="#b6">(Go et al., 2009</ref>) because it has a large quantity of data and is based on Twitter. The sentiment tweet message is labeled using an automated classification approach in this dataset. The accuracy is more than 80% when using a machine learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this study we examined and analyzed various models and data augmentation strategies for sarcasm detection. First, we go through data augmentation methods; then, we discuss the structure and hyperparameters of these models in this section. The codes of all models are available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Generator-based</head><p>For this augmentation method, we used GPT-2 <ref type="bibr" target="#b19">(Radford et al., 2019</ref>) generative model to generate 4000 tweets for both sarcastic and non-sarcastic classes. Then we selected 2000 tweets of each class randomly to increase dataset quantity and have more sarcastic samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Mutation-based</head><p>We used three distinct ways to change the data in this method: eliminating, replacing with synonyms, and shuffling. These processes were used in the following order: shuffling, deleting, and replacing. The removal and replacement were carried out systematically. We used the words' roots to create a synonym dictionary. Synonym dictionary is created by scarping the Thesaurus website 2 . When a term was chosen to be swapped with its synonyms, we chose one of the synonyms randomly ( <ref type="figure">Figure  1)</ref>. We tried each combination of these processes to find the best data augmentation combination (a total of seven).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Support Vector Machine (SVM)</head><p>We utilized SVM to discover the optimal approaches for dataset preprocessing and word embeddings. For data augmentation, we employed both generator-based and mutation-based methods. We also put other data preprocessing approaches to the test, such as link removal, emoji removal, stop word removal, stemming, and lemmatizing. We utilized TF-IDF, Word2Vec <ref type="bibr" target="#b15">(Mikolov et al., 2013)</ref>, and BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> for word embedding. We found that using a regularization value of 10 and a Radial Basis Function (RBF) kernel, BERT word embedding, and no data preprocessing will give us the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">LSTM-based Methods</head><p>We begin with the intuition that a memory model can help us reach a better result. So we started with Long Short Term Memory (LSTM) model <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>. We used one LSTM layer followed by time distributed dense layer. We repeated these two layers one more time, and then we used another LSTM layer followed by two dense layers. This model and all of the following models in this section were trained in 10 epochs.</p><p>In addition, we used Bidirectional Long Short Term Memory (BLSTM). Using bidirectional will run the inputs in two directions, one from past to future and the other from future to past. We used one BLSTM layer for this network, followed by a time-distributed dense layer. We repeated these two layers one more time, and then we used another BLSTM layer followed by two dense layers. Furthermore, we combined LSTM and BLSTM with Convolutional Neural Networks (CNNs). CNN layers for feature extraction on input data are paired with LSTM to facilitate sequence prediction in the CNN-LSTM architecture. Although this model is often employed for video datasets, <ref type="bibr" target="#b21">(Rehman et al., 2019)</ref> demonstrated that it could perform better in sentiment analysis tasks. We used three 1D convolutional layers followed by a 1D global max-pooling layer for the convolutional part. We used these layers at the end of LSTM-based networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">BERT-based Methods</head><p>The use of bidirectional training of transformer and a prominent attention mode for language modeling is BERT's fundamental technological breakthrough <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>. The researchers describe a new Masked Language Model (MLM) approach that permits bidirectional training in previously tricky models. They found that bidirectionally trained language models can have a better understanding of language context and flow than unidirectional ones.</p><p>Robustly Optimized BERT or RoBERTa has a nearly identical architecture to BERT, however, the researchers made some minor adjustments to its architecture and training technique to enhance the results on BERT architecture .</p><p>We used both RoBERTa with twitter-robertabase, which has been trained on near 58 million tweets and finetuned for sentiment analysis with the TweetEval benchmark and BERT with bert-base from Huggingface <ref type="bibr" target="#b31">(Wolf et al., 2019)</ref>. For both models, we employed five epochs, batch size of 32, 500 warmup steps, and a weight decay of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Attention-based Methods</head><p>One of the most important achievements in deep learning research in the recent decade is the attention mechanism <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. The Encoder-Decoder model's restriction of encoding the input sequence to one fixed-length vector to decode each output time step is addressed via an attention mechanism. This difficulty is thought to be more prevalent when decoding extended sequences.</p><p>We start with the assumption that if a model with an attention layer is trained to identify sarcasm at the sentence level, the sarcastic words will be the ones the attention layer learns to value. As a result, we added an attention layer to our LSTM-based and BERT-based models. The results will be discussed further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Google's T5</head><p>Google's T5 <ref type="bibr" target="#b20">(Raffel et al., 2019)</ref> text-to-text model outperformed the human baseline on the GLUE, SQuAD, and CNN/Daily Mail datasets and earned a remarkable 88.9 on the SuperGLUE language benchmark.</p><p>We fine-tuned T5 for our problem and dataset by giving the sarcastic label the target and the tweets as the source. We used two epochs, batch size of 4,  512 tokenization max length, Adam epsilon of 1e-8, word decay of 0, no warmup steps, and learning rate of 3e-4 ( <ref type="figure" target="#fig_0">Figure 2</ref>) 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section we report the results of our models introduced in Section 4. It's important to note that after the competition, we discovered that none of our preprocessing strategies improved the performance of our model. So we were able to get an F1-sarcastic of 0.414 without using any preprocessing methods, which was 0.034 higher than our performance in the competition, which was based on the best combination of preprocessing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Support Vector Machine (SVM)</head><p>The optimum augmentation technique, preprocessing method, and word embedding were all determined using the SVM model. Without any augmentation, BERT obtained the greatest F1-sarcastic of 0.2862, compared to 0.2541 and 0.0924 for Word2Vec and TF-IDF, respectively.</p><p>We have also looked at several ways of data augmentation. The F1-sarcastics for shuffling with replacing words, only word elimination, just shuffling, and shuffling with word elimination were the highest in the mutation-based augmentation (Table 2). We also tried these data augmentation and GPT-2 data augmentation on RoBERTa because the results were close, and we found that merely word removal was the best data augmentation. The following results are based on no data preprocessing, BERT word embedding, and mutation-based data augmentation utilizing only word removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LSTM-based Methods</head><p>LSTM obtained an F1-sarcastic of 0.2176 using BERT word embeddings, mutation-based data augmentation, and no preprocessing, whereas BLSTM's F1-sarcastic was 0.2439 using BERT word embeddings, mutation-based data augmentation, and no preprocessing. By adding CNN layers, the F1-sarcastic of the LSTM was increased to 0.2453, and the BLSTM was increased to 0.2751. The CNN model's F1-sarcastic was 0.2263.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">BERT-based Methods</head><p>We employed a mutation-based data augmentation approach with no preprocessing for BERT-based procedures. We got an F1-sarcastic of 0.323 using BERT. We achieved our best result with RoBERTa with an F1-sarcastic of 0.414, which was better than LOANT <ref type="bibr" target="#b8">(Guo et al., 2021</ref>) model on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Attention-based Methods</head><p>Adding attention layers to this job was not helpful, and it decreased our models' performance. RoBERTa's F1-sarcastic dropped to 0.2959 using the attention layer. LSTM model with the attention layer earned an F1-sarcastic of 0.2145. The F1-sarcastic of BLSTM with attention layer was 0.2336.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Google's T5</head><p>Based on the hyperparameters listed in the Section 4, our F1-sarcastic for this model is 0.4038. However, we believe that we may get better results by increasing the tokenization max length, increasing the batch size, and utilizing the t5-large pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we reviewed and contrasted a number of sarcasm detection methods. To improve the performance of our model, we experimented with two different types of augmentation. In the job of sarcasm detection, we observed that mutation-based data augmentation can assist us in achieving better results than generative-based data augmentation. Additionally, we tested with other deep-learning techniques, including RNN and BERT-based models. Our best system, an ensemble model, has an F1-sarcastic of 0.414.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Fine-tuning T5 model for sarcasm detection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Example of Sarcastic and Non-Sarcastic tweets.</figDesc><table><row><cell>Tweet</cell><cell>Sarcastic</cell><cell>Sarcasm Type</cell></row><row><cell>Oh my goodness. It's the first week of the</cell><cell></cell><cell></cell></row><row><cell>summer holidays and @name has found</cell><cell>Sarcastic</cell><cell>['Sarcasm']</cell></row><row><cell>his recorder Give.Me.Strength.</cell><cell></cell><cell></cell></row><row><cell>90% of adulthood is just refilling your @name pitcher.</cell><cell>Sarcastic</cell><cell>['Irony', 'overstatement']</cell></row><row><cell>True bliss is laying in an ice</cell><cell></cell><cell></cell></row><row><cell>cold bath during the hottest part of the year</cell><cell>Non-Sarcastic</cell><cell>[]</cell></row><row><cell>Figure 1: Effect of shuffling, word elimination, and re-</cell><cell></cell><cell></cell></row><row><cell>placing with synonyms on a tweet sample.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>F1-sarcastic and accuracy for different data augmentation methods on SVM model with BERT word embedding and no preprocessing.</figDesc><table><row><cell>Data Augmentation</cell><cell cols="2">F1-sarcastic Accuracy</cell></row><row><cell>Shuffling</cell><cell>0.305</cell><cell>0.7471</cell></row><row><cell>Shuffling + Replacing</cell><cell>0.301</cell><cell>0.741</cell></row><row><cell>Shuffling + Removing</cell><cell>0.306</cell><cell>0.747</cell></row><row><cell>Removing</cell><cell>0.301</cell><cell>0.747</cell></row><row><cell>GPT-2</cell><cell>0.292</cell><cell>0.675</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Best results for each model using iSarcasm dataset and mutation-based data augmentation.</figDesc><table><row><cell>Model</cell><cell cols="2">F1-sarcastic Accuracy</cell></row><row><cell>SVM</cell><cell>0.3064</cell><cell>0.7478</cell></row><row><cell>LSTM-based</cell><cell>0.2751</cell><cell>0.7251</cell></row><row><cell>BERT-based</cell><cell>0.414</cell><cell>0.8634</cell></row><row><cell>Attention-based</cell><cell>0.2959</cell><cell>0.7793</cell></row><row><cell>Google's T5</cell><cell>0.4038</cell><cell>0.8124</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/AmirAbaskohi/SemEval2022-Task6-Sarcasm-Detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.thesaurus.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We were not able to test a larger version of the model with better hyperparameters due to resource constraints</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We want to convey our heartfelt gratitude to Prof. Yadollah Yaghoobzadeh, who provided us with invaluable advice during our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.yelp.com/dataset_challenge" />
		<title level="m">Yelp. yelp open dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2022 Task 6: iSarcas-mEval, Intended Sarcasm Detection in English and Arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Ibrahim Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</title>
		<meeting>the 16th International Workshop on Semantic Evaluation (SemEval-2022)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Naver labs europe&apos;s systems for the wmt19 machine translation robustness task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioan</forename><surname>Calapodescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Roux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06488</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised recognition of sarcasm in twitter and amazon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth conference on computational natural language learning</title>
		<meeting>the fourteenth conference on computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01794</idno>
		<title level="m">Teruko Mitamura, and Eduard Hovy. 2020. Genaug: Data augmentation for finetuning text generators</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision. CS224N project report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: a closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Gonz?lez-Ib?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Latent-optimized adversarial neural transfer for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09261</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzana</forename><surname>Ili?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Balazs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09795</idno>
		<title level="m">Deep contextualized word representations for detecting sarcasm and irony</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining twitter for finegrained political opinion polarity classification, ideology detection and sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeepa</forename><surname>Kannangara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="751" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Augmenting data for sarcasm detection with unlabeled conversation context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hankyol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06259</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prahal</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07414</idno>
		<title level="m">Sarcasm detection using hybrid neural network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sculpting data for ml: The first act of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigyasa</forename><surname>Grover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03123</idno>
		<title level="m">isarcasm: A dataset of intended sarcasm</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hybrid cnn-lstm model for improving accuracy of movie reviews sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">Kamran</forename><surname>Anwar Ur Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basit</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqar</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="26597" to="26613" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From humor recognition to irony detection: The figurative language of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A multidimensional approach for detecting irony in twitter. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="239" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sarcasm as contrast between a positive sentiment and negative situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalindra De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="704" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02856</idno>
		<title level="m">Reasoning with sarcasm by reading inbetween</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognition of sarcasm in microblogging based on sentiment analysis and coherence identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyoros</forename><surname>Tungthamthiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoaki</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masnizah</forename><surname>Mohd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="383" to="405" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval-2018 task 3: Irony detection in english tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Van Hee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ronique</forename><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Systematizing creativity: A computational view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Am?lcar</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>P?rez Y P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Creativity</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A corpus for research on deliberation and debate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean E Fox</forename><surname>Tree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="812" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust machine translation with domain sensitive pseudo-sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08393</idno>
	</analytic>
	<monogr>
		<title level="m">Baidu-osu wmt19 mt robustness shared task system report. arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
