<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Cross-Modal Context Graph for Visual Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Queens University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Cross-Modal Context Graph for Visual Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual grounding is a ubiquitous building block in many vision-language tasks and yet remains challenging due to large variations in visual and linguistic features of grounding entities, strong context effect and the resulting semantic ambiguities. Prior works typically focus on learning representations of individual phrases with limited context information. To address their limitations, this paper proposes a languageguided graph representation to capture the global context of grounding entities and their relations, and develop a crossmodal graph matching strategy for the multiple-phrase visual grounding task. In particular, we introduce a modular graph neural network to compute context-aware representations of phrases and object proposals respectively via message propagation, followed by a graph-based matching module to generate globally consistent localization of grounding phrases. We train the entire graph neural network jointly in a two-stage strategy and evaluate it on the Flickr30K Entities benchmark. Extensive experiments show that our method outperforms the prior state of the arts by a sizable margin, evidencing the efficacy of our grounding framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Integrating visual scene and natural language understanding is a fundamental problem toward achieving human-level artificial intelligence, and has attracted much attention due to rapid advances in computer vision and natural language processing <ref type="bibr" target="#b10">(Mogadala, Kalimuthu, and Klakow 2019)</ref>. A key step in bridging vision and language is to build a detailed correspondence between a visual scene and its related language descriptions. In particular, the task of grounding phrase descriptions into their corresponding image has become an ubiquitous building block in many vision-language applications, such as image retrieval <ref type="bibr" target="#b6">(Justin et al. 2015;</ref><ref type="bibr" target="#b10">Nam et al. 2019</ref>  <ref type="bibr" target="#b7">Kottur et al. 2018)</ref>.</p><p>General visual grounding typically attempts to localize object regions that correspond to multiple noun phrases in image descriptions. Despite significant progress in solving vision <ref type="bibr" target="#b14">(Ren et al. 2015;</ref><ref type="bibr" target="#b23">Zhang et al. 2017)</ref> or language <ref type="bibr" target="#b13">(Peters et al. 2018;</ref><ref type="bibr" target="#b2">Devlin et al. 2018</ref>) tasks, it remains challenging to establish such cross-modal correspondence between objects and phrases, mainly because of large variations in object appearances and phrase descriptions, strong context dependency among these grounding entities, and the resulting semantic ambiguities in their representations <ref type="bibr" target="#b13">(Plummer et al. 2015;</ref><ref type="bibr" target="#b13">Plummer et al. 2018)</ref>.</p><p>Many existing works on visual grounding tackle the problem by localizing each noun phrase independently via phrase-object matching <ref type="bibr" target="#b13">(Plummer et al. 2015;</ref><ref type="bibr" target="#b13">Plummer et al. 2018;</ref><ref type="bibr" target="#b22">Yu et al. 2018b;</ref>. However, such grounding strategy tends to ignore visual and linguistic context, thus leading to matching ambiguity or errors for complex scenes. Only a few grounding approaches take into account context information <ref type="bibr" target="#b11">(Pelin, Leonid, and Markus 2019;</ref><ref type="bibr" target="#b1">Chen, Kovvuri, and Nevatia 2017)</ref> or phrase relationship <ref type="bibr" target="#b13">Plummer et al. 2017</ref>) when representing visual or phrase entities. While they partially alleviate the problem of grounding ambiguity, their context or relation representations have several limitations for capturing global structures in language descriptions and visual scenes. First, for language context, they typically rely on chainstructured LSTMs defined on description sentences, which have difficulty in encoding long-range dependencies among phrases. In addition, most methods simply employ off-theshelf object detectors to generate object candidates for crossmodal matching. However, it is inefficient to encode visual context for those objects due to a high ratio of false positives in such object proposal pools. Furthermore, when incorporating phrase relations, these methods often adopt a stagewise strategy that learns representations of noun phrases and their relationship separately, which is sub-optimal for the overall grounding task.</p><p>In this work, we propose a novel cross-modal graph network to address the aforementioned limitations for multiple-phrase visual grounding. Our main idea is to exploit the language description to build effective global context representations for all the grounding entities and their relations, which enables us to generate a selective set of high-quality object proposals from an image and to develop a contextaware cross-modal matching strategy. To achieve this, we design a modular graph neural network consisting of four main modules: a backbone network for extracting basic language and visual features, a phrase graph network for encoding phrases in the sentence description, a visual object graph network for computing object proposal features and a graph similarity network for global matching between phrases and object proposals.</p><p>Specifically, given an image and its textual description, we first use the backbone network to compute the language embedding for the description, and to generate an initial set of object proposals. To incorporate language context, we construct a language scene graph from the description (e.g., <ref type="bibr" target="#b15">Schuster et al. 2015;</ref><ref type="bibr" target="#b18">Wang et al. 2018b)</ref> in which the nodes are noun phrases, and the edges encode relationships between phrases. Our second module, phrase graph network, is defined on this language scene graph and computes a context-aware phrase representation through message propagation on the phrase graph. We then use the phrase graph as a guidance to build a visual scene graph, in which the nodes are object proposals relevant to our phrases, and the edges encode the same type of relations as in the phrase graph between object proposals. The third network module, visual object graph network, is defined on this derived graph and generates a context-aware object representation via message propagation. Finally, we introduce a graph similarity network to predict the global matching of those two graph representations, taking into account similarities between both graph nodes and relation edges.</p><p>We adopt a two-stage strategy in our model learning, of which the first stage learns the phrase graph network and visual object features while the second stage trains the entire deep network jointly. We validate our approach by extensive experiments on the public benchmark Flickr30K Entities <ref type="bibr" target="#b13">(Plummer et al. 2015)</ref>, and our method outperforms the prior state of the art by a sizable margin. To better understand our method, we also provide the detailed ablative study of our context graph network.</p><p>The main contributions of our work are three-folds:</p><p>? We propose a language-guided graph representation, capable of encoding global contexts of phrases and visual objects, and a globally-optimized graph matching strategy for visual grounding.</p><p>? We develop a modular graph neural network to implement the graph-based visual grounding, and a two-stage learning strategy to train the entire model jointly.</p><p>? Our approach achieves new state-of-the-art performance on the Flickr30K Entities benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Visual Grounding: In general, visual grounding aims to localize object regions in an image corresponding to multiple noun phrases from a sentence that describes the un-derlying scene.  proposed an attention mechanism to attend to relevant object proposals for a given phrase and designed a loss for phrase reconstruction. <ref type="bibr" target="#b13">Plummer et al. (2018)</ref> presented an approach to jointly learn multiple text-conditioned embedding in a single endto-end network. In DDPN <ref type="bibr" target="#b22">(Yu et al. 2018b</ref>), they learned a diversified and discriminate proposal network to generate higher quality object candidates. Those methods grounded each phrase independently, ignoring the context information in image and language. Only a few approaches attempted to solve visual grounding by utilizing context cues. <ref type="bibr" target="#b8">Chen et al. (2017)</ref> designed an additional reward by incorporating context phrases and train the whole network by reinforcement learning. <ref type="bibr">Dongan et al. (2019)</ref> took context into account by adopting chain-structured LSTMs network to encode context cues in language and image respectively. In our work, we aim to build cross-modal graph networks under the guidance of language structure to learn global context representation for grounding entities and object candidates.</p><p>Referring Expression: Referring expression comprehension is closely related to visual grounding task, which attempts to localize expressions corresponding to image regions. Unlike visual grounding, those expressions are typically region-level descriptions without specifying grounding entities. <ref type="bibr" target="#b10">Nagaraja et al. (2016)</ref> proposed to utilize LSTMs to encode visual and linguistic context information jointly for referring expression. <ref type="bibr" target="#b21">Yu et al. (2018a)</ref> developed modular attention network, which utilized language-based attention and visual attention to localize the relevant regions. <ref type="bibr">Wang et al. (2019)</ref> applied self-attention mechanism on sentences and built a directed graph over neighbour objects to model their relationships. All the above-mentioned methods fail to explore the structure of the expression explicitly. Our focus is to exploit the language structure to extract cross-modal context-aware representations.</p><p>Structured Prediction: Structured prediction is a framework to solve the problems whose output variables are mutually dependent or constrained. <ref type="bibr" target="#b6">Justin et al. (2015)</ref> proposed the task of scene graph grounding to retrieve images, and formulated the problem as structured prediction by taking into account both object and relationship matching. To explore the semantic relations in visual grounding task, <ref type="bibr" target="#b19">Wang et al. (2016)</ref> tried to introduce a relational constraint between phrases, but limited their relations to possessive pronouns only. <ref type="bibr" target="#b13">Plummer et al. (2017)</ref> extended the relations to attributes, verbs, prepositions and pronouns, and performed global inference during test stage. We extend these methods by exploiting the language structure to get context-aware cross-modal representations and learn the matching between grounding entities and their relations jointly.</p><p>3 Problem Setting and Overview</p><p>The task of general visual grounding aims to localize a set of object regions in an image, each corresponding to a noun phrase in a sentence description of the image. Formally, given an image I and a description Q, we denote a set of noun phrases for grounding as P = {p i } N i=1 and their cor- the Phrase Graph Network is defined on the a parsed language scene graph to refine language representations; the Visual Object Graph Network is defined on a visual scene graph which is constructed under the guidance of the phrase graph to refine visual object feature; finally a Graph Similarity Network predicts the global matching of those two graph representations. Solid circles denote noun phrase features while solid squares represent relation phrase features. Hollow circles and squares denote visual object and relation features respectively.</p><formula xml:id="formula_0">responding locations as B = {b i } N i=1 where b i ? R 4 is the</formula><p>bounding box parameters. Our goal is to predict the set B for a given set P from the input I and Q.</p><p>To this end, we adopt a hypothesize-and-match strategy that first generates a set of object proposals O = {o m } M m=1 and then formulates the grounding task as a matching problem, in which we seek to establish a cross-modal correspondence between the phrase set P and the object proposal set O. This matching task, nevertheless, is challenging due to large variations in visual and linguistic features, strong context dependency among the grounding entities and the resulting semantic ambiguities in pairwise matching.</p><p>To tackle those issues, we propose a language-guided approach motivated by the following three key observations: First, language prior can be used to generate a graph representation of noun phrases and their relations, which captures the global context dependency more effectively than chain-structured models. In addition, the object proposals generated by detectors typically have a high ratio of false positives, and hence it is difficult to encode visual context for each object. We can exploit language structure to guide proposal pruning and build a better context-aware visual representation. Finally, the derived phrase graph structure also includes the phrase relations, which provide additional constraints in the matching for mitigating ambiguities.</p><p>We instantiate these ideas by designing a cross-modal graph network for the visual grounding task, which consists of four main modules: a) a backbone network that extracts basic linguistic and visual features; b) a phrase graph network defined on a language scene graph built from the description to compute the context-aware phrase representations; c) a visual graph network defined on a visual scene graph of object proposals constructed under the guidance of the phrase graph, and encodes context cues for the object representations via message propagation; and d) a graph similarity network that predicts a global matching of the two graph representations. The overall model is shown in <ref type="figure" target="#fig_1">Fig. 1</ref> and we will describe the details of each module in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-modal Graph Network</head><p>We now introduce our cross-modal graph matching strategy, including the model design of four network modules and the overall inference pipeline, followed by our two-stage model training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Backbone Network</head><p>Our first network module is a backbone network that takes as input the image I and description Q, and generates corresponding visual and linguistic features. The backbone network consists of two sub-networks: a convolutional network for generating object proposals and a recurrent network for encoding phrases.</p><p>Specifically, we adopt the ResNet-101 ) as our convolutional network to generate feature map ? with channel dimension of D 0 . We then apply a Region Proposal Network (RPN) <ref type="bibr" target="#b14">(Ren et al. 2015)</ref> to generate an initial set of object proposals O = {o m } M m=1 , where o m ? R 4 denotes object location (i.e. bounding box parameters). For each o m ? O, we use RoI-Align <ref type="bibr" target="#b5">(He et al. 2017</ref>) and average pooling to compute a feature vector x a om ? R D0 . We also encode the relative locations of conv-features as a spatial feature vector x s om (See Suppl. for details), which is fused with x a om to produce the object representation:</p><formula xml:id="formula_1">x om = F vf ([x a om ; x s om ])<label>(1)</label></formula><p>where x om ? R D , F vf is a multilayer network with fully connected layers and [; ] is the concatenate operation.</p><p>For the language features, we generate an embedding of noun phrase p i ? P. To this end, we first encode each word in sentence Q into a sequence of word embedding {h t } t=1...T with a Bi-directional GRU <ref type="bibr" target="#b2">(Chung et al. 2014)</ref>, where T is the number of words in sentence. We then compute the phrase representation x pi by taking average pooling on the word representations in each p i :</p><formula xml:id="formula_2">[h 1 , h 2 , . . . , h T ] = BiGRU p (Q)</formula><p>(2)</p><formula xml:id="formula_3">x pi = 1 |p i | t?pi h t i = 1, ? ? ? , N (3) where BiGRU p denotes the bi-directional GRU, h t , x pi ? R D and h t = [ ? h t ; ? h t ]</formula><p>is the concatenation of forward and backward hidden states for t-th word in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Phrase Graph Network</head><p>To encode the context dependency among phrases, we now introduce our second module, the phrase graph network, which refines the initial phase embedding features by incorporating phrase relations cues in the description.</p><p>Phrase Graph Construction Specifically, we first build a language scene graph from the image description by adopting an off-the-shelf scene graph parser 1 , which also extracts the phrase relations R = {r ij } from Q, where r ij is a relationship phrase that connects p i and p j . We denote the language scene graph as G L = {P, R} where P and R are the nodes and edges set respectively. Similar to the phrases in Sec. 4.1, we compute an embedding x rij for r ij ? R based on a second bi-directional GRU, denoted as BiGRU r .</p><p>On top of the language scene graph, we construct a phrase graph network that refines the linguistic features through message propagation. Concretely, we associate each node p i in the graph G L with its embedding x pi , and each edge r ij with its vector representation x rij . We then define a set of message propagation operators on the graph to generate context-aware representations for all the nodes and edges as follows.</p><p>Phrase Feature Refinement We introduce two types of message propagation operators to update the node and edge feature respectively. First, to enrich each phrase relation with its subject and object nodes, we send out messages from the noun phrases, which are encoded by their features, to update the relation representation via aggregation:</p><formula xml:id="formula_4">x c rij = x rij + F l e ([x pi ; x pj ; x rij ])<label>(4)</label></formula><p>where x c rij ? R D is the context-aware relation feature, and F l e is a multilayer network with fully connected layers. The second message propagation operator update each phrase node p i by aggregating features from all its neighbour nodes N (i) and edges via an attention mechanism:</p><formula xml:id="formula_5">x c pi = x pi + j?N (i) w pij F l p ([x pj ; x c rij ])<label>(5)</label></formula><p>where x c pi is the context-aware phrase feature, F l p is a multilayer network, and w pij is an attention weight between node p i and p j , which is defined as follows:</p><formula xml:id="formula_6">w pij = Softmax j?N (i) (F l p ([x pi ; x c rij ]) F l p ([x pj ; x c rij ]))<label>(6)</label></formula><p>Here Softmax is a softmax function to compute normalized attention values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Object Graph Network</head><p>Similar to the language counterpart, we also introduce a visual scene graph to capture the global scene context for each object proposal, and to build our third module, the visual object graph network, which enriches object features with their contexts via message propagation over the visual graph.</p><p>Visual Scene Graph Construction Instead of using a noisy dense graph <ref type="bibr" target="#b6">(Hu et al. 2019)</ref>, we propose to construct a visual scene graph relevant to the grounding task by exploiting the knowledge of our phrase graph G L . To this end, we first prune the object proposal set to keep the objects relevant to the grounding phrases, and then consider only the pairwise relations induced by the phrase graph. Specifically, we adopt the method in <ref type="bibr" target="#b13">(Plummer et al. 2015;</ref> to select a small set of highquality proposals O i for each phrase p i . To achieve this, we first compute a similarity score ? p i,m for each phraseboxes pair p i , o m and a phrase-specific regression offset ? p i,m ? R 4 for o m based on the noun phrase embedding x c pi and each object feature x om as follows:</p><formula xml:id="formula_7">? p i,m = F p cls (x c pi , x om ), ? p i,m = F p reg (x c pi , x om ) (7)</formula><p>where F p cls and F p reg are two-layer fully-connected networks which transform the input features as in <ref type="bibr" target="#b9">(Lili et al. 2016)</ref>.</p><p>We then select the top K(K M ) for each phrase p i based on the similarity score ? p i,m , and apply the regression offsets ? p i,m to adjust locations of the selected proposals. We denote the refined proposal set of p i as O i = {o i,k } K k=1 and all the refined proposals as V = ? N i=1 O i . For each pair of the object proposals o i,k , o j,l , we introduce an edge u ij,kl if there is a relation r ij exists in the phrase relation set R. Denoting the edge set as U = {u ij,kl }, we define our visual scene graph as G V = {V, U}.</p><p>Built on top of the visual scene graph, we introduce a visual object graph network that augments the object features with their context through message propagation. Concretely, as in Sec. 4.1, we extract an object feature x o i,k for each proposal o i,k in V. Additionally, for each edge u ij,kl in the graph G V , we take a union box region of two object o i,k and o j,l , which is the minimum box region covering both objects, and compute its visual relation feature x u ij,kl . To do this, we extract a convolution feature x a u ij,kl from ? by RoI-Align, and as in the object features, fuse it with a geometric feature x s u ij,kl encoding location of two objects (See Suppl. for details). We then develop a set of message propagation operators on the graph to generate context-aware representations for all the nodes and edges in the following.</p><p>Visual Feature Refinement Similar to Sec. 4.2, we introduce two types of message propagation operators to refine the object and relation features respectively. Specifically, we first update relation features by fusing with their subject and object node features:</p><formula xml:id="formula_8">x c u ij,kl = x u ij,kl + F v e ([x o i,k ; x o j,l ; x u ij,kl ]) (8) where F v</formula><p>e is a multilayer network with fully connected layers. The second type of message update each object node o i,k by aggregating features from all its neighbour nodes and corresponding edges via the same attention mechanism:</p><formula xml:id="formula_9">x c o i,k = x o i,k + j,l ? ij,kl F v o ([x o j,l ; x c u ij,kl ]) (9) ? ij,kl = Softmax j,l (F v o ([x o i,k ; x c u ij,kl ]) F v o ([x o j,l ; x c u ij,kl ]) where x c o i,k is the context-aware object feature, F v o</formula><p>is a multilayer network and ? ij,kl is the attention weight between object o i,k and o j,l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Graph Similarity Network</head><p>Given the phrase and visual scene graph, we formulate the visual grounding as a graph matching problem between two graphs. To solve this, we introduce a graph similarity network to predict the node and edge similarities between the two graphs, followed by a global inference procedure to predict the matching assignment.</p><p>Formally, we introduce a similarity score ? i,k for each noun phrase and visual object pair x c pi , x c o i,k , and an edge similarity score ? ij,kl for each phrase and visual relation pair x c ri,j , x c u ij,kl . For the node similarity ? i,k , we first predict a similarity between the refined features x c pi , x c o i,k as in Sec. 4.3, using two-layer fully-connected networks to compute the similarity score and the object offset as follows,</p><formula xml:id="formula_10">? g i,k = F g cls (x c pi , x c o i,k ) ? g i,k = F g reg (x c pi , x c o i,k ) (10)</formula><p>We then fuse this with the score used in object pruning to generate the node similarity: ? i,k = ? p i,k ?? g i,k . The predicted offset is applied to the proposals in the prediction outcome. For the edge similarity, we take the same method as in the node similarity prediction, using a multilayer network F r cls to predict the edge similarity score ? ij,kl :</p><formula xml:id="formula_11">? ij,kl = F r cls (x c rij , x c u ij,kl )<label>(11)</label></formula><p>Given the node and edge similarity scores, we now assign each phrase-object pair a binary variable s i,k ? {0, 1} indicating whether o i,k is the target location of p i . Assuming only one proposal is selected, i.e., K k=1 s i,k = 1, our subgraph matching can be formulated as a structured prediction problem as follows:</p><formula xml:id="formula_12">s * =arg max s i,k ? i,k s i,k + ? i,j,k,l ? ij,kl s i,k ? s j,l s.t. K k=1 s i,k = 1; i = 1, . . . , N<label>(12)</label></formula><p>where ? is a weight balancing the phrase and relation scores. We solve the assignment problem by an approximate algorithm based on exhaustive search with a maximal depth (see Suppl. for detail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Learning</head><p>We adopt a pre-trained ResNet-101 network and an off-theshelf RPN in our backbone network, and train the remaining network modules. In order to build the visual scene graph, we adopt a two-stage strategy in our model learning. The first stage learns the phrase graph network and object features by a phrase-object matching loss and a box regression loss. We use the learned sub-modules to select a subset of proposals and construct the rest of our model. The second stage trains the entire deep model jointly with a graph similarity loss and a box regression loss. Specifically, for a noun phrase p i , the ground-truth for matching scores ?</p><formula xml:id="formula_13">p i = {? p i,m } M m=1 and ? g i = {? g i,k } K k=1 are defined as soft label distributions Y p i = {y p i,m } M m=1 and Y g i = {y g i,k } K k=1</formula><p>respectively, based on the IoU between proposal bounding boxes and their ground-truth <ref type="bibr" target="#b22">(Yu et al. 2018b)</ref>.</p><p>Similarly, we compute the ground-truth offset ? p * i,m between b i and o m , ? g * i,k between b i and o i,k . In addition, the ground-truth for matching scores ? r ij = {? ij,kl } K k,l=1 are defined as Y r ij = {y r ij,kl } K k,l=1 based on the IoU between a pair of object proposals o i,k , o j,l and their ground-truth locations b i , b j <ref type="bibr" target="#b20">(Yang et al. 2018)</ref>.</p><p>After normalizing Y p i , Y g i and Y r ij to probability distributions, we define the matching loss L p mat and regression loss L p reg in the first stage as follows:</p><formula xml:id="formula_14">L p mat = i L ce (? p i , Y p i ) L p reg = i 1 ||Y p i || 0 m I(y p i,m &gt; 0)L sm (? p i,m , ? p * i,m ) (13)</formula><p>where L ce is the Cross Entropy loss and L sm is the Smooth-L1 loss. For the second stage, the node matching loss L g mat , edge matching loss L r mat and regression loss L g reg are defined as:</p><formula xml:id="formula_15">L g mat = i L ce (? g i , Y g i ), L r mat = i,j L ce (? r ij , Y r ij ) L g reg = i 1 ||Y g i || 0 k I(y g i,k &gt; 0)L sm (? g i,k , ? g * i,k ) (14)</formula><p>Here || * || 0 is the L0 norm and I is the indicator function. Finally the total loss L can be defined as:</p><formula xml:id="formula_16">L = L p mat + ? 1 ? L p reg + ? 2 ? L g mat + ? 3 ? L r mat + ? 4 ? L g reg<label>(15)</label></formula><p>where ? 1 , ? 2 , ? 3 , ? 4 are weighting coefficients for balancing loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Metrics</head><p>We evaluate our approach on Flickr30K Entities <ref type="bibr" target="#b13">(Plummer et al. 2015)</ref> dataset, which contains 32k images, 275k bounding boxes, and 360k noun phrases. Each image is associated with five sentences description and the noun phrases Methods Accuracy(%) SMPL  42.08 NonlinearSP <ref type="bibr" target="#b19">(Wang, Li, and Lazebnik 2016)</ref> 43.89 GroundeR  47.81 MCB <ref type="bibr" target="#b3">(Fukui et al. 2016)</ref> 48.69 RtP <ref type="bibr" target="#b13">(Plummer et al. 2015)</ref> 50.89 Similarity Network <ref type="bibr" target="#b17">(Wang et al. 2018a)</ref> 51.05 IGOP <ref type="bibr" target="#b20">(Yeh et al. 2017)</ref> 53.97 SPC+PPC <ref type="bibr" target="#b13">(Plummer et al. 2017)</ref> 55.49 SS+QRN <ref type="bibr" target="#b1">(Chen, Kovvuri, and Nevatia 2017)</ref> 55.99 CITE <ref type="bibr" target="#b13">(Plummer et al. 2018)</ref> 59.27 SeqGROUND <ref type="bibr" target="#b11">(Pelin, Leonid, and Markus 2019)</ref> 61.60 Our approach  67.90 DDPN <ref type="bibr" target="#b22">(Yu et al. 2018b)</ref> 73.30 Our approach  76.74</p><p>are provided with their corresponding bounding boxes in the image. Following , if a single noun phrase corresponds to multiple ground-truth bounding boxes, we merge the boxes and use the union region as their ground-truth. We adopt the standard dataset split as in <ref type="bibr" target="#b13">Plummer et al. (2015)</ref>, which separates the dataset into 30k images for training, 1k for validation and 1k for testing. We consider a noun phrase grounded correctly when its predicted box has at least 0.5 IoU with its ground-truth location. The grounding accuracy (i.e., Recall@1) is the fraction of correctly grounded noun phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We generate an initial set of M = 100 object proposals with a RPN from Anderson et al. (2018) 2 . We use the output of ResNet C4 block as our feature map ? with channel dimension D 0 = 2048 and the visual object features are obtained by applying RoI-Align with resolution 14?14 on ?. The embedding dimension D of phrase and visual representation is set as 1024. In visual graph construction, we select the most K = 10 relevant object candidates for each noun phrase.</p><p>For model training, we use SGD optimizer with initial learning rate 5e-2, weight decay 1e-4 and momentum 0.9. We train 60k iterations with batch-size 24 totally and decay the learning rate 10 times in 20k and 40k iterations respectively. The loss weights of regression terms ? 1 and ? 4 are set to 0.1 while matching terms ? 2 and ? 3 are set to 1. During the test stage, we search an optimal weight ? * ? [0, 1] on val set and apply it to test set directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Comparisons</head><p>We report the performance of the proposed framework on the Flickr30K Entities test set and compare it with several the state-of-the-art approaches. Here we consider two model configurations for proper comparisons, which use an ResNet-50 3 and an ResNet-101 as their backbone network, respectively.</p><p>As shown in Tab. 1, our approach outperforms the prior methods by a large margin in both settings. In particular, 2 It is based on FasterRCNN <ref type="bibr" target="#b14">(Ren et al. 2015)</ref> with ResNet-101 as its backbone, trained on Visual Genome dataset <ref type="bibr" target="#b8">(Krishna et al. 2017)</ref>. We use its RPN to generate object proposals.</p><p>3 Model details of ResNet-50 backbone are included in Suppl. our model with ResNet-101 backbone achieves 76.74% in accuracy, which improves 3.44% compared to DDPN <ref type="bibr" target="#b22">(Yu et al. 2018b)</ref>. For the setting that uses ResNet-50 backbone and a pretrained RPN on MSCOCO <ref type="bibr" target="#b10">(Lin et al. 2014</ref>) dataset, we can see that our model achieves 67.90% in accuracy and outperforms SeqGROUND by 6.3%. We also show detailed comparisons per coarse categories in Tab. 2 and it is evident that our approach achieves better performances consistently on most categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>In this section, we perform several experiments to evaluate the effectiveness of individual components, investigate hyper-parameter K and the impact of relations feature in two graphs in our framework with ResNet-101 as the backbone on Flickr30k val set 4 , which is shown in Tab. 3 and Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline:</head><p>The baseline first predicts the similarity score and regression offset for each phrase-box pair x pi , x om , and then selects the most relevant proposal followed by applying its offset. Our baseline grounding accuracy achieves 73.46% with ResNet-101 backbone.</p><p>Phrase Graph Net (PGN): PGN propagate language context cues via the scene graph structure effectively. The noun phrases feature can not only be aware of long-term semantic contexts from the other phrases but also enriched by its relation phrases representation. The experiment shows that our PGN can improve the accuracy from 73.46% to 74.40%.</p><p>Proposal Pruning (PP): The quality of proposals generation plays an important role in visual grounding task. Here we take proposal pruning operation by utilizing PGN, which can help reduce more ambiguous object candidates with language contexts. We can see a significant improvement of 1.1% accuracy.</p><p>Visual Object Graph Net (VOGN): When integrating the VOGN into the whole framework, we can achieve 75.85% accuracy, which is better than the direct matching with the phrase graph. This suggests that the object representation can be more discriminative after conducting message passing among context visual object features 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structured Prediction (SP):</head><p>The aforementioned PGN and VOGN take the context cues into consideration during their nodes matching. Our approach, by contrast, explicitly  (a1) A man wearing jeans and a button up dress shirt is holding a camera while standing next to a woman in black pants and a beige jacket (a2) A man in a striped shirt hugging a blond short-haired woman with a black apron on (b1) A guy sitting in a chair with a mug on the table next to him (b2) A man in a black jacket is riding a horse on a public sidewalk (d1) A reporter is being taped during the storm (d2) One monkey is jumping near another monkey on the top of a wooden structure takes the cross-modal relation matching into account and predicts the final result via a global optimization. We can see further improvement of accuracy from 75.85% to 76.19%.</p><p>Hyper-parameter K and Relations Feature: In Tab.4, our framework achieves the highest accuracy when K = 10 while K = 5 will result in performance dropping from 76.19% to 74.97% due to the lower proposals recall. When K = 20, our model will get a comparable performance but consume more computation resources and inference time.</p><p>We also perform experiments to show the impact of relation phrases and visual relations in PGN and VOGN in Tab. 3. For PGN, the performance will drop from 74.40% to 74.11% without phrase relations x c rij . And we can see 0.41% performance drop when ignoring both phrase relations x c rij and visual relations x c u ij,kl in PGN and VOGN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Visualization Results</head><p>We show some qualitative visual grounding results in <ref type="figure" target="#fig_2">Fig.2</ref> to demonstrate the capabilities of our framework in challenging scenarios. In (a1) and (a2), our framework is able to successfully localize multiple entities in the long sentences without ambiguity. With the help of VOGN, we can see that our model localize a mug close to man correctly rather than another mug in the left bottom in (b1). Column 3 shows that relations constraint can help refine the final prediction. The last column is failure cases. Our model cannot ground objects in images correctly with severe visual ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a context-aware cross-modal graph network for visual grounding task. Our method exploits a graph representation for language description, and transfers the linguistic structure to object proposals to build a visual scene graph. Then we use message propagation to extract global context representations both for the grounding entities and visual objects. As a result, it is able to conduct a global matching between both graph nodes and relation edges. We present a modular graph network to instantiate our core idea of context-aware cross-modal matching. Moreover, we adopt a two-stage strategy in our model learning, of which the first stage learns a phrase graph network and visual object features while the second stage trains the entire deep network jointly. Finally, we achieve the state-of-the-art performances on Flickr30K Entities benchmark, and outperform other approaches by a sizable margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Spatial Feature of Union Region</head><p>We generate a two-channel binary mask for o i,k and o j,l separately where locations within object proposal o i,k , o j,l fill 1 and others fill 0. Then the two-channel binary mask is resized to 64 ? 64. And we use multiple fully connected layers to embed it to a geometric feature vector x s u ij,kl ? R 256 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Scene Graph Parser</head><p>For a given sentence, we use a public toolkit 1 to generate a language scene graph, in which nodes encode noun phrases and edges are the relationships between them. In this language scene graph parser, a dependency parser is first applied to the input sentence and then hand-crafted rules are employed to generate language scene graphs. However, we observe some issues associated with the off-the-shelf parser: 1) noun phrases in the parses sometimes do not correspond to the given phrases; 2) some phrases and their relationships are still missing the in parses.</p><p>To address the aforementioned limitations, we perform additional post-processing on the Flickr30K Entities dataset. First, we take all given phrases as graph nodes. For each phrase, we pick a noun phrase in the parse that has a maximum word overlap with this given phrase. We then assign the parsed relations to these nodes. However, there are still some isolated nodes in the resulting graph. We further recall some missing relations by taking advantage of the coarse categories of the given phrases. Specifically, for an isolated phrase, if its type is clothing or bodyparts, we find a phrase with the type of people as its subject, and assign a relationship wear / have to them. If there are multiple phrases with the type of people in the graph nodes, we select the one that has a minimum word distance in the sentence with the isolated phrase. The motivation of our rules design comes from the observation that most of clothing / bodyparts phrases are related to a people phrase, and their relationships are generally wear / have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Solving Structured Prediction</head><p>We solve the structured prediction problem by taking an exhaustive search on all the possibilities of s in Equ. 12 with a maximal depth when noun phrase number N is less than 6, and applying only node matching between the phrase graph and visual scene graph otherwise. The motivation of the solving strategy comes from the observation that 96.12% language scene graphs in Flickr30K dataset have less than 6 nodes. The complexity of exhaustive search with a maximal depth is K N , which is not time-consuming when N is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model details with ResNet-50 backbone</head><p>We take an off-the-shelf object detector with ResNet-50 as its backbone to generate the initial set of proposals. It is based on FasterRCNN and pre-trained on the MSCOCO dataset <ref type="bibr">(Lin et al.2014)</ref>. Other settings are same to the model with ResNet-10 backbone. During the training stage, we use SGD optimizer with initial learning rate 1e-1, weight decay 1e-4 and momentum 0.9. The model is trained with 60k iterations totally with batch size 24, and decay the learning rate 10 times in 20k and 40k iterations respectively. In order to investigate the effectiveness the individual component of our framework with ResNet-50 backbone, we also conduct a series of ablation studies. As shown in Tab. 1, the accuracy shows the same growth trend compared to ResNet-101 backbone. In particular, we can observe a significant performance improvement when adopting proposal pruning over baseline model, which improves the accuracy from 60.31% to 66.77%. This indicates that proposal pruning is critical for visual grounding task when the object detector doesn't perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ablations with ResNet-50 backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Additional Experiments on VOGN</head><p>To validate the effectiveness of VOGN, we conduct some additional experiments as shown in Tab. 2. In the baseline model, we compute the similarity score and regression offset for each phrase-box pair x pi , x om . Then we adopt proposal pruning strategy over baseline model without PGN, which can improve grounding accuracy from 73.46% to 74.6%. Furthermore, we add our VOGN under this setting and observe a significant improvement from 74.60% to 75.59%, which indicates the visual object representation can be more discriminative with its context cues. Finally, the performance will drop sharply from 75.59% to 74.80% without considering visual relations feature x c u ij,kl during message passing, which suggests that visual relations play an important role in computing attention among objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>), image captioning (Li et al. 2017; Feng * Both authors contributed equally to the work. This work was supported by Shanghai NSF Grant (No. 18ZR1425100) and NSFC Grant (No. 61703195). The research of 3rd author is supported by the Discovery Grant of Natural Sciences and Engineering Research Council of Canada. Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. et al. 2019), visual question answering (Mun et al. 2018; Cadene et al. 2019) and visual dialogue (Das et al. 2017;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Model Overview: There are four modules in our network, the Backbone Network extracts basic linguistic and visual features;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of phrase grounding results in Flickr30K val set. The colored bounding boxes, which are predicted by our approach, correspond to the noun phrases in the sentences with the same color. The dot boxes denote the predicted results without relations constraint, while the white boxes are ground-truths and the red boxes are the incorrect predictions. The last column is the failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of spatial feature embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of pairwise geometric feature embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results Comparison on Flickr30k test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of phrases grounding accuracy over coarse categories on Flickr30K test set.</figDesc><table><row><cell>Methods SMPL GroundR RtP IGOP SS+QRN SPC+PPC CITE SeqGROUND</cell><cell cols="7">people clothing bodyparts animal vehicles instruments scene other 57.89 34.61 15.87 55.98 52.25 23.46 34.22 26.23 61.00 38.12 10.33 62.55 68.75 36.42 58.18 29.08 64.73 46.88 17.21 65.83 68.72 37.65 51.39 31.77 68.17 56.83 19.50 70.07 73.72 39.50 60.38 32.45 68.24 47.98 20.11 73.94 73.66 29.34 66.00 38.32 71.69 50.95 25.24 76.23 66.50 35.80 51.51 35.98 73.20 52.34 30.59 76.25 75.75 48.15 55.64 42.83 76.02 56.94 26.18 75.56 66.00 39.36 68.69 40.60</cell></row><row><cell cols="2">Ours (RN-50) Ours (RN-101) 86.82 83.06</cell><cell>63.35 79.92</cell><cell>24.28 53.54</cell><cell>84.94 90.73</cell><cell>78.25 84.75</cell><cell>55.56 63.58</cell><cell>61.67 52.05 77.12 58.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on Flickr30K val set.</figDesc><table><row><cell>Components Methods PGN PP VOGN SP Acc(%) Baseline ----73.46 ---74.40 --75.50 -75.85 Ours 76.19</cell><cell>Components (w/o relations feature) PGN PP VOGN Acc(%) ----(w/o x c rij ) --74.11 (w/o x c rij ) -75.32 (w/o x c rij ) (w/o x c u ij,kl ) 75.44 ----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of K proposals on Flickr30K val set.</figDesc><table><row><cell>K Acc(%) 74.97 76.19 76.07 5 10 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Learning Cross-modal Context Graph Networks for Visual Grounding Supplementary Material Yongfei Liu 1 * Bo Wan 1 * Xiaodan Zhu 2 Xuming He 1 1 ShanghaiTech University 2 Queens University {liuyf3, wanbo, hexm}@shanghaitech.edu.cn xiaodan.zhu@queensu.ca We generate a coordinate map ? with the same spatial size as the convolution feature map ?. The coordinate map ? consists of two channels, indicating the x, y coordinates for each pixel in ?, and normalized by the feature map center. For each object proposal o m ? R 4 , we crop a coordinate map from ? with RoI-Align and embed it into a spatial feature vector x s om ? R 256 by multiple fully connection layers.</figDesc><table><row><cell>1 Cross-modal Graph Network</cell></row><row><cell>1.1 Spatial Feature of Object</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies on Flickr30K val set with ResNet-50 backbone.</figDesc><table><row><cell cols="6">Components Methods PGN PP VOGN SP Acc(%)</cell></row><row><cell>Baseline Ours</cell><cell>-</cell><cell>--</cell><cell>---</cell><cell>----</cell><cell>60.31 62.45 67.51 67.77 68.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Additional Experments of VOGN with ResNet-101 backbone on Flickr30K val set Components</figDesc><table><row><cell cols="3">Methods PGN PP</cell><cell>VOGN</cell><cell cols="2">SP Acc(%)</cell></row><row><cell>Baseline</cell><cell>----</cell><cell>-</cell><cell>--(w/o x c uij,kl )</cell><cell>----</cell><cell>73.46 74.60 75.59 74.80</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vacancy/SceneGraphParser. We refine the language scene graph for the visual grounding task by rule-based post-processing and more details are included in Suppl.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We include ablations of ResNet-50 backbone in Suppl. 5 See Suppl. for more experiments that analyze the VOGN.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and topdown attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kovvuri</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<idno>arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<idno type="arXiv">arXiv:1905.04405</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Image retrieval using scene graphs</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kottur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Scene graph generation from objects, phrases and region captions</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to specialize with knowledge distillation for visual question answering</title>
		<idno type="arXiv">arXiv:1907.09358</idno>
	</analytic>
	<monogr>
		<title level="m">Trends in integration of vision and language research: A survey of tasks, datasets, and methods</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural sequential phrase grounding (seqground)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">;</forename><surname>Pelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leonid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<idno>Plummer et al. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">NeuIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Grounding of textual phrases in images by reconstruction</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Vision and Language (VL15)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene graph parsing as dependency parsing</title>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpretable and globally optimal prediction for textual grounding using image concepts</title>
	</analytic>
	<monogr>
		<title level="m">NeuIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<idno type="arXiv">arXiv:1805.03508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
