<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grounded Situation Recognition with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyeong</forename><surname>Cho</surname></persName>
							<email>junhyeong99@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE POSTECH Pohang</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseok</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE POSTECH Pohang</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Lee</surname></persName>
							<email>hyeonjun1882@postech.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI POSTECH Pohang</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<email>suha.kwak@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE POSTECH Pohang</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI POSTECH Pohang</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grounded Situation Recognition with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grounded Situation Recognition (GSR) is the task that not only classifies a salient action (verb), but also predicts entities (nouns) associated with semantic roles and their locations in the given image. Inspired by the remarkable success of Transformers in vision tasks, we propose a GSR model based on a Transformer encoder-decoder architecture. The attention mechanism of our model enables accurate verb classification by capturing high-level semantic feature of an image effectively, and allows the model to flexibly deal with the complicated and image-dependent relations between entities for improved noun classification and localization. Our model is the first Transformer architecture for GSR, and achieves the state of the art in every evaluation metric on the SWiG benchmark. Our code is available at https://github.com/jhcho99/gsrtr. Figure 1: Predictions of our model on the SWiG dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 2</ref><p>: The overall architecture of our model (GSRTR). It mainly consists of two components: Transformer Encoder for verb prediction, and Transformer Decoder for grounded noun prediction. Diagram is best viewed in colored version.</p><p>However, it still remains challenging and less explored to expand such models for detailed and comprehensive understanding of natural scenes, e.g., recognizing what happens and who are involved with which roles. Image captioning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref> and scene graph generation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> have been studied in this context. These tasks aim at reasoning about image contents in detail and describing them through natural language captions or relation graphs of objects. However, quality evaluation of natural language captions is not straightforward, and scene graphs are limited in terms of expressive power as they represent an action only by a triplet of subject, predicate, and object.</p><p>Grounded Situation Recognition (GSR) <ref type="bibr" target="#b16">[17]</ref> is a comprehensive scene understanding task that resolves the above limitations. It originates from Situation Recognition (SR) <ref type="bibr" target="#b27">[28]</ref>, the task of predicting a salient action, entities taking part of the action, and their roles altogether given an image. In SR, an action and entities are called verb and nouns, respectively, and the set of semantic roles of the entities in an action is termed frame; a frame is defined for each verb as prior knowledge by FrameNet <ref type="bibr" target="#b3">[4]</ref>, a lexical database of English. Then SR is typically done by predicting a verb then assigning a noun to each role given by the frame of the verb. GSR has been introduced to further address localization (i.e., bounding box estimation) of the nouns in the image, which is missing in SR. It is thus more challenging yet enables more detailed scene understanding in comparison with SR.</p><p>The major challenge in GSR is two-fold. The first is the difficulty of verb prediction. This is caused by the fact that a verb is a high-level concept embodied by multiple entities; as illustrated in <ref type="figure">Fig. 1</ref>, images of the same verb often vary significantly due to different entities interacting in different ways. The second is the difficulty of modeling complicated relations between entities. Since an action (i.e., verb) is performed by multiple entities (i.e., nouns) related to each other, individual noun recognition per role is definitely suboptimal; relations between nouns have to be considered for improved noun prediction and localiza-tion. However, modeling such relations is challenging since they are latent and depending on an input image. Inspired by the recent success of Transformers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>, we present in this paper a new model, dubbed GSRTR, that addresses the aforementioned challenges through the attention mechanism. As illustrated in <ref type="figure">Fig. 2</ref>, it has an encoder-decoder architecture based on Transformer. The encoder takes as input a verb token and image features from a CNN backbone. The token then goes through self-attention blocks in the encoder and is finally processed by a verb classifier on top. Thanks to the self-attention with the image features, the encoder can capture rich and high-level semantic information for accurate verb prediction. Meanwhile, the decoder predicts a grounded noun per role, where target roles are determined by the frame of the target verb. It thus takes as input semantic role queries of target roles as well as image features given by the encoder; a semantic role query is obtained by a concatenation of two embedding vectors, one for its role and the other for a verb, which are learnable parameters dedicated to the role and verb, respectively. Each semantic role query is converted to a feature vector through attention blocks, then used to predict a noun class, a box coordinate and a box existence probability of its role. The attention blocks in our decoder allow to capture complicated and image-dependent relations among roles effectively and flexibly. Contributions: Our GSRTR is the first Transformer architecture dedicated to GSR. Furthermore, its encoder-decoder architecture is carefully designed to address major challenges of the task. The efficacy of GSRTR is validated on the SWiG dataset <ref type="bibr" target="#b16">[17]</ref>, the standard benchmark for GSR, where it clearly outperforms existing models <ref type="bibr" target="#b16">[17]</ref> in every evaluation metric. We also provide in-depth analysis on behaviors of GSRTR, which demonstrates that it has the capability of drawing attentions on local areas relevant to verb and grounded nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Situation Recognition: Situation Recognition (SR) is the task of predicting a salient action (verb) and entities (nouns) taking part of the action. Yatskar et al. <ref type="bibr" target="#b27">[28]</ref> present the imSitu dataset as benchmark of Situation Recognition and propose Conditional Random Field (CRF) model. Their following work <ref type="bibr" target="#b28">[29]</ref> figures out that sparsity of training examples compared to large output space could be problematic, and alleviates it through tensorcomposition function. Since then, there have been attempts to model the relations among semantic roles. Inspired by image captioning task, Mallya and Lazebnik <ref type="bibr" target="#b14">[15]</ref> adopt a Recurrent Neural Network (RNN) architecture to model the relations in the predefined order. Li et al. <ref type="bibr" target="#b8">[9]</ref> use a Gated Graph Neural Network (GGNN) <ref type="bibr" target="#b9">[10]</ref> to capture relations among roles, and Suhail and Sigal <ref type="bibr" target="#b19">[20]</ref> propose a modified GGNN to learn context-aware relations among roles depending on the content of the image. Cooray et al. <ref type="bibr" target="#b1">[2]</ref> formulate the relation modeling as an interdependent query based visual reasoning problem. Grounded Situation Recognition: Recently, Grounded Situation Recognition (GSR) has been introduced by Pratt et al. <ref type="bibr" target="#b16">[17]</ref> to further address localization of entities, which is missing in SR. They propose the Situation With Groundings (SWiG) dataset that provides bounding box annotations in addition to the imSitu dataset. They also propose Joint Situation Localizer (JSL) model which consists of a verb classifier and a RNN based object detector. The object detector sequentially produces noun and its bounding box prediction via the predefined role order. Compared with JSL, our GSRTR can flexibly capture the relations among the semantic roles rather than the predefined order. Furthermore, the verb prediction process in our model can capture long-range interactions of semantic concepts via a Transformer encoder.</p><p>Transformer in Vision Tasks: Dosovitskiy et al. <ref type="bibr" target="#b2">[3]</ref> propose a standard Transformer encoder architecture <ref type="bibr" target="#b21">[22]</ref> for image classification task. This model, called ViT, takes image patches flattened, linearly transformed, and combined with positional encodings as input with a classification token. On the other hand, the encoder of GSRTR takes image features from a CNN backbone as input, and is combined with a decoder for grounded noun prediction. Carion et al. <ref type="bibr" target="#b0">[1]</ref> view object detection as a direct set prediction and bipartite matching problem, and propose a Transformer encoder-decoder architecture for object detection accordingly. Their model, called DETR, introduces learnable embeddings called object queries as inputs of the decoder, each of which is in charge of a certain image region and a set of bounding box candidates. Instead of the object queries, GSRTR uses semantic role queries, each of which focuses on entities taking part of a specified action with a specific role. Similar follow-ups to DETR: There have been attempts, including our GSRTR, to apply DETR to other domains such as video instance segmentation <ref type="bibr" target="#b23">[24]</ref>, video action recognition <ref type="bibr" target="#b30">[31]</ref> and human-object-interaction detection <ref type="bibr" target="#b33">[34]</ref>. Their models use latent queries for a Transformer decoder in the similar way, but GSRTR has notable differences. While their models employ a fixed number of latent queries in the decoder, GSRTR constructs a variable number of queries depending on a given image. Also, to the best of our knowledge, GSRTR is the first attempt to explicitly leverage the output of a Transformer encoder for building queries used in a Transformer decoder; semantic role queries use the verb embedding corresponding to the predicted verb from the encoder output at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Inspired by ViT <ref type="bibr" target="#b2">[3]</ref> and DETR <ref type="bibr" target="#b0">[1]</ref>, we propose a novel model called Grounded Situation Recognition TRansformer (GSRTR) to address the challenging GSR task; the architecture of GSRTR is illustrated in <ref type="figure">Fig. 2</ref>. This section first provides a formal definition of GSR, then describes details of our model architecture, training and inference procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Let V, R, and N denote the sets of verbs, roles, and nouns defined in the task, respectively. For each verb v ? V, a set of semantic roles, denoted by R v ? R, is predefined as its frame by FrameNet <ref type="bibr" target="#b3">[4]</ref>. For example, the frame of a verb Catching is a set of semantic roles R Catching = {Agent, Caught Item, Tool, Place} ? R. Also, a pair of a noun n ? N and its bounding box b ? R 4 is called a grounded noun. The goal of GSR is to predict a verb v of an input image and assign a grounded noun to each role in R v . Formally speaking, a prediction of GSR is in the form of</p><formula xml:id="formula_0">S = (v, F v ), where F v = {(r, n r , b r ) | n r ? N ? { / 0 n } , b r ? R 4 ? { / 0 b } for r ? R v }; /</formula><p>0 n and / 0 b mean unknown and not grounded, respectively. For example, the prediction for the leftmost image in <ref type="figure">Fig. 1</ref> is given by S = Catching, (Agent, Bear, ), (Caught Item, Fish, ), (Tool, Mouth, ), (Place, River, / 0 b ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder for Verb Prediction</head><p>A CNN backbone first processes an input image to extract its feature map X img ? R c?h?w , where c is the number of channels and h ? w is the resolution of X img . Then X img is fed to a 1 ? 1 convolution layer for reducing the channel size to d, and flattened, leading to flattened images features F img ? R d?hw . Like the classification token used in ViT <ref type="bibr" target="#b2">[3]</ref>, we append a learnable verb embedding f v ? R d to F img , forming an input of the encoder F ? R d?(1+hw) . The encoder is a stack of six layers, each of which consists of a Multi-Head Self-Attention (MHSA) block and a Feed Forward Network (FFN) block. Also, we apply Pre-Layer Normalization (Pre-LN) <ref type="bibr" target="#b24">[25]</ref> before the MHSA and FFN blocks. Positional encodings are added to the input of each encoder layer. Please refer to the supplementary material for more details of the encoder.</p><p>The output of the encoder, denoted by E ? R d?(1+hw) , is split into a verb feature e v ? R d and hw image features E img ? R d?hw . The former is fed to the verb classifier, which in turn produces a logit vector z v ? R |V| as a result of verb classification. On the other hand, the latter will be used as observations for the decoder. Note that by exploiting the attention mechanism through the encoder layers, the verb token can effectively aggregate relevant semantic features of an image for accurate verb classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder for Grounded Noun Prediction</head><p>In addition to the image features E img given by the encoder, the decoder takes as input semantic role queries to predict corresponding nouns and their bounding boxes, inspired by the object queries in DETR <ref type="bibr" target="#b0">[1]</ref>. To be specific, a semantic role query w (v,r) ? R d is obtained by a concatenation of a verb embedding vector w v ? R d v and a role embedding vector w r ? R d r (d = d v + d r ), both of which are learnable parameters; v is the ground-truth verb at training time and the predicted verb at inference time, while r ? R v . The number of semantic role queries fed to the decoder is thus |R v |.</p><p>The decoder is a stack of six layers, each of which consists of a MHSA block, a Multi-Head Attention (MHA) block, and a FFN block; Pre-LN is applied before each of the blocks. The first decoder layer input is set to zero. In each decoder layer, each semantic role query w (v,r) is added to each key and query of the MHSA block and added to each query of the MHA block. The image features E img serve as keys and values in the MHA block of each decoder layer. Through the MHSA block in each decoder layer, semantic role queries flexibly capture the role relations ( <ref type="figure">Fig. 4</ref>). From the MHA block in each decoder layer, each semantic role query attends to image features considering image-dependent relations ( <ref type="figure" target="#fig_0">Fig. 3)</ref>.</p><p>Through the decoder, each semantic role query w (v,r) is converted to an output feature. The output feature of each role r ? R v is in turn fed to three branches: One for noun classification, another for bounding box regression, and the other for predicting existence of its bounding box. The noun classifier produces a noun logit vector z n r ? R |N ?{ / 0 n }| . The bounding box regressor predictsb r = (? x ,? y ,?,?) ? [0, 1] 4 , indicating the normalized center coordinate, height, and width of a box relative to the image size. This predicted box coordinate is transformed into top-left and bottom-right coordinate representationb r = (x 1 ,? 1 ,x 2 ,? 2 ) ? R 4 . Finally, the box existence predictor produces a box existence probability p b r ? [0, 1]. Please refer to the supplementary material for more details of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>The total loss for training GSRTR is a linear combination of five losses: A verb classification loss, a noun classification loss, a bounding box existence loss, a L 1 box regression loss, and a Generalized IoU (GIoU) <ref type="bibr" target="#b17">[18]</ref> box regression loss. The verb classification loss L v is the cross entropy between the verb prediction probability p v = Softmax(z v ) and the ground-truth verb distribution. The noun classification loss L n is formulated as the average of individual noun classification losses over the semantic roles, and is given by</p><formula xml:id="formula_1">L n = 1 |R v | ? r?R v CrossEntropy(p n r , t n r ),<label>(1)</label></formula><p>where p n r denotes the noun prediction probability for each role r and t n r indicates the groundtruth noun distribution for each role r. The bounding box existence loss L exist is the average of individual bounding box existence loss over the semantic roles, and is given by</p><formula xml:id="formula_2">L exist = 1 |R v | ? r?R v CrossEntropy(p b r ,t b r ),<label>(2)</label></formula><p>where p b r denotes the bounding box existence probability for each role r and t b r ? {0, 1} specifies the existence of the ground-truth bounding box for each role r (i.e., t b r = 1 when b r = / 0 b ). The L 1 box regression loss L L 1 is defined as the average of individual L 1 distances between predicted and ground-truth bounding boxes over semantic roles for which groundtruth bounding boxes exist, and are given by</p><formula xml:id="formula_3">L L 1 = 1 |R v | ? r?R v b r ? b r 1 ,<label>(3)</label></formula><p>whereR v = {r | r ? R v and b r = / 0 b } is the set of roles associated with bounding boxes. Finally, the GIoU box regression loss L GIoU <ref type="bibr" target="#b17">[18]</ref> is formulated as the average of individual GIoU losses over roles for which ground-truth bounding boxes exist, and are given by</p><formula xml:id="formula_4">L GIoU = 1 |R v | ? r?R v 1 ? |b r ?b r | |b r ?b r | ? |C(b r ,b r ) \ b r ?b r | |C(b r ,b r )| ,<label>(4)</label></formula><p>where C(b r , b r ) denotes the smallest box enclosing predicted boxb r and ground-truth box b r for each role r. GIoU loss is a scale-invariant loss and it compensates for scale-variant L 1 loss. The total loss L total is formulated as</p><formula xml:id="formula_5">L total = ? v L v + ? n L n + ? exist L exist + ? L 1 L L 1 + ? GIoU L GIoU , where ? v , ? n , ? exist , ? L 1 , ? GIoU &gt; 0 are hyperparameters.</formula><p>At inference time, our method predicts a verbv = arg max v p v then constructs corresponding semantic role queries w (v,r) for all r ? Rv. Each w (v,r) is used by the decoder to produce corresponding output noun logit z n r , bounding boxb r and bounding box existence probability p b r . Note that if p b r &lt; 0.5, the predicted bounding boxb r is ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metrics</head><p>SWiG <ref type="bibr" target="#b16">[17]</ref> dataset is composed of 75k, 25k and 25k images for the train, development and test set respectively. There are |V| = 504 verbs, |R| = 190 roles, and 1 ? |R v | ? 6 semantic roles per verb. We use about 10k nouns, the number of noun classes in the train set. The annotation for each image consists of a verb, a bounding box for each semantic role, and three nouns (from three annotators) for each semantic role. The predicted verb and grounded nouns are measured by five metrics: verb, value, valueall, grounded-value, and grounded-value-all. The verb metric denotes a verb prediction accuracy. The value metric denotes a noun prediction accuracy from its semantic role. The value-all metric denotes that all nouns corresponding to semantic roles are correctly predicted. The grounded-value metric denotes a grounded noun prediction accuracy for its semantic role. Note that the grounded noun prediction is considered correct if it correctly predicts noun and bounding box. The bounding box prediction is considered correct if it correctly predicts bounding box existence and the predicted box has an Intersection-over-Union (IoU) value of at least 0.5 with the ground-truth box. The grounded-value-all metric denotes that all grounded nouns corresponding to semantic roles are correctly predicted. The requirements for each metric are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Because the number of roles per verb is different and the number of images per verb could be different, all above metrics are calculated for each verb and then averaged over them.</p><p>Since these metrics depend heavily on the verb accuracy, the metrics are reported in 3 settings: top-1 predicted verb, top-5 predicted verbs and ground-truth verb. In top-1 predicted verb setting, five metrics are reported: a top-1 predicted verb accuracy, two noun metrics and two grounded noun metrics. If the top-1 predicted verb is incorrect, the noun and grounded noun metrics are considered incorrect. In top-5 predicted verbs setting, five metrics are reported: a top-5 predicted verbs accuracy, two noun metrics and two grounded noun metrics. If the ground-truth verb is not included in the top-5 predicted verbs, the noun and grounded noun metrics are considered incorrect, too. In ground-truth verb setting, four metrics are reported: two noun metrics and two grounded noun metrics. From the groundtruth verb assumed to be known, noun and grounded noun predictions are taken from the model by conditioning on the ground-truth verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following previous work <ref type="bibr" target="#b16">[17]</ref>, we use ImageNet-pretrained ResNet-50 backbone <ref type="bibr" target="#b6">[7]</ref> except Feature Pyramid Network (FPN) <ref type="bibr" target="#b10">[11]</ref>. The ResNet-50 backbone produces the image features X img ? R c?h?w from the input image where c = 2048. The hidden dimensions of each semantic role query, verb token and image feature are 512 (d = 512). The verb embedding dimension and role embedding dimension are 256 (d v = d r = 256). We use learnable 2D embeddings for the positional encodings. The number of heads for all MHSA and MHA blocks is 8. We use 2 fully connected layers with ReLU activation function for the four followings: the FFN blocks in the encoder and decoder, the verb classifier, the noun classifier, and the bounding box existence predictor. The size of hidden dimensions are 2048, 2d, 2d, and 2d, respectively. The dropout rates are 0.15, 0.3, 0.3, and 0.2, respectively. The bounding box regressor is 3 fully connected layers with ReLU activation function and 2d hidden dimensions, using 0.2 dropout rate. The label smoothing regularization <ref type="bibr" target="#b20">[21]</ref> is used for the target verb and noun labels with label smoothing factor 0.3 and 0.2, respectively. We use AdamW <ref type="bibr" target="#b13">[14]</ref> optimizer with the learning rate 10 ?4 (10 ?5 for the backbone), weight decay 10 ?4 , ? 1 = 0.9 and ? 2 = 0.999. We set the max gradient clipping value to 0.1 and train the BatchNorm layers in the backbone. The training epoch is 40 with batch size 16 per GPU on four 12GB TITAN Xp GPUs, which takes about 20 hours. The loss coefficients are ? v = ? n = 1 and ? exist = ? L 1 = ? GIoU = 5. Data Augmentation: Random Color Jittering, Random Gray Scaling, Random Scaling and Random Horizontal Flipping are used. The hue, saturate and brightness scale in random color jittering set to 0.1. The scale of random gray scaling sets to 0.3. The scales of random scaling set to 0.5, 0.75 and 1.0. The probability of random horizontal flipping sets to 0.5. Final Noun Loss: In SWiG, three noun annotations exist per role. For each noun annotation, we calculate the loss (Eq. 1). The final noun loss is the summation of the three noun losses. Batch Training: The number of semantic roles ranges from 1 to 6 depending on the frame of a verb. In GSRTR, the semantic role queries are constructed as much as the number of semantic roles. To ensure batch training, zero padding is used for each output of grounded noun prediction branches. We ignore the padded outputs in the loss computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Results</head><p>Quantitative Comparison with Previous Work: <ref type="table" target="#tab_1">Table 2</ref> quantitatively compares our model with previous work on the dev and test splits of SWiG dataset. In all evaluation metrics, GSRTR achieves the state-of-the-art accuracy. In the dev set, compared with JSL, GSRTR achieves the top-1 predicted verb and top-5 predicted verbs accuracies of 41.06% (+1.46%p) and 69.46% (+1.75%p), respectively. In ground-truth verb setting, GSRTR achieves the value and grounded-value accuracies 74.27% (+0.74%p) and 58.33% (+0.83%p), respectively. Note that previous work uses two ResNet-50 backbones and FPN, while our GSRTR only uses a single ResNet-50 backbone without FPN. Existing models in <ref type="bibr" target="#b16">[17]</ref> have about 108 million parameters, but our GSRTR only has about 83 million parameters. Although GSRTR has less backbone capacity and less parameters, it achieves the state-of-the-art accuracy in every evaluation metric. In addition, the reason for the small improvement by GSRTR in terms of grounded-value metrics is that these metrics require correct predictions of verb, noun and bounding box as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Existing models in <ref type="bibr" target="#b16">[17]</ref> are trained separately in terms of verb prediction part and grounded noun prediction part, while our GSRTR is trained in an end-to-end manner. For this reason, it is difficult to fairly compare the training time of ours with existing models. However, we can reasonably guess that GSRTR takes less training time than others. GSRTR takes about 20 hours with four 12GB TITAN Xp GPUs for whole training, but other models take about 20 hours with four 24GB TITAN RTX GPUs only for training of grounded noun prediction part. For the comparison of inference time, we compare GSRTR with JSL which was the previous state-of-the-art. We evaluate the models on the test set in the same environment with one 2080Ti GPU. GSRTR takes 21.69 ms (46.10 FPS) and JSL takes 80.00 ms (12.50 FPS) on the average of 10 trials. Effect of Verb Embedding Concatenation: We also quantitatively show the effect of verb embedding concatenation in the semantic role query. If we do not concatenate the verb embedding (i.e., d v = 0 and d r = d), the accuracies in the ground-truth verb setting decrease by around 1.3 ? 2.3%p (GSRTR w/o VE in <ref type="table" target="#tab_1">Table 2</ref>). It demonstrates that the verb embedding concatenation is helpful for grounded noun prediction.   <ref type="figure">Fig. 4(a)</ref> focus on the role Place, i.e., the forest (Place) is highly related to the monkey (Agent) and the vine (Carrier) given the verb Swinging. Meanwhile, the role Place in <ref type="figure">Fig. 4(b)</ref> focuses on the role Carrier, i.e., the golf club (Carrier) is highly related to the golf course (Place) given the verb Swinging. It shows that the relations among roles can be adaptively captured depending on the context of a given image.</p><p>(a) (b) <ref type="figure">Figure 4</ref>: Visualization on Role Relations for two Swinging images. We visualize the attention scores between semantic role pairs computed in the MHSA block of the last decoder layer. Attention scores are represented as column-wise sum to 1. Verb Token Attention Map on Image Features: In <ref type="figure" target="#fig_1">Figure 5</ref>, the rightmost column shows the semantic regions where the verb token focuses on are similar. The verb token can capture the key feature (e.g., tugged item) to infer the salient action. Each row shows the transition of attention maps through the encoder layers, e.g., focusing on the tugged item gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>There have been many studies in image retrieval by computing the similarities between the visual representations of images. But, they do not work well for getting the retrieval results which have similar situations with respect to semantics or object arrangements. Grounded-Semantic-Aware Image Retrieval enables image retrieval in the aspects of main activity and key objects with their arrangements, as shown in <ref type="figure">Figure 6</ref>. This retrieval uses the results of verb prediction and grounded noun prediction instead of visual representations. The predictions of main activity (verb) and entities (nouns) enable image retrieval for similar semantics, and the predictions of entity locations enable image retrieval for similar object arrangements. In this retrieval, we compute the GrSitSim(I, J) <ref type="bibr" target="#b16">[17]</ref> as similarity score function between im- <ref type="figure">Figure 6</ref>: Grounded-Semantic-Aware Image Retrieval results on the dev set. The retrieval results have similar semantics and object arrangements with the query image. In this retrieval, the similarity between two images is computed by the results of verb prediction and grounded noun prediction as in <ref type="bibr" target="#b16">[17]</ref>. . Note that we ignore the predicted bounding box if its existence probability is less than 0.5. We calculate the similarity between two images I and J as follows:</p><formula xml:id="formula_6">GrSitSim(I, J) = max ? ? ? ? ? ? ? 1 [v I i =v J j ] 2 ? i ? j ? |RvI i | |Rv I i | ? k=1 1 [n I i,k =n J j,k ] ? 1 + IoU(b I i,k ,b J j,k ) 1 ? i, j ? 5 ? ? ? ? ? ? ? .<label>(5)</label></formula><p>GrSitSim(I, J) is computed by the results of verb prediction and grounded noun prediction for image I and J. The similarity is not zero when at least one verb is shared in the top-5 verb predictions for image I and J. The similarity is maximized when the top-1 verb predictions and noun predictions of two images are same, and the sizes and locations of predicted bounding boxes are same. For this reason, we can get the retrieval result which has similar semantics and object arrangements in Grounded-Semantic-Aware Image Retrieval. Thus, we can apply this image retrieval to the applications where semantics and object arrangements are important, e.g., search engine using semantics and object arrangements of images. Grounded Situation Recognition models produce complete predictions with respect to the semantic roles corresponding to a verb. Thus, the models can answer the following questions more strictly, "What is the main activity" (verb), "Who is participating in the main activity" (role Agent), "What does the actor use in the main activity" (role Tool), "Where is the actor in the image" (entity location of role Agent), etc. For this reason, the models are useful for predetermined questions on situations. Taking advantages of these properties, we can apply the models for industry such as unmanned surveillance system or service robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose the first Transformer architecture for GSR, which achieves the state-of-the-art accuracy on every evaluation metric. Our model, GSRTR, can capture high-level semantic feature, and flexibly deal with the complicated and image-dependent role relations. We perform extensive experiments and qualitatively illustrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>This provides more details of our model, further analyses on it, additional ablation studies and experimental results. Section A.1 describes the transformer architecture of GSRTR in detail, Section A.2 performs the ablation studies on GSRTR, and Section A.3 provides more qualitative examples of the total prediction of GSRTR. Finally, a more thorough qualitative analysis on attention of GSRTR is illustrated in Section A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Detailed Transformer Architecture</head><p>Transformer Encoder-Decoder: The detailed transformer architecture of GSRTR is given in <ref type="figure" target="#fig_2">Figure A1</ref>. The encoder takes as input a verb token and flattened image features, and then produces a verb feature and image features. Along with image features given by the encoder, the decoder takes as input semantic role queries, and then produces output features corresponding to the semantic roles. The encoder is a stack of six encoder layers and the decoder is a stack of six decoder layers. Each encoder layer consists of a Multi-Head Self-Attention (MHSA) block and a Feed-Forward Network (FFN) block. Each decoder layer consists of a MHSA block, a Multi-Head Attention (MHA) block, and a FFN block. We use Pre-Layer Normalization (Pre-LN) <ref type="bibr" target="#b24">[25]</ref>, i.e., LayerNorm is used before each MHSA block, MHA block, and FFN block, and also before the verb feature and before the decoder output features corresponding to the semantic roles. The skip connection, using 0.15 dropout rate, is given by:</p><formula xml:id="formula_7">x + Dropout (Block(LayerNorm (x))) , (A.1)</formula><p>where x ? R d and Block denotes one of the MHSA block, MHA block, and FFN block. Note that we use d = 512. The FFN block is 2 fully-connected layers with ReLU activation function and 2048 hidden dimensions, using 0.15 dropout rate, and it is given by:</p><formula xml:id="formula_8">FFN(x) = W 2 (Dropout (max(W 1 x + b 1 , 0))) + b 2 , (A.2) where x ? R d , W 1 ? R 2048?d , b 1 ? R 2048 , W 2 ? R d?2048 , and b 2 ? R d .</formula><p>We use Xavier initialization <ref type="bibr" target="#b4">[5]</ref> for the learnable parameters in the encoder and decoder.</p><p>Multi-Head Attention: MHA takes as input a query sequence X Q ? R d?n Q and a key-value sequence X KV ? R d?n KV , where n Q denotes the query sequence length and n KV denotes the key-value sequence length. MHSA corresponds to the case when the query sequence is same with the key-value sequence in MHA, i.e., when X Q = X KV in MHA. MHA is formulated as:</p><formula xml:id="formula_9">MHA(X Q , X KV ) = W O Head 1 (X Q , X KV ) ; ? ? ? ; Head H (X Q , X KV ) , (A.3)</formula><p>where H is the number of heads, [; ] denotes a concatenation and W O ? R d?d denotes an output projection. Note that we use H = 8. Head m denotes each attention function with linear projections for m = 1, ? ? ? , H, and it is given by: Attn denotes an attention function which transforms a query sequence Q ? R d ?n Q into an output sequence, whose element is a weighted sum of a value sequence V ? R d ?n KV . For i th query q i ? R d , each weight of the sum is computed by a softmax function (i.e., Softmax) after a scaled dot-product between the i th query q i and a key sequence K ? R d ?n KV . In other words, the i th element of the attention function output from the query sequence Q, key sequence K, and value sequence V is given by:</p><formula xml:id="formula_10">Head m (X Q , X KV ) = Attn W m Q X Q ,W m K X KV ,W m V X KV , (A.4) where W m Q ,W m K ,W m V ? R d</formula><formula xml:id="formula_11">Attn i (Q, K,V ) = ? j Softmax j 1 ? d q i K v j , (A.5)</formula><p>where Softmax j denotes the j th output of the softmax function and v j ? R d denotes the j th value.</p><p>The where X Q = X KV and X Q ? R d?(1+hw) .</p><p>The MHSA and MHA blocks in the decoder: Along with the image features given by the encoder, the decoder takes as input a sequence of the semantic role queries. Additionally to Section 3.3, each semantic role query w (v,r) per semantic role r ? R v can formulate a sequence with arbitrary role orders, leading to the semantic role query sequence S v ? R d?|R v | . Note that the initial decoder input is set to zero. In each MHSA block of the decoder, the semantic role query sequence S v is added to the query and key inputs of the attention function. In other words, the m th attention function in each MHSA block of the decoder is given by:</p><formula xml:id="formula_12">Head m (X Q , X KV ) = Attn W m Q (X Q + S v ) ,W m K (X KV + S v ),W m V X KV , (A.7)</formula><p>where X Q = X KV and X Q ? R d?|R v | . In each MHA block of the decoder, the semantic role query sequence S v are added to the query inputs of the attention function, and positional encodings P are added to the key inputs of the attention function. In other words, the m th attention function in each MHA block of the decoder is given by:</p><formula xml:id="formula_13">Head m (X Q , X KV ) = Attn W m Q (X Q + S v ) ,W m K (X KV + P) ,W m V X KV , (A.8)</formula><p>where X Q ? R d?|R v | and X KV ? R d?hw . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Ablation Studies</head><p>We study the effect on the number of layers and the location of LayerNorm in GSRTR. Our experiments are evaluated on the dev and test splits of SWiG dataset <ref type="bibr" target="#b16">[17]</ref>, and the results are compared with the proposed model and setting in Section 4.2.</p><p>The effect on the number of layers in the encoder and decoder is shown at the first and second row of each set in <ref type="table" target="#tab_0">Table A1</ref>. GSRTR w/ 4 layers denotes that each of the transformer encoder and decoder has four layers, and GSRTR w/ 8 layers denotes that each has eight layers. In ground-truth verb setting, the noun and grounded noun accuracies of both models decrease. The top-1 predicted verb and top-5 predicted verbs accuracies of both models marginally fluctuate.</p><p>The effect on the location of LayerNorm in GSRTR is shown at the third row of each set in <ref type="table" target="#tab_0">Table A1</ref>. GSRTR w/ Post-LN denotes that LayerNorm is placed between skip connections, leading to Post-Layer Normalization (Post-LN) <ref type="bibr" target="#b24">[25]</ref> transformer architecture. In all evaluation metrics of each set, the accuracies of GSRTR w/ Post-LN decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More Qualitative Results of Our Model</head><p>In top-1 predicted verb setting on the test split of the SWiG dataset, the prediction results of GSRTR are shown in <ref type="figure" target="#fig_4">Figure A2</ref>, <ref type="figure" target="#fig_0">Figure A3</ref> and <ref type="figure" target="#fig_6">Figure A4</ref>. The SWiG dataset has three noun annotations for each semantic role. The noun prediction is considered correct if the predicted noun matches one of the three noun annotations. The box prediction is considered correct if the model correctly predicts box existence and the predicted box has an Intersection-over-Union (IoU) value of at least 0.5 with the ground-truth box. Note that the grounded noun prediction is considered correct if the predicted noun and predicted box are correct. <ref type="figure" target="#fig_4">Figure A2</ref> shows the correct grounded noun prediction results. <ref type="figure" target="#fig_0">Figure A3</ref> shows the failure cases of box prediction. There are incorrect box predictions when bounding boxes have extreme aspect ratios (e.g., the boxes of the role Tool in the Surfing and the Coloring image), or small scales (e.g., the box of the role Agent in the Mowing image and the box of the role Tool in the Helping image). <ref type="figure" target="#fig_6">Figure A4</ref> shows the failure cases of noun prediction, including incorrect box predictions. Even in the failure cases, there are the cases where GSRTR reasonably predicts nouns. For example, in the Tilting image, GSRTR predicts that the noun of the role Place is Outdoors, which is similar to the first annotation Outside. In the Curling image, GSRTR predicts that the nouns of the role Agent and Place are Person and / 0, which are enough to describe the given image. There is also the case where GSRTR inappropriately predicts nouns. In the Chasing image, GSRTR predicts that the noun of the role Chasee is Zebra, whereas the three noun annotations are Bull, Calf, and Cow.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Qualitative Analysis on Attention</head><p>Role Attention Map on Image Features: In <ref type="figure" target="#fig_1">Figure A5</ref>, <ref type="figure" target="#fig_8">Figure A6</ref> and <ref type="figure" target="#fig_9">Figure A7</ref>, each column shows the difference of attention maps among roles. Each row shows the transition of attention maps through the decoder layers. In <ref type="figure" target="#fig_1">Figure A5</ref>, the role Decorated focuses on the decorated stuff and the role Item focuses on the decoration item. <ref type="figure" target="#fig_8">Figure A6</ref> shows that GSRTR can understand the given image and distinguish between the role Agent and the role Victim. <ref type="figure" target="#fig_8">Figure A6</ref> and <ref type="figure" target="#fig_9">Figure A7</ref> show that GSRTR can figure out the background for the role Place in the given image.  Visualization of Role Relations: GSRTR captures the relations among roles in the similar way if the situations of the given images are similar. In <ref type="figure" target="#fig_10">Figure A8</ref>, the role Vehicle focuses on the role Place, i.e., the runway (Place) and the railway station (Place) are highly related to the airplane (Vehicle) and the train (Vehicle) given the verb Boarding, respectively. In <ref type="figure" target="#fig_11">Figure A9</ref>, the role Obstacle and the role Tool focus on the role Place, i.e., the cliff (Place) is highly related to the rock (Obstacle) and the rope (Tool) given the verb Climbing.   Verb Token Attention Map on Image Features: GSRTR can capture the key feature to infer the salient action. <ref type="figure" target="#fig_2">Figure A10</ref> and <ref type="figure" target="#fig_2">Figure A11</ref> show that GSRTR focuses on the bitten part and the falling agent, respectively. The rightmost column shows that the semantic regions where the verb token focuses on are similar for the same verb. Each row shows the transition of attention maps through the encoder layers. <ref type="figure" target="#fig_2">Figure A11</ref>: Verb Token Attention Map on Image Features for three Falling images. Each row consists of an image and attention maps from the MHSA block in each encoder layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Role Attention Map on Image Features for a Sketching image from the MHA block in each decoder layer. The left labels are the semantic roles of the verb Sketching. The rightmost column images and labels are predicted bounding boxes and nouns of our model. Role Attention Map on Image Features: In Figure 3, each column shows the difference of attention maps among semantic roles. For example, at Layer 6, the role Agent focuses on the woman, and the role Place focuses on the road and yard. Each row shows the transition of attention maps through the decoder layers. For example, in the role Material, the attention map gradually focuses on the paper in the image through the decoder layers. It shows that the semantic role queries can focus on the region related to them. Visualization on Role Relations: In Figure 4, two images show different context for a verb Swinging. The role Agent and Carrier in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Verb Token Attention Map on Image Features for three Tugging images. Each row consists of an image and attention maps from the MHSA block in each encoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure A1 :</head><label>A1</label><figDesc>?d denotes linear projection of m th head for key, query, and value, respectively. The linear projection matrices are learnable parameters, which are not shared across the MHA and MHSA blocks in the encoder and decoder layers. Note that we use The detailed transformer architecture of GSRTR. A verb token and flattened image features are used for the first encoder layer input (black line in Encoder). Zero input is used for the first decoder layer input (black line in Decoder). Positional encodings are added to the keys and queries of the MHSA block in each encoder layer and the keys of the MHA block in each decoder layer (red line). Semantic role queries are added to the keys and queries of the MHSA block in each decoder layer and the queries of the MHA block in each decoder layer (blue line). We omit Dropout in this diagram. d = 64, where d = d H .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>MHSA block in the encoder: The encoder takes as input a verb token and flattened image features. The positional encodings P ? R d?hw are used, where hw denotes the length of flattened image features. The positional encodings P are 2D learnable embeddings, and they are used at the attention function of each MHSA block in the encoder. To be specific, the positional encodings are added to the corresponding image features, which are used as the key and query inputs at the attention function. For the verb token, we append zero to the positional encodings, leading to P ? R d?(1+hw) . As a result, the positional encodings P are added to the key and query inputs of the attention function in each MHSA block of the encoder. Thus, the m th attention function in each MHSA block of the encoder is given by: Head m (X Q , X KV ) = Attn W m Q X Q + P ,W m K X KV + P ,W m V X KV , (A.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A2 :</head><label>A2</label><figDesc>Correct grounded noun predictions of GSRTR in top-1 predicted verb setting on the test set. For each semantic role, three annotators record noun annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A3 :</head><label>A3</label><figDesc>Incorrect box predictions of GSRTR in top-1 predicted verb setting on the test set. The dashed box denotes incorrect box prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A4 :</head><label>A4</label><figDesc>Incorrect noun predictions of GSRTR in top-1 predicted verb setting on the test set. The incorrect noun predictions are highlighted in red color. The dashed box denotes incorrect box prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A5 :</head><label>A5</label><figDesc>Role Attention Map on Image Features for a Decorating image from the MHA block in each decoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A6 :</head><label>A6</label><figDesc>Role Attention Map on Image Features for a Apprehending image from the MHA block in each decoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A7 :</head><label>A7</label><figDesc>Role Attention Map on Image Features for a Smelling image from the MHA block in each decoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A8 :</head><label>A8</label><figDesc>Visualization on Role Relations for two Boarding images from the MHSA block in the last decoder layer. Attention scores are represented as column-wise sum to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A9 :</head><label>A9</label><figDesc>Visualization on Role Relations for two Climbing images from the MHSA block in the last decoder layer. Attention scores are represented as column-wise sum to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A10 :</head><label>A10</label><figDesc>Verb Token Attention Map on Image Features for three Biting images. Each row consists of an image and attention maps from the MHSA block in each encoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Requirements for each metric.</figDesc><table><row><cell></cell><cell></cell><cell>requirement</cell><cell></cell></row><row><cell>metric</cell><cell>correct verb</cell><cell>correct noun for a semantic role for all semantic roles correct nouns</cell><cell>correct bounding box correct bounding boxes for a semantic role for all semantic roles</cell></row><row><cell>verb</cell><cell></cell><cell></cell><cell></cell></row><row><cell>value</cell><cell></cell><cell></cell><cell></cell></row><row><cell>value-all</cell><cell></cell><cell></cell><cell></cell></row><row><cell>grounded-value</cell><cell></cell><cell></cell><cell></cell></row><row><cell>grounded-value-all</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation on the SWiG dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">top-1 predicted verb</cell><cell></cell><cell cols="3">top-5 predicted verbs</cell><cell></cell><cell></cell><cell cols="2">ground-truth verb</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell></row><row><cell>set</cell><cell>model</cell><cell cols="12">verb value value-all value value-all verb value value-all value value-all value value-all value value-all</cell></row><row><cell></cell><cell>ISL [17]</cell><cell>38.83 30.47</cell><cell>18.23</cell><cell>22.47</cell><cell>7.64</cell><cell>65.74 50.29</cell><cell>28.59</cell><cell>36.90</cell><cell>11.66</cell><cell>72.77</cell><cell>37.49</cell><cell>52.92</cell><cell>15.00</cell></row><row><cell>dev</cell><cell cols="2">JSL [17] GSRTR w/o VE (Ours) 40.81 32.05 39.60 31.18</cell><cell>18.85 19.31</cell><cell>25.03 25.64</cell><cell>10.16 10.31</cell><cell>67.71 52.06 69.33 53.09</cell><cell>29.73 29.78</cell><cell>41.25 42.01</cell><cell>15.07 15.36</cell><cell>73.53 72.55</cell><cell>38.32 37.07</cell><cell>57.50 57.00</cell><cell>19.29 18.93</cell></row><row><cell></cell><cell>GSRTR (Ours)</cell><cell>41.06 32.52</cell><cell>19.63</cell><cell>26.04</cell><cell>10.44</cell><cell>69.46 53.69</cell><cell>30.66</cell><cell>42.61</cell><cell>15.98</cell><cell>74.27</cell><cell>39.24</cell><cell>58.33</cell><cell>20.19</cell></row><row><cell></cell><cell>ISL [17]</cell><cell>39.36 30.09</cell><cell>18.62</cell><cell>22.73</cell><cell>7.72</cell><cell>65.51 50.16</cell><cell>28.47</cell><cell>36.60</cell><cell>11.56</cell><cell>72.42</cell><cell>37.10</cell><cell>52.19</cell><cell>14.58</cell></row><row><cell>test</cell><cell cols="2">JSL [17] GSRTR w/o VE (Ours) 40.61 31.87 39.94 31.44</cell><cell>18.87 19.01</cell><cell>24.86 25.21</cell><cell>9.66 9.69</cell><cell>67.60 51.88 69.75 53.25</cell><cell>29.39 29.67</cell><cell>40.60 41.65</cell><cell>14.72 14.93</cell><cell>73.21 72.32</cell><cell>37.82 36.75</cell><cell>56.57 56.03</cell><cell>18.45 18.02</cell></row><row><cell></cell><cell>GSRTR (Ours)</cell><cell>40.63 32.15</cell><cell>19.28</cell><cell>25.49</cell><cell>10.10</cell><cell>69.81 54.13</cell><cell>31.01</cell><cell>42.50</cell><cell>15.88</cell><cell>74.11</cell><cell>39.00</cell><cell>57.45</cell><cell>19.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>age I and J. For an image I, we compute the top-5 verb predictionsv I 1 , ...,v I</figDesc><table><row><cell></cell><cell>5 . For each verb</cell></row><row><cell>predictionv I i , we predict nounsn I i,1 , ...,n I i,|Rv I i</cell><cell>| and bounding boxesb I i,1 , ...,b I i,|Rv I</cell></row></table><note>i |</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A1 :</head><label>A1</label><figDesc>Ablation studies on our model (GSRTR). -all value value-all verb value value-all value value-all value value-all value value-all</figDesc><table><row><cell></cell><cell></cell><cell cols="3">top-1 predicted verb</cell><cell></cell><cell cols="3">top-5 predicted verbs</cell><cell></cell><cell></cell><cell cols="2">ground-truth verb</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell></row><row><cell cols="4">set verb value valuedev model GSRTR w/ 4 layers 40.26 31.88 19.20 GSRTR w/ 8 layers 40.49 32.10 19.46 GSRTR w/ Post-LN 40.18 31.50 18.54</cell><cell>25.44 25.69 25.20</cell><cell>10.20 10.39 9.89</cell><cell>69.34 53.52 69.11 53.34 68.82 52.72</cell><cell>30.33 30.62 29.30</cell><cell>42.29 42.35 41.79</cell><cell>15.69 15.88 15.27</cell><cell>74.09 74.07 73.30</cell><cell>38.88 39.12 37.60</cell><cell>57.97 58.27 57.50</cell><cell>19.75 19.92 19.34</cell></row><row><cell></cell><cell>GSRTR</cell><cell>41.06 32.52</cell><cell>19.63</cell><cell>26.04</cell><cell>10.44</cell><cell>69.46 53.69</cell><cell>30.66</cell><cell>42.61</cell><cell>15.98</cell><cell>74.27</cell><cell>39.24</cell><cell>58.33</cell><cell>20.19</cell></row><row><cell></cell><cell cols="2">GSRTR w/ 4 layers 40.87 32.21</cell><cell>19.13</cell><cell>25.35</cell><cell>9.83</cell><cell>69.87 53.78</cell><cell>30.25</cell><cell>41.97</cell><cell>15.22</cell><cell>73.89</cell><cell>38.42</cell><cell>57.00</cell><cell>18.88</cell></row><row><cell>test</cell><cell cols="2">GSRTR w/ 8 layers 40.83 32.20 GSRTR w/ Post-LN 40.31 31.72</cell><cell>19.17 18.69</cell><cell>25.49 25.03</cell><cell>10.03 9.56</cell><cell>69.47 53.40 69.86 53.57</cell><cell>30.07 29.89</cell><cell>41.99 41.99</cell><cell>15.35 15.14</cell><cell>73.75 73.33</cell><cell>38.54 37.76</cell><cell>57.20 56.70</cell><cell>19.19 18.78</cell></row><row><cell></cell><cell>GSRTR</cell><cell>40.63 32.15</cell><cell>19.28</cell><cell>25.49</cell><cell>10.10</cell><cell>69.81 54.13</cell><cell>31.01</cell><cell>42.50</cell><cell>15.88</cell><cell>74.11</cell><cell>39.00</cell><cell>57.45</cell><cell>19.67</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-Based Context Aware Reasoning for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilini</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4736" to="4745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Background to Framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="250" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-scale Orderless Pooling of Deep Convolutional Activation Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention on Attention for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. Situation Recognition with Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4173" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully Convolutional Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Mortazavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11546" to="11556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel scene classification model combining ResNet based transfer learning and data augmentation with a filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="191" to="206" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent Models for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="455" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta Pseudo Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11557" to="11568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grounded Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="314" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Still Image Action Recognition by Predicting Spatial-Temporal Pixel Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjaneh</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2019.00019</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mixture-Kernel Graph Attention Network for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10363" to="10372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-End Video Instance Segmentation With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On Layer Normalization in the Transformer Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Commonly Uncommon: Semantic Sparsity in Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7196" to="7205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image Captioning with Semantic Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal Query Networks for Fine-grained Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4486" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single Image Action Recognition using Semantic Body Part Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-End Human Object Interaction Detection with HOI Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11825" to="11834" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
