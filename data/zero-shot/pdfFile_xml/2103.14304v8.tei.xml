<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D human pose estimation</term>
					<term>Transformer</term>
					<term>Strided convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the great progress in 3D human pose estimation from videos, it is still an open problem to take full advantage of a redundant 2D pose sequence to learn representative representations for generating one 3D pose. To this end, we propose an improved Transformer-based architecture, called Strided Transformer, which simply and effectively lifts a long sequence of 2D joint locations to a single 3D pose. Specifically, a Vanilla Transformer Encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce the redundancy of the sequence, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively shrink the sequence length and aggregate information from local contexts. The modified VTE is termed as Strided Transformer Encoder (STE), which is built upon the outputs of VTE. STE not only effectively aggregates long-range information to a single-vector representation in a hierarchical global and local fashion, but also significantly reduces the computation cost. Furthermore, a full-to-single supervision scheme is designed at both full sequence and single target frame scales applied to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision and hence helps produce smoother and more accurate 3D poses. The proposed Strided Transformer is evaluated on two challenging benchmark datasets, Human3.6M and HumanEva-I, and achieves state-of-the-art results with fewer parameters. Code and models are available at https://github.com/Vegetebird/StridedTransformer-Pose3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3 D human pose estimation is a classic computer vision task that aims to estimate 3D joint locations of a human body from images or videos. This task has drawn tremendous attention in the past decades <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> since it plays a significant role in wide applications, such as clinic <ref type="bibr" target="#b4">[5]</ref>, computer animation <ref type="bibr" target="#b5">[6]</ref>, action recognition <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b15">[16]</ref>, and human-robot interaction <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Many state-of-the-art approaches adopt a two-stage pipeline (i.e., 2D-to-3D lifting method) <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, ? ? ? STE VTE <ref type="figure">Fig. 1</ref>: Our Strided Transformer Encoder (STE) takes the outputs of Vanilla Transformer Encoder (VTE) as input (yellow) and generates a 3D pose for the target frame as output (top). The self-attention mechanism (blue) concentrates on global context and the strided convolution (green) aggregates information from local contexts.</p><p>which first estimates 2D keypoints and then lifts them to 3D space. Although the 2D-to-3D lifting method benefits from the reliable performance of 2D pose detectors, it is still a highly ill-posed problem due to the inherent ambiguity in depth, since multiple 3D interpretations can be projected to the same 2D pose in the image space.</p><p>To alleviate this problem, temporal context information has been investigated by many researchers. Some methods <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> leverage past and future data in the sequence to predict the 3D pose of the target frame. For instance, Cai et al. <ref type="bibr" target="#b23">[24]</ref> presented a local-to-global graph convolutional network to exploit spatio-temporal relations to estimate 3D keypoints from a 2D pose sequence. However, these approaches have small temporal receptive fields and limited temporal correlation windows, thus suffering from modeling long-range dependencies.</p><p>Vanilla Transformer <ref type="bibr" target="#b24">[25]</ref> is developed for exploiting longrange dependencies and achieves tremendous success in natural language processing <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and computer vision <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b31">[32]</ref>. It consists of a self-attention module and a positionwise feed-forward network (FFN). The self-attention module computes pairwise dot-product among all input elements to capture global-context information, and the FFN acts as pattern detectors over the input across all layers <ref type="bibr" target="#b32">[33]</ref>. Such a design looks like a good choice for the 2D-to-3D pose lifting method to capture long-range dependencies. However, there are several shortcomings in the Vanilla Transformer Encoder (VTE) <ref type="bibr" target="#b24">[25]</ref>: (i) The full-length sequence in the forward pass across all layers contains significant redundancy for video-based pose estimation as nearby poses are quite similar, as illustrated in <ref type="figure" target="#fig_0">Fig. 2.</ref> (ii) The time and memory complexity of the attention operation grows quadratically with the input length, making it very expensive to process long sequences. Thus, the receptive field may be forced to decrease in real-time applications, whereas a large receptive field is important to enhance the estimation consistency <ref type="bibr" target="#b33">[34]</ref>. (iii) The VTE architecture is less capable to extract fine-grained local feature patterns, which is well-known to be crucial for computer vision tasks. To mitigate these issues, we propose to gradually merge nearby poses to shrink the sequence length until one representation of the target pose is acquired. An alternative is to perform pooling operation after the FFN <ref type="bibr" target="#b26">[27]</ref>. However, lots of valuable information will be lost using pooling operation, and the local information can not be well exploited. Motivated by the previous methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref> that are able to elegantly handle variable-length sequences via temporal convolutions, we propose to replace fully-connected layers in FFN with strided convolutions to progressively reduce the sequence length. The modified Transformer is dubbed Strided Transformer Encoder (STE), as shown in <ref type="figure">Fig. 1</ref>. With the proposed STE, we can model both global and local information in a hierarchical architecture, and the computation in FFN can be traded off for constructing a deeper model to boost the model capacity.</p><p>Although the STE can aggregate long-range information to a single-pose representation, it remains a question whether this single representation is enough to represent a long sequence and how to make this representation work in improving the performance. We observe that directly supervising the model at a single target frame scale always breaks temporal smoothness among video frames, while only supervising at a full sequence scale cannot explicitly learn a specific representation for the target frame. These observations encourage us to develop a method that can effectively embed both scales into a learnable framework. Therefore, based on the outputs of VTE and STE, a full-to-single supervision scheme is designed at both full and single scales, which can impose extra temporal smoothness constraints at the full sequence scale and refine the estimation at the single target frame scale. This scheme brings great benefits in producing smoother and more accurate 3D poses.</p><p>The proposed architecture is called Strided Transformer, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Extensive experiments are conducted on two standard 3D human pose estimation datasets, i.e., Human3.6M <ref type="bibr" target="#b34">[35]</ref> and HumanEva-I <ref type="bibr" target="#b35">[36]</ref>. Experimental results show that the proposed method achieves state-of-the-art performance.</p><p>Our contributions are summarized as follows:</p><p>? We propose a new Transformer-based architecture for 3D human pose estimation called Strided Transformer, which can simply and effectively lift a long 2D pose sequence to a single 3D pose. ? To reduce the sequence redundancy and computation cost, Strided Transformer Encoder (STE) is introduced to gradually reduce the temporal dimensionality and aggregate long-range information into a single-vector representation of pose sequences in a hierarchical global and local fashion. ? A full-to-single supervision scheme is designed to impose extra temporal smoothness constraints during training at the full sequence scale and further refine the estimation at the single target frame scale. ? State-of-the-art results are achieved with fewer parameters on two commonly used benchmark datasets, making our method a strong baseline for Transformer-based 3D pose estimation.</p><p>II. RELATED WORK At the early stage of applying deep neural networks on 3D pose estimation task, many methods <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b39">[40]</ref> learned the direct mapping from RGB images to 3D poses (i.e., one-stage pose estimation). However, these methods require sophisticated architectures with high computation costs, which are impractical in realistic applications.</p><p>Two-stage pose estimation. Two-stage methods formulate the problem of 3D human pose estimation as 2D keypoint detection followed by 2D-to-3D lifting estimation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Recent works show that 3D locations of body joints can be efficiently and effectively recovered using detected 2D poses from state-of-the-art 2D pose detectors, and this 2Dto-3D pose lifting method outperforms one-stage approaches. For example, Martinez et al. <ref type="bibr" target="#b18">[19]</ref> lifted 2D joint locations to 3D space via a fully-connected residual network. Fang et al. <ref type="bibr" target="#b40">[41]</ref> proposed a pose grammar model to encode the human body configuration of human poses from 2D space to 3D space. To improve the generalization of the trained 2D-to-3D pose estimator, Gong <ref type="bibr" target="#b42">[43]</ref> introduced a pose augmentation framework that is differentiable. We also follow this two-stage pipeline because it is widely adopted among the state-of-theart methods in this domain. Video pose estimation. Recently, many approaches tried to exploit temporal information <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref> to improve the accuracy and the smoothness of the estimated 3D pose sequence. To predict temporally consistent 3D poses, Hossain et al. <ref type="bibr" target="#b22">[23]</ref> designed a sequence-to-sequence network with LSTM. Pavllo et al. <ref type="bibr" target="#b19">[20]</ref> introduced a fully convolutional model based on dilated temporal convolutions. Cai et al. <ref type="bibr" target="#b23">[24]</ref> directly chose the 3D pose of the target frame from the outputs of the proposed graph-based method and then fed it to a refinement model. To produce smoother 3D sequences, Wang et al. <ref type="bibr" target="#b43">[44]</ref> designed an U-shaped graph convolutional network and involved motion modeling into learning. However, the temporal connectivity of these architectures is inherently limited and is mainly constrained to simple sequential correlations. Different from most existing works that employed LSTMbased <ref type="bibr" target="#b22">[23]</ref>, graph-based <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, or temporal convolutional networks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b44">[45]</ref> to exploit temporal information, we propose a Transformer-based architecture to capture longrange dependencies from input 2D pose sequences. Furthermore, compared with previous methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref> that either utilize a refinement model or use a motion loss to improve estimations, we design a full-to-single supervision scheme that refines the intermediate predictions to produce smoother and more accurate estimations.</p><p>Visual Transformers. Transformer models first proposed in <ref type="bibr" target="#b24">[25]</ref> are commonly used in various language tasks. Recently, Transformers have shown promising performance in many computer vision tasks, such as object detection <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> and image classification <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. DETR <ref type="bibr" target="#b45">[46]</ref> presented a new Transformer-based design for object detection systems. ViT <ref type="bibr" target="#b47">[48]</ref> proposed to apply a standard Transformer architecture directly to sequential image patches for image classification. METRO <ref type="bibr" target="#b49">[50]</ref> introduced a Transformer frame-work to reconstruct 3D human pose and mesh from a single image. However, METRO focused on the one-stage pose estimation and ignores the temporal information across frames. Unlike DETR <ref type="bibr" target="#b45">[46]</ref>, ViT <ref type="bibr" target="#b47">[48]</ref>, or METRO <ref type="bibr" target="#b49">[50]</ref> that directly apply Transformer to images, we utilize a Transformer-based architecture to effectively map 2D keypoints to 3D poses. Additionally, efficient strided convolutions are incorporated into Transformer models to address the redundancy problem for the video-based 3D pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we first present an overview of the proposed Strided Transformer for 3D human pose estimation from a 2D video stream, and then show how our Transformer-based architecture learns a representative single-pose representation from redundant sequences resulting in an enhanced estimation. Finally, the complexity analysis of our network is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The overall framework of our proposed method is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. Given a sequence of the estimated 2D poses P = {p 1 , . . . , p T } from videos, we aim at reconstructing 3D joint locations X ? R J?3 for a target frame (center frame), where p t ? R J?2 denotes the 2D joint locations at frame t, T is the number of video frames, and J is the number of joints. The network contains a Vanilla Transformer Encoder (VTE) followed by a Strided Transformer Encoder (STE), which is trained in a full-to-single prediction scheme at both full sequence and single target frame scales. Specifically, VTE is first used to model long-range information and is supervised by the full sequence scale to enforce temporal smoothness. Then, the proposed STE aggregates the information to generate one target pose representation and is supervised by the single target frame scale to produce more accurate estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Strided Transformer Encoder</head><p>Despite the substantial performance gains achieved by Transformers <ref type="bibr" target="#b24">[25]</ref> in many computer vision tasks, the fulllength token representation makes it unsuitable for many video-based vision tasks that only require a single-vector representation of a sequence. To this end, STE is proposed to gradually compress the sequence of hidden states and model both global and local information in a hierarchical architecture. Each layer of the proposed STE consists of a multi-head selfattention (MSA) and a convolutional feed-forward network (CFFN).</p><p>1) Multi-head self-attention: The core mechanism of the Transformer model is MSA <ref type="bibr" target="#b24">[25]</ref>. Suppose there are a set of queries (Q), keys (K), and values (V ) of dimension d m . Then the MSA can be computed as:</p><formula xml:id="formula_0">head i = Self-Attn QW Q i , KW K i , V W V i ,<label>(1)</label></formula><formula xml:id="formula_1">MSA(Q, K, V ) = Concat (head 1 , . . . , head h ) W O , (2) where Self-Attn(Q, K, V ) = softmax QK T / ? d k V and W Q i ? R dm?d k , W K i ? R dm?d k , W V i ? R dm?dv , and W O ? R hdv?dm are parameter matrices.</formula><p>The hyperparameter h is the number of multi-attention heads, d m is the dimension of the model, and</p><formula xml:id="formula_2">d k = d v = d m /h in our implementation.</formula><p>2) Convolutional feed-forward network: In the existing fully-connected (FC) layers in the FFN of VTE (Eq. (3)), it always maintains a full-length sequence of hidden representations across all layers with a high computation cost. It contains significant redundancy for video-based pose estimation, as nearby poses are quite similar. However, to reconstruct more accurate 3D body joints of the target frame, crucial information should be extracted from the entire pose sequences. Therefore, it requires selectively aggregating useful information.</p><p>To tackle this issue, inspired by the previous works <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref> that employ temporal convolutions to effectively shrink the sequence length, we make modifications to the generic FFN. Given the input feature vector Z ? R T ?Din with T sequences and D in channels to generate an output of (T , D out ) features, the operation performed by FC in FFN can be formulated as:</p><formula xml:id="formula_3">FC t,dout (z) = Din i w dout,i * z t,i .<label>(3)</label></formula><p>If 1D convolution is considered with kernel size K and strided factor S, a strided convolution in CFFN can be computed as:</p><formula xml:id="formula_4">Conv S(t),cout (z) = Din i K k w dout,i,k * z S(t? K?1 2 +k),i . (4)</formula><p>In this way, fully-connected layers in FFN of VTE are replaced with strided convolutions. The modified VTE is termed as Strided Transformer Encoder (STE), which can be represented as:</p><formula xml:id="formula_5">Z n?1 = Z n?1 + MSA(LN(Z n?1 )),<label>(5)</label></formula><formula xml:id="formula_6">Z n = MaxPool(? n?1 ) + CFFN(LN(? n?1 )),<label>(6)</label></formula><p>where LN(?) denotes the layer normalization, MaxPool(?) denotes the max pooling operation, and n ? [1, . . . , N ] is the index of STE layers. The STE is a hierarchical global and local architecture, where the self-attention mechanism models global context Here, N 1 and N 2 denote the number of layers of the two modules, respectively. The hyperparameters k, s, d m and d f are the kernel size, the strided factor, the dimension, and the number of hidden units. The max pooling operation is applied to the residuals to match the temporal dimensions. and the strided convolution helps capture local contexts, as presented in <ref type="figure" target="#fig_2">Fig. 4</ref> (right). It gradually merges the nearby poses to a short sequence length representation, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. Importantly, through such a hierarchical design, the redundancy of the sequence and the computation cost can be reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>In this section, we describe how to use the proposed Transformer-based network architecture to estimate 3D human poses from a sequence of 2D poses. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, the proposed network is composed of four components: a pose embedding, a Vanilla Transformer Encoder (VTE), a Strided Transformer Encoder (STE), and a regression head.</p><p>1) Pose embedding: Given a sequence of the estimated 2D poses P ? R T ?J?2 , the pose embedding first concatenates (x, y) coordinates of the J joints for each frame to tokens P ? R T ?(J?2) , and then embeds each token to a high dimensional feature Z 0 ? R T ?dm using a 1D convolutional layer with d m channels, followed by batch normalization, dropout, and a ReLU activation.</p><p>2) Vanilla Transformer Encoder: Suppose that the VTE consists of N 1 layers, the learnable position embedding E 1 ? R T ?dm is used before the first layer of VTE, which can be formulated as follows:  Convolutional feed-forward networks are in blue where (3, 3, 256) denotes kernels of size 3 with strided factor 3 and 256 output channels. The tensor sizes are shown in parentheses, e.g., <ref type="bibr" target="#b26">(27,</ref><ref type="bibr" target="#b33">34)</ref> denotes 27 frames and 34 channels. Due to strided convolutions, the max pooling operation is applied to the residuals to match the shape of subsequent tensors.</p><formula xml:id="formula_7">Z 0 1 = Z 0 + E 1 .<label>(7)</label></formula><p>Then, given the embedded feature Z 0 1 , the VTE layers can be represented as:</p><formula xml:id="formula_8">Z n?1 1 = Z n?1 1 + MSA(LN(Z n?1 1 )),<label>(8)</label></formula><formula xml:id="formula_9">Z n 1 =? n?1 1 + FFN(LN(? n?1 1 )),<label>(9)</label></formula><p>where n ? [1, . . . , N 1 ] is the index of VTE layers. It can be expressed by using a function of a VTE layer VTE(?):</p><formula xml:id="formula_10">Z n 1 = VTE(Z n?1 1</formula><p>).</p><p>3) Strided Transformer Encoder: For the STE, it is built upon the outputs of VTE and takes the Z N1 1 ? R T ?dm as input. The learnable position embeddings E 2 ? R S(t)?dm with strided factor S are used for every layer of STE due to the different sequence lengths. Then, the STE layers can be represented as follows:</p><formula xml:id="formula_12">Z n 2 = STE(Z n?1 2 + E n 2 ),<label>(11)</label></formula><p>where n ? [1, . . . , N 2 ] is the index of STE layers, Z 0 2 = Z N1 1 , and STE(?) denotes the function of an STE layer whose details can be found in Eq. (5) and Eq. (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Regression head:</head><p>In order to perform the regression, a batch normalization and a 1D convolutional layer are applied to the outputs of VTE and STE, Z N1 1 ? R T ?dm and Z N2 2 ? R 1?dm , respectively. Finally, the outputs of 3D pose prediction areX and X, whereX ? R T ?J?3 and X ? R J?3 are predictions of the 3D pose sequence and the 3D joint locations of the target frame, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Full-to-Single Prediction</head><p>The iterative refinement scheme, aimed at producing predictions in multiple processing stages, is effective for 3D pose estimation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Motivated by the success of such iterative processing, we also consider a refinement scheme. A full-to-single scheme is proposed to incorporate both full sequence and single target frame scales constraints into the framework. This scheme further refines the intermediate predictions to produce more accurate estimations rather than using a single component with a single output. More precisely, the full sequence scale can enforce temporal smoothness and the single target frame scale helps learn a specific representation for the target frame.</p><p>1) Full sequence scale: The first step is to supervise at full sequence scale by imposing extra temporal smoothness constraints during training from the output of VTE followed by a regression head. A sequence loss L f is used to improve upon single frame predictions for temporal consistency over a sequence. This loss ensures that the estimated 3D pose sequencesX ? R T ?J?3 coincide with the ground truth 3D joint sequences Y ? R T ?J?3 :</p><formula xml:id="formula_13">L f = T t=1 J i=1 Y t i ?X t i 2 ,<label>(12)</label></formula><p>whereX t i and Y t i represent the sequence of estimated 3D poses and ground truth 3D joint locations of joint i at frame t, respectively.</p><p>2) Single target frame scale: In the second step, the supervision is adopted on the output of STE followed by a regression head. A single-frame loss L s is used to refine the estimation at the single target frame scale. It minimizes the distance between the estimated 3D pose X ? R J?3 and the target ground truth 3D joint annotation Y ? R J?3 :</p><formula xml:id="formula_14">L s = J i=1 Y i ? X i 2 ,<label>(13)</label></formula><p>where X i and Y i represent the target frame's estimated 3D pose and ground truth 3D joint locations of joint i, respectively.</p><p>3) Loss function: In our implementation, the model is supervised at both full sequence scale and single target frame scale. We train the entire network in an end-to-end manner with the total loss:</p><formula xml:id="formula_15">L = ? f L f + ? s L s ,<label>(14)</label></formula><p>where ? f and ? s are weighting factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Complexity Analysis</head><p>In this section, we use floating-point operations (FLOPs) to measure the computational cost and analyze the compression ratio of our proposed Strided Transformer network. Given the sequence length t, dimension d m = d f /2 = d, strided factor s, and kernel size k, the FLOPs of a VTE layer F n V T E and an STE layer F n ST E can be computed by:</p><formula xml:id="formula_16">F n V T E (t, d) = F n M SA (t, d) + F n F F N (t, d) = 8td 2 + 2t 2 d,<label>(15)</label></formula><formula xml:id="formula_17">F n ST E (t, d, s) = F n M SA (t, d, s) + F n CF F N (t, d, s) = (6 + 2s ?1 k)td 2 + 2t 2 d,<label>(16)</label></formula><p>where F n M SA , F n F F N , and F n CF F N are the FLOPs of the MSA, FFN, and CFFN, respectively.</p><p>Then if we consider N layers of VTE and STE with input sequence length T , dimension D, strided factor S, and kernel size K, the encoder-wise FLOPs of VTE F V T E can be formulated as:</p><formula xml:id="formula_18">F V T E = N F n V T E = N (8T D 2 + 2T 2 D),<label>(17)</label></formula><p>the encoder-wise FLOPs of STE F ST E can be formulated as:</p><formula xml:id="formula_19">F ST E = N n=1 F n ST E = N n=1 ( 6 + 2KS ?1 S n?1 )T D 2 + 2 S 2(n?1) T 2 D .<label>(18)</label></formula><p>For our 27-frame Strided Transformer, which contains N 1 VTE layers and N 2 STE layers with N 1 = N 2 = N = 3, S = 3, and K = 3. In this case, the compression ratio ? can be computed by:</p><formula xml:id="formula_20">? = 2F V T E F V T E + F ST E = 2 1 + ? ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_21">? = F ST E F V T E = 468D + 91T 972D + 243T .<label>(20)</label></formula><p>We have lim D?? ? = 1.35 with a fixed T . Thus, the compression ratio ? of our 27-frame Strided Transformer is 1.35.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Evaluation</head><p>The proposed method is evaluated on two challenging benchmark datasets, i.e., Human3.6M <ref type="bibr" target="#b34">[35]</ref> and HumanEva-I <ref type="bibr" target="#b35">[36]</ref>. Human3.6M dataset is the largest publicly available dataset for 3D human pose estimation, which consists of 3.6 million images captured from 4 synchronized cameras with 50 Hz. There are 7 professional subjects performing 15 daily activities such as "Waiting", "Smoking", and "Posing". Following the standard protocol in prior works <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, 5 subjects (S1, S5, S6, S7, S8) are used for training and 2 subjects (S9 and S11) are used for evaluation. The frames from all views are trained by a single model for all actions. HumanEva-I is a much smaller dataset with fewer subjects and actions compared to Human3.6M. Following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, our model is trained for all subjects (S1, S2, S3) and all actions (Walk, Jog, Box).</p><p>Three standard evaluation protocols are used in the experiments. The mean per joint position error (MPJPE) is the average Euclidean distance between the ground truth and predicted positions of the joints, which is referred to as protocol #1 in many works <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Procrustes analysis MPJPE (P-MPJPE) is adopted, where the estimated 3D pose is aligned to the ground truth in translation, rotation, and scale. This protocol is referred to as protocol #2 <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, we also report the mean per joint velocity error (MPJVE) corresponding to the MPJPE of the first derivative of the 3D pose sequences. This metric measures the smoothness of predictions over time and is vital for videobased 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In our experiments, the proposed Strided Transformer contains N 1 = N 2 = 3 encoder layers, h = 8 attention heads, d m = 256 dimensions, and d f = 512 hidden units for both VTE and STE. The kernel sizes k f and k m are set to 1 and 3 in all STE layers, respectively. The strided factor s f is set to 1, and s m is set to {3, 3, 3} for the receptive field of 27 frames, {9, 3, 3} for 81, {3, 9, 9} for 243, and {3, 9, 13} for 351. The weighting factors ? f and ? s are set to 1.</p><p>All experiments are conducted on the PyTorch framework with one GeForce GTX 3090 GPU. The network is trained using Amsgrad optimizer. An initial learning rate of 0.001 is used with a shrink factor of 0.95 applied after each epoch. The same refine module as <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref> is adopted. We only apply horizontal flip augmentation during training/test stages. The 2D poses can be obtained by performing any classic 2D pose detections or directly using the 2D ground truth. Following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b58">[59]</ref>, the cascaded pyramid network (CPN) <ref type="bibr" target="#b59">[60]</ref> is used for Human3.6M and Mask R-CNN [61] is adopted for HumanEva-I to obtain 2D poses for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-art Results</head><p>Our method is compared with previous state-of-the-art approaches on Human3.6M dataset. The performance of our 351-frame model with CPN input is reported in <ref type="table" target="#tab_1">Table I</ref>. Our method outperforms the state-of-the-art methods on Hu-man3.6M under all metrics (43.7 mm on protocol #1 and 35.2 mm on protocol #2). <ref type="table" target="#tab_1">Table II</ref> compares the computational complexity, MPJPE, and frame per second (FPS) with several state-of-the-art methods in different receptive fields on Human3.6M. Our model is lightweight and the number of parameters hardly increases with the increased receptive fields, which is practical for real-time applications. Compared with temporal convolutional networks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b44">[45]</ref>, our proposed Transformer-based network requires fewer total parameters with competitive performance for 3D pose estimation in videos. Besides, even though the inference speed of the proposed model is lower than <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b44">[45]</ref>, it still has an acceptable FPS for real-time inference. <ref type="figure" target="#fig_5">Fig. 7</ref> shows some qualitative comparisons with state-of-theart methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which indicates that our methods can produce more accurate 3D predictions.</p><p>To further explore the upper bound of our method, the results from 2D ground truth inputs are reported in <ref type="table" target="#tab_1">Table III</ref>. It can be seen that our method achieves the best result (28.5 mm in MPJPE), outperforming all other methods. This  demonstrates if a more robust 2D pose detection is available, our Strided Transformer can produce more accurate 3D poses.</p><p>As shown in <ref type="table" target="#tab_1">Table IV</ref>, with the supervision of full sequence scale, our method reduces the MPJVE by 15.4% (from 2.6 mm to 2.2 mm), achieving smoother predictions with lower MPJVE than other models. It indicates that the full-to-single supervision scheme can enhance temporal smoothness and produce vastly smoother poses.</p><p>To evaluate the generalizability of our model to smaller datasets, experiments are conducted on HumanEva-I based on Mask R-CNN 2D detections and 2D ground truth. The results in <ref type="table" target="#tab_5">Table V</ref> demonstrate that our method achieves promising results on all kinds of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>Input sequence length. The MPJPE results of our model with different sequence lengths (between 1 and 351) on Human3.6M are shown in <ref type="figure" target="#fig_4">Fig. 6 (a)</ref>. It can be seen that our proposed method obtains larger gains under both 2D pose inputs (CPN and GT) with more input frames used for predictions, but the error saturates past a certain point. This is expected since directly lifting 3D poses from disjointed 2D poses leads to temporally incoherent outputs <ref type="bibr" target="#b61">[62]</ref>. It is worth mentioning that our method gets a better result with T = 351 (43.7 mm) than T = 243 (44.0 mm), while the performance decreases with longer inputs (T &gt; 243) in <ref type="bibr" target="#b33">[34]</ref>. This indicates that our method equipped with the global self-attention mechanism is powerful in modeling longrange dependencies. Meanwhile, with the help of STE, our method can learn the representative representation from long sequences. Next, we choose T = 27 on Human3.6M in the following ablation experiments as a compromise between the accuracy and computational complexity.</p><p>2D detections. For the 2D-to-3D pose lifting task, the accuracy of the 2D detections directly influences the results of 3D pose estimation <ref type="bibr" target="#b18">[19]</ref>. To show the effectiveness of our method on different 2D pose detectors, we carry out experiments with the detections from Stack Hourglass (SH) <ref type="bibr" target="#b62">[63]</ref>, Detectron <ref type="bibr" target="#b19">[20]</ref>, and CPN <ref type="bibr" target="#b59">[60]</ref>. Moreover, to test the tolerance of our method to different levels of noise, we also train our network by 2D ground truth (GT) with various levels of additive Gaussian noises. The results are shown in <ref type="figure" target="#fig_4">Fig. 6 (b)</ref>. It can be observed that the MPJPE of 3D poses increases linearly with the two-norm errors of 2D poses. Besides, our method performs well on different 2D inputs, indicating the effectiveness and robustness of our method.</p><p>Model hyperparameters. As shown in <ref type="table" target="#tab_1">Table VI</ref>, we first   Walk Jog Box S1 S2 S3 S1 S2 S3 S1 S2 S3 Avg. analyze the effect of the number of VTE layers. Empirically, it can be found that the performance cannot be improved when naively stacking multiple standard Transformer encoder layers. Notably, our model equipped with STE is more accurate at the same number of Transformer encoder layers and comparable model parameters. For example, our method (N 1 = 3 and N 2 = 3) has better performance and fewer FLOPs than the model of N 1 = 6 at the same d m = 256 and d f = 512 (46.9 mm vs. 47.9 mm, 0.128G vs. 0.174G). In addition, our STE (N 2 = 3, 0.041G) also has fewer FLOPs than standard Transformer encoder (N 1 = 3, 0.087G) with similar parameters, which achieves 2.1? less computation. It verifies the effectiveness of our proposed STE in reducing computation cost and boosting performance. Then, we investigate the influence of various hyperparameters combinations to find the optimal network architecture. It can be observed that using 3 encoder layers of both VTE and STE modules, 256 dimensions, and 512 hidden units achieves the best performance. Strided factor. We observe that the strided factor of STE used in our Strided Transformer has an impact on the estimation performance. Here, we study the influence of using different design choices of strided factor of STE. The experimental results are depicted in <ref type="table" target="#tab_1">Table VII</ref>. It shows that using a strided factor s m = {3, 3, 3} has the best performance. This demonstrates the benefit of gradually reducing the temporal dimensionality with a small strided factor.</p><p>Prediction scheme. We further examine the proposed prediction scheme of full sequence scale and single target frame scale by using five different designs: (i) Full: the STE of our proposed method is replaced with VTE, and the new architecture is only supervised by the full sequence scale (the sequence loss). (ii) Single: the proposed method is only supervised by the single target frame scale (single-frame loss). (iii) Fullto-full: the architecture consists of six VTE layers, whose first three layers and final three layers are both supervised by the sequence loss. (iv) Single-to-single: VTE and STE of the proposed method are both supervised by the single-frame loss. (v) Full-to-single: our proposed method. In <ref type="table" target="#tab_1">Table VIII</ref>, it can be observed that the schemes of considering only one prediction manner (i, ii, iii, iv) decay performance, and our full-to-single prediction scheme (v) is the best. The empirical results indicate that our proposed full-to-single mechanism is    crucial for performance improvement. Model components. As shown in <ref type="table" target="#tab_1">Table IX</ref>, an ablation study is performed to assess the effectiveness of different components of our method. We select the center frame of intermediate predictions from VTE as final results, which increases the MPJPE by 1.2 mm (from 46.9 mm to 48.1 mm). It proves that the scheme of intermediate supervision can further improve estimation accuracy. Next, we perform pooling operation after FFN of VTE following <ref type="bibr" target="#b26">[27]</ref> and then replace STE of our proposed method with it. The new architecture is termed as Pooling Transformer, and its error increases by 0.4 mm, which highlights that our STE can preserve more valuable information than Pooling Transformer by exploiting  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head><p>Attention visualization. Our method is easily interpretable through visualizing the attention score across frames to explain what the target frame relies on. Visualization results of the multi-head attention maps of the first attention layers from VTE and STE (243-frame model) are shown in <ref type="figure">Fig. 8</ref>. The left map shows strong attention close to the input frames <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, while the right map mainly pays strong attention to the center frame across all the sequences. This is expected since the proposed full-to-single strategy enables the VTE and STE modules to learn different representations: (i) VTE selectively identifies important sequences that are close to the input frames and enforces temporal consistency across frames. (ii) STE learns a specific representation from the input sequences  <ref type="figure">Fig. 8</ref>: Multi-head attention maps (h = 8) from VTE and STE of our 243-frame model. It illustrates that the self-attention mechanism systematically assigns a weight distribution to frames, all of which might benefit the inference. Brighter color indicates higher attention score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ours ATTN-TCN TCN GCN <ref type="figure">Fig. 9</ref>: Qualitative comparisons on challenging in-thewild videos with previous state-of-the-art methods, ATTN-TCN <ref type="bibr" target="#b33">[34]</ref>, TCN <ref type="bibr" target="#b19">[20]</ref>, and GCN <ref type="bibr" target="#b23">[24]</ref>. The last row shows the failure case, where the 2D detector has failed badly.</p><p>using both past and future data, improving the representation ability of features to reach an optimal inference for the target frame. Note that a few attention head maps are sparse due to the different temporal patterns or semantics. 3D reconstruction visualization. We further evaluate our method on challenging in-the-wild videos from YouTube. <ref type="figure">Fig. 9</ref> shows the qualitative comparisons with the previous state-of-the-art methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>. We use the same 2D detector (cascaded pyramid network <ref type="bibr" target="#b59">[60]</ref>) to obtain 2D poses and then feed them to the models for a fair comparison. Despite the challenging samples with complex actions and fast movements, the proposed method can produce realistic and structurally plausible 3D predictions outperforming previous works. This demonstrates our method is robust to partial occlusions and tolerant to depth ambiguity. The last row shows the failure case caused by a big 2D detection error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we investigate the suitableness of applying a Transformer-based network to the task of video-based 3D human pose estimation. From the proposed Strided Transformer with Strided Transformer Encoder (STE) and full-to-single supervision scheme, we show how the representative singlepose representation can be learned from redundant sequences.</p><p>The key is to reasonably use strided convolutions in the Transformer architecture to aggregate long-range information into a single-vector pose in a hierarchical global and local fashion. Meanwhile, the computation cost can be reduced significantly. Moreover, our full-to-single supervision scheme enhances temporal smoothness and further refines the representation for the target frame. Comprehensive experiments on two benchmark datasets demonstrate that our method achieves superior performance compared with state-of-the-art methods.</p><p>Although our method can reduce the computation cost of Transformers, the computational complexity and runtime cost of our method are still larger than temporal convolutional networks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b44">[45]</ref>, indicated in <ref type="table" target="#tab_1">Table II</ref>. It is well acknowledged that the strong performance of Transformers comes at high computational costs. Note that the scope of this paper only targets improving FFN in the Transformer model. Future works may include designing a more efficient self-attention mechanism and extending our Strided Transformer to solve multi-view 3D human pose estimation. In addition, we hope that our approach would bring inspiration to the field of skeleton-based representation learning, e.g., action recognition, motion prediction, pose tracking, and so on. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Example of 2D pose sequences of 27 consecutive frames (520 ms) on Human3.6M dataset (captured from 50 Hz cameras). It contains huge redundant information as nearby poses are same. The rectangle denotes the center frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of our proposed Strided Transformer for predicting the 3D joint locations of the target frame (center frame) from the estimated 2D pose sequences. It mainly consists of a Vanilla Transformer Encoder (VTE) and a Strided Transformer Encoder (STE). The network first models long-range information via VTE and then aggregates the information into one target pose representation from the proposed STE. The model is trained end-to-end at both full sequence and single target frame scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The network architecture of our proposed Strided Transformer. The left is the VTE and the right is the STE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>An instantiation of the proposed Strided Transformer network. It reconstructs the target 3D body joints by progressively reducing the sequence length. The input consists of 2D keypoints for a receptive field of 27 frames with J = 17 joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>(a) Ablation studies on different sequence lengths of our method on Human3.6M with the MPJPE metric. (b) The impact of 2D detections on Human3.6M. Here, N (0, ? 2 ) represents the Gaussian noise with mean zero and ? is the standard deviation. (CPN) -Cascaded Pyramid Network; (SH) Stack Hourglass; (GT) -2D ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative comparisons with the previous state-of-the-art methods, TCN<ref type="bibr" target="#b19">[20]</ref> and ATTN-TCN<ref type="bibr" target="#b33">[34]</ref> on Human3.6M dataset. Wrong estimations are highlighted by red circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Visual results of our proposed method on Human3.6M dataset (first 3 rows) and HumanEva-I dataset (last 2 rows). Qualitative results on challenging wild videos. The number is the frame index of input videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative comparisons on Human3.6M under protocol #1 and protocol #2, where ? indicates the temporal information used in each method. Best in bold, second-best underlined.</figDesc><table><row><cell>Protocol #1</cell><cell>Dir.</cell><cell>Disc</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>Martinez et al. [19] ICCV'17</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. [41] AAAI'18</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Lee et al. [22] ECCV'18  ?</cell><cell>40.2</cell><cell>49.2</cell><cell>47.8</cell><cell>52.6</cell><cell>50.1</cell><cell>75.0</cell><cell>50.2</cell><cell>43.0</cell><cell>55.8</cell><cell>73.9</cell><cell>54.1</cell><cell>55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>Xu et al. [42] CVPR'21</cell><cell>45.2</cell><cell>49.9</cell><cell>47.5</cell><cell>50.9</cell><cell>54.9</cell><cell>66.1</cell><cell>48.5</cell><cell>46.3</cell><cell>59.7</cell><cell>71.5</cell><cell>51.4</cell><cell>48.6</cell><cell>53.9</cell><cell>39.9</cell><cell>44.1</cell><cell>51.9</cell></row><row><cell>Gong et al. [43] CVPR'21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.2</cell></row><row><cell>Cai et al. [24] ICCV'19  ?</cell><cell>44.6</cell><cell>47.4</cell><cell>45.6</cell><cell>48.8</cell><cell>50.8</cell><cell>59.0</cell><cell>47.2</cell><cell>43.9</cell><cell>57.9</cell><cell>61.9</cell><cell>49.7</cell><cell>46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell>Pavllo et al. [20] CVPR'19  ?</cell><cell>45.2</cell><cell>46.7</cell><cell>43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell>57.3</cell><cell>65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Lin et al. [51] BMVC'19  ?</cell><cell>42.5</cell><cell>44.8</cell><cell>42.6</cell><cell>44.2</cell><cell>48.5</cell><cell>57.1</cell><cell>42.6</cell><cell>41.4</cell><cell>56.5</cell><cell>64.5</cell><cell>47.4</cell><cell>43.0</cell><cell>48.1</cell><cell>33.0</cell><cell>35.1</cell><cell>46.6</cell></row><row><cell>Xu et al. [52] CVPR'20  ?</cell><cell>37.4</cell><cell>43.5</cell><cell>42.7</cell><cell>42.7</cell><cell>46.6</cell><cell>59.7</cell><cell>41.3</cell><cell>45.1</cell><cell>52.7</cell><cell>60.2</cell><cell>45.8</cell><cell>43.1</cell><cell>47.7</cell><cell>33.7</cell><cell>37.1</cell><cell>45.6</cell></row><row><cell>Liu et al. [34] CVPR'20  ?</cell><cell>41.8</cell><cell>44.8</cell><cell>41.1</cell><cell>44.9</cell><cell>47.4</cell><cell>54.1</cell><cell>43.4</cell><cell>42.2</cell><cell>56.2</cell><cell>63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>Zeng et al. [53] ECCV'20  ?</cell><cell>46.6</cell><cell>47.1</cell><cell>43.9</cell><cell>41.6</cell><cell>45.8</cell><cell>49.6</cell><cell>46.5</cell><cell>40.0</cell><cell>53.4</cell><cell>61.1</cell><cell>46.1</cell><cell>42.6</cell><cell>43.1</cell><cell>31.5</cell><cell>32.6</cell><cell>44.8</cell></row><row><cell>Wang et al. [44] ECCV'20  ?</cell><cell>40.2</cell><cell>42.5</cell><cell>42.6</cell><cell>41.1</cell><cell>46.7</cell><cell>56.7</cell><cell>41.4</cell><cell>42.3</cell><cell>56.2</cell><cell>60.4</cell><cell>46.3</cell><cell>42.2</cell><cell>46.2</cell><cell>31.7</cell><cell>31.0</cell><cell>44.5</cell></row><row><cell>Chen et al. [45] TCSVT'21  ?</cell><cell>41.4</cell><cell>43.5</cell><cell>40.1</cell><cell>42.9</cell><cell>46.6</cell><cell>51.9</cell><cell>41.7</cell><cell>42.3</cell><cell>53.9</cell><cell>60.2</cell><cell>45.4</cell><cell>41.7</cell><cell>46.0</cell><cell>31.5</cell><cell>32.7</cell><cell>44.1</cell></row><row><cell>Ours  ?</cell><cell>40.3</cell><cell>43.3</cell><cell>40.2</cell><cell>42.3</cell><cell>45.6</cell><cell>52.3</cell><cell>41.8</cell><cell>40.5</cell><cell>55.9</cell><cell>60.6</cell><cell>44.2</cell><cell>43.0</cell><cell>44.2</cell><cell>30.0</cell><cell>30.2</cell><cell>43.7</cell></row><row><cell>Protocol #2</cell><cell>Dir.</cell><cell>Disc</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>Martinez et al. [19] ICCV'17</cell><cell>39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Pavlakos et al. [54] CVPR'18</cell><cell>34.7</cell><cell>39.8</cell><cell>41.8</cell><cell>38.6</cell><cell>42.5</cell><cell>47.5</cell><cell>38.0</cell><cell>36.6</cell><cell>50.7</cell><cell>56.8</cell><cell>42.6</cell><cell>39.6</cell><cell>43.9</cell><cell>32.1</cell><cell>36.5</cell><cell>41.8</cell></row><row><cell>Liu et al. [55] ECCV'20</cell><cell>35.9</cell><cell>40.0</cell><cell>38.0</cell><cell>41.5</cell><cell>42.5</cell><cell>51.4</cell><cell>37.8</cell><cell>36.0</cell><cell>48.6</cell><cell>56.6</cell><cell>41.8</cell><cell>38.3</cell><cell>42.7</cell><cell>31.7</cell><cell>36.2</cell><cell>41.2</cell></row><row><cell>Gong et al. [43] CVPR'21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.1</cell></row><row><cell>Cai et al. [24] ICCV'19  ?</cell><cell>35.7</cell><cell>37.8</cell><cell>36.9</cell><cell>40.7</cell><cell>39.6</cell><cell>45.2</cell><cell>37.4</cell><cell>34.5</cell><cell>46.9</cell><cell>50.1</cell><cell>40.5</cell><cell>36.1</cell><cell>41.0</cell><cell>29.6</cell><cell>33.2</cell><cell>39.0</cell></row><row><cell>Lin et al. [51] BMVC'19  ?</cell><cell>32.5</cell><cell>35.3</cell><cell>34.3</cell><cell>36.2</cell><cell>37.8</cell><cell>43.0</cell><cell>33.0</cell><cell>32.2</cell><cell>45.7</cell><cell>51.8</cell><cell>38.4</cell><cell>32.8</cell><cell>37.5</cell><cell>25.8</cell><cell>28.9</cell><cell>36.8</cell></row><row><cell>Pavllo et al. [20] CVPR'19  ?</cell><cell>34.1</cell><cell>36.1</cell><cell>34.4</cell><cell>37.2</cell><cell>36.4</cell><cell>42.2</cell><cell>34.4</cell><cell>33.6</cell><cell>45.0</cell><cell>52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Xu et al. [52] CVPR'20  ?</cell><cell>31.0</cell><cell>34.8</cell><cell>34.7</cell><cell>34.4</cell><cell>36.2</cell><cell>43.9</cell><cell>31.6</cell><cell>33.5</cell><cell>42.3</cell><cell>49.0</cell><cell>37.1</cell><cell>33.0</cell><cell>39.1</cell><cell>26.9</cell><cell>31.9</cell><cell>36.2</cell></row><row><cell>Liu et al. [34] CVPR'20  ?</cell><cell>32.3</cell><cell>35.2</cell><cell>33.3</cell><cell>35.8</cell><cell>35.9</cell><cell>41.5</cell><cell>33.2</cell><cell>32.7</cell><cell>44.6</cell><cell>50.9</cell><cell>37.0</cell><cell>32.4</cell><cell>37.0</cell><cell>25.2</cell><cell>27.2</cell><cell>35.6</cell></row><row><cell>Wang et al. [44] ECCV'20  ?</cell><cell>32.9</cell><cell>35.2</cell><cell>35.6</cell><cell>34.4</cell><cell>36.4</cell><cell>42.7</cell><cell>31.2</cell><cell>32.5</cell><cell>45.6</cell><cell>50.2</cell><cell>37.3</cell><cell>32.8</cell><cell>36.3</cell><cell>26.0</cell><cell>23.9</cell><cell>35.5</cell></row><row><cell>Ours  ?</cell><cell>32.7</cell><cell>35.5</cell><cell>32.5</cell><cell>35.4</cell><cell>35.9</cell><cell>41.6</cell><cell>33.0</cell><cell>31.9</cell><cell>45.1</cell><cell>50.1</cell><cell>36.3</cell><cell>33.5</cell><cell>35.1</cell><cell>23.9</cell><cell>25.0</cell><cell>35.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Quantitative comparisons with state-of-the-art methods in different receptive fields on Human3.6M. The computational complexity, MPJPE, and frame per second (FPS) are reported. FPS is computed on a single GeForce GTX 2080 Ti GPU.</figDesc><table><row><cell>Model</cell><cell cols="5">T Param (M) FLOPs (G) MPJPE (mm) FPS</cell></row><row><cell cols="2">Pavllo et al. [20] 27</cell><cell>8.56</cell><cell>0.017</cell><cell>48.8</cell><cell>1492</cell></row><row><cell cols="2">Pavllo et al. [20] 81</cell><cell>12.75</cell><cell>0.025</cell><cell>47.7</cell><cell>1121</cell></row><row><cell cols="2">Pavllo et al. [20] 243</cell><cell>16.95</cell><cell>0.033</cell><cell>46.8</cell><cell>863</cell></row><row><cell>Chen et al. [45]</cell><cell>27</cell><cell>31.88</cell><cell>0.061</cell><cell>45.3</cell><cell>410</cell></row><row><cell>Chen et al. [45]</cell><cell>81</cell><cell>45.53</cell><cell>0.088</cell><cell>44.6</cell><cell>315</cell></row><row><cell cols="2">Chen et al. [45] 243</cell><cell>59.18</cell><cell>0.116</cell><cell>44.1</cell><cell>264</cell></row><row><cell cols="2">Ours (27 frames) 27</cell><cell>4.01</cell><cell>0.128</cell><cell>46.9</cell><cell>118</cell></row><row><cell cols="2">Ours (81 frames) 81</cell><cell>4.06</cell><cell>0.392</cell><cell>45.4</cell><cell>112</cell></row><row><cell cols="2">Ours (243 frames) 243</cell><cell>4.23</cell><cell>1.372</cell><cell>44.0</cell><cell>108</cell></row><row><cell cols="2">Ours (351 frames) 351</cell><cell>4.34</cell><cell>2.142</cell><cell>43.7</cell><cell>105</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Quantitative comparisons of MPJPE in millimeter on Human3.6M under protocol #1, using ground truth 2D joint locations as input. ? means the method utilizing temporal information. Best in bold.</figDesc><table><row><cell>Protocol #1</cell><cell>Dir.</cell><cell>Disc</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell cols="3">Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>Martinez et al. [19] ICCV'17</cell><cell>37.7</cell><cell>44.4</cell><cell>40.3</cell><cell>42.1</cell><cell>48.2</cell><cell>54.9</cell><cell>44.4</cell><cell>42.1</cell><cell>54.6</cell><cell>58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>Lee et al. [22] ECCV'18  ?</cell><cell>32.1</cell><cell>36.6</cell><cell>34.3</cell><cell>37.8</cell><cell>44.5</cell><cell>49.9</cell><cell>40.9</cell><cell>36.2</cell><cell>44.1</cell><cell>45.6</cell><cell>35.3</cell><cell>35.9</cell><cell>30.3</cell><cell>37.6</cell><cell>35.5</cell><cell>38.4</cell></row><row><cell>Pavllo et al. [20] CVPR'19  ?</cell><cell>35.2</cell><cell>40.2</cell><cell>32.7</cell><cell>35.7</cell><cell>38.2</cell><cell>45.5</cell><cell>40.6</cell><cell>36.1</cell><cell>48.8</cell><cell>47.3</cell><cell>37.8</cell><cell>39.7</cell><cell>38.7</cell><cell>27.8</cell><cell>29.5</cell><cell>37.8</cell></row><row><cell>Cai et al. [24] ICCV'19  ?</cell><cell>32.9</cell><cell>38.7</cell><cell>32.9</cell><cell>37.0</cell><cell>37.3</cell><cell>44.8</cell><cell>38.7</cell><cell>36.1</cell><cell>41.0</cell><cell>45.6</cell><cell>36.8</cell><cell>37.7</cell><cell>37.7</cell><cell>29.5</cell><cell>31.6</cell><cell>37.2</cell></row><row><cell>Xu et al. [42] CVPR'21</cell><cell>35.8</cell><cell>38.1</cell><cell>31.0</cell><cell>35.3</cell><cell>35.8</cell><cell>43.2</cell><cell>37.3</cell><cell>31.7</cell><cell>38.4</cell><cell>45.5</cell><cell>35.4</cell><cell>36.7</cell><cell>36.8</cell><cell>27.9</cell><cell>30.7</cell><cell>35.8</cell></row><row><cell>Liu et al. [34] CVPR'20  ?</cell><cell>34.5</cell><cell>37.1</cell><cell>33.6</cell><cell>34.2</cell><cell>32.9</cell><cell>37.1</cell><cell>39.6</cell><cell>35.8</cell><cell>40.7</cell><cell>41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell>Chen et al. [45] TCSVT'21  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.3</cell></row><row><cell>Zeng et al. [53] ECCV'20  ?</cell><cell>34.8</cell><cell>32.1</cell><cell>28.5</cell><cell>30.7</cell><cell>31.4</cell><cell>36.9</cell><cell>35.6</cell><cell>30.5</cell><cell>38.9</cell><cell>40.5</cell><cell>32.5</cell><cell>31.0</cell><cell>29.9</cell><cell>22.5</cell><cell>24.5</cell><cell>32.0</cell></row><row><cell>Ours</cell><cell>27.1</cell><cell>29.4</cell><cell>26.5</cell><cell>27.1</cell><cell>28.6</cell><cell>33.0</cell><cell>30.7</cell><cell>26.8</cell><cell>38.2</cell><cell>34.7</cell><cell>29.1</cell><cell>29.8</cell><cell>26.8</cell><cell>19.1</cell><cell>19.8</cell><cell>28.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Results show the velocity error (MPJPV) of our methods and other state-of-the-arts on Human3.6M. Here, * denotes our result without the supervision of full sequence scale. Best in bold.</figDesc><table><row><cell>MPJPV</cell><cell cols="5">Dir. Disc Eat Greet Phone</cell><cell cols="3">Photo Pose Purch.</cell><cell>Sit</cell><cell>SitD.</cell><cell cols="2">Smoke Wait</cell><cell cols="4">WalkD. Walk WalkT. Avg.</cell></row><row><cell>Pavllo et al. [20] CVPR'19</cell><cell>3.0</cell><cell>3.1</cell><cell>2.2</cell><cell>3.4</cell><cell>2.3</cell><cell>2.7</cell><cell>2.7</cell><cell>3.1</cell><cell>2.1</cell><cell>2.9</cell><cell>2.3</cell><cell>2.4</cell><cell>3.7</cell><cell>3.1</cell><cell>2.8</cell><cell>2.8</cell></row><row><cell>Lin et al. [51] BMVC'19</cell><cell>2.7</cell><cell>2.8</cell><cell>2.1</cell><cell>3.1</cell><cell>2.0</cell><cell>2.5</cell><cell>2.5</cell><cell>2.9</cell><cell>1.8</cell><cell>2.6</cell><cell>2.1</cell><cell>2.3</cell><cell>3.7</cell><cell>2.7</cell><cell>3.1</cell><cell>2.7</cell></row><row><cell>Chen et al. [45] TCSVT'21</cell><cell>2.7</cell><cell>2.8</cell><cell>2.0</cell><cell>3.1</cell><cell>2.0</cell><cell>2.4</cell><cell>2.4</cell><cell>2.8</cell><cell>1.8</cell><cell>2.4</cell><cell>2.0</cell><cell>2.1</cell><cell>3.4</cell><cell>2.7</cell><cell>2.4</cell><cell>2.5</cell></row><row><cell>Wang et al. [44] ECCV'20</cell><cell>2.3</cell><cell>2.5</cell><cell>2.0</cell><cell>2.7</cell><cell>2.0</cell><cell>2.3</cell><cell>2.2</cell><cell>2.5</cell><cell>1.8</cell><cell>2.7</cell><cell>1.9</cell><cell>2.0</cell><cell>3.1</cell><cell>2.2</cell><cell>2.5</cell><cell>2.3</cell></row><row><cell>Ours  *</cell><cell>2.8</cell><cell>2.8</cell><cell>2.1</cell><cell>3.2</cell><cell>2.2</cell><cell>2.5</cell><cell>2.6</cell><cell>2.8</cell><cell>1.8</cell><cell>2.4</cell><cell>2.1</cell><cell>2.3</cell><cell>3.5</cell><cell>3.0</cell><cell>2.6</cell><cell>2.6</cell></row><row><cell>Ours</cell><cell>2.4</cell><cell>2.5</cell><cell>1.8</cell><cell>2.8</cell><cell>1.8</cell><cell>2.2</cell><cell>2.2</cell><cell>2.5</cell><cell>1.5</cell><cell>2.0</cell><cell>1.8</cell><cell>1.9</cell><cell>3.2</cell><cell>2.5</cell><cell>2.1</cell><cell>2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell>: Quantitative results on HumanEva-I dataset under</cell></row><row><cell>protocol #2. Best in bold, second-best underlined. (MRCNN)</cell></row><row><cell>-Mask-RCNN; (GT) -2D ground truth.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation study on the hyperparameters of our model on Human3.6M under protocol #1. N 1 and N 2 are the number of VTE and STE layers, respectively. d m and d f are the dimensions and the number of hidden units.</figDesc><table><row><cell cols="3">N 1 N 2 dm</cell><cell>d f</cell><cell cols="3">Param (M) FLOPs (G) MPJPE (mm)</cell></row><row><cell>2</cell><cell>-</cell><cell cols="2">512 2048</cell><cell>6.36</cell><cell>0.342</cell><cell>47.9</cell></row><row><cell>3</cell><cell>-</cell><cell cols="2">512 2048</cell><cell>9.51</cell><cell>0.514</cell><cell>47.8</cell></row><row><cell>4</cell><cell>-</cell><cell cols="2">512 2048</cell><cell>12.66</cell><cell>0.685</cell><cell>48.0</cell></row><row><cell>5</cell><cell>-</cell><cell cols="2">512 2048</cell><cell>15.82</cell><cell>0.856</cell><cell>48.4</cell></row><row><cell>6</cell><cell>-</cell><cell cols="2">512 2048</cell><cell>18.97</cell><cell>1.028</cell><cell>49.3</cell></row><row><cell>2</cell><cell>-</cell><cell>256</cell><cell>512</cell><cell>1.08</cell><cell>0.058</cell><cell>47.8</cell></row><row><cell>3</cell><cell>-</cell><cell>256</cell><cell>512</cell><cell>1.61</cell><cell>0.087</cell><cell>47.6</cell></row><row><cell>4</cell><cell>-</cell><cell>256</cell><cell>512</cell><cell>2.13</cell><cell>0.116</cell><cell>47.8</cell></row><row><cell>5</cell><cell>-</cell><cell>256</cell><cell>512</cell><cell>2.66</cell><cell>0.145</cell><cell>47.7</cell></row><row><cell>6</cell><cell>-</cell><cell>256</cell><cell>512</cell><cell>3.19</cell><cell>0.174</cell><cell>47.9</cell></row><row><cell>-</cell><cell>3</cell><cell>256</cell><cell>512</cell><cell>2.42</cell><cell>0.041</cell><cell>48.0</cell></row><row><cell>2</cell><cell>3</cell><cell>256</cell><cell>512</cell><cell>3.48</cell><cell>0.099</cell><cell>47.4</cell></row><row><cell>3</cell><cell>3</cell><cell>256</cell><cell>512</cell><cell>4.01</cell><cell>0.128</cell><cell>46.9</cell></row><row><cell>2</cell><cell>3</cell><cell cols="2">512 2048</cell><cell>22.18</cell><cell>0.589</cell><cell>47.4</cell></row><row><cell>3</cell><cell>3</cell><cell cols="2">512 2048</cell><cell>25.33</cell><cell>0.761</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation study on the strided factor of STE with the receptive field T = 3 ? 3 ? 3 = 27. The evaluation is performed on Human3.6M under protocol #1.</figDesc><table><row><cell>Layers</cell><cell>Strided factor</cell><cell>MPJPE (mm)</cell></row><row><cell>3</cell><cell>3, 3, 3</cell><cell>46.9</cell></row><row><cell>3</cell><cell>3, 9, 1</cell><cell>47.5</cell></row><row><cell>3</cell><cell>9, 3, 1</cell><cell>47.3</cell></row><row><cell>2</cell><cell>3, 9</cell><cell>47.2</cell></row><row><cell>2</cell><cell>9, 3</cell><cell>47.1</cell></row><row><cell>1</cell><cell>27</cell><cell>47.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII :</head><label>VIII</label><figDesc>Ablation study on different prediction schemes. The evaluation is performed on Human3.6M under protocol #1. ? represents the performance gap between the methods and ours.</figDesc><table><row><cell>Prediction scheme</cell><cell>MPJPE (mm)</cell><cell>?</cell></row><row><cell>Full</cell><cell>47.9</cell><cell>1.0</cell></row><row><cell>Single</cell><cell>48.3</cell><cell>1.4</cell></row><row><cell>Full-to-full</cell><cell>47.4</cell><cell>0.5</cell></row><row><cell>Single-to-single</cell><cell>48.5</cell><cell>1.6</cell></row><row><cell>Full-to-single</cell><cell>46.9</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX :</head><label>IX</label><figDesc>Ablation study on each component of our network architecture on Human3.6M under protocol #1.</figDesc><table><row><cell>Method</cell><cell>MPJPE (mm)</cell></row><row><cell>Ours, proposed</cell><cell>46.9</cell></row><row><cell>Ours, intermediate predictions</cell><cell>48.1</cell></row><row><cell>Ours, Pooling Transformer</cell><cell>47.3</cell></row><row><cell>w/o VTE</cell><cell>48.0</cell></row><row><cell>w/o STE</cell><cell>47.6</cell></row><row><cell cols="2">local contexts to aggregate information. Removing VTE (only</cell></row><row><cell cols="2">trained with single-frame loss) leads to a 1.1 mm increase</cell></row><row><cell cols="2">in MPJPE error. Besides, removing STE (only trained with</cell></row><row><cell cols="2">sequence loss) increases the MPJPE to 47.6 mm. These results</cell></row><row><cell cols="2">validate the importance of both VTE and STE modules in our</cell></row><row><cell cols="2">Strided Transformer, where VTE mainly models long-range</cell></row><row><cell cols="2">information and STE focuses on aggregating information in a</cell></row><row><cell>hierarchical global and local fashion.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1888" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3-d reconstruction of human body shape from a single commodity depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="123" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3dbodynet: Fast reconstruction of 3d animatable human body shape from a single commodity depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munteanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion capture assisted animation: Texturing and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 29th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth pooling based large-scale 3-d action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1051" to="1061" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust 3d action recognition through sampling local appearances and global distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1932" to="1947" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning composite latent structures for 3d human action representation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2195" to="2208" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Constructing stronger and faster baselines for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15125</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning multi-granular spatio-temporal graph network for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the 29th ACM International Conference on Multimedia (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4334" to="4342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory attention networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unik: A unified framework for real-world skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08580</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Action recognition using 3d histograms of texture and a multi-class boosting classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4648" to="4660" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitemporal depth motion maps-based local binary patterns for 3-d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="22" to="590" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human 3d pose estimation with a tilting camera for social mobile robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garcia-Salguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-A</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">4943</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teaching robots to predict human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Weaklysupervised cross-view 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zihang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guokun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">L V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey on visual transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transreid: Transformer-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04378</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Trear: Transformer-based rgb-d egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03904</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting better feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1469" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformer guided geometry model for flow-based unsupervised visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transformer feed-forward layers are key-value memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14913</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature boosting network for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="501" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8575" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13985</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Anatomyaware 3d human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09760</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep videobased 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Convbert: Improving bert with span-based dynamic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS), 2020. VI. APPENDIX</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
