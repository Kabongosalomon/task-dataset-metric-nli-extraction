<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan?1</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel</orgName>
								<address>
									<addrLine>Hill 2 Huggingface</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei?1</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel</orgName>
								<address>
									<addrLine>Hill 2 Huggingface</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
							<email>thomas@huggingface.co</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel</orgName>
								<address>
									<addrLine>Hill 2 Huggingface</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel</orgName>
								<address>
									<addrLine>Hill 2 Huggingface</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video understanding relies on perceiving the global content and modeling its internal connections (e.g., causality, movement, and spatio-temporal correspondence). To learn these interactions, we apply a mask-then-predict pre-training task on discretized video tokens generated via VQ-VAE. Unlike language, where the text tokens are more independent, neighboring video tokens typically have strong correlations (e.g., consecutive video frames usually look very similar), and hence uniformly masking individual tokens will make the task too trivial to learn useful representations. To deal with this issue, we propose a block-wise masking strategy where we mask neighboring video tokens in both spatial and temporal domains. We also add an augmentation-free contrastive learning method to further capture the global content by predicting whether the video clips are sampled from the same video. We pre-train our model on uncurated videos and show that our pre-trained model can reach state-of-the-art results on several video understanding datasets (e.g., SSV2, Diving48). Lastly, we provide detailed analyses on model scalability and pre-training method design. 2 Preprint. Under review.</p><p>Unsupervised representation learning, with the promise of learning from large-scale unlabeled data, has drawn increasing attention in recent years, in both computer vision and natural language processing (NLP) communities. Most mainstream self-supervised methods can be categorized into three general directions: generative, denoising, and discriminative <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Generative and denoising methods seek to generate or reconstruct corrupted text/image/video tokens according to their empirical distributions. In generative and auto-regressive methods, next tokens are predicted given a causal context <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61]</ref> while denoising methods seek to reconstruct corrupted or masked tokens given an extended context <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref> For text, since the tokens (words or subwords <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b66">67]</ref>) are discrete and has relatively high entropy rate, language modeling has became the de-facto approach for pre-training models for most natural language tasks <ref type="bibr" target="#b52">[53]</ref>. In the case of images, generative approaches often operate on pixel space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b69">70]</ref>, which can be extremely expensive for larger input size like videos and has hence limited the widespread adoption of these methods. Recently, discretizing images and videos with discrete variational auto-encoders (VQ-VAE),</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, state-of-the-art self-supervised methods have been exploring different directions for pre-training images and text representations, with Contrastive Learning (CL) providing strong results for vision representation learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59]</ref>, and Language Modeling (LM) becoming the de-facto standard in natural language processing (NLP) pre-training <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b68">69]</ref>. These two approaches are quite different from each other. A contrastive objective compares positive/negative examples at a coarse/sample level, focusing on global content (e.g., for image classification) while a token modeling objective predict missing tokens from context at a much finer/sub-sample level to model sequential and short range interactions between tokens (e.g. in text generation tasks). Interestingly, video understanding naturally combines both types of requirements. 2D processing along the spatial dimensions of the video bears similarity to image processing, while 1D processing along the temporal dimension often involves modeling sequential events and short range coherence.</p><p>Hence, in this work, we propose to combine both text and image representation learning approaches for improved video pre-training, taking advantage of recent advances in self-supervised methods of both fields. We name our method as VIMPAC: VIdeo pre-training via Masked token Prediction And Contrastive learning. From language research, we adopt a 'masked language model' pre-training objective <ref type="bibr" target="#b15">[16]</ref> where a model is trained to reconstruct local masked regions in images or videos. From the computer vision world, we borrow a contrastive learning objective, specifically the InfoNCE <ref type="bibr" target="#b45">[46]</ref> objective applied on positive/negative video samples. While the masked language model objective Equal contribution. <ref type="bibr" target="#b1">2</ref> Code is released at https://github.com/airsplay/vimpac. The pretrained checkpoints and scripts will be soon open-sourced in HuggingFace transformers. encourages models to learn low-level semantics and sequential interaction, the contrastive loss provide a supervision for the models to learn more global and separable representations that are useful for many downstream tasks (e.g., action classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57]</ref>). Combining both objectives allow to provide training signal covering complementary aspects of a video signal: while short range correlations can be predominantly modeled from the training signal of the mask-and-predict task, the contrastive learning objective can provide signal on a more coarse-grained global-context and semantic level.</p><p>However, unlike language and its compact vocabulary of discrete tokens, videos are typically represented as RGB pixels in an almost continuous, high dimensional vector space. Naively masking pixels in videos induces a prohibitive computation cost while also tending to over-emphasize local details. To overcome these issues, we first tokenize input videos using the latent codes of a pretrained Vector Quantized-Variational Auto-Encoder (VQ-VAE) <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62]</ref> to encode them in smaller quantized representations on which a reconstruction model can then be trained with a masked token modeling objective. In practice, we also discovered that models trained with a uniform random token masking strategy can fail to learn meaningful and useful visual representations as neighboring pixels may contain very similar and correlated content (in particular along the temporal frame axis), making the task of predicting a randomly masked token from its visible neighbors easy. We therefore also introduce a block-masking scheme for videos by simultaneously masking video tokens in a contiguous 3D spatio-temporal block. Reconstructing such an extended spatio-temporal cube requires performing long-range predictions, forcing the models to learn a more complex set of relations between the video tokens, resulting in better visual representations.</p><p>Our contrastive learning approaches also departs from previous work in several aspects. First, since we apply the contrastive objective on token-discretized video samples and in combination with the token modeling loss, we observe strong performance without requiring the usual extensive set of data augmentations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48]</ref>. Second, we are able to leverage positive clip pairs that are temporally distant from each other (can be as far as 400 seconds away), while previous work favors using positives within a shorter range (maximum 36 seconds for uncurated videos in <ref type="bibr" target="#b20">[21]</ref> or 10 seconds in <ref type="bibr" target="#b47">[48]</ref>).</p><p>We evaluate the performances of our method VIMPAC on several video understanding datasets including two temporally-heavy tasks, SSV2 and Diving48 on which it achieves state-of-the-art results with regard to both self-supervised and supervised pre-training works and a set of more spatial-heavy datasets (UCF101, HMDB51, and Kinetics-400) on which it achieve competitive results with regards to the literature. Overall, taking advantage of VQ-VAE discretized video tokens, we present a method for self-supervised learning of video representations that combines two general streams of research in self-supervision: masked language modeling and contrastive learning. Our contribution is 3-folds: (i) We apply the mask-then-predict task to video understanding and introduce the use of block masking. (ii) We propose a contrastive learning method which is able to achieve strong performance without spatial data augmentation. (iii) We empirically show that this method can achieve state-of-the-art results on several video classification datasets. We also present comprehensive ablation studies to analyze the various aspects of our proposed approach.  <ref type="figure">Figure 1</ref>: Illustration of our VIMPAC framework. Frames are sampled from the video clip and discretized by VQ-VAE encoder. The tokens from VQ-VAE are then block-masked (in light yellow blocks). The model is self-supervised by two tasks: 1) mask-then-predict task predicts the masked tokens from visible context; 2) contrastive learning task classifies the positive examples (details in <ref type="figure">Fig. 2</ref>) with the feature of the additional [CLS] token. For space limit, we only show 2 frames and a smaller token map.</p><p>has been explored in compression and generative setups <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b67">68]</ref>. Such approaches avoid modeling pixel-level details and have enabled the use of generative models for images and videos <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b63">64]</ref>. Differing from these works, our framework investigates the use of such quantized representations in a denoising/reconstruction setup rather than generative, which has been shown in the NLP community to learn better representations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref>. Moreover, beyond simply applying MLM to the video tokens, we propose a block masking strategy to reduce the strong local correlation in neighboring video tokens. This 3D block masking strategy is inspired from recent span-masking schemes <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> for language modeling.</p><p>The other direction of research, which our framework combines, is discriminative methods which start from the hypothesis that learning to reconstruct local details is not necessary for learning good visual representations. In some of these approaches, an objective is constructed around hand-crafted heuristics tasks like spatial arrangement, color, playback speed or frame order predictions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b71">72]</ref>. Another line of discriminative approaches is contrastive learning which aims at training a model to be able to recognize different views (e.g., different augmentation of images or different temporal samples of videos) of the same image or video, as a way to learn general representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. This direction of research is reminiscent of sentence-order prediction tasks introduced in NLP <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref> with the goal of predicting whether two text sequences should be juxtaposed or not, an approach challenged in more recent literature <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref>. In the present work, inspired by visual representation rather than text representation learning literature, we adapt the contrastive learning approach to video by training a model to differentiate pairs of clips from a single video from pairs of clips from disparate videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we present our proposed video pre-training method VIMPAC (illustrated in <ref type="figure">Fig. 1</ref>) as well its detailed components. We first introduce the mask-then-predict task in Sec. 3.1, and then the contrastive learning task in Sec. 3.2. Lastly, we discuss how these two tasks are combined in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mask-then-Predict Task</head><p>Suppose that a video clip input comprises T frames tf 1 , f 2 , . . . , f T u, the mask-then-predict task learns video representations by predicting the masked contents from their spatio-temporal context. Denote the set of mask-token locations as M , we learn to predict the original tokens tx t,i,j u (see  <ref type="figure">Figure 2</ref>: Illustration of pre-training task details. In (a), block masking constructs the 3D-contiguous masking cube instead of independently sampling masked tokens (i.e., i.i.d masking). In (b), given the reference video clip, the positive clip is randomly sampled from the same video (video 1) while negative clips are sampled from other videos (video 2 and video 3). No spatial augmentations are applied to the raw video clips. details below) by optimizing the negative log-likelihood:</p><formula xml:id="formula_0">L mask "?1 |M | ? t,i,jPM log p t,i,j`xt,i,j | tx t 1 ,i 1 ,j 1 u t 1 ,i 1 ,j 1 PM C?,<label>(1)</label></formula><p>where M C is the complement of M and thus indicates the unmasked context.</p><p>Video Quantization with VQ-VAE. Since directly applying mask-then-predict over raw pixels and masking/predicting pixels leads to prohibitive computational costs and also tends to make the model overfit on detailed low-level visual information, we quantize the input videos with Vector Quantized-Variational Auto Encoder (VQ-VAE) <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62]</ref>. The VQ-VAE encoder takes an image as input and produces a token map, where the tokens belong to a predefined vocabulary V of cardinal 'vocabulary size'. The VQ-VAE decoder then tries to reconstruct the original image from these latent codes. In our method, we use a frozen and pretrained generic VQ-VAE encoder as a compressor that converts an input from an original input space R H?W?3 into a discretized space rV s H 8?W 8 . We independently apply the VQ-VAE encoder to each frame f t inside a clip. <ref type="bibr" target="#b2">3</ref> We keep the VQ-VAE weights frozen and do not finetune or adapt this model on our corpus.</p><p>Block Masking For sampling tokens to mask, the original BERT methods proposes the i.i.d. (independent and identically distributed) random mask M iid that constitutes of masked tokens:</p><formula xml:id="formula_1">M iid " tpt, i, jq | U t,i,j r0, 1s ? ?u,<label>(2)</label></formula><p>where U t,i,j r0, 1s is the uniform distribution from 0 to 1 and ? is the threshold. Intuitively, ? is the expectation of masked-token ratio and hence controls the difficulty of our mask-then-predict task. In our early experiments, we found it easy to infer a masked token from its direct spatio-temporal neighbours (e.g., neighboring frames in a video tend to look similar thus contain similar tokens).</p><p>To overcome this issue, we propose to use block masking (see <ref type="figure">Fig. 2</ref> (a)), which masks continuous tokens inside spatio-temporal blocks. For each mask block B, we randomly sample lower (B?, 0 ) and upper boundaries (B?, 1 ) for each of the temporal (T ), height (H), and width (W ) dimensions. The direct product of the intervals delimited by these boundaries constructs the block mask. The final mask M block is the union of them:</p><formula xml:id="formula_2">M block " ? B rB T,0 , B T,1 s?rB H,0 , B H,1 s?rB W,0 , B W,1 s.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Learning</head><p>Contrastive learning aims to distinguishing positive pairs from negative pairs (see <ref type="figure">Fig. 2</ref>  Spatially-Heavy Datasets Temporally-Heavy Datasets <ref type="figure">Figure 3</ref>: Example videos from temporally-heavy (left) and spatially-heavy (right) datasets. For the spatially-heavy video example, we can easily recognize the action 'apply eye makeup' from any of its frames. However, in order to recognize the action 'open the bottle cap' from the temporally-heavy data example, we need to consider the relations between multiple frames.</p><p>InfoNCE <ref type="bibr" target="#b45">[46]</ref> loss is used to distinguishes the positive feature pair (f i , f 1 i ) from the negative pairs</p><formula xml:id="formula_3">? ttpf i , f k q, pf i , f 1 k qu | k ? iu for each clip c i : L InfoNCE piq "?log exp pf J i f 1 i {?q ? k?i exp pf J i f k {?q`? k exp pf J i f 1 k {?q ,<label>(4)</label></formula><p>which we combine with the symmetric loss L 1 InfoNCE piq for paired clip sample c 1 i . The final loss for a mini batch L cl is the average loss for all n clips in the mini-batch:</p><formula xml:id="formula_4">L cl " 1 n n ? i"1 L InfoNCE piq`1 n n ? i"1 L 1 InfoNCE .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-Training Objective</head><p>We combine the two pre-training methods discussed above to define the overall objective as:</p><formula xml:id="formula_5">L " L mask`? ?L cl ,<label>(6)</label></formula><p>where ? is a hyperparameter controlling the weight of the contrastive loss and multiplying the temperature ? will smooth training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>. The inputs for both tasks are shared in mini-batches with the contrastive learning loss using the same block-masked inputs necessary for the mask-then-predict task. We highlight that the masked tokens for the denoising task are the only noise introduced in the contrastive learning, and that no other data augmentation is applied to raw pixels, in contrast to previous vision contrastive learning methods in which data-augmentation was paramount to the final performances of the model. This phenomenon is empirically studied in Sec. 5.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modeling</head><p>The model architecture follows the standard transformer architecture in its post-layer-norm variant <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b62">63]</ref> with two more recent additions: divided temporal-spatial attention <ref type="bibr" target="#b5">[6]</ref>, and sparse spatial attention <ref type="bibr" target="#b13">[14]</ref>. The model embedding layer maps the discrete tokens tx t,i,j u of a quantized input video (see Sec. 3.1) into dense vectors and sum them with positional embeddings. The backbone transformer model then outputs corresponding features th t,i,j u. We append an additional [CLS] token to each input sequence following <ref type="bibr" target="#b15">[16]</ref> and use its output feature h cls as a representation for the whole video. For pre-training, we use two heads: a 2-layer MLP after each token outputs th t,i,j u for the mask-then-predict task following BERT <ref type="bibr" target="#b15">[16]</ref>, and a 3-layer MLP after the CLS output h cls for the contrastive learning task following SimCLR <ref type="bibr" target="#b10">[11]</ref>. For fine-tuning on classification tasks, we remove the pre-training heads and add a fully-connected layer to the [CLS] output h cls . Please see Sec. A for a detailed model description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For pre-training, we use the HowTo100M dataset <ref type="bibr" target="#b44">[45]</ref>. This dataset is constructed by searching YouTube videos with a list of text queries, it is significantly larger and more diverse than humanannotated datasets such as Kinetics 400 <ref type="bibr" target="#b7">[8]</ref>. HowTo100M has 1.2M uncurated videos, with an average duration of 6.5 minutes. We only use videos and do not use other signals such as ASR captions in this dataset. For downstream evaluation, we experiment with several action classification datasets: UCF101 <ref type="bibr" target="#b55">[56]</ref>, HMDB51 <ref type="bibr" target="#b36">[37]</ref>, Kinetics-400 <ref type="bibr" target="#b8">[9]</ref>, SSV2 <ref type="bibr" target="#b25">[26]</ref>, and Diving48 <ref type="bibr" target="#b40">[41]</ref>. It is important to note that in many cases, actions in UCF101, HMDB51, and Kinetics-400 can be recognized from a single frame of the video, thus these datasets are 'spatially-heavy'. As a consequence, image-level methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49]</ref> show competitive results without modeling the temporal interactions inside the videos. To test the video model's ability beyond recognizing static images, we lay our focus on 'temporally-heavy' datasets (SSV2 and Diving48), in which action recognition from a single frame is more difficult. For example, it is almost impossible to distinguish two SSV2 classes open something and close something without reasoning across frames, and the same for different diving classes in Diving48.</p><p>We show examples of temporally-heavy and spatially-heavy datasets in <ref type="figure">Fig. 3</ref>. More examples are in Sec. G.1. Additional dataset details and statistics are presented in Sec. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>Our model shapes follow BERT LARGE with 24 layers and hidden size 1024, but with halved attention head size and MLP intermediate size as in <ref type="bibr" target="#b13">[14]</ref>. For pre-training, we train the model for 100 epochs on the HowTo100M dataset with frames sampled at 2 FPS. We create the training inputs by sampling two clips from each video as described in Sec. 3.2. To reduce computation cost, the first 90 epochs are trained with a smaller input resolution (#frames T =5 and frame size S=128) and we increase the spatial resolution (T =5, S=256) for the last 10 epochs following <ref type="bibr" target="#b15">[16]</ref>. Positional embeddings are interpolated as in <ref type="bibr" target="#b17">[18]</ref> when input resolution changes. Importantly, our pre-training scheme does not involve spatial augmentations: all frames are resized and centered cropped without random flipping, color distortion, etc. We use a batch size of 1024 in pre-training. The number of negative clips used for contrastive learning is 255 for the first 90 epochs and 127 for the last 10 epochs. The number of negative pairs used in our ablation analyses is kept constant at 127. <ref type="bibr" target="#b3">4</ref> For fine-tuning, we use more input frames (T =10 and S=256), and batch size 128. We sample frames at 2 FPS for datasets with longer videos (i.e., UCF101 and Kinetics-400), and sample 4 FPS for datasets with shorter videos (i.e., HMDB51, SSV2, Diving48). During inference, we follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> to use 3 spatial crops and 10 temporal crops (in total 30 crops), and average their prediction scores as the final score. <ref type="bibr" target="#b4">5</ref> All models are trained with AdamW <ref type="bibr" target="#b42">[43]</ref> optimizer with linear warm-up and linear learning rate decay. We observe similar pre-training instability as reported in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> and follow their practice to sequentially choose learning rate at 1e-3, 5e-4, 3e-4, ..., until convergence. More details for experiments are listed in Sec. C. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare our primary results with previous work in <ref type="table" target="#tab_2">Table 1</ref>. We expand the results that are most related to our work: self-supervised training on uncurated videos and supervised pre-training with transformers. For other results, we select the best-performing models to our knowledge and denote their reference in the table.</p><p>Our model VIMPAC sets the new state of the art on the two temporally-heavy datasets SSV2 and Diving48, where we achieve 2.7% and 4.5% absolute improvement, respectively, over previous best models among all self-supervised and supervised pre-trained methods. This is especially surprising considering the two previous SotA models ViViT <ref type="bibr" target="#b1">[2]</ref> and TimeSformer <ref type="bibr" target="#b5">[6]</ref> both use large-scale supervised pre-training, and ViViT also uses various regularization techniques (e.g., stochastic depth <ref type="bibr" target="#b30">[31]</ref>, random augment <ref type="bibr" target="#b14">[15]</ref> and mixup <ref type="bibr" target="#b70">[71]</ref>). VIMPAC also achieves competitive results on other three spatially-heavy datasets: UCF101, HMDB51, and Kinetics-400. As discussed in Sec. 4.1, recognizing actions in SSV2 and Diving48 require a strong temporal reasoning ability, while in the other datasets, spatial understanding is dominant. Some relatively low results of our VIMPAC (e.g., K400) are thus possibly due to the VQ-VAE spatial information loss. To illustrate this, we show a comparison between the SotA models 6 with temporal modeling in the first row and the ones without in the second row ('w/o Temporal') of <ref type="table" target="#tab_2">Table 1</ref>. Note the gaps between these two types of models are significantly larger for temporally-heavy datasets (SSV2) than spatially-heavy datasets (UCF101, Kinetics-400), demonstrating the importance of temporal modeling for temporally-heavy datasets. We also show the methods pre-trained on HowTo100M that take other modalities to help video learning thus beyond the scope of visual self-supervised learning.</p><p>Previous self-supervised pre-training such as MoCo <ref type="bibr" target="#b20">[21]</ref> are good at global understanding, but the pre-training schema does not consider the internal interactions inside videos (especially for the temporal dimensions). As a result, it could reach or even outperform the supervised alternatives on UCF101. However, it shows lower results on SSV2 compared to the transformer models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> (although with different backbone models) that warm up from image-pre-trained models and learn the temporal interactions directly from the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We also analyze the model's scalability and the effectiveness of our pre-training methods. To save computation, for all analyses, we use a smaller model (6-layer transformer with hidden dimension 512) and smaller input resolution (5 input frames with spatial size 128, i.e., T =5, S=128) throughout this section, unless otherwise stated. We also perform pre-training with fewer epochs (i.e., 10). For downstream tasks, we use the same input resolution as pre-training (i.e., T =5, S=128), and we use 2 temporal crops for inference. All results are reported on the train-val split 1 if applicable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scalability</head><p>Model. In <ref type="table" target="#tab_3">Table 2</ref>, we illustrate the scalability of our method with different model sizes (i.e., number of layers and hidden dimensions). Larger models have more parameters ('Params') and higher computational cost (measured by the normalized pre-training 'Speed'). To evaluate the pre-training tasks performance, we provide both pre-training metrics (mask-then-predict accuracy denoted by 'Mask-Accu.', and contrastive learning loss denoted by 'CL-Loss') and UCF101 downstream finetuning results. As the size of the model grows, the fine-tuning results show consistent improvement with the pre-training metrics. Note that for the last row in <ref type="table" target="#tab_3">Table 2</ref>, we halve the attention head and MLP intermediate dimensions.</p><p>Input Resolution. In <ref type="table" target="#tab_4">Table 3</ref>, we show model scalability over input resolution (i.e., #frames T and frame size S). With the same frame size S, longer clips perform better than shorter clips (e.g., T =10, S=128 is better than T =5, S=128). With the same number of input frames T , larger frame size improves the performance (e.g., T =10, S=256 is better than T =10, S=128). For each pre-training resolution, we also try to fine-tune under a full-resolution with T =10, S=256 (denoted as 'UCF101-Full-Reso.'). As in pre-training, fine-tuning with larger resolution generally improves the results. Although longer and smaller clips (T =10, S=128) show better results than shorter and larger clips (T =5, S=256) when using the same pre-training and fine-tuning resolutions, they show different trends with the full-resolution fine-tuning. Increasing frame size during fine-tuning (the second block in <ref type="table" target="#tab_4">Table 3</ref>) only improves the UCF101 result by 0.4, while increasing the clip length (the third block) improves the UCF101 result by 3.8. These results call for a need of pre-training with large spatial size, and we follow this practice in our large-scale experiments as in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pre-Training Methods</head><p>We analyze the key designs of our two pre-training tasks. When analyzing the mask-then-predict task in Sec. 5.2.2, we exclude the contrastive learning loss (by setting loss ratio ?=0) to preclude potential side effects. However, we still use masked prediction loss when assessing the contrastive learning task in Sec. 5.2.3 as we observe very low performance with only contrastive learning objective. We first compare different pre-training tasks and the non-pre-training results. As shown in <ref type="table" target="#tab_5">Table 4</ref>, maskthen-predict is good at temporally-heavy datasets (SSV2, Diving48) while contrastive learning improves the spatially-heavy datasets. We also compare with the non-pre-training results (the first row of <ref type="table" target="#tab_5">Table 4</ref>) and observe that both tasks significantly improve the results. We notice that these non-pretraining results are lower than previous from-scratch models, which might be caused by the difficulty in training video transformers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> and the information loss in our input quantization process <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The Impact of Pre-Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Mask-then-Predict</head><p>Block Masking versus I.I.D. Masking. We first compare our proposed block masking strategy and the uniform i.i.d. masking strategy (discussed in Sec. 3.1 and illustrated in <ref type="figure">Fig. 2</ref>). As shown in    ('Mask-Accu.'), it shows lower downstream results ('UCF101') than block masking. The higher mask accuracy is possibly due to the easier i.i.d. mask-then-predict task. To illustrate this, we show that simply copy-pasting from nearest neighbours yields reasonable reconstruction results in <ref type="figure" target="#fig_1">Fig. 4</ref>. More discussions are provided in Sec. G.2. The existence of such a trivial solution potentially prevents the model from learning useful video representations for downstream tasks. Meanwhile, we also find that the model with larger input frame size 256 benefits more from the block masking strategy, because the adjacent tokens are closer in the original 2D image for these larger frames. Hence, the spatial locality is amplified.</p><p>Masking Ratio. In <ref type="table" target="#tab_8">Table 6</ref>, we study the impact of masking ratio, by varying the number of masked blocks for block masking. Empirically, the result differences among different masking ratios are marginal and the original BERT's 15% masking ratio (with roughly 5 masking blocks) works slightly better. Thus we always select the number of mask blocks whose induced masking ratio is closest to 15%. The detailed choices of number of masking blocks are listed in Sec. B.1.</p><p>Reconstruction Visualization. In <ref type="figure" target="#fig_2">Fig. 5</ref>, we show our model reconstruction results from block masking. The centering block in the image is masked, and our model can reconstruct the image patch with the spatio-temporal context. Details are in Sec. G.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Contrastive Learning</head><p>Positive Sampling Distance. As illustrated in Sec. 3.2 and <ref type="figure">Fig. 2.(b)</ref>, we uniformly sample positive clip pairs across the whole video without any distance restriction. To analyze the effect of such a sampling strategy, We perform a set of experiments by varying the maximum sampling distance d max (in seconds) between two positive clips. The results are shown in <ref type="table" target="#tab_9">Table 7</ref>. d max =8 denotes our default setup without any distance restriction. d max =0 samples two same clips, and d max =10 samples two positive clips with a maximum distance of 10 seconds. Although previous contrastive learning methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref> favor the sampling of temporal positives within a shorter range (e.g., maximum 36 seconds for uncurated videos in <ref type="bibr" target="#b20">[21]</ref>), we observe a performance gain when using larger distance.</p><p>We also want to emphasize that the results with d max =10 and d max =0 are not better than the model pre-trained with only mask-then-predict (UCF101 accuracy 68.3), which suggests that short-range contrastive learning does not improve upon our mask-then-predict task. This is potentially because our mask-then-predict already gives the model the ability to model local interactions, thus contrastive learning objective can only be useful when it focuses on longer-range interactions.</p><p>Number of Negative Samples. Previous constrastive learning methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref> benefit from more negative samples. In this section, we show that the number of negative samples has less impact on our method when mask-then-predict task is added. As shown in <ref type="table" target="#tab_10">Table 8</ref>, we experiment with    Input Masking as Augmentation. Most selfsupervised visual contrastive learning methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref> suffer from a large drop when removing strong spatial augmentations. In contrast, our pre-training does not use any spatial augmentations on raw frames, such as flipping and random cropping. However, as we tie the input between mask-then-predict and contrastive learning to reduce computation cost, the random masking noise is naturally introduced. We here investigate its impact in <ref type="table" target="#tab_11">Table 9</ref>. When pre-trained jointly with mask-thenpredict, adding mask noise improves UCF101 accuracy by +2.0; however, when pre-trained without it, adding mask noise hurts the performance (-1.6). We hypothesize that this is due to the large input mismatches between pre-training and fine-tuning when mask-then-predict objective is not applied. Noisy masking creates 'holes' in the input token maps during pre-training, while for fine-tuning the input token maps are intact. When mask-then-predict task is applied, it guides the model to fill these holes, thus reducing this mismatch and allowing the contrastive learning task to benefit from noisy masking as a type of regularization. In contrast, this input mismatch becomes dominant when only using the contrastive learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented the video pre-training framework VIMPAC that introduces mask-then-predict task to video self-supervised learning. mask-then-predict task helps model spatio-temporal interactions that is important for video understanding. We used the VQ-VAE quantizer and propose the block masking method that is essential to overcome the strong locality in video. The contrastive learning task is also added to learn separable global features. Different from previous methods, our contrastive learning did not use data augmentation over raw frames and is less sensitive to the temporal sampling distribution for positive pairs. We showed that our frameworks could achieve state-of-the-art performance on two temporally-heavy dataset (SSV2 and Diving48) and reach competitive results on other datasets. Detailed analyses were provided regarding the model scalability and task design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Architecture</head><p>As described in Sec. 3.4, we use a transformer model on top of the discrete video tokens generated by VQ-VAE. Since transformers have different variants, we here show details of our architecture for clarity. The design of our method largely follows the practice in BERT <ref type="bibr" target="#b15">[16]</ref>, TimeSformer <ref type="bibr" target="#b5">[6]</ref>, Sparser Transformer <ref type="bibr" target="#b13">[14]</ref>, ViViT <ref type="bibr" target="#b1">[2]</ref>, and MoCoV3 <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Backbone Model</head><p>Embedding. Given the video frames tf t P R H?W | t P rT su, we first use VQ-VAE <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62]</ref> to discrete them into video tokens tx t,i,j P rV s | t P rT s, i P r?s, j P r? su. We next use an embedding layer (embedding) that to map these discrete tokens to continuous vectors. Since transformer layers are permutation-invariant, we follow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> to add positional information into the input. The positional embedding (pos) is factorized as a sum of the temporal embedding pos T , the height embedding pos H , and the width embedding pos W . This factorization reduces the number of trainable parameters to encode positional information, which empirically shows a slightly better result. Finally, a Layer-Normalization <ref type="bibr" target="#b2">[3]</ref> layer is added to get the initial hidden outputs h 0 t,i,j :</p><formula xml:id="formula_6">h 0 t,i,j " LayerNormpembeddingpx t,i,j q`pospt, i, jqq,<label>(7)</label></formula><p>pospt, i, jq " pos T ptq`pos H piq`pos W pjq, (8) where we use the superscript 0 to denote that it is the hidden outputs before the first transformer layer.</p><p>Attention Blocks. Before introducing the detailed model architecture, we first describe the basic building components: the attention block. An attention block is built based on the attention operator (i.e., 'Attn') with a residual connection. The attention operator takes a single query vector x and its context ty i u as input. It first computes the attention score between x and each context vector y i , then the attention scores are normalized by the softmax. Lastly, the output is a weighted-sum over all the context vectors (transferred by a 'value' matrix W value ):</p><p>Attnpx, ty i uq " ? i softmax i tpW query xq J W key y i uW value y i .</p><p>To compose the attention block from the previous attention operator, the residual connection and layer normalization (i.e., 'LayerNorm') are added. We follow the original transformer model <ref type="bibr" target="#b62">[63]</ref> that uses a post-layer-norm layout: AttnBlockpx, ty i uq " LayerNormpx`W out Attnpx, ty i uqq.</p><p>In order to reduce computational cost and memory, we also adapt the attention block suggested in Sparse Transformer <ref type="bibr" target="#b13">[14]</ref> that takes two sets of context vectors ty i u and tz j u as input. This special attention block computes attention for the two context-vector sets separately and concatenates their output together. In our case, suppose ty i u and tz j u are the rows and columns of a square matrix, then it reduces the computation cost of calculating attention scores from ?pn 4 q to ?pn 2 q, where n is the number of rows/columns:</p><p>AttnBlockpx, ty i u, tz j uq " LayerNormpx`W out rAttnpx, ty i uq, Attnpx, tz j uqsq</p><p>Spatio-Temporal Transformer Layer. The spatio-temporal transformer layer is composed with the previously-introduced attention blocks and an additional MLP block. The l-th layer takes the output of the previous layer th l?1 t,i,j u as input and outputs the hidden states th l t,i,j u. We separate the attention into two attention blocks: the temporal attention block AttnBlock TIME and the spatial attention block AttnBlock SPACE . Without loss of generality, we will use g TIME and g SPACE to denote the intermediate results from temporal and spatial attention blocks, respectively. First, the temporal attention block attends to the tokens at the same spatial location but in different frames (i.e., at different timesteps): th l?1 t,i,j | t P rT su. Next, the spatial attention block attends to the tokens in the same frame: tg T t,i,j | pi, jq P r?s?r? su. To reduce the computational cost, we incorporate the sparse attention block <ref type="bibr" target="#b13">[14]</ref> (detailed in the previous paragraph) that factorizes the attention over height and width: tg T t,i,j | i P r?su, tg T t,i,j | j P r? su. The MLP block has two fully-connected layers with GeLU <ref type="bibr" target="#b29">[30]</ref> activation in the middle. Overall, the formula of one spatio-temporal transformer layer is:</p><formula xml:id="formula_10">g TIME t,i,j " AttnBlock TIME ph l?1 t,i,j , th l?1 t,i,j | t P rT suq (12) g SPACE t,i,j " AttnBlock SPACE pg T t,i,j , tg TIME t,i,j | i P r?su, tg TIME t,i,j | j P r? suq (13) h l t,i,j " LayerNormpg SPACE t,i,j`M LPpg S t,i,j qq<label>(14)</label></formula><p>[CLS] Token. Following the practice in BERT <ref type="bibr" target="#b15">[16]</ref> design, we add a special [CLS] (abbreviation of 'classification') token and take its output as the representation of the whole sequence. We follow TimeSformer <ref type="bibr" target="#b5">[6]</ref> to compute the its output: the [CLS] token attends over the context separately and then the outputs are averaged. We take the temporal attention layer as an example. Suppose h l?1 cls is the [CLS] feature vector output by layer l?1, then the temporal attention layer do the following computation:</p><formula xml:id="formula_11">g TIME cls " 1 H 1 W ? i ? j AttnBlock TIME ph l?1 cls , th l?1 t,i,j | t P rT suq.<label>(15)</label></formula><p>The other attention blocks process the [CLS] token similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pre-Training and Fine-Tuning Heads</head><p>Pre-training or fine-tuning usually requires a few additional modules (i.e., heads) on top of the transformer layers that convert the output features to the desired probabilities or vectors. We next describe the heads used in our pre-training and fine-tuning process.</p><p>Token Head for Mask-then-Predict. We first define the prediction head over the tokens following BERT <ref type="bibr" target="#b15">[16]</ref>. It first processes the last-layer hidden outputs h L t,i,j using a fully-connected layer (with GELU activation <ref type="bibr" target="#b29">[30]</ref> and layer normalization <ref type="bibr" target="#b2">[3]</ref>): u t,i,j " LayerNorm`GELUpW token ph L t,i,j q`b token q?.</p><p>In our mask-then-predict method (Sec. 3.1), we will predict the masked tokens (i.e., the token before masking) from their context. We thus further convert this hidden vector into a distribution over the token vocabulary: P t,i,j po t,i,j " kq " softmax k tW word u t,i,j`bword u.</p><p>The weight W word is shared with input word embedding layer embedding <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47]</ref> while the bias b word is trained independently. Contrastive Learning Head Next we discuss the pre-training heads for contrastive learning. It is on top of the [CLS] hidden output h CLS . We encode the hidden state with MLP. We use batch normalization <ref type="bibr" target="#b31">[32]</ref> inside the MLP head following the practice in <ref type="bibr" target="#b12">[13]</ref>.</p><formula xml:id="formula_14">f CLS " MLP CLS ph CLS q<label>(18)</label></formula><p>This f CLS feature is used in computing the contrastive loss as in Sec. 3.2.</p><p>FC Layer for Fine-Tuning. When fine-tuning for action classification task, we add a fullyconnected (FC) layer to the [CLS] output h cls . We initialize its weight and bias to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Special Tokens</head><p>Besides the V token types introduced in the vocabulary of the VQ-VAE (see Sec. 3.1), we add several special tokens into the 'vocabulary', namely a [CLS] token is introduced as a stub for the whole-video representation, a [PAD] token is used when the actual clip length is less than the model's expected input length. For the mask-then-predict task, we follow BERT <ref type="bibr" target="#b15">[16]</ref> to replace the masked tokens with a specific [MASK] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Pre-Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Masking Blocks</head><p>As described in Sec. 3.1, we mask the tokens by blocks (a cube-shape set of tokens). To avoid masking all the tokens in the clip, we control the maximum block length for the time domain, height, and width. For spatial dimensions (i.e., height and width), the maximum length is half of the full length (e.g., the maximum block length will be 16 for a token map of length 32). For temporal dimension (i.e., the clip length), the maximum length will be 2/3 (round up) of the full length so that it allows long-range modeling. Under these constraints, we uniformly sample a fixed number of mask blocks and take their union to construct the final mask. The number of blocks is decided by the induced masking ratio, which depends on the input resolutions. In <ref type="table" target="#tab_2">Table 10</ref>, we show the induced masking ratio w.r.t. different input resolutions and #masking blocks. We take the VQ-VAE <ref type="bibr" target="#b61">[62]</ref> provided in DALL-E <ref type="bibr" target="#b50">[51]</ref> that has a compression factor of 8, thus the length of the token map is always 1/8 of the frame size. For each input resolution, we select the number of blocks (shown in bold in <ref type="table" target="#tab_2">Table 10</ref>) whose induced masking ratio is closet to 15% following BERT <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Contrastive Learning Loss</head><p>For completeness, we list the two losses used in contrastive learning here. The first loss for clip c i from video i is:</p><formula xml:id="formula_15">L InfoNCE piq "?log exp pf J i f 1 i {?q ? k?i exp pf J i f k {?q`? k exp pf J i f 1 k {?q (19)</formula><p>The symmetric loss L 1 InfoNCE piq for feature of the other clip sample c 1 i from video i (and its feature f 1 i ) is:   <ref type="bibr" target="#b13">[14]</ref>. For the pre-training heads, we follow BERT to take the intermediate dimension of the token-head to be the same as the backbone's hidden dimension. For the CLS head, we take 3 layers in MLP and 4096 intermediate dimensions. The output dimension is 256. We test with different number of layers and hidden dimensions of CLS head and generally find that larger head gives better results (as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48]</ref>). This CLS head contributes to about 1% pre-training computational cost overhead. Training Hyperparameters. We list the training hyperparameters in <ref type="table" target="#tab_2">Table 12</ref>. Most of the hyperparameters are inherited from previous works to allow fair comparison and reduce tuning effort. For optimizer hyperparameters, we mostly follow the implementation of DALL-E <ref type="bibr" target="#b50">[51]</ref> and BERT <ref type="bibr" target="#b15">[16]</ref>. SSV2 follows the epoch number in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref>. To reduce the computational cost, we pre-extract the VQ-VAE tokens thus we employ a fixed set of spatial data augmentations. As listed in the bottom of <ref type="table" target="#tab_2">Table 12</ref>, we exclude any color distortion and gray scale augmentation. We resize the video clip to the desired frame size and center-crop it during pre-training. For downstream tasks, we resize the video clip to frame size or 1.5 times of the frame size, then crop the clip (with frame-size by frame-size spatial size) from the top-left, center, and bottom-right. We apply (horizontal) flip to the raw frames, thus a total of 12 spatial augmentations are extracted (12 = 2 resize?3 crops?2 flip/no-flip). The only exception is SSV2. This dataset needs to distinguish left/right motions thus we exclude the flip augmentation and only use the center crop during inference following previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Following previous works <ref type="bibr" target="#b19">[20]</ref>, we increase the training epochs for the non-pre-training models (in Sec. 5.2.1) by 4?for small datasets (Diving48, UCF101, HMDB51) and 1.5?for larger datasets (SSV2, Kinetics-400).</p><formula xml:id="formula_16">L 1 InfoNCE piq "?log exp pf 1J i f i {?q ? k exp pf 1J i f k {?q`? k?i exp pf 1J i f 1 k {?q (20)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dataset Details</head><p>In <ref type="table" target="#tab_2">Table 13</ref>, we list the key statistics of the datasets used in our paper. HowTo100M is our pretraining datasets that has long-duration uncurated videos. The videos are collected from YouTube by searching key phrases thus the scale could be easily increased. SSV2 and Kinetics-400 are two large downstream datasets, where SSV2 focuses more on the actions and Kinetics-400 focuses more on the scenes. Diving48, UCF101, HMDB51 are three small datasets. Different from previous datasets on classifying different action types (thus might be potentially inferred from single frames), Diving48 studies the three stages (takeoff, flight, and entry) of competitive diving. Thus achieving good results on Diving48 requires an understanding of the whole video clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Computational Cost</head><p>The pre-training takes around 8.9K V100 GPU hours. This computational cost is at the same level as ViT <ref type="bibr" target="#b17">[18]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Analysis Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Model Architecture Comparisons</head><p>Attention Layouts. We here compare different alternative model architectures in <ref type="table" target="#tab_2">Table 14</ref>. We first experiment with different attention layouts discussed in Sec. 3.4. We consider the sparse attention as proposed in <ref type="bibr" target="#b13">[14]</ref> and the sequential attention blocks as in <ref type="bibr" target="#b5">[6]</ref>. The 'TxHxW' model is the basic attention module that takes the flattened tokens as input (of shape T?H?W). At each layer, each  <ref type="table" target="#tab_2">Table 15</ref>: Impact of masking ratio. All models are pre-trained with only mask-then-predict task. #Blocks is the number of masking blocks. Default setup is underlined.</p><p>Strategy #Blocks Ratio Mask-Accu.? UCF101 ?  <ref type="figure">Figure 7</ref>: Nearest-neighbour reconstruction of block masking and i.i.d masking. We mask tokens at different ratios and reconstruct them by simply copying their spatial or spatio-temporal neighbours. Even under heavy masking (e.g., 45% masked), this simple reconstruction strategy still yields a reasonable results for i.i.d masking, e.g., we can easily recognize the action 'petting cat' from the reconstructed images, especially the one reconstructed from spatio-temporal neighbours. However, this becomes significantly more difficult when using block masking.  <ref type="figure">Figure 8</ref>: Masked-token model reconstruction for SSV2 and Kinetics-400 datasets. Comparing 1. and 4., our model could redraw temporally-consistent and spatially-plausible patches for the masked regions.</p><p>decoder to generate the RGB images. For the default 15% masking ratio, i.i.d. masking is recoverable while block masking causes striped noisy patterns. <ref type="bibr" target="#b6">7</ref> We also test with the extreme case of masking 45% tokens (in <ref type="figure">Fig. 7 (d)</ref>, (e)). The block-masked images are hard to reconstruct, however, some objects in reconstructed images from i.i.d. masking are still recognizable. When comparing images under the same masking strategy, recovered images using spatio-temporal neighbours is better than using only spatial neighbours, especially when comparing the images under 45% i.i.d. masking (i.e., <ref type="bibr">(d)</ref>.2 and (e).2 in <ref type="figure">Fig. 7)</ref>. Overall, these results indicate that using i.i.d. masking in mask-then-predict task has a potential trivial solution by copying the neighbourhood, while block-masking resolves this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Model Reconstruction</head><p>Since our model is trained with mask-then-predict task, it is able to reconstruct masked tokens. In this section, we showcase the reconstructed video frames by our final model (i.e., 24 layers, 1024 dimensions, 5 clip length, and 256 frame size). As shown in <ref type="figure">Fig. 8</ref>, we provide two examples from the SSV2 and Kinetics-400 dataset. We uniformly sample 5 consecutive frames from the video at 1 frame per second. We show the original frames in the first rows ( <ref type="figure">Fig. 8.(a)</ref>.1, (b).1). As illustrated in Sec. G.1, the temporally-heavy SSV2 dataset has object motions between frames while the spatially-heavy Kinetics-400 dataset has almost static frames. In the second rows, we show the images after VQ-VAE compression. To do this, we first use VQ-VAE encoder to encode the images, and then use VQ-VAE decoder to reconstruct the images, without any corruptions in between. We see that there is some information loss caused by the VQ-VAE compression (e.g., the text 'comfort' in <ref type="figure">Fig. 8.(a)</ref>.1). It potentially contributes to relative lower results on spatially-heavy datasets. In the third and fourth rows, we illustrate the masked area and the prediction from our model. As shown in <ref type="figure">Fig. 8.(a)</ref>.4, our model could faithfully redraw the shape and texture of the object. As shown in <ref type="figure">Fig. 8.(b)</ref>.4, the shape of the cat's head is pretty similar to the original frames while the shading is different (but still consistent in different frames).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b)). For each video video i , we uniformly and independently sample two clips c i , c 1 i as a positive pair, while the clips in a batch belonging to other videos are used to construct negative pairs. A model (described in Sec. 3.4) processes clips c i , c 1 i to build respective vector representations f i , f 1 i and an (a) Dataset: SSV2, Action: "Open the bottle cap" (b) Dataset: UCF101, Action: "ApplyEyeMakeup"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Nearest-neighbour reconstruction of block masking and i.i.d masking.Figure (a)is the original image, and other figures are reconstructions under different masking strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Model reconstruction results for SSV2 and Kinetics-400 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art. Our model outperforms previous works on SSV2 and Diving48 dataset while showing competitive results on other datasets. UCF101 and HMDB51 are average over three train-val splits. V,A,T refer to Visual, Audio, and Text modalities, respectively. AS and HTM are Audio Set<ref type="bibr" target="#b22">[23]</ref> and HowTo100M<ref type="bibr" target="#b44">[45]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Modality Pre-Train Dataset</cell><cell cols="5">Temporally-Heavy SSV2 [26] Diving48 [41] UCF101 [57] HMDB51 [38] K400 [9] Spatially-Heavy</cell></row><row><cell>Previous SotA</cell><cell>V</cell><cell>-</cell><cell>65.4 [2]</cell><cell>81.0 [6]</cell><cell>98.7 [34]</cell><cell>85.1 [34]</cell><cell>84.8 [2]</cell></row><row><cell>w/o Temporal</cell><cell>V</cell><cell>-</cell><cell>36.6 [6]</cell><cell>-</cell><cell>92.0 [49]</cell><cell>-</cell><cell>77.6 [6]</cell></row><row><cell cols="2">Self-supervised Pre-Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>K400 Self-Sup.</cell><cell>V</cell><cell>Kinetics-400 [9]</cell><cell>55.8 [21]</cell><cell>-</cell><cell>96.3 [21]</cell><cell>75.0 [21]</cell><cell>-</cell></row><row><cell>MIL-NCE [44]</cell><cell>V+T</cell><cell>HTM [45]</cell><cell>-</cell><cell>-</cell><cell>91.3</cell><cell>61.0</cell><cell>-</cell></row><row><cell>MMV [1]</cell><cell>V+A+T</cell><cell cols="2">AS [23]+HTM [45] -</cell><cell>-</cell><cell>95.2</cell><cell>75.0</cell><cell>-</cell></row><row><cell>MoCo [21]</cell><cell>V</cell><cell>IG-Uncurated [24]</cell><cell>53.2</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>-</cell></row><row><cell>VIMPAC</cell><cell>V</cell><cell>HTM [45]</cell><cell>68.1</cell><cell>85.5</cell><cell>92.7</cell><cell>65.9</cell><cell>77.4</cell></row><row><cell cols="2">Supervised Pre-Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>K400 Sup.</cell><cell>V</cell><cell>Kinetics-400 [9]</cell><cell>63.1 [20]</cell><cell>-</cell><cell>96.8 [60]</cell><cell>82.5 [65]</cell><cell>81.5 [36]</cell></row><row><cell cols="2">TimeSformer [6] V</cell><cell cols="2">ImageNet-21K [54] 62.3</cell><cell>81.0</cell><cell>-</cell><cell>-</cell><cell>80.7</cell></row><row><cell>ViViT [2]</cell><cell>V</cell><cell cols="2">ImageNet-21K [54] 65.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Impact of model size. 'Params' is the number of parameters. 'Speed' is the normalized pre-training speed measured by #videos/second on one V100 GPU. 'Mask-Accu.' and 'CL-Loss' are mask-then-predict accuracy and contrastive learning loss to indicate the pre-training performance. 'UCF101' is the fine-tuning accuracy on UCF101 dataset. First line is defautly used in analysis and the configuration producing final results are underlined.</figDesc><table><row><cell cols="7">Layers Dim Params Speed Mask-Accu.? CL-Loss ? UCF101?</cell></row><row><cell>6</cell><cell>512</cell><cell>29.4M</cell><cell>32.0</cell><cell>17.2</cell><cell>1.06</cell><cell>69.4</cell></row><row><cell>6</cell><cell>768</cell><cell>63.0M</cell><cell>21.0</cell><cell>17.7</cell><cell>1.03</cell><cell>75.0</cell></row><row><cell>12</cell><cell>512</cell><cell>54.7M</cell><cell>18.1</cell><cell>17.9</cell><cell>1.02</cell><cell>76.6</cell></row><row><cell>12</cell><cell cols="2">768 119.7M</cell><cell>11.2</cell><cell>18.4</cell><cell>1.00</cell><cell>78.1</cell></row><row><cell>24</cell><cell cols="2">1024 210.1M</cell><cell>5.0</cell><cell>18.7</cell><cell>0.98</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Impact of input resolutions T and S. 'Mask-Accu.' and 'CL-Loss' are the pre-training metrics. 'UCF101' indicates the UCF101 fine-tuning results with the pre-training resolution. 'UCF101-Full-Reso.' indicates the full-resolution fine-tuning with T =10 and S=256.</figDesc><table><row><cell>?</cell></row></table><note>#frames T Frame Size S Params Pre-train Speed Mask-Accu.? CL-Loss? UCF101? UCF101-Full-Reso.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Impact of pre-training tasks. 'MP'=Mask-then-Predict, 'CL'=Contrastive Learning task.</figDesc><table><row><cell>MP CL</cell><cell cols="5">Temporally-Heavy SSV2 Diving48 UCF101 HMDB51 K400 Spatially-Heavy</cell></row><row><cell></cell><cell>1.2</cell><cell>10.0</cell><cell>41.3</cell><cell>19.0</cell><cell>41.0</cell></row><row><cell></cell><cell>32.5</cell><cell>26.3</cell><cell>57.1</cell><cell>30.7</cell><cell>47.0</cell></row><row><cell></cell><cell>41.4</cell><cell>37.2</cell><cell>68.3</cell><cell>35.3</cell><cell>53.7</cell></row><row><cell></cell><cell>41.1</cell><cell>37.5</cell><cell>69.4</cell><cell>37.8</cell><cell>54.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 ,</head><label>5</label><figDesc></figDesc><table><row><cell>(a) Original Image</cell><cell>(b) 15% block masking</cell><cell>(c) 15% i.i.d masking</cell><cell>(d) 45% block masking</cell><cell>(e) 45% i.i.d masking</cell></row></table><note>although the i.i.d. masking achieves higher pre-training mask-token-prediction accuracy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Impact of masking strategies. Models are pre-trained with only mask-then-predict.</figDesc><table><row><cell cols="4">Strategy Frame-Size S Mask-Accu.? UCF101 ?</cell></row><row><cell>block</cell><cell>128</cell><cell>17.6</cell><cell>68.3</cell></row><row><cell>i.i.d.</cell><cell>128</cell><cell>24.3</cell><cell>63.5 (-4.8)</cell></row><row><cell>block</cell><cell>256</cell><cell>11.2</cell><cell>69.5</cell></row><row><cell>i.i.d.</cell><cell>256</cell><cell>19.5</cell><cell>61.4 (-8.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Impact of masking ratios.</figDesc><table><row><cell>Models are</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Impact of maximum sampling distance d max (seconds) between two positive clips.</figDesc><table><row><cell cols="4">d max Mask-Accu.? CL-Loss? UCF101?</cell></row><row><cell>8</cell><cell>17.2</cell><cell>1.06</cell><cell>69.4</cell></row><row><cell>30</cell><cell>17.3</cell><cell>0.77</cell><cell>69.0 (-0.4)</cell></row><row><cell>10</cell><cell>17.4</cell><cell>0.61</cell><cell>68.3 (-1.1)</cell></row><row><cell>0</cell><cell>17.5</cell><cell>0.41</cell><cell>66.7 (-2.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Impact of #negative samples.</figDesc><table><row><cell cols="4">#neg-samples Mask-Accu.? CL-Loss? UCF101?</cell></row><row><cell>128 -1</cell><cell>17.2</cell><cell>1.06</cell><cell>69.4</cell></row><row><cell>256 -1</cell><cell>17.1</cell><cell>1.30</cell><cell>69.2</cell></row><row><cell>512 -1</cell><cell>17.2</cell><cell>1.56</cell><cell>70.4</cell></row><row><cell>1024 -1</cell><cell>17.0</cell><cell>1.86</cell><cell>69.8</cell></row></table><note>different contrastive learning sample sizes (i.e., n in Sec. 3.2 which is 1 + number of negative samples) and always accumulate the gradients to 1024 samples before updating the parameters. Although increasing sample size makes the contrastive learning task harder (reflected by 'CL-Loss'), it does not show clear evidence of improving UCF101 downstream performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">MP CL-Mask Mask-Accu.? CL-Loss? UCF101?</cell></row><row><cell>-</cell><cell>1.07</cell><cell>57.1</cell></row><row><cell>-</cell><cell>1.08</cell><cell>55.5</cell></row><row><cell>17.2</cell><cell>1.04</cell><cell>67.4</cell></row><row><cell>17.2</cell><cell>1.06</cell><cell>69.4</cell></row></table><note>Impact of mask augmentation in contrastive learning. 'MP'=Mask-then- Predict. 'CL-Mask'=Use input mask in CL. Default setup is underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Induced Masking ratio w.r.t. to different input resolutions and #masking blocks. The numbers of blocks/masking ratio for each resolution setting used in our experiments are shown in bold.</figDesc><table><row><cell></cell><cell>Input Resolution</cell><cell></cell><cell></cell><cell cols="3">#Masking Blocks</cell></row><row><cell cols="3">Length Frame Size Token Map Size</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>5</cell><cell>128</cell><cell>16</cell><cell cols="4">11.9 14.5 17.0 19.4 21.7</cell></row><row><cell>5</cell><cell>256</cell><cell>32</cell><cell cols="4">10.6 13.1 15.2 17.5 19.5</cell></row><row><cell>10</cell><cell>128</cell><cell>16</cell><cell cols="4">10.4 12.8 15.0 17.1 19.2</cell></row><row><cell>10</cell><cell>256</cell><cell>32</cell><cell cols="4">9.3 11.4 13.4 15.4 17.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Model Configuration. The 'Small' model is mainly used in the analysis (Sec. 5) while 'Large-Half' model is mainly used in the results (Sec. 4.3) for the final large-scale experiments. 'Vocab Size' is the number of token types in our model, defined by the pre-trained VQ-VAE model<ref type="bibr" target="#b50">[51]</ref>.</figDesc><table><row><cell></cell><cell>Small (in Sec. 5)</cell><cell cols="2">Base Large-Half (in Sec. 4.3)</cell></row><row><cell>Layers</cell><cell>6</cell><cell>12</cell><cell>24</cell></row><row><cell>Dimensions</cell><cell>512</cell><cell>768</cell><cell>1024</cell></row><row><cell>Attention Heads</cell><cell>8</cell><cell>12</cell><cell>16</cell></row><row><cell>Attention Head Dim</cell><cell>64</cell><cell>64</cell><cell>32</cell></row><row><cell>MLP Intermediate Size</cell><cell>2048</cell><cell>3072</cell><cell>2048</cell></row><row><cell>Vocab Size</cell><cell>8192</cell><cell>8192</cell><cell>8192</cell></row><row><cell>Params</cell><cell>29.4M</cell><cell>119.7M</cell><cell>210.1M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Training Hyperparameters.In this section, we show our model configuration and training hyperparameters in details to support the reproducibility of our experiments.Model Configuration. Our model configuration details is shown inTable 11. Most analysis results (Sec. 5) take 'Small' models and our final results (Sec. 4.3) take 'Large-Half' model. Other models are used in Sec. 5.1. The final 'Large-Half' model halves the attention head dimension and MLP intermediate size as in</figDesc><table><row><cell></cell><cell cols="7">'Pre-Train-L' is our final Large model in Sec. 4.3 that takes a</cell></row><row><cell cols="8">large-half model. 'Pre-Train-S' is the small pre-training in analysis (Sec. 5). 'K-400' is Kinetics-400.</cell></row><row><cell cols="8">*The batch size for pre-training is the number of samples in updating the weights. Since we use</cell></row><row><cell cols="8">gradient accumulation, it is not correlated to the number of negative examples in contrastive learning.</cell></row><row><cell></cell><cell cols="2">Pre-Train-S Pre-Train-L</cell><cell>SSV2</cell><cell cols="4">Diving48 UCF101 HMDB51 K-400</cell></row><row><cell>Optimization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of Epochs</cell><cell>100</cell><cell>10</cell><cell>22</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>30</cell></row><row><cell>Number of Updates</cell><cell>120K</cell><cell>12K</cell><cell>29K</cell><cell>5.8K</cell><cell>3.7K</cell><cell>1.4K</cell><cell>48K</cell></row><row><cell>Learning Rate</cell><cell>3e-4</cell><cell>1e-3</cell><cell cols="5">1e-4 for small/base model, 5e-5 for large-half model</cell></row><row><cell>Warm-Up Ratio</cell><cell>0.05</cell><cell>0.1</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>LR Decay</cell><cell>Linear</cell><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell></cell><cell></cell></row><row><cell>Backbone Dropout</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>Last FC Dropout</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell></cell><cell></cell><cell></cell><cell>AdamW</cell><cell></cell><cell></cell></row><row><cell>Batch Size</cell><cell>1024*</cell><cell></cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell></row><row><cell>Weight-Decay</cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell></row><row><cell>Adam Beta1</cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>Adam Beta2</cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell>0.999</cell><cell></cell><cell></cell></row><row><cell>Adam Epsilon</cell><cell>1e-8</cell><cell></cell><cell></cell><cell></cell><cell>1e-8</cell><cell></cell><cell></cell></row><row><cell>Grad-Clipping Norm</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>Data Augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color Distortion/Gray-Scale</cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell>No</cell><cell></cell><cell></cell></row><row><cell>Training Spatial Resize</cell><cell cols="2">1 (Frame Size)</cell><cell></cell><cell cols="3">2 (Frame Size, Frame Size * 1.25)</cell><cell></cell></row><row><cell>Training Spatial Crops</cell><cell>1 (Center)</cell><cell></cell><cell cols="4">3 (Top-Left, Center, Bottom-Right)</cell><cell></cell></row><row><cell>Training Temporal Crops</cell><cell cols="2">2 (Random Uniform)</cell><cell></cell><cell cols="2">1 (Random Uniform)</cell><cell></cell><cell></cell></row><row><cell>Inference Spatial Resize</cell><cell cols="2">1 (Frame Size)</cell><cell></cell><cell cols="2">1 (Frame Size)</cell><cell></cell><cell></cell></row><row><cell>Inference Temporal Crops</cell><cell cols="2">1 (Random Uniform)</cell><cell></cell><cell></cell><cell>10 (Uniform)</cell><cell></cell><cell></cell></row><row><cell>Training Spatial Flip</cell><cell>No</cell><cell></cell><cell>No</cell><cell></cell><cell>Yes</cell><cell></cell><cell></cell></row><row><cell>Inference Spatial Crops</cell><cell>1 (Center)</cell><cell></cell><cell>1 (Center)</cell><cell cols="4">3 (Top-Left, Center, Bottom-Right)</cell></row><row><cell cols="2">C Experiment Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Key statistics of video datasets used in this paper. HowTo100M is used for pre-training while others are downstream datasets. The number of training/validation examples in HMDB51 and UCF101 are reported for the train-val split 1.</figDesc><table><row><cell></cell><cell>HowTo100M</cell><cell>SSV2</cell><cell cols="4">Diving48 UCF101 HMDB51 Kinetics-400</cell></row><row><cell>Training</cell><cell>1238791</cell><cell>168913</cell><cell>15027</cell><cell>9537</cell><cell>3570</cell><cell>205418</cell></row><row><cell>Validation</cell><cell>-</cell><cell>24777</cell><cell>1970</cell><cell>3783</cell><cell>1530</cell><cell>17686</cell></row><row><cell>Number of Classes</cell><cell>-</cell><cell>174</cell><cell>48</cell><cell>101</cell><cell>51</cell><cell>400</cell></row><row><cell>Average Video Duration</cell><cell>6.5min</cell><cell>4s</cell><cell>6s</cell><cell>7s</cell><cell>4s</cell><cell>10s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>supervised training (5.5K hours on ImageNet-21K<ref type="bibr" target="#b53">[54]</ref> and 12.8K on JFT<ref type="bibr" target="#b57">[58]</ref>). It is also at the same level of supervised training a model on Kinetics-400 dataset (6.4K for SlowFast<ref type="bibr" target="#b19">[20]</ref>, about 5.6K for TimeSformer-L<ref type="bibr" target="#b5">[6]</ref>). For fine-tuning, SSV2, Diving48, UCF101, HMDB51, and Kinetics-400 take 1K, 200, 150, 40, 2K GPU hours, respectively. For analysis, the pre-training takes about 160 GPU hours. Besides the final model training, energy is also spent on tuning the model and finding the best configuration. As shown in Sec. 5.2.3, our method is more robust to the hyperparameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Results of different attention-module layouts and layer-normalization positions. 'Speed' is the normalized pre-training speed (i.e., number of samples / GPU / second). Models are pre-trained on HowTo100M for 10 epochs. The result numbers represent UCF101 accuracy.Params Speed Pre-LayerNorm Post-LayerNorm</figDesc><table><row><cell>TxHxW</cell><cell>23.1M</cell><cell>12.6</cell><cell>-</cell><cell>65.9</cell></row><row><cell>T,HxW [6]</cell><cell>27.1M</cell><cell>20.0</cell><cell>-</cell><cell>69.0</cell></row><row><cell>T,H,W</cell><cell>35.8M</cell><cell>26.4</cell><cell>69.0</cell><cell>69.6</cell></row><row><cell cols="2">T,H|W (ours) 29.4M</cell><cell>32.0</cell><cell>67.6</cell><cell>69.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Impact of contrastive learning loss weight ?. Default setup is underlined.</figDesc><table><row><cell>?</cell><cell>Mask-Accu.? CL-Loss? UCF101?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We do not use the Video-VQVAE<ref type="bibr" target="#b63">[64]</ref> method since the image-trained VQVAE<ref type="bibr" target="#b50">[51]</ref> has been pretrained on a very large image corpus and as a consequence cover a much more diverse set of visual scenes and elements.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">During pre-training, we always accumulate the gradient to a batch size of 1024 before updating the weights but use different numbers of negative examples. We analyze this effect in Sec. 5.2.3.<ref type="bibr" target="#b4">5</ref> As in<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, we observe that the performance is saturated at 4"5 temporal crops for our model. For SSV2, we use 10 crops following<ref type="bibr" target="#b20">[21]</ref>. Details are in Sec. C</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Some SotA models are pre-trained with extremely large (weakly-)supervised datasets, e.g., IG65M<ref type="bibr" target="#b23">[24]</ref> in<ref type="bibr" target="#b33">[34]</ref> and JFT-300M<ref type="bibr" target="#b57">[58]</ref> in<ref type="bibr" target="#b1">[2]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We highlight the masking region inFig. 7(b)and show the raw RGB images in other cases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was granted access to the HPC resources of IDRIS under the allocation 20XX-AD011011621R1 made by GENCI. This work is supported by DARPA MCS Grant N66001-19-2-4031, DARPA KAIROS Grant FA8750-19-2-1004, and ARO-YIP Award W911NF-18-1-0336. Additionally, Hao Tan is supported by Bloomberg Data Science Ph.D. Fellowship and Jie Lei is supported by Adobe Research Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9922" to="9931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Navier-stokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<editor>I-I. IEEE</editor>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>pages arXiv-2104, 2021. 5, 6, 9</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oops! predicting unintentional action in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14558</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzy?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Late temporal modeling in 3d cnn architectures with bert for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Esat Kalfaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aydin Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5792" to="5801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Movinets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11511</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>A?ron van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01950</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Predicting video with vqvae. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hallucinating idt descriptors and i3d optical flow features for action recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8698" to="8708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using vq-vae and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Axial Attention&apos; in [6]). The &apos;T, H|W&apos; model is our default model that sequentially conduct temporal attention and spatial attention, where the spatial attention are parallel into the height attention and width attention</title>
	</analytic>
	<monogr>
		<title level="j">As shown in Table</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>HxW&apos; model separates the temporal attention and spatial attention (&apos;Divided Space-Time&apos; in [6]). The &apos;T,H,W&apos; model processes three attention sequentially</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Besides the architectures listed above, we also consider the pre-layer-norm (used in GPT and ViT) and post-layer-norm (used in BERT) variation. We empirically find that post-layer-norm architecture is better for our pre-training tasks as</title>
	</analytic>
	<monogr>
		<title level="m">Pre-Layer-Normalization vs. Post-Layer-Normalization</title>
		<imprint/>
	</monogr>
	<note>shown in Table 14 (comparing the last 2 columns</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Noisy Masking for Mask-then-Predict Our default masking strategy replaces all masked tokens with a special [MASK] symbol. We also experiment with BERT&apos;s masking strategy that only replaces 80% of masked tokens to the MASK symbol. For other tokens, 10% are randomly sampled from the &apos;vocabulary&apos; and 10% are kept the same. For smaller experiments, the two masking strategies show similar results. However, this BERT&apos;s noisy masking strategy has lower convergence stability on the larger model pre-training</title>
		<imprint/>
	</monogr>
	<note>The pre-training diverges after about 10 epochs (out of the 100 epochs</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Masking We test the effect of different masking ratios. In the main text, we control the number of blocks for block masking. In Table 15, we here also show the results of matched masking ratio for i.i.d. masking for completeness. Empirically, the result differences among various masking ratios are marginal and the original BERT&apos;s 15% masking ratio (with roughly 5 masking blocks) works slightly better. Thus we always select the number of mask blocks whose induced masking ratio is closest to 15%. For all masking ratios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I D</forename><surname>Block-Masking</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Masking Ratio for. block masking shows significantly better results than the i.i.d. masking</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Since the loss have been calibrated by multiplying the temperature, ?=1 shows stable results and ?=0.5 is slightly better. Setting ?=2.0 will let the model to focus mostly on contrastive learning task and its result is worse than pure mask-then-predict pre-training (i.e., ?=0.0). We also list the pure contrastive learning pre-training results here (denoted as ?=8?but it excludes the mask-then-predict loss and set ?=1.0) for reference. ) Dataset: SSV2, Action</title>
		<imprint/>
	</monogr>
	<note>In Table 16, we show the impact of loss weight ? (see Sec. 3.3. Open the bottle cap&quot; (b) Dataset: Diving48, Action:[ &apos;Forward&apos;, &apos;15som&apos;, &apos;NoTwis&apos;, &apos;PIKE&apos;</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dataset: UCF101, Action</title>
	</analytic>
	<monogr>
		<title level="m">ApplyEyeMakeup&quot; (c) Dataset:HMDB51, Action: &quot;Ride_Horse</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<title level="m">Dataset: Kinetics400, Action: &quot;PettingCat&quot; Spatially-Heavy Datasets Temporally-Heavy Datasets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Data samples from temporally-heavy and spatially-heavy datasets. While temporally-heavy datasets need the temporal information to make decisions, most actions in spatially-heavy datasets could be inferred from single frames</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">We here show equally-distributed frames from the video and the label of the video clip. Note that we do not cherry-pick the data but aim for showing the nature of each dataset. Overall, understanding in temporally-heavy datasets needs temporal modeling, whereas the action labels of spatially-heavy datasets could be inferred from a single frame</title>
	</analytic>
	<monogr>
		<title level="m">G Visualizations G.1 Temporally-Heavy vs. Spatially-Heavy Datasets We illustrate the differences between temporally-heavy and spatially-heavy datasets in Fig. 6</title>
		<imprint/>
	</monogr>
	<note>Kinetics-</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">the action label could be inferred from any single sampled frame. These observations result in the pretty high frame-level accuracy (i.e., not modeling temporal interactions)</title>
		<imprint/>
	</monogr>
	<note>in Sec. 4.3</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Masking Strategy Comparisons We propose to use block masking (in Sec. 3.1) since i.i.d. masking may lead to trivial solutions for the mask-then-predict task given the strong localities in videos. We illustrate this point in Fig. 7 with a simple copy-paste reconstruction method. Specifically, after masking, we first replace the masked tokens with their nearest visible neighbours (i.e., the unmasked token that has the shortest distance in spatial or spatio-temporal domain</title>
		<imprint/>
	</monogr>
	<note>and then forward the reconstructed tokens to the VQ-VAE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
