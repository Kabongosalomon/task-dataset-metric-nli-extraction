<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPARSE MIXTURE-OF-EXPERTS ARE DOMAIN GENER- ALIZABLE LEARNERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Mila-Quebec AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nvidia Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPARSE MIXTURE-OF-EXPERTS ARE DOMAIN GENER- ALIZABLE LEARNERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-ofthe-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE). Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms. * Equal contribution arXiv:2206.04046v4 [cs.CV] 30 Sep 2022</p><p>Published as a conference paper at ICLR 2023 be promising for DG. To verify this intuition, we evaluate a transformer-based model and compare it with CNN-based architectures of equivalent computational overhead, as shown in <ref type="figure">Fig. 1(a)</ref>. To our surprise, a vanilla ViT-S/16 (Dosovitskiy et al., 2021) trained with empirical risk minimization (ERM) outperforms ResNet-50 trained with SOTA DG algorithms <ref type="bibr" target="#b7">(Cha et al., 2021b;</ref><ref type="bibr" target="#b40">Rame et al., 2021;</ref> Shi et al., 2021)  on DomainNet, OfficeHome and VLCS datasets, despite the fact that both architectures have a similar number of parameters and enjoy close performance on in-distribution domains. We theoretically validate this effect based on the algorithmic alignment framework <ref type="bibr" target="#b61">(Xu et al., 2020a;</ref>. We first prove that a network trained with the ERM loss function is more robust to distribution shifts if its architecture is more similar to the invariant correlation, where the similarity is formally measured by the alignment value defined in <ref type="bibr" target="#b61">Xu et al. (2020a)</ref>. On the contrary, a network is less robust if its architecture aligns with the spurious correlation. We then investigate the alignment between backbone architectures (i.e., convolutions and attentions) and the correlations in these datasets, which explains the superior performance of ViT-based methods.</p><p>To further improve the performance, our analysis indicates that we should exploit properties of invariant correlations in vision tasks and design network architectures to align with these properties. This requires an investigation that sits at the intersection of domain generalization and classic computer vision. In domain generalization, it is widely believed that the data are composed of some sets of attributes and distribution shifts of data are distribution shifts of these attributes <ref type="bibr" target="#b59">(Wiles et al., 2021)</ref>. The latent factorization model of these attributes is almost identical to the generative model of visual attributes in classic computer vision (Ferrari &amp; Zisserman, 2007). To capture these diverse attributes, we propose a Generalizable Mixture-of-Experts (GMoE), which is built upon sparse mixture-of-experts (sparse MoEs)  and vision transformer (Dosovitskiy et al.,  2021). The sparse MoEs were originally proposed as key enablers for extremely large, but efficient models (Fedus et al., 2022). By theoretical and empirical evidence, we demonstrate that MoEs are experts for processing visual attributes, leading to a better alignment with invariant correlations. Based on our analysis, we modify the architecture of sparse MoEs to enhance their performance in DG. Extensive experiments demonstrate that GMoE achieves superior domain generalization performance both with and without DG algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">CONTRIBUTIONS</head><p>In this paper, we formally investigate the impact of the backbone architecture on DG and propose to develop effective DG methods by backbone architecture design. Specifically, our main contributions are summarized as follows:</p><p>A Novel View of DG: In contrast to previous works, this paper initiates an exploration of the backbone architecture in DG. Based on algorithmic alignment <ref type="bibr" target="#b61">(Xu et al., 2020a)</ref>, we prove that a network is more robust to distribution shifts if its architecture aligns with the invariant correlation, whereas less robust if its architecture aligns with spurious correlation. The theorems are verified on synthetic and real datasets.</p><p>A Novel Model for DG: Based on our theoretical analysis, we propose Generalizable Mixture-of-Experts (GMoE) and prove that it enjoys a better alignment than vision transformers. GMoE is built upon sparse mixture-of-experts  and vision transformer (Dosovitskiy et al.,  2021), with a theory-guided performance enhancement for DG.</p><p>Excellent Performance: We validate GMoE's performance on all 8 large-scale datasets of Do-mainBed. Remarkably, GMoE trained with ERM achieves SOTA performance on 7 datasets in the train-validation setting and on 8 datasets in the leave-one-domain-out setting. Furthermore, the GMoE trained with DG algorithms achieves better performance than GMoE trained with ERM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>2.1 NOTATIONS Throughout this paper, a, a, A stand for a scalar, a column vector, a matrix, respectively. O(?) and ?(?) are asymptotic notations. We denote the training dataset, training distribution, test dataset, and test distribution as E tr , D tr , E te , and D te , respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">MOTIVATIONS</head><p>Generalizing to out-of-distribution (OOD) data is an innate ability for human vision, but highly challenging for machine learning models <ref type="bibr">(Recht et al., 2019;</ref><ref type="bibr">Geirhos et al., 2021;</ref><ref type="bibr">Ma et al., 2022)</ref>. Domain generalization (DG) is one approach to address this problem, which encourages models to be resilient under various distribution shifts such as background, lighting, texture, shape, and geographic/demographic attributes.</p><p>From the perspective of representation learning, there are several paradigms towards this goal, including domain alignment <ref type="bibr">(Ganin et al., 2016;</ref><ref type="bibr">Hoffman et al., 2018)</ref>, invariant causality prediction <ref type="bibr" target="#b1">(Arjovsky et al., 2019;</ref><ref type="bibr" target="#b19">Krueger et al., 2021)</ref>, meta-learning <ref type="bibr">(Bui et al., 2021;</ref><ref type="bibr" target="#b69">Zhang et al., 2021c)</ref>, ensemble learning <ref type="bibr" target="#b30">(Mancini et al., 2018;</ref><ref type="bibr" target="#b7">Cha et al., 2021b)</ref>, and feature disentanglement <ref type="bibr" target="#b63">(Wang et al., 2021;</ref><ref type="bibr">Zhang et al., 2021b)</ref>. The most popular approach to implementing these ideas is to design a specific loss function. For example, <ref type="bibr">DANN (Ganin et al., 2016)</ref> aligns domain distributions by adversarial losses. Invariant causal prediction can be enforced by a penalty of gradient norm <ref type="bibr" target="#b1">(Arjovsky et al., 2019)</ref> or variance of training risks <ref type="bibr" target="#b19">(Krueger et al., 2021)</ref>. Meta-learning and domain-specific loss functions <ref type="bibr">(Bui et al., 2021;</ref><ref type="bibr" target="#b69">Zhang et al., 2021c)</ref> have also been employed to enhance the performance. Recent studies have shown that these approaches improve ERM and achieve promising results on large-scale DG datasets <ref type="bibr" target="#b59">(Wiles et al., 2021)</ref>.</p><p>Meanwhile, in various computer vision tasks, the innovations in backbone architectures play a pivotal role in performance boost and have attracted much attention <ref type="bibr">(He et al., 2016;</ref><ref type="bibr" target="#b13">Hu et al., 2018;</ref><ref type="bibr">Liu et al., 2021)</ref>. Inspired by these empirical successes, we conjecture that backbone architecture design would</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ATTRIBUTE FACTORIZATION</head><p>The attribute factorization <ref type="bibr" target="#b59">(Wiles et al., 2021)</ref> is a realistic generative model under distribution shifts. Consider a joint distribution of the input x and corresponding attributes a 1 , ? ? ? , a K (denoted as a 1:K ) with a i ? A i , where A i is a finite set. The label can depend on one or multiple attributes. Denote the latent factor as z, the data generation process is given by z ? p(z), a i ? p(a i |z), x ? p(x|z), p(a 1:K , x) = p(a 1:K ) p(x|z)p(z|a 1:K )dz. <ref type="bibr" target="#b76">(1)</ref> The distribution shift arises if different marginal distributions of the attributes are given but they share the same conditional generative process. Specifically, we have p train (a 1:K ) = p test (a 1:K ), but the generative model in equation 1 is shared across the distributions, i.e., we have p test (a 1:K , x) = p test (a 1:K ) p(x|z)p(z|a 1:K )dz and similarly for p train . The above description is abstract and we will illustrate with an example. Example 1. (DSPRITES <ref type="bibr">(Matthey et al., 2017)</ref>) Consider A 1 = {red, blue} and A 2 = {ellipse, square}. The target task is a shape classification task, where the label depends on attribute a 2 . In the training dataset, 90% ellipses are red and 50% squares are blue, while in the test dataset all the attributes are distributed uniformly. As the majority of ellipses are red, the classifier will use color as a shortcut in the training dataset, which is so-called geometric skews <ref type="bibr">(Nagarajan et al., 2020)</ref>. However, this shortcut does not exist in the test dataset and the network fails to generalize.</p><p>In classic computer vision, the attributes are named visual attributes and they follow a similar data generation process <ref type="bibr">(Ferrari &amp; Zisserman, 2007)</ref>. We shall discuss them in detail in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ALGORITHMIC ALIGNMENT</head><p>We first introduce algorithmic alignment, which characterizes the easiness of IID reasoning tasks by measuring the similarity between the backbone architecture and target function. The alignment is formally defined as the following. Definition 1. (Alignment; <ref type="bibr" target="#b61">(Xu et al., 2020a)</ref>) Let N denote a neural network with n modules {N i } n i=1 and assume that a target function for learning y = g(x) can be decomposed into n functions f 1 , ? ? ? , f n . The network N aligns with the target function if replacing N i with f i , it outputs the same value as algorithm g. The alignment value between N and f is defined as</p><formula xml:id="formula_0">Alignment(N , f, , ?) := n ? max i M(f i , N i , , ?),<label>(2)</label></formula><p>where M(f i , N i , , ?) denotes the sample complexity measure for N i to learn f i with precision at failure probability ? under a learning algorithm when the training distribution is the same as the test distribution.</p><p>In Definition 1, the original task is to learn f , which is a challenging problem. Intuitively, if we could find a backbone architecture that is suitable for this task, it helps to break the original task into simpler sub-tasks, i.e., to learn f 1 , ? ? ? , f n instead. Under the assumptions of algorithmic alignment <ref type="bibr" target="#b61">(Xu et al., 2020a)</ref>, f can be learned optimally if the sub-task f 1 , ? ? ? , f n can be learned optimally. Thus, a good alignment makes the target task easier to learn, and thus improves IID generalization, which is given in Theorem 3 in Appendix B.1. In Section 3, we extend this framework to the DG setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ON THE IMPORTANCE OF NEURAL ARCHITECTURE FOR DOMAIN GENERALIZATION</head><p>In this section, we investigate the impact of the backbone architecture on DG, from a motivating example to a formal framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A MOTIVATING EXAMPLE: CNNS VERSUS VISION TRANSFORMERS</head><p>We adopt DomainBed <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021)</ref> as the benchmark, which implements SOTA DG algorithms with ResNet50 as the backbone. We test the performance of ViT trained with ERM on (a) Performance comparison.  this benchmark, without applying any DG method. The results are shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. To our surprise, ViT trained with ERM already outperforms CNNs with SOTA DG algorithms on several datasets, which indicates that the selection of the backbone architecture is potentially more important than the loss function in DG. In the remaining of this article, we will obtain a theoretical understanding of this phenomenon and improve ViT for DG by modifying its architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UNDERSTANDING FROM A THEORETICAL PERSPECTIVE</head><p>The above experiment leads to an intriguing question: how does the backbone architecture impact the network's performance in DG? In this subsection, we endeavor to answer this question by extending the algorithmic alignment framework <ref type="bibr" target="#b61">(Xu et al., 2020a)</ref> to the DG setting.</p><p>To have a tractable analysis for nonlinear function approximation, we first make an assumption on the distribution shift. Assumption 1. Denote N 1 as the first module of the network (including one or multiple layers) of the network. Let p train,N1 (s) and p test,N1 (s) denote the probability density functions of features after N 1 . Assume that the support of the training feature distribution covers that of the test feature distribution, i.e., max s p test,N 1 (s) p train,N 1 (s) ? C, where C is a constant independent of the number of training samples. Remark 1. (Interpretations of Assumption 1) This condition is practical in DG, especially when we have a pretrained model for disentanglement (e.g., on DomainBed (Gulrajani &amp; Lopez-Paz, 2021)). In Example 1, the training distribution and test distribution have the same support. In DomainNet, although the elephants in quickdraw are visually different from the elephants in other domains, the quickdraw picture's attributes/features (e.g., big ears and long noise) are covered in the training domains. From a technical perspective, it is impossible for networks trained with gradient descent to approximate a wide range of nonlinear functions in the out-of-support regime <ref type="bibr" target="#b62">(Xu et al., 2020b)</ref>. Thus, this condition is necessary if we do not impose strong constraints on the target functions.</p><p>We define several key concepts in DG. The target function is an invariant correlation across the training and test datasets. For simplicity, we assume that the labels are noise-free. Assumption 2. (Invariant correlation) Assume there exists a function g c such that for training data, we have g c (N 1 (x)) = y, ?x ? E tr , and for test data, we have P Dte [ g c (N 1 (x)) ? y ? ] &gt; 1 ? ?.</p><p>We then introduce the spurious correlation <ref type="bibr" target="#b59">(Wiles et al., 2021)</ref>, i.e., some attributes are correlated with labels in the training dataset but not in the test dataset. The spurious correlation exists only if the training distribution differs from the test distribution and this distinguishes DG from classic PAC-learning settings <ref type="bibr" target="#b61">(Xu et al., 2020a)</ref>. Assumption 3. (Spurious correlation) Assume there exists a function g s such that for training data g s (N 1 (x)) = y, ?x ? E tr , and for test data, we have P Dte [ g s (N 1 (x)) ? y &gt; ?( )] &gt; 1 ? ?.</p><p>The next theorem extends algorithmic alignment from IID generalization (Theorem 3) to DG. Theorem 1. (Impact of Backbone Architecture in Domain Generalization) Denote N = {N 2 , ? ? ? , N n }. Assuming we train the neural network with ERM, and Assumption 1, 2, 3 hold, we have the following statements:</p><formula xml:id="formula_1">1. If Alignment(N , g c , , ?) ? |E tr |, we have P Dte [ N (x) ? y ? O( )] &gt; 1 ? O(?); 2. If Alignment(N , g s , , ?) ? |E tr |, we have P Dte [ N (x) ? y &gt; ?( )] &gt; 1 ? O(?). Remark 2.</formula><p>(Interpretations of Theorem 1) By choosing a sufficiently small , only one of Alignment(N , g c , , ?) ? |E tr | and Alignment(N , g s , , ?) ? |E tr | holds. Thus, Theorem 1 shows that the networks aligned with invariant correlations are more robust to distribution shifts. In Appendix B.2, we build a synthetic dataset that satisfies all the assumptions. The experimental results exactly match Theorem 1. In practical datasets, the labels may have colored noise, which depends on the spurious correlation. Under such circumstances, the network should rely on multiple correlations to fit the label well and the correlation that best aligns with the network will have the major impact on its performance. Please refer to Appendix B.1 for the proof.</p><p>ViT with ERM versus CNNs with DG algorithms We now use Theorem 1 to explain the experiments in the last subsection. The first condition of Theorem 1 shows that if the neural architecture aligns with the invariant correlation, ERM is sufficient to achieve a good performance. In some domains of OfficeHome or DomainNet, the shape attribute has an invariant correlation with the label, illustrated in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. On the contrary, a spurious correlation exists between the attribute texture and the label. According to the analysis in <ref type="bibr" target="#b36">Park &amp; Kim (2022)</ref>, multi-head attentions (MHA) are low-pass filters with a shape bias while convolutions are high-pass filters with a texture bias. As a result, a ViT simply trained with ERM can outperform CNNs trained with SOTA DG algorithms.</p><p>To improve ViT's performance, Theorem 1 suggests that we should exploit the properties of invariant correlations. In image recognition, objects are described by functional parts (e.g., visual attributes), with words associated with them <ref type="bibr" target="#b73">(Zhou et al., 2014)</ref>. The configuration of the objects has a large degree of freedom, resulting in different shapes among one category. Therefore, functional parts are more fundamental than shape in image recognition and we will develop backbone architectures to capture them in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GENERALIZABLE MIXTURE-OF-EXPERTS FOR DOMAIN GENERALIZATION</head><p>In this section, we propose Generalizable Mixture-of-Experts (GMoE) for domain generalization, supported by effective neural architecture design and theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MIXTURE-OF-EXPERTS LAYER</head><p>In this subsection, we introduce the mixture-of-experts (MoE) layer, which is an essential component of GMoE. One ViT layer is composed of an MHA and an FFN. In the MoE layer, the FFN is replaced by mixture-of-experts and each expert is implemented by an FFN . Denoting the output of the MHA as x, the output of the MoE layer with N experts is given by</p><formula xml:id="formula_2">f MoE (x) = N i=1 G(x) i ? E i (x) = N i=1 TOP k (Softmax(W x)) ? W 2 FFNi ?(W 1 FFNi x),<label>(3)</label></formula><p>where W is the learnable parameter for the gate, W 1 FFNi and W 2 FFNi are learnable parameters for the i-th expert, ?(?) is a nonlinear activation function, and TOP k (?) operation is a one-hot embedding that sets all other elements in the output vector as zero except for the elements with the largest k values where k is a hyperparameter. Given x in as the input of the MoE layer, the update is given by</p><formula xml:id="formula_3">x = f MHA (LN(x in )) + x in , x out = f MoE (LN(x)) + x,</formula><p>where f MHA is the MHA layer, LN represents layer normalization, and x out is the output of the MoE layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VISUAL ATTRIBUTES, CONDITIONAL STATEMENTS, AND SPARSE MOES</head><formula xml:id="formula_4">Algorithm 1: Conditional Statements Define intervals I i ? R, i = 1, ? ? ? , M Define functions h i , , i = 1, ? ? ? , M + 1 switch h 1 (x) do case I i do apply h i+1 to x</formula><p>In real world image data, the label depends on multiple attributes. Capturing diverse visual attributes is especially important for DG. For example, the definition of an elephant in the Oxford dictionary is "a very large animal with thick grey skin, large ears, two curved outer teeth called tusks, and a long nose called a trunk". The definition involves three shape attributes (i.e., large ears, curved outer teeth, and a long nose) and one texture attribute (i.e., thick grey skin). In the IID ImageNet task, using the most discriminative attribute, i.e., the thick grey skin <ref type="bibr">(Geirhos et al., 2018)</ref>, is sufficient to achieve high accuracy. However, in DomainNet, elephants no longer have grey skins while the long nose and big ears are preserved and the network relying on grey skins will fail to generalize.</p><p>To efficiently capture the visual attributes and combine them for DG, conditional statements (e.g., IF/ELSE in programming) are needed. First, to obtain these diverse attributes, different filters should be applied to different regions of the image. For example, the operation for curved outer teeth is that if the patches belong to the teeth, we apply a filter to obtain its shape. Second, for image recognition, the network should leverage conditional statements to integrate multiple visual attributes for learning the definition of this class. For example, the recognition of an elephant should check if the key attributes are placed in the proper position. The general conditional statements are shown in Algorithm 1. In literature, the MoE layer is considered as an effective approach to implement conditional computations <ref type="bibr" target="#b43">Riquelme et al., 2021)</ref>. We formalize this intuition in the next theorem. </p><formula xml:id="formula_5">? (N + 1) ? max (M * P , M(G, h 1 , , ?)) , if N &lt; M, (N + 1) ? max max i?{1,??? ,M } M(f FFNi , h i+1 , , ?), M(G, h 1 , , ?) , if N ? M,<label>(4)</label></formula><p>where M(?, ?, ?, ?) is defined in Definition 1, and M * P is the optimal objective value of the following optimization problem:</p><formula xml:id="formula_6">P :minimize I1,???I N max i?{1,??? ,N } M(f FFNi , ([1 Ij ] j?Ii ? h 1 ) T ? [h j ] j?Ii , , ?) subject to ? N i=1 I i = {2, 3, ? ? ? , M + 1}, ,<label>(5)</label></formula><p>where 1 Ij is the indicator function on interval I j .</p><p>Remark 3. (Interpretations of Theorem 2) In algorithmic alignment, the network better aligns with the algorithm if the alignment value in equation 2 is lower. The alignment value between MoE and conditional statements depends on the product of N + 1 and a sample complexity term. When we increase the number of experts N , the alignment value first decreases as multiple experts decompose the original conditional statements into several simpler tasks. As we further increase N , the alignment value increases because of the factor N + 1 in the product. Therefore, the MoE aligns better with conditional statements than with the original FFN (i.e., N = 1). In addition, to minimize equation <ref type="formula" target="#formula_6">5</ref>  <ref type="figure" target="#fig_2">Figure 2</ref>: Overview architecture of GMoE. The cosine router distributes normalized image patches of different visual attributes to corresponding experts. Our analysis and experiments (in Section 4.2 and Section 5.4) demonstrate that an expert is potentially responsible for a group of similar visual attributes.</p><p>Routing scheme Linear routers (i.e., equation 3) are often adopted in MoEs for vision tasks <ref type="bibr" target="#b43">(Riquelme et al., 2021)</ref> while recent studies in NLP show that the cosine router achieves better performance in cross-lingual language tasks <ref type="bibr">(Chi et al., 2022)</ref>. For the cosine router, given input x ? R d , the embedding W x ? R de is first projected onto a hypersphere, followed by multiplying a learned embedding E ? R de?N . Specifically, the expression for the gate is given by</p><formula xml:id="formula_7">G(x) = TOP k Softmax E T W x ? W x E ,</formula><p>where ? is a hyper-parameter. In the view of image processing, E can be interpreted as the cookbook for visual attributes <ref type="bibr">(Ferrari &amp; Zisserman, 2007;</ref><ref type="bibr" target="#b73">Zhou et al., 2014)</ref> and the dot product between E and W x with 2 normalization is a matched filter. We opine that the linear router would face difficulty in DG. For example, the elephant image (and its all patches) in the Clipart domain is likely more similar to other images in the Clipart domain than in other domains. The issue can be alleviated with a codebook for visual attributes and matched filters for detecting them. Please refer to Appendix D.5 for the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of MoE layers</head><p>Every-two and last-two are two commonly adopted placement methods in existing MoE studies <ref type="bibr" target="#b43">(Riquelme et al., 2021;</ref><ref type="bibr" target="#b20">Lepikhin et al., 2021)</ref>. Specifically, every-two refers to replacing the even layer's FFN with MoE, and last-two refers to placing MoE at the last two even layers. For IID generalization, every-two often outperforms last-two <ref type="bibr" target="#b43">(Riquelme et al., 2021)</ref>. We argue that last-two is more suitable for DG as the conditional sentences for processing visual attributes are high-level. From experiments, we empirically find that last-two achieves better performance than every-two with fewer computations. Please refer to Appendix C.1 for more discussions and Appendix D.5 for the ablation study.</p><p>The overall backbone architecture of GMoE is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. To train diverse experts, we adopt the perturbation trick and load balance loss as in <ref type="bibr" target="#b43">Riquelme et al. (2021)</ref>. Due to space limitation, we leave them in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the performance of GMoE on large-scale DG datasets and present model analysis to understand GMoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DOMAINBED RESULTS</head><p>In this subsection, we evaluate GMoE on DomainBed (Gulrajani &amp; Lopez-Paz, 2021) with 8 benchmark datasets: PACS, VLCS, OfficeHome, TerraIncognita, DomainNet, SVIRO, Wilds-Camelyon and Wilds-FMOW. Detailed information on datasets and evaluation protocols are provided in Appendix D.1. We present results in <ref type="table" target="#tab_1">Table 1</ref> with train-validation selection, which include baseline methods and recent SOTA DG algorithms and GMoE trained with ERM. The results demonstrate that GMoE without DG algorithms already outperforms counterparts on almost all the datasets. Meanwhile, GMoE has excellent performance in leave-one-domain-out criterion, and we leave the results in Appendix D.3 due to space limit. In the lower part of <ref type="table" target="#tab_1">Table 1</ref>, we test our methods on three largescale datasets: SVIRO, Wilds-Camelyon, and Wilds-FMOW. The three datasets capture real-world distribution shifts across a diverse range of domains. We adopt the data preprocessing and domain split in DomainBed. As there is no previous study conducting experiments on these datasets with DomainBed criterion, we only report the results of our methods, which reveal that GMoE outperforms the other two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GMOE WITH DG ALGORITHMS</head><p>GMoE's generalization ability comes from its internal backbone architecture, which is orthogonal to existing DG algorithms. This implies that the SOTA DG algorithms can be applied to improve the GMoE's performance. To validate this idea, we apply two SOTA DG algorithms to GMoE, including one modifying loss functions approaches (Fish) and one adopting model ensemble (Swad). The results in <ref type="table" target="#tab_2">Table 2</ref> demonstrate that adopting GMoE instead of ResNet-50 brings significant accuracy promotion to these DG algorithms. In this subsection, we create a challenging task, singlesource domain generalization, to focus on generalization ability of backbone architecture. Specifically, we train the model only on data from one domain, and then test the model on multiple domains to validate its performance across all domains. This is a challenging task as we cannot rely on multiple domains to identify invariant correlations, and popular DG algorithms cannot be applied. We compare several models mentioned in above analysis (e.g., ResNet, ViT, GMoE) with different scale of parameters, including their float-point-operations per second (flops), IID and OOD accuracy. From the results in <ref type="table" target="#tab_3">Table 3</ref>, we see that GMoE's OOD generalization gain over ResNet or ViT is much larger than that in the IID setting, which shows that GMoE is suitable for challenging domain generalization. Due to space limitation, we leave experiments with other training domains in Appendix D.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SINGLE-SOURCE DOMAIN GENERALIZATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MODEL ANALYSIS</head><p>In this subsection, we present diagnostic datasets to study the connections between MoE layer and the visual attributes.</p><p>Diagnostic datasets: CUB-DG We create CUB-DG from the original Caltech-UCSD Birds (CUB) dataset <ref type="bibr" target="#b56">(Wah et al., 2011)</ref>. We stylize the original images into another three domains, Candy, Mosaic and Udnie. The examples of CUB-DG are presented in <ref type="figure" target="#fig_5">Fig. 3</ref> has provided the location of visual attributes, we first match a visual attribute with its nearest 9 patches, and then correlate visual attribute and its current 9 patches' experts. In <ref type="figure" target="#fig_5">Fig. 3(b)</ref>, we show the 2D histogram correlation between the selected experts and attributes.</p><p>Expert Selection To further understand the expert selections of the entire image, we record the router's selection for each patch (in GMoE-S/16, we process an image into 16 ? 16 patches), and then visualize the top-1 selections for each patch in <ref type="figure" target="#fig_5">Fig. 3</ref>(c). In this image, we draw different visual attributes of the bird with large circles in different colors. We see consistent relationship with <ref type="bibr">Fig. 3(b)</ref>. For example, we see <ref type="formula" target="#formula_9">(1)</ref> experts 0 and 2 are consistently routed with patches that are in the background area; <ref type="formula" target="#formula_0">(2)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>This paper is an initial step in exploring the impact of the backbone architecture in domain generalization. We proved that a network is more robust to distribution shifts if its architecture aligns well with the invariant correlation, which is verified on synthetic and real datasets. Based on our theoretical analysis, we proposed GMoE and demonstrated its superior performance on DomainBed.</p><p>As for future directions, it is interesting to develop novel backbone architectures for DG based on algorithmic alignment and classic computer vision. (1) Invariant Learning: Aligning domain distributions and finding invariance across domains has been often studied with empirical results and theoretical proofs <ref type="bibr">(Ganin et al., 2016;</ref><ref type="bibr" target="#b70">Zhao et al., 2019a)</ref>. Specifically, researchers have explicitly sought aligning feature distributions based on the maximum mean discrepancy (MMD) <ref type="bibr" target="#b23">(Li et al., 2018b)</ref>, second order correlation <ref type="bibr" target="#b47">(Sun &amp; Saenko, 2016)</ref>, moment matching <ref type="bibr" target="#b37">(Peng et al., 2019)</ref>, etc. Besides aligning feature distributions, Arjovsky et al. <ref type="bibr" target="#b1">(Arjovsky et al., 2019)</ref> proposed IRM to learn an ideal invariant classifier on top of the representation space, which has inspired many follow-up works <ref type="bibr" target="#b19">(Krueger et al., 2021;</ref><ref type="bibr" target="#b0">Ahuja et al., 2021;</ref>. These studies implement invariant learning via loss function designs. In this paper, we show that if we have a suitable backbone architecture, ERM is sufficient to find the invariant correlations, which indicates that the backbone architecture is as important as loss function design.</p><p>(2) Ensemble Learning and Meta-learning: Apart from learning invariant features and correlations across the domain, model-specific or domain-specific information helps improve the performance on DG datasets. The ensemble learning methods combine different models to exploit model-specific information <ref type="bibr" target="#b30">(Mancini et al., 2018;</ref><ref type="bibr">Seg? et al., 2020;</ref><ref type="bibr">Arpit et al., 2021;</ref><ref type="bibr" target="#b57">Li et al., 2022)</ref>. In addition, SWAD <ref type="bibr" target="#b7">(Cha et al., 2021b)</ref> inhibits models from being overfit to local sharp minima by averaging model weights below a validation loss threshold. Meta-learning-based approaches <ref type="bibr" target="#b22">(Li et al., 2018a;</ref><ref type="bibr">Dou et al., 2019;</ref><ref type="bibr" target="#b69">Zhang et al., 2021c;</ref><ref type="bibr">Bui et al., 2021)</ref> leverage domain-specific feature to improve the accuracy on each domain.</p><p>(3) Data Manipulation: Diverse training data are helpful for improving generalization, and researchers have proposed different manipulation/augmentation techniques <ref type="bibr" target="#b35">(Nazari &amp; Kovashka, 2020;</ref><ref type="bibr" target="#b42">Riemer et al., 2019)</ref>, domain randomization <ref type="bibr" target="#b64">(Yue et al., 2019;</ref><ref type="bibr" target="#b65">Zakharov et al., 2019)</ref>. Furthermore, a line of works <ref type="bibr" target="#b38">(Qiao et al., 2020;</ref><ref type="bibr" target="#b27">Liu et al., 2018;</ref><ref type="bibr" target="#b71">Zhao et al., 2019b)</ref> have exploited generating data samples to enhance the model generalization ability.</p><p>(4) Neural Architecture Search: Recently, neural architecture search-based methods were employed for DG. In <ref type="bibr" target="#b67">Zhang et al. (2021a)</ref>, the lottery ticket hypothesis was extended to DG settings: a biased full network contains an unbiased subnetwork that can achieve better OOD performance. The network pruning method was further adopted to identify such subnetworks. Differentiable architecture search was employed in <ref type="bibr" target="#b3">Bai et al. (2021)</ref> to find a good CNN architecture for DG. These works are independent of backbone architecture design.</p><p>One unique advantage of the proposed method is that it is orthogonal to the above approaches, meaning that the performance of GMoE might be enhanced when combined with these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 VISION TRANSFORMERS</head><p>Originated from the machine translation tasks, Transformer <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> has recently received great attention in computer vision <ref type="bibr">(Dosovitskiy et al., 2021;</ref><ref type="bibr">Liu et al., 2021;</ref><ref type="bibr" target="#b57">Wang et al., 2022)</ref> for their unprecedented performance in image recognition, semantic segmentation, and visual question answering. In addition to strong performance, some recent works empirically demonstrate the robustness of the ViTs over CNNs in terms of adversarial noise <ref type="bibr">(Qin et al., 2021)</ref> and distribution shifts . Nevertheless, the theoretical understandings remain elusive and this paper theoretically validates these effects. The analysis also motivates us to employ mixture-of-experts to enhance the performance of ViT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SPARSE MIXTURE-OF-EXPERTS</head><p>Mixture-of-Experts models, or MoEs, make use of the outputs from several sub-models (experts) through an input-dependent routing mechanism for stronger model performance <ref type="bibr" target="#b15">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b16">Jordan &amp; Jacobs, 1994)</ref>. This training paradigm has led to the development of a plethora of methods for a wide ranges of applications <ref type="bibr" target="#b14">(Hu et al., 1997;</ref><ref type="bibr" target="#b49">Tani &amp; Nolfi, 1999)</ref>. However, integrating MoEs and big models will inevitably introduce even larger model sizes and longer inference time. Sparse MoEs  were proposed with their routers to select only a few experts so that the inference time is on-par with the standalone counterpart. They were considered as promising ways to scaling up vision models <ref type="bibr" target="#b43">(Riquelme et al., 2021)</ref>. Existing works conjectured that the effectiveness of MoE comes from conditional computations and this is theoretically verified in Theorem 2. In addition, we show that MoE is a powerful architecture for DG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 OUT-OF-DISTRIBUTION GENERALIZATION THEORY</head><p>The classic PAC learning <ref type="bibr" target="#b52">(Valiant, 1984)</ref> is only applicable to IID settings and recently there have been a line of works to investigate the theory of OOD generalization, including structured causal models (SCM) <ref type="bibr" target="#b1">(Arjovsky et al., 2019;</ref><ref type="bibr" target="#b0">Ahuja et al., 2021;</ref> and extrapolation theory <ref type="bibr" target="#b62">(Xu et al., 2020b;</ref><ref type="bibr" target="#b75">Ziyin et al., 2020;</ref>. The SCM-based approaches often assume that the invariant correlation is linear <ref type="bibr" target="#b1">(Arjovsky et al., 2019;</ref><ref type="bibr" target="#b0">Ahuja et al., 2021;</ref>. The extrapolation theory focused on the situation where the supports of training distribution and test distribution are different <ref type="bibr" target="#b62">(Xu et al., 2020b;</ref><ref type="bibr" target="#b75">Ziyin et al., 2020;</ref>, where the networks have very limited ability for nonlinear function approximation <ref type="bibr" target="#b62">(Xu et al., 2020b)</ref>. Different from existing works, this paper studies the situation where the invariant correlation is an arbitrary nonlinear function, and one of our main focuses is to justify why these assumptions hold in DG of vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 ALGORITHMIC ALIGNMENT</head><p>The algorithmic alignment framework <ref type="bibr" target="#b61">(Xu et al., 2020a)</ref> was originally proposed to understand the effectiveness of specialized network structures in reasoning tasks. Specifically, it characterizes the generalization bound of reasoning tasks by studying the alignment between the reasoning algorithm and the computation graph of the neural network. The underlying intuition is that if they align well, the neural network only needs to learn simple functions, which leads to better sample efficiency. This framework was further extended in  to investigate the impact of backbone architectures on the robustness of noisy labels. This paper adopts algorithmic alignment to provide a novel perspective of DG and develops a new architecture based on it.</p><p>B THEOREMS AND PROOFS B.1 PROOF OF THEOREM 1</p><p>We first state the algorithmic alignment theorem in the IID setting and then extend it to the DG setting. </p><formula xml:id="formula_8">N i 's. Suppose f = A({x i , y i }) and f = A({x i , y i }). For any x, f (x) ?f (x) ? L 0 ? max i x i ?x i , where x i is the i-th coordinate of x.</formula><p>(b) Sequential learning. We train N i sequentially: N 1 has input samples {x</p><formula xml:id="formula_9">(1) i , f 1 (x (1) i )} N i=1 , with x<label>(1)</label></formula><p>i obtained from the training dataset. For j &gt; 1, the inputsx <ref type="bibr">(j)</ref> i for N j are the outputs from the previous modules, but labels are generated by the correct functions f j?1 , ? ? ? , f 1 onx</p><formula xml:id="formula_10">(1) i . (c) Lipschitzness. The learned functionsf j satisfy f j (x) ?f j (x) ? L 1 x ?x for some L 1 . Under assumptions (a)(b)(c), Alignment(N , g, , ?) ? M implies there is a learning algorithm A such that P x?D [ N (x) ? g(x) ? O( )] ? 1 ? O(?),</formula><p>where N is the network generated by A on the training data {x i , y i } M i=1 . Remark 4. (Explanations of Assumptions in Theorem 3) The first and third assumptions are common assumptions in machine learning and are practical. The second assumption is impractical as we do not have auxiliary labels. However, the same pattern is observed for end-to-end learning in experiments <ref type="bibr" target="#b61">(Xu et al., 2020a;</ref>.</p><p>We now prove Theorem 3. Theorem 4. (Impact of Backbone Architecture in DG) Let N = {N 2 , ? ? ? , N n }. Assuming we train the neural network with ERM, and Assumption 1, 2, 3, we have the following statements:</p><formula xml:id="formula_11">1. If Alignment(N , g c , , ?) ? |E tr |, we have P Dte [ N (x) ? y ? O( )] &gt; 1 ? O(?); 2. If Alignment(N , g s , , ?) ? |E tr |, we have P Dte [ N (x) ? y &gt; ?( )] &gt; 1 ? O(?).</formula><p>Proof. The proof is mainly based on Theorem 3. We tackle the distribution shift in x with Lemma 1 and analyze the distribution shift in y with Assumption 2, 3.</p><p>First condition: From Theorem 3,</p><formula xml:id="formula_12">P x?Dtr [ N (x) ? g c (N 1 (x)) &gt; O( )] ? O(?).</formula><p>By Lemma 1, we have</p><formula xml:id="formula_13">P x?Dte [ N (x) ? g c (N 1 (x)) ? O( )] ? CP x?Dtr [ N (x) ? g c (x) &lt; O( )] ? O(?). (6)</formula><p>Combing with the second condition of Assumption 2, we have</p><formula xml:id="formula_14">P Dte [ N (x) ? g(x) ? O( )] ?P Dte [ g c (N 1 (x)) ? g(x) ? O( )] ? P Dte [ N (x) ? g c (N 1 (x)) ? O( )] ?(1 ? ?)(1 ? O(?)) = 1 ? O(?),</formula><p>where the first inequality follows O( ) + O( ) = O( ), and the second inequality follows the second condition of Assumption 2 and equation 6.</p><p>Second condition: Following the proof of equation 6, we obtain</p><formula xml:id="formula_15">P Dte [ N (x) ? g s (N 1 (x)) ? O( )] ? 1 ? O(?).<label>(7)</label></formula><p>Combing with the second condition of Assumption 3, we have</p><formula xml:id="formula_16">P Dte [ N (x) ? g(x) &gt; ?( )] ?P Dte [ g s (N 1 (x)) ? g(x) &gt; ?( )] ? P Dte [ N (x) ? g s (N 1 (x)) ? O( )] ?(1 ? ?)(1 ? O(?)) = 1 ? O(?),</formula><p>where the first inequality follows ?(?) ? O(?) = O(?), and the second inequality follows the second condition of Assumption 3 and equation 7. This finishes the proof for Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SYNTHETIC EXPERIMENTS FOR THEOREM 1</head><p>In this subsection, we set up synthetic experiments to illustrate Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 NOISELESS VERSION</head><p>We first set up experiments following the assumptions in Theorem 1.</p><p>Training dataset generation: Consider data pairs (x, y) ? (R K ) P ? {1, ? ? ? , K} in the training and validation datasets generated as follows.</p><p>? Generate label y ? {1, ? ? ? , K} uniformly.</p><p>? Generate K mutually orthogonal feature vectors {c k } such that c k ? R K and c k 2 = 1.</p><p>? Generate x as a collection of P patches: x = (x <ref type="bibr" target="#b76">(1)</ref> , ? ? ? , x (P ) ) ? (R d ) P -Pixel-level feature. For the first patch, the y-th pixel is set as 1 and other pixels drawn from N (0, 1). -Patch-level feature. Uniformly select one and only one patch given by c y .</p><p>-Random noise. The rest patches are Gaussian noise drawn independently from N (0, I K ).</p><p>The generation of the validation dataset follows that of the training dataset.</p><p>Test dataset generation: Two OOD test datasets are given. The first dataset differs from the training dataset in the pixel-level feature. Specifically, we set the (y + 1)mod K-th pixel of the first patch as 1 and other pixels in the first patch follow N (0, 1), i.e., making the pixel-level features spurious. The second dataset is distinguished from the training dataset such that the patch-level feature is c (y+1)mod K , i.e., making patch-level features spurious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone architectures:</head><p>We adopt MLPs and fully convolutional networks (FCNs) as MLPs align with pixel-level features while FCNs align with patch-level features. It is clear that both MLPs and FCNs can fit the label only from pixel-level features or patch-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters in experiments:</head><p>We set P = 10, K = 4. We generate 100, 000 samples for the training dataset and 2, 000 test samples for the validation and test datasets. We use a three-layer MLP with hidden size {40, 100, 100, 4} and a two-layer FCN with 20 filters.</p><p>The experimental results are shown in <ref type="figure" target="#fig_8">Fig. 4</ref>. The MLP completely fails on the second dataset while the FCN completely fails on the first dataset, which is consistent with Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 NOISY VERSION</head><p>In the previous experiment, both pixel-level features and patch-level features can perfectly fit the label. We give a noisy version as follows:</p><p>1. Pixel-level feature. Given a probability value p 1 . With probability p 1 , the y-th pixel of the first patch is set as 1 and other pixels are drawn from N (0, 1). With probability 1 ? p 1 , the y -th (y = y) pixel of the first patch is set as 1 and other pixels are drawn from N (0, 1).  (g) p1 = 0.9, p2 = 1.  2. Patch-level feature. Given a probability value p 2 . With probability p 2 , uniformly select one and only one patch given by c y . With probability 1 ? p 2 , uniformly select one and only one patch given by c y (y = y).</p><p>The experiments with different p 1 and p 2 are shown in <ref type="figure" target="#fig_10">Fig. 5</ref>. When the signal-to-noise ratios of pixel-level feature and patch-level feature are similar, the MLP learns the pixel-level feature, which corresponds to its alignment. It shows that the network prefers learning the correlation that aligns with its architecture, even when exploiting other correlations may lead to performance gains. When the difference between p 1 and p 2 is large, the network will still learn its aligned correlation at the early stage (with above 90% accuracy on Test1), but may change when the number of epochs is large. This suggests that early stopping is beneficial to DG as suggested in <ref type="bibr" target="#b7">Cha et al. (2021b)</ref> under the condition that the backbone aligns to some invariant correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 PROOF OF THEOREM 2</head><p>Theorem 5. An MoE module in equation 3 with N experts and k = 1 aligns the conditional sentences in Algorithm 1 with</p><formula xml:id="formula_17">Alignment = ? ? ? (N + 1) ? max (M * P , M(G, h 1 , , ?)) , if N &lt; M, (N + 1) ? max max i?{1,??? ,M } M(f FFNi , h i+1 , , ?), M(G, h 1 , , ?) , if N ? M,</formula><p>where M(?, ?, ?, ?) is defined in Definition 1, and M * P is the optimal objective value of the following optimization problem:</p><formula xml:id="formula_18">P :minimize I1,???I N max i?{1,??? ,N } M(f FFNi , ([1 Ij ] j?Ii ? h 1 ) T ? [h j ] j?Ii , , ?) subject to ? N i=1 I i = {2, 3, ? ? ? , M + 1}, where 1 Ii is the indicator function on interval I i .</formula><p>Proof. For N ? M , we assign one expert for each function. For N &lt; M , similar functions should be assigned to one expert to minimize the sample complexity.</p><p>Case N ? M : We define the mapping H(x) = [1 I1 , ? ? ? , 1 I M ] ? R M , where 1 Ii is the indicator function on interval I i . To show the alignment between MoE and conditional sentences, we define the module functions f 1 , ? ? ? , f N and neural network modules N 1 , ? ? ? , N N as</p><formula xml:id="formula_19">f i = ? ? ? H ? h 1 , if i = 1, h i+1 , if 1 ? i ? M + 1, 0, o.w., N i = ? ? ? G, if i = 1, f FFNi , if 1 ? i ? M + 1, f FFNi , o.w.,</formula><p>Replacing N i with f i , the MoE network simulates Algorithm 1. Thus, for N &gt; M , MoE algorithmically aligns conditional sentences with</p><formula xml:id="formula_20">Alignment = (N + 1) ? max max i M(f FFNi , h i+1 , , ?), M(G, h 1 , , ?)<label>(8)</label></formula><p>where M(?, ?, ?, ?) is defined in Definition 1.</p><p>Case N &lt; M : We define the mapping H 0 (x) = [1 ? j?I 1 Ij , ? ? ? , 1 ? j?I N Ij ] ? R N , where I i is the solution to the optimization problem in equation 5. To show the alignment between MoE and conditional sentences, we define the module functions f 1 , ? ? ? , f N and neural network modules N 1 , ? ? ? , N N as</p><formula xml:id="formula_21">f i = H 0 ? h 1 , if i = 1, ([1 Ij ] j?Ii?1 ? h 1 ) T ? [h j ] j?Ii?1 , o.w., N i = G, if i = 1, f FFNi , o.w.</formula><p>Replacing N i with f i , the MoE network simulates Algorithm 1. Thus, for N &lt; M , MoE algorithmically aligns with conditional sentences with Alignment = (N + 1) ? max (M * P , M <ref type="figure" target="#fig_1">(G, h 1 , , ?)</ref>) where M * P is the optimal objective value to equation 5 and M(?, ?, ?, ?) is defined in Definition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 TECHNICAL LEMMAS</head><p>Lemma 1. Under Assumption 1, for a given functions f (?) and an interval A,</p><formula xml:id="formula_22">P Dtr [f (N 1 (x)) ? A] &lt; ? implies P Dte [f (N 1 (x)) ? A] ? C?.</formula><p>Proof. We denote the inverse mapping as f ?1 (A) = {s|f (s) ? A}. Note that</p><formula xml:id="formula_23">P Dte [f (N 1 (x)) ? A] = s?f ?1 (A) p test,N1 (s)ds ?C s?f ?1 (A) p train,N1 (s)ds = CP Dtr [f (N 1 (x)) ? A]</formula><p>This finishes the proof for Lemma 1.</p><p>Remark 5. Assumption 1 is similar to ?-exploration in batch reinforcement learning (RL), which aims at controlling the distribution shift in RL <ref type="bibr">(Xie &amp; Jiang, 2021)</ref>. Lemma 1 suggests that the generalization with the same training and test support resembles the IID generalization. Unfortunately, this is the largest distribution shift we can assume as the impossibility of nonlinear out-of-support generalization is proved in <ref type="bibr" target="#b62">Xu et al. (2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATIONS C.1 DETAILED ARCHITECTURE</head><p>For image input X, we first process the images as a sequence of equal-sized patches x i ? X using a 1-layer convolutional neural network with layer normalization. Then those patches are added with positional embeddings, and the patch embeddings (tokens) are ready to be processed by later L blocks.</p><p>To make a fair comparison with other reported methods on DomainBed, we choose the ViT-S/16 which has similar parameters and run-time memory cost with ResNet-50 as the basic architecture for our main model GMoE-S/16. The chosen ViT-S/16 model has an input patch size of 16 ? 16, 6 heads in multi-head attention layers, and 12 transformer blocks.</p><p>We consider two-layer configurations, every-two and last-two, that are widely adopted in Sparse MoE design <ref type="bibr" target="#b43">(Riquelme et al., 2021)</ref>. The detailed architectures are illustrated in <ref type="figure" target="#fig_11">Figure 6</ref>. Besides, we also consider evaluating GMoE's generalization performance with larger scale models (e.g., ViT-Base).</p><p>V-MoE <ref type="bibr" target="#b43">(Riquelme et al., 2021</ref>) trains 32 experts model on ImageNet-21K dataset. However, considering the number of training data, we need to adopt a smaller number of experts. More experts require larger datasets to ensure that every expert is adequately trained since each expert only sees a small portion of the dataset. Domain generalization datasets are usually 1-2 orders of magnitude smaller than ImageNet-21K. Based on this fact, each GMoE block contains 6 experts. For each image patch, the cosine router selects the TOP-2 index out of 6 and routes the patch to the corresponding 2 experts.</p><p>We put the ablation study results on layer configuration and larger size backbone model on Sec. D.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 MODEL INITIALIZATION</head><p>To make a fair comparison with other algorithms from DomainBed (mostly with ResNet-50 pretrained on ImageNet-1K), we also utilize the pretrained ViT models on ImageNet-1K from DeiT to initialize our GMoE. Our practice avoids the generalization performance improvement of GMoE coming from a stronger pretrained model (e.g., directly from V-MoE pretrained on ImageNet-21K). Meanwhile, there is no released pretrained MoE model on ImageNet-1K. In <ref type="table" target="#tab_7">Table 4</ref>, we see ViT-S/16 and GMoE-S/16 achieve excellent generalization performance even with fewer parameters and smaller architecture.</p><p>In the GMoE block, we initialize the multiple experts with pretrained ViT block's FFN in the same position (e.g., block-8 and 10) and multiply it by E copies (E is the number of experts). For cosine routers, we randomly initialize them in each GMoE block, to make them evenly choose experts at first.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 PERTURBATION</head><p>Another specific design for the router is adding Gaussian noise to increase stability. Hard routing brings sparsity benefits while also causing discontinuities in routing. In a TOP-2 selection routing, a small perturbation would cause order changes if the largest output and second-largest output are close. In domain generalization, data come from diverse domains and we need to learn a model to capture the invariant semantic meaning of an object. Existing work <ref type="bibr">(Chen et al., 2022)</ref> theoretically claims that adding noise to the gating function provides a smooth transition of the gating function during training. Empirically, we find adding noise with a standard deviation of 1 N would empirically improve performance and stabilize training dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 AUXILIARY LOSS</head><p>It has been observed that the gating network tends to converge to a self-reinforcing imbalanced state, where a few experts are more frequently activated than others. Following <ref type="bibr" target="#b43">Riquelme et al. (2021)</ref>; , we define an importance loss L imp to encourage balanced usage of experts. The importance of expert e is defined as the normalization of the gating function's output in a batch of images:</p><formula xml:id="formula_24">imp e (X) = x?X g(x) e<label>(9)</label></formula><p>And the loss L imp is defined by the squared coefficient of variation of imp(X):</p><formula xml:id="formula_25">L imp = STD(imp(X)) MEAN(imp(X)) 2</formula><p>While importance loss seeks that all experts have on average similar output routing weights, it is still likely some experts would not be routed (see example in <ref type="table" target="#tab_8">Table 5</ref>). To address this issue, besides balancing across patches, we also need to encourage balanced assignment across experts. Ideally, we expect a balanced assignment via a load loss L load . However, the assignment is a discrete value; then we expect a proxy to make it differentiable in back-propagation. Following the idea in <ref type="bibr" target="#b43">Riquelme et al. (2021)</ref>; ; <ref type="bibr" target="#b33">Mustafa et al. (2022)</ref>, for each token x and each expert e i , we could compute the probability of e i once being selected in TOP k after SoftMax operation, and then kept if re-sampling again only the noise for expert e i . In other words, we are computing the probability that having expert e i still being among the TOP k while re-sampling again only the noise of the expert. This probability is formally defined as in Eq. 10, where ? k the K-th largest entry after SoftMax operation and ? is the cumulative distribution function of a Gaussian distribution.</p><formula xml:id="formula_26">p e (x) = 1 ? ? ? k ? (E T W x) e ?<label>(10)</label></formula><p>Then the expert load could be defined as load e (X) = x?X p e (x) and the whole experts load is defined as load(X) = {load e (X)} E e=1 .The load loss L load is defined by</p><formula xml:id="formula_27">L load (X) = STD(load(x)) MEAN(load(x)) 2</formula><p>With the above adaptations for DG tasks, GMoE still works with a simple overall loss:</p><formula xml:id="formula_28">L(X) = L classification (X) + 1 2 ?(L imp (X) + L load (X))</formula><p>with a hyperparameter ? &gt; 0 to balance the main task loss and the auxiliary loss, in which usually we view equal importance of the load and importance loss. Besides DomainBed, we also conduct experiments on the self-created CUB-DG dataset for model analysis. In the following, we will provide the details of different datasets.</p><p>Dataset Details In <ref type="table" target="#tab_10">Table 6</ref>, we provide statistics for the 8 datasets in DomainBed, as well as our self-created dataset CUB-DG (e.g., number of domains, categories and images).  In detail, the 9 multi-domain image classification datasets are comprised of: For train-validation selection, we split each training domain into training and validation subsets. Then, we pool the validation subsets of each training domain to create an overall validation set. Finally, we choose the model maximizing the accuracy on the overall validation set, and report the final accuracy on one leave-out test domain.</p><p>For leave-one-domain-out validation selection, we train the model on all training domains and one domain out as the validation set. We choose the model maximizing the accuracy on the leave-out validation domain, and report accuracy on another leave-out test domain. We should emphasize that the leave-one-domain-out validation setting means we choose two domains as leave-out domains. This term is used in conformity in literature <ref type="bibr">(Cha et al., 2022;</ref><ref type="bibr" target="#b7">2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard error bars</head><p>We train the model three times with random seeds on weight initializations. The mean and standard error of these repetitions are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 TRAINING DETAILS &amp; HYPERPARAMETERS</head><p>In this section, we report the training details of our experiments. We optimize models using Adam optimizer <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref> with slightly different parameters on different datasets (see <ref type="table" target="#tab_12">Table 8</ref>).</p><p>The training and inference batch size is set to 32 for each domain.</p><p>Since Gulrajani &amp; Lopez-Paz (2021) only evaluates baselines for 5K iterations on DomainNet, while 5K iterations are less than 2 epochs on DomainNet. This results in insufficient training on this dataset and it can not fully reveal models' performance with different algorithms. Subsequent state-of-the-art works <ref type="bibr" target="#b7">(Cha et al., 2021b</ref>) <ref type="bibr">(Cha et al., 2022)</ref> re-evaluated ERM and proposed their methods on 15,000 iterations of DomainNet.</p><p>To compare with those SOTA counterparts, we report the results of 15K iterations on DomainNet in <ref type="table" target="#tab_1">Table 1</ref>. However, we also train GMoE for 5K iterations to make a fair comparison with both types of algorithms (see <ref type="table" target="#tab_13">Table 9</ref>). For the rest of the datasets on DomainBed, we train GMoE for 5K iterations following the original setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 LEAVE-ONE-DOMAIN-OUT RESULTS</head><p>We have demonstrated the experimental results in <ref type="table" target="#tab_1">Table 1</ref> with train-validation selection. In this section, we will further provide the results of GMoE on leave-one-domain-out selection, which is a more challenging setting. In this setting, we should train GMoE on 5 datasets with in total of 61 individual experiments and summarize them as average performance. Due to the huge amount of experiments, recent methods rarely evaluate their methods on such setting, except for those provided    <ref type="table" target="#tab_1">Table 12</ref>, we see that the last two configuration largely exceeds every two configuration, thus verifying our idea that visual attributes exist in relatively high-level signals. This result also demonstrates that the cosine router and larger model lead to better generalization performance.    <ref type="table" target="#tab_1">Table 13</ref>.</p><p>Due to the space limit, we use abbreviations for some terms. In detail, they are:</p><p>? LR Opt. stands for learning rate optimization.</p><p>? TrivialAug. stands for trivial augmentation (M?ller &amp; Hutter, 2021).</p><p>? Ep. stands for pre-training epochs.</p><p>? Rand Er. stands for random erasing <ref type="bibr" target="#b72">(Zhong et al., 2017)</ref>.</p><p>? Label Sm. stands for label smoothing <ref type="bibr" target="#b48">(Szegedy et al., 2016)</ref>.</p><p>? FixRes Mt. stands for FixRes mitigations <ref type="bibr" target="#b50">(Touvron et al., 2019)</ref>.</p><p>? WDT stands for weight decay tuning.</p><p>? IRT stands for inference resize tuning <ref type="bibr" target="#b50">(Touvron et al., 2019)</ref>.</p><p>The results in <ref type="table" target="#tab_1">Table 14</ref> demonstrate that GMoE and ViT could still remain higher performance than the strong pre-trained model ResNet V2 due to the benefits of the backbone architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 COMPUTATIONAL COST COMPARISON</head><p>Having shown the effects of GMoE's performance on different domain generalization benchmarks, we now conduct efficiency analysis with respect to iteration time and run-time memory during training and inference. Since algorithms developed from DomainBed mainly adopt the same architectures (e.g., ResNet50, Linear Classifier), traditional model complexity measures such as flops and model parameters can not truly reflect the difference in efficiency. By evaluating the above two metrics, different algorithms with more complex loss design or gradient constraints will cause larger overheads with respect to iteration time or run-time memory. From the results in <ref type="table" target="#tab_1">Table 15</ref>, we observe that GMoE achieves relatively low run-time memory and training step time among other competitors.  <ref type="bibr">DANN (Ganin et al., 2016)</ref>, Fish <ref type="bibr">(Shi et al., 2021)</ref> and Fishr <ref type="bibr" target="#b40">(Rame et al., 2021)</ref>). Among them, DANN and Fish are widely discussed and cited papers, and Fishr is considered the best algorithm of its kind so far.</p><p>In this experiment, all three invariant learning methods, including ERM, adopt ResNet-50 as the backbone model. GMoE does not have any specific loss design and constraint. The only difference is the model architecture. In <ref type="table" target="#tab_1">Table 16</ref>, we see that GMoE, the new backbone model for DG, is significantly more effective than those methods specifically designed to learn invariance with certain losses and gradient constraints. Experts Selection Following the analysis in Sec. 5.4, we will provide more visualization results. In <ref type="figure" target="#fig_15">Figure 8</ref>-11, we provide the experts selection results for the same image on four domains (with four types of stylization). We see that, in most cases, the same visual attributes in different domains are handled by the same experts. For example, in <ref type="figure" target="#fig_15">Figure 8</ref>, right eye marked by green dot ( ) is processed by Expert 5 across four domains. The results reveal that experts are invariant in dealing with visual attributes across domains.</p><p>Multi-head Attention Visualization Ideally, the multi-head attention mechanism can be viewed as jointly attending to multiple places by ensembling multiple attention heads. Each attention head would focus on its specific attention relationship between all patches and inherently collaborate with each other.  However, recent studies <ref type="bibr">(Cordonnier et al., 2020)</ref> demonstrate that multiple-head attention (MHA) layers could learn redundant key/query projections, i.e. some heads might attend to similar features in input space. This issue demonstrates that two heads head i and head j are computing the same key/query representations up to a unitary matrix I ? R d?d such that W Qi = W Qj I and W Ki = W Kj I. In this case, even though the two heads are computing identical attention scores, i.e. W Qi II T W T Ki , the concatenation [W Qi , W Qj ] ? R d?2d can also be full rank, which indicates that some attention heads would focus on the same content but are agnostic to each other. We opine that the mixture-of-experts (MoE) layer could alleviate redundancy and increase disentanglement to some extent.</p><p>In <ref type="figure" target="#fig_12">Figure 7</ref>, presented in a grid fashion, the MHA layer focuses on different signals (features) in different heads while MoE layer disentangles the information by handling each patch to different experts. This mechanism extends the network sparsity and hence is able to improve the model generalizability. In sum, the MoE layer is designed to leverage different experts to capture distinct visual attributes, as well as their correlations (via attention mechanism) to each other.</p><p>To see more about the learned representation in multi-head attention layer, we provide the visualization of last layer's attention activations of ViT and GMoE on <ref type="figure" target="#fig_1">Figure 12</ref>              </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) Performance comparison of ViT-S/16 (w/21.8M trainable parameters) with ERM and the ResNet-50 (w/25.6M) backbone with SOTA DG algorithms. ERM(RN-50), Fishr(RN-50), Swad(RN-50/Ensemble) denotes the ResNet-50 trained with ERM, Fish (Shi et al., 2021) , Fishr (Rame et al., 2021), and Swad (Cha et al., 2021b), respectively. (b) Image examples from DomainNet. Each row shows one class and each column shows one domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 .</head><label>2</label><figDesc>An MoE module in equation 3 with N experts and k = 1 aligns with the conditional statements in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>expert 3 mainly focuses on the back and tail areas; (3) expert 4 mainly watches the breast and belly areas; (4) expert 5 mainly focuses on head and crown. More examples are given in Appendix E.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>CUB-DG image examples. (b) Visual attributes &amp; experts correlation. (c) Expert selection. (a) CUB-DG image examples. (b) Visual attributes. (c) Expert selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>(a) Examples of CUB-DG datasets from four domains. (b)The y-axis corresponds to 15 + 1 attributes (15 visual attributes + 1 background). The x-axis corresponds to the selected expert id. (c) Finetuned GMoE's router decision of block-10. The image is from CUB-DG's natural domain with color dots denoting visual attributes (e.g., beak and breast types).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 3 .</head><label>3</label><figDesc>(Alignment improves IID generalization;<ref type="bibr" target="#b61">(Xu et al., 2020a)</ref>) Fix and ?. Given a target function g and a neural network N , suppose {x i } M i=1 are i.i.d. samples drawn from some distribution D, and let y i = g(x i ). Assumptions:(a) Algorithm stability. Let A be a learning algorithm for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Performance of the MLP and FCN on the synthetic dataset. MLP aligns with the invariant correlations in test dataset 1 while FCN aligns with the invariant correlations in test dataset 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>p1 = 0.8, p2 = 0.9. p1 = 0.7, p2 = 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>MLP' s performance with different p 1 , p 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Diagrams of detailed GMoE architecture with two types of layer configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Diagram of a GMoE block, where different attention heads attend to different aspects of relations between patches. Different experts handle the learned attention in different patches. The router has been omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>13 14 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 :</head><label>8</label><figDesc>Expert selection visualization on block-10 of GMoE-S/16. Images are from different domains on CUB-DG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 :</head><label>9</label><figDesc>Expert selection visualization on block-10 of GMoE-S/16. Images are from different domains on CUB-DG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 10 :</head><label>10</label><figDesc>Expert selection visualization on block-10 of GMoE-S/16. Images are from different domains on CUB-DG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 11 :</head><label>11</label><figDesc>Expert selection visualization on block-10 of GMoE-S/16. Images are from different domains on CUB-DG. attention (from head 0-5). Right ones are original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 12 :</head><label>12</label><figDesc>Multi-head attention visualization on the last block of ViT-S/16 and GMoE-S/16. Images are from Real domain in DomainNet. attention (from head 0-5). Right ones are original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 13 :</head><label>13</label><figDesc>Multi-head attention visualization on the last block of ViT-S/16 and GMoE-S/16. Images are from Real domain in DomainNet. attention (from head 0-5). Right ones are original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 14 :</head><label>14</label><figDesc>Multi-head attention visualization on the last block of ViT-S/16 and GMoE-S/16. Images are from Real domain in DomainNet. attention (from head 0-5). Right ones are original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 15 :</head><label>15</label><figDesc>Multi-head attention visualization on the last block of ViT-S/16 and GMoE-S/16. Images are from Real domain in DomainNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overall out-of-domain accuracies with train-validation selection criterion. The best result is highlighted in bold. GMoE achieves the best performance on PACS, VLCS, OfficeHome and DomainNet while ranks in the third-best on TerraIncognita. ? 0.5 77.4 ? 0.3 67.5 ? 0.5 47.2 ? 0.4 41.2 ? 0.2 IRM [ArXiv 20]<ref type="bibr" target="#b1">(Arjovsky et al., 2019)</ref> 83.5 ? 0.8 78.5 ? 0.5 64.3 ? 2.2 47.6 ? 0.8 33.9 ? 2.8 DANN [JMLR 16] (Ganin et al., 2016) 84.6 ? 1.1 78.7 ? 0.3 68.6 ? 0.4 46.4 ?</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell>OfficeHome TerraInc</cell><cell>DomainNet</cell></row><row><cell>ERM (ResNet50) (Vapnik, 1991)</cell><cell cols="3">85.7 0.8</cell><cell>41.8 ? 0.2</cell></row><row><cell>CORAL [ECCV 16] (Sun &amp; Saenko, 2016)</cell><cell cols="3">86.0 ? 0.2 77.7 ? 0.5 68.6 ? 0.4 46.4 ? 0.8</cell><cell>41.8 ? 0.2</cell></row><row><cell>MMD [CVPR 18] (Li et al., 2018b)</cell><cell cols="3">85.0 ? 0.2 76.7 ? 0.9 67.7 ? 0.1 42.2 ? 1.4</cell><cell>39.4 ? 0.8</cell></row><row><cell>FISH [ICLR 22] (Shi et al., 2021)</cell><cell cols="3">85.5 ? 0.3 77.8 ? 0.3 68.6 ? 0.4 45.1 ? 1.3</cell><cell>42.7 ? 0.2</cell></row><row><cell>SWAD [NeurIPS 21] (Cha et al., 2021a)</cell><cell cols="3">88.1 ? 0.1 79.1 ? 0.1 70.6 ? 0.2 50.0 ? 0.3</cell><cell>46.5 ? 0.1</cell></row><row><cell>Fishr [ICML 22] (Rame et al., 2021)</cell><cell cols="3">85.5 ? 0.2 77.8 ? 0.2 68.6 ? 0.2 47.4 ? 1.6</cell><cell>41.7 ? 0.0</cell></row><row><cell>MIRO [ECCV 22] (Cha et al., 2022)</cell><cell cols="3">85.4 ? 0.4 79.0 ? 0.0 70.5 ? 0.4 50.4 ? 1.1</cell><cell>44.3 ? 0.2</cell></row><row><cell cols="4">ERM (ViT-S/16) [ICLR 21] (Dosovitskiy et al., 2021) 86.2 ? 0.1 79.7 ? 0.0 72.2 ? 0.4 42.0 ? 0.8</cell><cell>47.3 ? 0.2</cell></row><row><cell>GMoE-S/16 (Ours)</cell><cell cols="3">88.1 ? 0.1 80.2 ? 0.2 74.2 ? 0.4 48.5 ? 0.4</cell><cell>48.7 ? 0.2</cell></row><row><cell>Algorithms</cell><cell cols="2">SVIRO</cell><cell>Wilds-Camelyon</cell><cell>Wilds-FMOW</cell></row><row><cell>ERM (ResNet50) (Vapnik, 1991)</cell><cell cols="2">85.7 ? 0.1</cell><cell>93.1 ? 0.2</cell><cell>40.6 ? 0.4</cell></row><row><cell>ERM (ViT-S/16) [ICLR 21] (Dosovitskiy et al., 2021)</cell><cell cols="2">89.6 ? 0.0</cell><cell>91.1 ? 0.1</cell><cell>44.8 ? 0.2</cell></row><row><cell>GMoE-S/16 (Ours)</cell><cell cols="2">90.3 ? 0.1</cell><cell>93.7 ? 0.2</cell><cell>46.6 ? 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>GMoE trained with SOTA DG algorithms.</figDesc><table><row><cell>Algorithm</cell><cell>DomainNet</cell></row><row><cell>GMoE</cell><cell>48.7</cell></row><row><cell>Fish (Rame et al., 2021)</cell><cell>42.7</cell></row><row><cell>GMoE w/Fish</cell><cell>48.8</cell></row><row><cell>Swad (Cha et al., 2021a)</cell><cell>46.5</cell></row><row><cell>GMoE w/Swad</cell><cell>49.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Single-source DG accuracy (%). Models are trained on Paint domain and tested (1) on Paint validation set (2) the rest 5 domains' validation sets on DomainNet. The flops are reference values to compare the model's computational efficiency. IID Imp. denotes IID improvement on Paint's validation set comparing with ResNet50. OOD Imp. denotes average OOD improvement across 5 test domains.</figDesc><table><row><cell>Flops</cell><cell>Paint</cell><cell cols="2">Clipart Info</cell><cell cols="4">Paint Quick Real Sketch</cell><cell cols="2">IID Imp. OOD Imp.</cell></row><row><cell>4.1G</cell><cell>ResNet50</cell><cell>37.1</cell><cell>12.9</cell><cell>62.7</cell><cell>2.2</cell><cell>49.3</cell><cell>33.3</cell><cell>-</cell><cell>-</cell></row><row><cell>7.9G</cell><cell>ResNet101</cell><cell>40.5</cell><cell>13.1</cell><cell>63.4</cell><cell>3.1</cell><cell>51.2</cell><cell>35.4</cell><cell>1.1%</cell><cell>12.4%</cell></row><row><cell>4.6G</cell><cell>ViT-S/16</cell><cell>42.7</cell><cell>15.9</cell><cell>69.0</cell><cell>5.0</cell><cell>56.4</cell><cell>37.0</cell><cell>10.0%</cell><cell>38.6%</cell></row><row><cell>4.8G</cell><cell>GMoE-S/16</cell><cell>43.5</cell><cell>16.1</cell><cell>69.3</cell><cell>5.3</cell><cell>56.4</cell><cell>38.0</cell><cell>10.5%</cell><cell>42.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(a). We evaluate GMoE and other DG algorithms that address domain-invariant representation (e.g., DANN). The results are in Appendix E.1, which demonstrate superior performance of GMoE compared with other DG algorithms. CUB-DG datasets provides rich visual attributes (e.g. beak's shape, belly's color) for each image. Those additional information enable us to measure the correlation between visual attributes and the router's expert selection.Visual Attributes &amp; Experts CorrelationWe choose GMoE-S/16 with 6 experts in each MoE layer. After training the model on CUB-DG, we perform forward passing with training images and save the routers top-1 selection. Since the MoE model routes patches instead of images and CUB-DG</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Gordon A. Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world.In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,UT, USA, June 18-22, 2018, pp. 6172-6180. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00646. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Christie_ Functional_Map_of_CVPR_2018_paper.html. 27 Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. Multi-head attention: Collaborate instead of concatenate. CoRR, 2020. URL https://arxiv.org/abs/2006.16362. 34 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.2 Attribute Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.3 Algorithmic Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3 On the Importance of Neural Architecture for Domain Generalization 3 3.1 A Motivating Example: CNNs versus Vision Transformers . . . . . . . . . . . . . 3 3.2 Understanding from a Theoretical Perspective . . . . . . . . . . . . . . . . . . . . 4 Mixture-of-Experts Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 4.2 Visual Attributes, Conditional Statements, and Sparse MoEs . . . . . . . . . . . . 6 4.3 Adapting MoE to Domain Generalization . . . . . . . . . . . . . . . . . . . . . . 6 DomainBed Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 GMoE with DG Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.3 Single-Source Domain Generalization Results . . . . . . . . . . . . . . . . . . . . 8 5.4 Model Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Domain Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Vision Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.3 Sparse Mixture-of-Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.4 Out-of-Distribution Generalization Theory . . . . . . . . . . . . . . . . . . . . . . 19 A.5 Algorithmic Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Synthetic Experiments for Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2.1 Noiseless Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2.2 Noisy Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Published as a conference paper at ICLR 2023 B.4 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Detailed Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2 Model Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.3 Perturbation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.4 Auxiliary loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 DomainBed Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.2 Training Details &amp; Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.3 Leave-one-domain-out Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.4 Single-source Domain Generalization Results . . . . . . . . . . . . . . . . . . . . 29 D.5 Ablation Study on Model Design . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.6 Ablation Study on ResNet with Strong Data Augmentation . . . . . . . . . . . . . 31 D.7 Computational cost comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . 32</figDesc><table><row><cell>1 Introduction 1.1 2 Preliminaries 2.1 4 Generalizable Mixture-of-Experts for Domain Generalization 4.1 5 Experimental Results 5.1 6 Conclusions A Related Works A.1 B Theorems and Proofs C.1 D Experimental Results B.1 C Implementations D.1 E Model Analysis</cell><cell>1 2 5 7 10 18 20 24 27 33</cell></row></table><note>E.1 CUB-DG Results &amp; Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . 33 A RELATED WORKSA.1 DOMAIN GENERALIZATION Domain Generalization (DG) aims to maintain the good performance of machine learning models even in the domains that are different from the training (source) domain. The following are the categories of mainstream domain generalization research.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the different backbone models used in the experiment. ImageNet-1K* denotes GMoE are initialized with ViT pretrained on ImageNet-1K and copy weights for different experts.</figDesc><table><row><cell>Backbone</cell><cell>ResNet-50</cell><cell>ResNet-101</cell><cell>ViT-S/16</cell><cell>GMoE-S/16</cell></row><row><cell cols="5">Pretrained Dataset ImageNet-1K ImageNet-1K ImageNet-1K ImageNet-1K*</cell></row><row><cell>Parameters</cell><cell>25.6M</cell><cell>42.9M</cell><cell>21.7M</cell><cell>33.8M</cell></row><row><cell>Flops</cell><cell>4.1G</cell><cell>7.9G</cell><cell>4.6G</cell><cell>4.8G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Simple example on TOP 1 score (before SoftMax operation) where average weights across patches (x 1,2,3 ) are balanced. However, Expert 2 has always been ignored.</figDesc><table><row><cell>Patch</cell><cell>Expert 1</cell><cell>Expert 2</cell><cell>Expert 3</cell><cell>Expert 4</cell><cell>Selected Expert</cell></row><row><cell>x1</cell><cell>0.9</cell><cell>0.4</cell><cell>0.1</cell><cell>0.2</cell><cell>Expert 1</cell></row><row><cell>x2</cell><cell>0.2</cell><cell>0.4</cell><cell>0.9</cell><cell>0.1</cell><cell>Expert 3</cell></row><row><cell>x3</cell><cell>0.1</cell><cell>0.4</cell><cell>0.2</cell><cell>0.9</cell><cell>Expert 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Statistics of DomainBed datasets.</figDesc><table><row><cell>Dataset</cell><cell>PACS</cell><cell>VLCS</cell><cell>OfficeHome</cell><cell>TerraInc</cell><cell>DomainNet</cell></row><row><cell># Domains</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>6</cell></row><row><cell># Classes</cell><cell>7</cell><cell>5</cell><cell>65</cell><cell>10</cell><cell>345</cell></row><row><cell># Examples</cell><cell>9,991</cell><cell>10,729</cell><cell>15,588</cell><cell>24,788</cell><cell>586,575</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Statistics of Wilds and CUB-DG. TerraInc stands for TerraIncognita, Camelyon stands for Wilds-Camelyon and FMOW stands for Wilds-FMOW.</figDesc><table><row><cell>Dataset</cell><cell>SVIRO</cell><cell>W-Camelyon</cell><cell>W-FMOW</cell><cell>CUB-DG</cell></row><row><cell># Domains</cell><cell>10</cell><cell>5</cell><cell>6</cell><cell>4</cell></row><row><cell># Classes</cell><cell>7</cell><cell>2</cell><cell>62</cell><cell>200</cell></row><row><cell># Examples</cell><cell>56,000</cell><cell>455,954</cell><cell>523,846</cell><cell>47,152</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters to reproduce best performance of GMoE on each dataset.</figDesc><table><row><cell>Hyperparameters</cell><cell>PACS</cell><cell>VLCS</cell><cell>OfficeHome</cell><cell>TerraInc</cell><cell>DomainNet</cell></row><row><cell>Learning Rate</cell><cell>3 ? 10 ?5</cell><cell>3 ? 10 ?5</cell><cell>1 ? 10 ?5</cell><cell>5 ? 10 ?5</cell><cell>5 ? 10 ?5</cell></row><row><cell>Weight Decay</cell><cell>0</cell><cell>1 ? 10 ?6</cell><cell>1 ? 10 ?6</cell><cell>1 ? 10 ?4</cell><cell>0</cell></row><row><cell cols="6">9. CUB-DG dataset is built from the original Caltech-UCSD Birds (CUB) dataset (Wah et al.,</cell></row><row><cell cols="6">2011). We stylize the original images into three domains, Candy, Mosaic, and Undie, while</cell></row><row><cell cols="6">keeping the original images as the Natural domain. Overall, CUB-DG has 4 domains d ?</cell></row><row><cell cols="6">{Natural, Candy, Mosaic, Udnie}. It contains 47,152 bird examples and are categorized into</cell></row><row><cell cols="6">200 classes. Each image has detailed annotations: 1 category label, 15 positional attributes</cell></row><row><cell cols="6">(e.g., left wing, back), 312 binary attributes (e.g., has_bill_shape=dagger), and 1</cell></row><row><cell cols="6">bounding box information. We treat the 15 positional attributes as visual attributes of a</cell></row><row><cell cols="6">bird image, and thus we could evaluate the correlation between visual attributes and expert</cell></row><row><cell>selections.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Evaluation protocols We evaluate our proposed method with two protocols on DomainBed (Gul-</cell></row><row><cell cols="2">rajani &amp; Lopez-Paz, 2021) benchmark.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Train-validation selection performance comparison with 5K and 15K training iterations, where train-val stands for train-validation selection and lodo stands for leave-one-domain-out selection. The place marked -is not applicable as stated in Sec. D.3. In theTable 10, we can find that GMoE outperforms previous methods in all 5 datasets.</figDesc><table><row><cell>Iterations</cell><cell>Algorithms &amp; Models</cell><cell>DomainNet (train-val)</cell><cell>DomainNet (lodo)</cell></row><row><cell></cell><cell>ERM (Vapnik, 1991)</cell><cell>41.2 ? 0.2</cell><cell>40.6 ? 0.2</cell></row><row><cell></cell><cell>IRM (Arjovsky et al., 2019)</cell><cell>33.9 ? 2.8</cell><cell>33.5 ? 3.0</cell></row><row><cell></cell><cell>DANN (Ganin et al., 2016)</cell><cell>41.8 ? 0.2</cell><cell>38.2 ? 0.2</cell></row><row><cell>5K</cell><cell>CORAL (Sun &amp; Saenko, 2016) MMD (Li et al., 2018b)</cell><cell>41.8 ? 0.2 39.4 ? 0.8</cell><cell>41.1 ? 0.1 23.4 ? 9.4</cell></row><row><cell></cell><cell>MLDG (Li et al., 2018a)</cell><cell>41.2 ? 0.1</cell><cell>41.0 ? 0.2</cell></row><row><cell></cell><cell>Fish (Shi et al., 2021)</cell><cell>42.7 ? 0.2</cell><cell>-</cell></row><row><cell></cell><cell>Fishr (Rame et al., 2021)</cell><cell>41.7 ? 0.2</cell><cell>-</cell></row><row><cell></cell><cell>ViT-S/16 (Dosovitskiy et al., 2021)</cell><cell>42.4 ? 0.0</cell><cell>42.0 ? 0.2</cell></row><row><cell></cell><cell>GMoE-S/16</cell><cell>44.6 ? 0.3</cell><cell>44.7 ? 0.2</cell></row><row><cell></cell><cell>Swad (Cha et al., 2021a)</cell><cell>46.5 ? 0.1</cell><cell>-</cell></row><row><cell>15K</cell><cell>MIRO (Cha et al., 2022)</cell><cell>47.4 ? 0.0</cell><cell>-</cell></row><row><cell></cell><cell>ViT-S/16 (Dosovitskiy et al., 2021)</cell><cell>47.1 ? 0.3</cell><cell>46.1 ? 0.4</cell></row><row><cell></cell><cell>GMoE-S/16</cell><cell>48.7 ? 0.2</cell><cell>48.4 ? 0.3</cell></row><row><cell>by DomainBed.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Overall out-of-domain accuracies () with leave-one-domain-out selection criterion. SINGLE-SOURCE DOMAIN GENERALIZATION RESULTS In this section, we will demonstrate the single source domain generalization results on DomainNet's 6 domains. We select different training domains in turn and take the remaining 5 domains as test domains. InTable 11, we show the OOD results when models are trained on only one domain and test on 5 test domains, as well as the IID results on the training domain's validation set. We specify IID and OOD results in different colors.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell>OfficeHome</cell><cell>TerraInc</cell><cell>DomainNet</cell></row><row><cell>ERM (ResNet50)</cell><cell>83.0 ? 0.7</cell><cell>77.2 ? 0.4</cell><cell>65.7 ? 0.5</cell><cell>41.4 ? 1.4</cell><cell>40.6 ? 0.2</cell></row><row><cell>IRM</cell><cell>81.5 ? 0.8</cell><cell>76.3 ? 0.6</cell><cell>64.3 ? 1.5</cell><cell>41.2 ? 3.6</cell><cell>33.5 ? 3.0</cell></row><row><cell>DANN</cell><cell>81.0 ? 1.1</cell><cell>76.9 ? 0.4</cell><cell>64.9 ? 1.2</cell><cell>44.4 ? 1.1</cell><cell>38.2 ? 0.2</cell></row><row><cell>CORAL</cell><cell>82.6 ? 0.5</cell><cell>78.7 ? 0.4</cell><cell>68.5 ? 0.2</cell><cell>46.3 ? 1.7</cell><cell>41.1 ? 0.1</cell></row><row><cell>MMD</cell><cell>83.2 ? 0.2</cell><cell>77.3 ? 0.5</cell><cell>60.2 ? 5.2</cell><cell>46.5 ? 1.5</cell><cell>23.4 ? 9.5</cell></row><row><cell>MLDG</cell><cell>77.2 ? 0.9</cell><cell>82.9 ? 1.7</cell><cell>66.1 ? 0.5</cell><cell>46.2 ? 0.9</cell><cell>41.0 ? 0.2</cell></row><row><cell>ViT-S/16</cell><cell>86.6 ? 0.1</cell><cell>78.3 ? 0.2</cell><cell>71.9 ? 0.2</cell><cell>41.9 ? 0.3</cell><cell>46.1 ? 0.2</cell></row><row><cell>GMoE-S/16</cell><cell>87.1 ? 0.3</cell><cell>80.0 ? 0.1</cell><cell>73.9 ? 0.2</cell><cell>46.7 ? 0.3</cell><cell>48.4 ? 0.2</cell></row><row><cell>D.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Single-source DG results on DomainNet. We alternatively choose 1 training domain and the other 5 as test domains. We report IID generalization results on training domain's validation set with grey color, and OOD generalization results on test domains' validation set with light cyan color. ABLATION STUDY ON MODEL DESIGN In order to validate discussions in Sec. 4.3 and Sec C.1, we conduct experiments with different layer configurations, as well as adopting larger backbone (e.g., ViT-Base) model. From the results in</figDesc><table><row><cell>Clipart</cell><cell>Clipart</cell><cell>Info</cell><cell>Paint</cell><cell>Quick</cell><cell>Real</cell><cell>Sketch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Train-validation selection performance comparison for different GMoE models. All GMoEs (S/16 and B/16) have E = 6 experts and L = 12 blocks. We specify the number of attention heads (H), the patch embedding size (D), layer configuration, and router's type in the table header.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Config. Router H</cell><cell>D</cell><cell>PACS</cell><cell>VLCS</cell><cell>OfficeHome TerraInc DomainNet</cell></row><row><cell cols="3">GMoE-S/16 Every 2 Linear</cell><cell cols="4">6 384 81.8 ? 0.2 75.0 ? 0.1</cell><cell>64.0 ? 0.4</cell><cell>32.5 ? 0.7</cell><cell>46.3 ? 0.3</cell></row><row><cell cols="3">GMoE-S/16 Every 2 Cosine</cell><cell cols="4">6 384 81.4 ? 0.1 74.8 ? 0.2</cell><cell>62.2 ? 0.4</cell><cell>40.9 ? 0.3</cell><cell>46.4 ? 0.2</cell></row><row><cell>GMoE-S/16</cell><cell>Last 2</cell><cell>Linear</cell><cell cols="4">6 384 87.8 ? 0.2 80.0 ? 0.0</cell><cell>72.7 ? 0.2</cell><cell>46.7 ? 0.2</cell><cell>48.3 ? 0.1</cell></row><row><cell>GMoE-S/16</cell><cell>Last 2</cell><cell>Cosine</cell><cell cols="4">6 384 88.1 ? 0.1 80.2 ? 0.2</cell><cell>74.2 ? 0.4</cell><cell>48.5 ? 0.4</cell><cell>48.7 ? 0.2</cell></row><row><cell>GMoE-B/16</cell><cell>Last 2</cell><cell cols="5">Cosine 12 768 89.4 ? 0.1 81.2 ? 0.1</cell><cell>77.2 ? 0.4</cell><cell>49.3 ? 0.3</cell><cell>51.3 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Comparison of different training recipes on ResNet-50 V2 and ViT-S/16.</figDesc><table><row><cell>ResNet-50 V2</cell><cell>600</cell></row></table><note>Recipe LR Opt. TrivialAug. Ep. Rand Er. Label Sm. FixRes Mt. WDT IRT IN1K</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Train-validation selection performance comparison for ViT, GMoE, and other DG algorithms with ResNet-50 V2 as backbone model. On DomainNet, results are reported with 15K iterations. ABLATION STUDY ON RESNET WITH STRONG DATA AUGMENTATIONIn this part, we will go into greater depth about the pre-trained model's effect on DG performance. As stated in Sec. C.2, GMoE is initialized from the same architecture ViT models, where pre-trained weights are from<ref type="bibr" target="#b51">Touvron et al. (2021)</ref>. In addition, the pre-training on ViT and ResNet models may adopt different training recipes. And it is generally believed that models with stronger data augmentation during pre-training could have better performance on downstream tasks.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell>OfficeHome</cell><cell>TerraInc</cell><cell>DomainNet</cell></row><row><cell>ERM (w/ ResNet-50 V2)</cell><cell>87.2</cell><cell>78.2</cell><cell>68.7</cell><cell>49.9</cell><cell>45.3</cell></row><row><cell>Fishr (w/ ResNet-50 V2)</cell><cell>87.5</cell><cell>77.9</cell><cell>70.4</cell><cell>51.7</cell><cell>47.0</cell></row><row><cell>ViT-S/16</cell><cell>86.2</cell><cell>79.7</cell><cell>72.2</cell><cell>42.0</cell><cell>47.1</cell></row><row><cell>GMoE-S/16</cell><cell>88.1</cell><cell>80.1</cell><cell>74.2</cell><cell>48.5</cell><cell>48.7</cell></row><row><cell cols="6">D.6 To verify this difference in detail, we consider comparing GMoE-S/16's pre-trained model, ViT-S/16,</cell></row><row><cell cols="6">with a stronger pre-trained ResNet-50 V2 model 1 with significant training tricks data augmentations.</cell></row><row><cell cols="4">The pre-training details on ViT-S/16 and ResNet V2 are listed in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 :</head><label>15</label><figDesc>Comparison of training/inference iteration time and run-time memory for a mini-batch. A mini-batch is formed with 160 images in 224 ? 224 resolutions from DomainBed. For both metrics, lower is better. .1 CUB-DG RESULTS &amp; VISUALIZATIONS Generalization across Image Stylization To evaluate the model performance in generalizing to image stylization, we conduct experiments following DomainBed setting on CUB-DG. We compare GMoE with other three invariant learning DG algorithms, (i.e.,</figDesc><table><row><cell>Training</cell><cell cols="7">ERM DANN IRM Fish Fishr SWAD VIT-S/16 GMoE-S/16</cell></row><row><cell>Step Time (s) ?</cell><cell>1.01</cell><cell>1.02</cell><cell>1.10 2.79</cell><cell>1.10</cell><cell>1.21</cell><cell>0.90</cell><cell>0.98</cell></row><row><cell cols="2">Run-time Memory (GB) ? 13.40</cell><cell>13.42</cell><cell cols="2">13.40 3.41 15.25</cell><cell>14.32</cell><cell>11.15</cell><cell>12.28</cell></row><row><cell>Inference</cell><cell cols="7">ERM DANN IRM Fish Fishr SWAD VIT-S/16 GMoE-S/16</cell></row><row><cell>Step Time (s) ?</cell><cell>0.32</cell><cell>0.33</cell><cell>0.32 0.33</cell><cell>0.34</cell><cell>0.33</cell><cell>0.28</cell><cell>0.30</cell></row><row><cell>Run-time Memory (GB) ?</cell><cell>1.82</cell><cell>1.83</cell><cell>1.82 1.82</cell><cell>1.83</cell><cell>1.84</cell><cell>0.76</cell><cell>1.05</cell></row></table><note>E</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 16 :</head><label>16</label><figDesc>Out-of-domain accuracy (%) in each domain on CUB-DG dataset. The best is in bold.</figDesc><table><row><cell>Algorithm</cell><cell>Candy</cell><cell>Mosaic</cell><cell>Natural</cell><cell>Udnie</cell></row><row><cell>ERM (ResNet50)</cell><cell>64.3</cell><cell>20.8</cell><cell>75.3</cell><cell>77.9</cell></row><row><cell>DANN</cell><cell>47.0</cell><cell>14.9</cell><cell>76.5</cell><cell>70.6</cell></row><row><cell>Fish</cell><cell>62.6</cell><cell>22.4</cell><cell>84.5</cell><cell>77.7</cell></row><row><cell>Fishr</cell><cell>62.4</cell><cell>24.3</cell><cell>78.8</cell><cell>73.8</cell></row><row><cell>GMoE-S/16</cell><cell>83.1</cell><cell>42.8</cell><cell>89.3</cell><cell>82.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">From TORCHVISION's Pretrained Models.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Invariance principle meets information bottleneck for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Gagnon-Audet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021</title>
		<editor>Marc&apos;Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021-12-06" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Irina Rish</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Invariant risk minimization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.02893.1,8" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ensemble of averages: Improving model selection and boosting performance in domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.10832.18" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nas-ood: Neural architecture search for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-H Gary</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01270-0_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01270-0_28.27" />
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploiting domain-specific features to enhance domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manh-Ha</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.09410.1" />
		<imprint>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SWAD: domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/bcb41ccdc4363c6848a1d760f26c28a0-Abstract.html.8" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Swad: Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Domain generalization by mutualinformation regularization with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.10789</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.10789.8" />
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards understanding mixture of experts in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.02813</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2208.02813.25" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the representation collapse of sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barun</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.09179</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.09179.7" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="https://proceedings.neurips.cc/paper/2018/hash/2e2079d63348233d91cad1fa9b1361e9-Abstract.html.1" />
		<title level="m">Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="8256" to="8266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A patient-adaptable ecg beat classifier using a mixture of experts approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu Hen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surekha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willis J</forename><surname>Palreddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tompkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1991.3.1.79</idno>
		<ptr target="https://doi.org/10.1162/neco.1991.3.1.79.19" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1994.6.2.181</idno>
		<ptr target="https://doi.org/10.1162/neco.1994.6.2.181.19" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980.28" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WILDS: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berton</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/koh21a.html.27" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/krueger21a.html.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qrwe7XHTmYb.7" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.591</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.591" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00566</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_Domain_Generalization_With_CVPR_2018_paper.html.8" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How does a neural network&apos;s architecture impact its robustness to noisy labels?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature-critic networks for heterogeneous domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Domain generalization using pretrained models without fine-tuning. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Kan Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.04600</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.04600.18" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified feature disentangler for multi-domain image translation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/84438b7aae55a0638073ef798e50b4ef-Abstract.html.19" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00986</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00986.1" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the principles of parsimony and self-consistency for the emergence of intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Best sources forward: Domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2018.8451318</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2018.8451318.1" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/,2017.3" />
		<title level="m">dsprites: Disentanglement testing sprites dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Trivialaugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multimodal contrastive learning with limoe: the language-image mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02770</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Understanding the failure modes of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Vaishnavh Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15775,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain generalization using shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honarvar</forename><surname>Narges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kovashka</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-66415-2_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-66415-2_45.18" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision -ECCV 2020 Workshops</title>
		<meeting>European Conference on Computer Vision -ECCV 2020 Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How do vision transformers work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songkuk</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06709</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00149</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00149" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to learn single domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengchun</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01257</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01257.19" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding and improving robustness of vision transformers through patch-based negative augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.07858.19" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fishr: Invariant gradient variances for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02934</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR, 2019. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to learn without forgetting by maximizing transfer and minimizing interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1gTShAct7.18" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 7th International Conference on Learning Representations</title>
		<meeting>The 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Andr? Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html.6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS 2021</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Batch normalization embeddings for deep domain generalization. CoRR, 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Seg?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.12672.18" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note>id=B1ckMDqlg. 2, 5, 6, 19</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gradient matching for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Seely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Synnaeve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.09937.2,4" />
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep CORAL: correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-49409-8_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-49409-8_35.8" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to perceive the world as articulated: an approach for hierarchical learning in sensory-motor systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Nolfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/touvron21a.html.31" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Principles of risk minimization for learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.19" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.572</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.572.27" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Image as a foreign language: Beit pretraining for all vision and vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Owais Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhojit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Som</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10442</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Variational disentanglement for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.05826.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A fine-grained analysis on distribution shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Sylvestre Alvise-Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylan</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cemgil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11328</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Batch value-function approximation with only realizability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<idno>PMLR, 2021. 24</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="11404" to="11413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">What can neural networks reason about?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representation</title>
		<meeting>International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2020-04-02" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11848</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards a theoretical framework of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanlong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">L</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00219</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00219.18" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deceptionnet: Network-driven domain randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00062</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00062.18" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Delving deep into the generalization of vision transformers under distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Can subnetwork structure be the key to out-of-distribution generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/zhang21a.html.19" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Towards principled disentanglement for domain generalization. CoRR, 2021b</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.13839.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adaptive risk minimization: Learning to adapt to domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/zhao19a.html.18" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/db9ad56c71619aeed9723314d1456037-Abstract.html.19" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arxiv. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Sparse invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural networks fail to learn periodic functions and how to fix it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Liu Ziyin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahito</forename><surname>Hartwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">2017) comprises four domains d ? {art, cartoons, photos, sketches}. This dataset contains 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pacs (li</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>991 examples of dimension (3, 224, 224) and 7 classes</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">2013) comprises photographic domains d ? {Caltech101, LabelMe, SUN09, VOC2007}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlcs (fang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>This dataset contains 10, 729 examples of dimension (3, 224, 224) and 5 classes</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">2017) includes domains d ? {art, clipart, product, real}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Office-Home</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkateswara</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>This dataset contains 15, 588 examples of dimension (3, 224, 224) and 65 classes</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">2018) contains photographs of wild animals taken by camera traps at locations d ? {L100, L38, L43, L46}. Our version of this dataset contains 24</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Terraincognita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beery</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>788 examples of dimension (3, 224, 224) and 10 classes</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">2019) has 6 domains d ? {clipart, infograph, painting, quickdraw, real, sketch}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Domainnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>This dataset contains 586, 575 examples of size (3, 224, 224) and 345 classes</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">2020) is a Synthetic dataset for Vehicle Interior Rear seat Occupancy across 10 different vehicles, the 10 domains. The domains are d ? {aclass, escape, hilux, i3, lexus, tesla, tiguan, tucson, x5, zoe}. This dataset for image classification contains 56</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sviro (cruz</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>000 image examples of size (3, 224, 224) and 7 classes</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">2021) dataset contains histopathological image slides collected and processed by different hospitals and curated by Wilds benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilds-Camelyon ; Koh</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>It contains 455,954 examples of dimension (3, 224, 224) and 2 classes from 5 hospitals</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">2018) and contains satellite images of 62 buildings or land classes across 6 regions. The image example is of size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Fmow (</forename><surname>Wilds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koh</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
	<note>2021) dataset is a variant of the functional map of the world dataset</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
