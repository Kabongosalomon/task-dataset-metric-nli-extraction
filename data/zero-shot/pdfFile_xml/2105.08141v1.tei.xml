<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
						</author>
						<title level="a" type="main">VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-trimmed videos</term>
					<term>pose</term>
					<term>activities of daily living</term>
					<term>embedding</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many attempts have been made towards combining RGB and 3D poses for the recognition of Activities of Daily Living (ADL). ADL may look very similar and often necessitate to model fine-grained details to distinguish them. Because the recent 3D ConvNets are too rigid to capture the subtle visual patterns across an action, this research direction is dominated by methods combining RGB and 3D Poses. But the cost of computing 3D poses from RGB stream is high in the absence of appropriate sensors. This limits the usage of aforementioned approaches in real-world applications requiring low latency. Then, how to best take advantage of 3D Poses for recognizing ADL? To this end, we propose an extension of a pose driven attention mechanism: Video-Pose Network (VPN), exploring two distinct directions. One is to transfer the Pose knowledge into RGB through a feature-level distillation and the other towards mimicking pose driven attention through an attention-level distillation. Finally, these two approaches are integrated into a single model, we call VPN++. We show that VPN++ is not only effective but also provides a high speed up and high resilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the representative baselines on 4 public datasets. Code is available at https://github.com/srijandas07/vpnplusplus. Index Terms-trimmed videos, pose, activities of daily living, embedding, attention. ! ? S. Das is with Stony Brook University,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L EARNING representations for human actions, taking into account only the RGB modality is not sufficient. As a consequence, a large corpus of research studies has been focusing on multi-modal action recognition. The most popular and effective method is the two-stream approach <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> where one stream models appearance by taking RGB frames and the other stream models short-term motion by taking optical flow frames. However, this method is effective on videos obtained from web <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> where the human actions have prominent motion patterns. But what about Activities of Daily Living (ADL) where actions have subtle motion and often pertain to have similar spatio-temporal patterns?</p><p>Activities of Daily Living (ADL) may look simple but their recognition is often more challenging than activities present in sport, movie or Youtube videos. ADL often have very low inter-class variance making the task of discriminating them from one another very challenging. The challenges characterizing ADL are illustrated in <ref type="figure">fig 1:</ref> (i) short and subtle actions like pouring water and pouring grain while making coffee ; (ii) actions exhibiting similar visual patterns while differing in motion patterns like rubbing hands and clapping; and finally, (iii) actions observed from different camera views. In the recent literature, the main focus is the recognition of actions from internet videos <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and very few studies have attempted to recognize ADL in indoor scenarios <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For instance, state-of-the- <ref type="figure">Fig. 1</ref>: Illustration of the challenges in Activities of Daily Living: fine-grained actions (top), actions with similar visual pattern (middle) and actions viewed from different cameras (below). art 3D convolutional networks like I3D <ref type="bibr" target="#b2">[3]</ref> pre-trained on huge video datasets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> have successfully boosted the recognition of actions from internet videos. But, these networks with similar spatio-temporal kernels applied across the whole space-time volume cannot address the complex challenges exhibited by ADL. Attention mechanisms have thus been proposed on top of these 3D convolutional networks to guide them along the regions of interest of the targeted actions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.  <ref type="bibr" target="#b6">[7]</ref> followed by Videopose3D <ref type="bibr" target="#b7">[8]</ref>. Early fusion indicates concatenation of features at the last layer before prediction whereas Late fusion indicates averaging the prediction from both modalities. Our proposed models (marked with bounding box): VPN-F, VPN-A and VPN++ mimicking Pose stream, outperforms all other RGB and Pose combining strategies, while being significantly faster. Late fusion of the distilled models with Pose stream further boosts the classification accuracy, but at the price of the model efficiency. Note that the model with input modalities denoted by RGB (+Poses) have been trained with RGB and Poses but do not require Poses at inference time.</p><p>Towards another approach, recent studies <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> have shown that human 3D poses provide a strong clue for understanding human-centric patterns in videos. Of course the use of 3D poses for human action analysis depends on (i) the availability of good quality 3D poses and (ii) architectures processing them. Thanks to algorithms like LCRNet++ <ref type="bibr" target="#b6">[7]</ref> and VideoPose3D <ref type="bibr" target="#b7">[8]</ref>, high quality 3D poses can be obtained from RGB without the requirement of depth sensors. Similarly, the advancement of Graph based CNN architectures <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> that take into account the human joint configurations have greatly impacted the skeleton based action recognition. Since skeleton based action recognition does not leverage the appearance information in videos, combining 3D poses and RGB is the need of the hour as studied in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>The most common strategy for combining RGB stream and 3D poses includes (i) feature or score level fusion <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>. As these modalities are heterogeneous, they must be processed by different kinds of network to show their effectiveness. This limits their performance in simple multi-modal fusion strategy <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[29]</ref>. Therefore, another approach adopted in recent days includes (ii) pose driven attention mechanisms <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>. However, these methods have improved the action recognition performance but they do not take into account the alignment of the RGB cues and the corresponding 3D poses. Therefore, we proposed a spatial embedding to project the visual features and the 3D poses in the same referential in <ref type="bibr" target="#b30">[30]</ref>.</p><p>Further, this embedding is accompanied by an attention network to recognize a large variety of human actions. Thus, VPN consists of a spatial embedding and an attention network. It exhibits the following properties through its modules: (i) a spatial embedding learns an accurate videopose embedding to enforce the relationships between the visual content and 3D poses, (ii) an attention network learns the attention weights with a tight spatio-temporal coupling for better modulating the RGB feature map, (iii) the attention network takes the spatial layout of the human body into account by processing the 3D poses through Graph Convolutional Networks (GCNs).</p><p>VPN to some extent overcomes the challenge of combining two modalities that are not only semantically different but also processed through heterogeneous networks. To go beyond these approaches, we study novel manners to combine efficiently RGB and 3D Poses. In particular, we aim at relaxing the need of high quality 3D poses, which are not always available. In <ref type="figure" target="#fig_0">Figure 2</ref>, we provide a plot of action classification accuracy vs average inference time on Toyota Smarthome <ref type="bibr" target="#b14">[15]</ref> dataset. From the plot, we observe that feature level fusion (Early Fusion) performs worse since such fusion mechanisms are often prone to over-fitting <ref type="bibr" target="#b31">[31]</ref> owing to an increase in the number of parameters of the network. Besides, Pose driven attention mechanism <ref type="bibr" target="#b30">[30]</ref> yields high classification accuracy compared to RGB <ref type="bibr" target="#b2">[3]</ref> and Poses <ref type="bibr" target="#b19">[20]</ref> individually or their score level fusion (Late Fusion). But these RGB+Poses based methods are significantly slower than the RGB ones.</p><p>To this end, we explore the concept of knowledge distillation to infuse pose stream into RGB stream. Towards this objective, we propose two levels of distillation -one taking an approach of feature level fusion and the other one benefiting from attention mechanism. First, we aim at transferring feature-level knowledge from Pose to RGB stream to learn discriminative representation for recognizing actions, we call this feature-level distillation model VPN-F. To learn VPN-F, we use contrastive learning for distilling the knowledge from Pose stream to RGB. Besides avoiding the computation of poses at inference time, VPN-F learns to maximize the salient information from both streams towards action recognition. Second, we mimic pose driven attention network as in VPN through RGB stream. This is performed by adding a self-attention block in the RGB stream that hallucinates attention weights learned through 3D poses for the task of action recognition. We call this attention-level distillation model VPN-A. As an end result, VPN-A learns to provide pose driven attention weights which not only improve the action classification accuracy but also eliminate the requirement of poses at inference time. Finally, we integrate both levels of distillation into a single model called VPN++. Our experiments confirm that VPN++ is 160 times faster than the state-of-the-art methods without compromising effectiveness in real-world scenarios as illustrated in <ref type="figure" target="#fig_0">fig. 2</ref>. We also show that VPN++ via distillation when combined with 3D Poses, if available, outperforms the state-of-the-art results on 4 public datasets. Thus, to sum up, by infusing Poses into RGB using distillation, we provide a choice of highly effective models to the community that can be leveraged based on their needs like low latency, low sensitivity towards noisy Poses, or none.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Significant improvement has been made in the action recognition domain after the advancement of 3D CNN <ref type="bibr" target="#b32">[32]</ref>. Carreira and Zisserman <ref type="bibr" target="#b2">[3]</ref> proposed a 3D CNN based fully convolutional network namely I3D which is pre-trained on huge datasets like Kinetics <ref type="bibr" target="#b4">[5]</ref> to capture discriminative spatio-temporal patterns within an action. With the success of I3D, holistic methods like Pseudo 3D CNN <ref type="bibr" target="#b33">[33]</ref>, Separable 3D CNN <ref type="bibr" target="#b34">[34]</ref>, slow-fast network <ref type="bibr" target="#b9">[10]</ref>, channelseparated CNN <ref type="bibr" target="#b35">[35]</ref>, and X3D <ref type="bibr" target="#b36">[36]</ref> have been fabricated for generic video datasets like Kinetics <ref type="bibr" target="#b4">[5]</ref>, UCF-101 <ref type="bibr" target="#b3">[4]</ref> and HMDB <ref type="bibr" target="#b5">[6]</ref>. But these networks with similar kernels applied across the whole space-time volume of a video, are too rigid to capture salient features for subtle patterns in ADL.</p><p>Recently several attention mechanisms have been proposed on top of the aforementioned 3D ConvNets to extract salient spatio-temporal patterns. For instance, Wang et al. <ref type="bibr" target="#b8">[9]</ref> have proposed a non-local module on top of I3D which computes the attention of each pixel as a weighted sum of the features of all pixels in the space-time volume. But this module relies too much on the appearance of the actions, i.e., pixel position within the space-time volume. As a consequence, this module though effective for the classification of actions in internet videos, fails to disambiguate ADL with similar motion and fails to address view-invariant challenges.</p><p>On the other hand, temporal evolution of 3D poses has been leveraged through sequential networks like LSTM and GRU for skeleton based action recognition <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>. Taking a step ahead, LSTMs have also been used for spatial and temporal attention mechanisms to focus on the salient human joints and key temporal frames <ref type="bibr" target="#b41">[40]</ref>. Another framework represents 3D poses as pseudo images to leverage the successful image classification CNNs for action classification <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b43">[42]</ref>. Moreover, skeleton based action recognition has made significant improvements with the advancement of Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The key idea is to feed a graph representation of a skeleton frame in these networks which are optimized for the task of action classification. These graph based methods make use of the spatial topology of the human body joints and thus are more effective than recurrent networks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b39">[38]</ref>. However, the skeleton based action recognition lacks in encoding the appearance information which is critical for ADL, such as in human-object interactions. Combining modalities: Combining the advantages of privileged modalities in order to make use of their complementary discriminative power has been exploited widely in action recognition domain. Two-stream architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> that learn separate features from optical flow and RGB modalities, outperform single modality approaches. Towards this direction, Ryoo et al. <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b45">[44]</ref> have proposed a Neural Search Architecture (NAS) to combine both RGB and Optical flow streams. In contrast to these methods, two complementary strategies are adopted to combine RGB and pose modalities. One is fusion of both modalities in feature space <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>. However, these modalities are heterogeneous and must be processed by different kinds of network to show their effectiveness. Combining these heterogeneous features from different modalities through feature/score fusion introduce noise resulting in a down-graded action recognition performance <ref type="bibr" target="#b46">[45]</ref>. The second is pose driven attention mechanisms to guide the RGB cues for action recognition as in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, the pose driven attention networks implemented through LSTMs, focus on the salient image features and the key frames. Then, with the success of 3D CNNs, 3D poses have been exploited to compute the attention weights of a spatiotemporal feature map. Das et al. <ref type="bibr" target="#b28">[28]</ref> have proposed a spatial attention mechanism on top of 3D ConvNets to weight the pertinent human body parts relevant for an action. Then, authors in <ref type="bibr" target="#b14">[15]</ref> have proposed a more general spatial and temporal attention mechanism in a dissociated manner. But these methods have the following drawbacks: (i) there is no accurate correspondence between the 3D poses and the RGB cues in the process of computing the attention weights <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>; (ii) the attention sub-networks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref> neglect the topology of the human body while computing the attention weights; (iii) the attention weights in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[28]</ref> provide identical spatial attention along the video. As a result, action pairs with similar appearance like jumping and hopping are mis-classified. Therefore in <ref type="bibr" target="#b30">[30]</ref>, we proposed a new spatial embedding to enforce the correspondences between RGB and 3D poses which has been missing in the state-of-the-art methods. The embedding is built upon an end-to-end learnable attention network. The attention network considers the human topology to better activate the relevant body joints for computing the attention weights. To the best of our knowledge, none of the previous action recognition methods have combined human topology with RGB cues. In addition, the proposed attention network couples the spatial and temporal attention weights in order to provide spatial attention weights varying along time.</p><p>However, all the above approaches including VPN rely on the availability of 3D Poses, which not only escalates the model inference time but also increases their sensitivity towards Pose quality. Therefore, we use the concept of distillation that not only learns discriminative video-pose representations for understanding actions but also relaxes the demand for Poses at inference time. Consequently, we adopt both strategies to enforce RGB stream to (i) mimic pose stream features, and (ii) emulate pose-driven attention mechanism. Distillation: Many approaches have exploited the concept of distillation for cross-modal knowledge transfer <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b51">[50]</ref>, <ref type="bibr" target="#b52">[51]</ref>. Towards action recognition, Garcia et al. <ref type="bibr" target="#b50">[49]</ref> proposed a distillation framework consisting of teacher-student networks that hallucinates depth features from RGB features. This distillation is performed via logits as well as by matching feature maps of RGB and depth networks. Similarly, distillation approaches dynamically leveraging complementary information across several modalities have been proposed in <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b52">[51]</ref>. Crasto et al. <ref type="bibr" target="#b51">[50]</ref> proposed MARS to train a RGB stream with standard cross-entropy loss along with mimicking the features learned by an optical flow stream. This mimicking is accomplished by a distillation loss that minimizes the euclidean distance between the learned features across both streams.</p><p>Thus, many distillation methods have been studied in the action recognition domain with OF and RGB, but not with RGB and Poses. Infusing Poses into RGB stream through distillation is not straightforward and includes two main challenges: (i) 2D RGB images with appearance information and 3D Poses with geometric details are fed to the teacher-student heterogeneous networks, limiting the knowledge transfer between them due to their asymmetric dimensionality; (ii) the teacher network, i.e. the Pose stream is not consistently effective on the entire data distribution. In fact, the Pose stream carries irrelevant features for actions that are discriminated using their appearance information. Therefore, we propose to minimize the distance between the features learned by RGB &amp; Poses while learning discriminative representation in the RGB feature space. Towards another approach with an effective teacher network, we perform online distillation (via collaborative learning) to transfer pose driven attention knowledge learned from VPN <ref type="bibr" target="#b30">[30]</ref> to RGB stream. Distillation methods like <ref type="bibr" target="#b53">[52]</ref>, <ref type="bibr" target="#b54">[53]</ref> are close to our approaches, however they are specific for image domain applications. In contrast, the extension of VPN: VPN++ is dedicated for combining cross-modal information pertaining to video domain applications. The feature-level and attention-level distillation mechanisms to infuse Poses into RGB stream through cross-modal knowledge distillation provide a practical model for combining RGB and 3D Poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO-POSE EMBEDDING MODELS</head><p>In this section, we first detail our previously proposed Video-Pose Network (VPN), followed by an elaborate description of the video-pose embedding models through distillation. We aim at building a video-pose network VPN++ which benefits from two levels of distillation -(i) featurelevel, and (ii) attention-level. At training time, the inputs to these models are RGB videos along with their corresponding 3D poses. These 3D poses could be obtained either from Kinect sensors using <ref type="bibr" target="#b55">[54]</ref> or from RGB images using pose estimation algorithms like LCRNet++ <ref type="bibr" target="#b6">[7]</ref> and Video-Pose3D <ref type="bibr" target="#b7">[8]</ref>. The RGB images and the 3D Poses are processed by a video backbone and a pose backbone respectively. In this work, the video backbones are usually 3D CNNs that take as input a stack of human cropped images from a video clip to compute the spatio-temporal representation of the clip. On the other hand, the Pose backbones are spatio-temporal Graph Convolutional Networks that take a stack of 3D Poses as a graphical input to model actions. At inference time, traditional VPN requires both RGB and 3D Poses to predict the actions. In contrast, VPN++ requires only the RGB videos at inference time to predict the action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: VPN</head><p>VPN can be thought as a layer which can be placed on top of any 3D convolutional backbone. VPN takes as input a 3D feature map (f ? R c?t?m?n ) and its corresponding 3D poses (P ) to perform two functionalities as shown in <ref type="figure" target="#fig_1">fig. 3</ref>. First, to provide an accurate alignment of the human joints with the feature map f . Second, to compute a modulated feature map (f ) which is further classified for action recognition. The modulated feature map (f ) is weighted along space and time as per its relevance. VPN exploits the highly informative 3D pose information to transform the visual feature map f and finally, compute the attention weights. This network has two major components: (I) an attention network and (II) a spatial embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Attention Network</head><p>The attention network consists of a Pose Backbone and a spatio-temporal Coupler (STC). The input poses along the video are processed in a Pose Backbone as shown in <ref type="figure" target="#fig_1">fig 3.</ref> The pose based inputs of VPN are the 3D human joint coordinates P ? R 3?J?tp stacked along t p temporal dimension, where J is the number of skeleton joints. The Pose Backbone processes these 3D poses to compute pose features h * which are used further in the attention network for computing the spatio-temporal attention weights. Next, the attention network in VPN learns the spatiotemporal attention weights A from the output of Pose Backbone in two steps as illustrated in <ref type="figure" target="#fig_2">fig. 4</ref>. In the first step, m ? n dimensional spatial and t dimensional temporal attention weights are classically trained as in <ref type="bibr" target="#b41">[40]</ref> to get the most important body parts and key frames for an action. This learning of spatial and temporal attention weights takes place in two streams (z 1 and z 2 ) consisting of dense layers each followed by relevant activations. In the second step, joint spatio-temporal attention weights are computed by performing a Hadamard product on the spatial and temporal attention weights. In order to perform this matrix multiplication, the spatial and temporal attention weights are inflated by duplicating the same attention weights in temporal and spatial dimension respectively. This two-step attention learning process enables the attention network to compute spatio-temporal attention weights in which the spatial saliency varies with time. The obtained attention weights are crucial to disambiguate actions with similar appearance as they may have dissimilar motion over time. Finally, the spatio-temporal attention weights A ? R t?m?n are linearly multiplied with the input video feature map f , followed by a residual connection with the original feature map f to output the modulated feature map f . The residual connection enables the network to retain the properties of the original visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Spatial Embedding of RGB and Pose</head><p>The objective of the embedding model is to provide tight correspondences between both pose and RGB modalities used in VPN. The state-of-the-art methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[28]</ref> attempt to provide the attention weights on the RGB feature map using 3D pose information without projecting them into the same 3D referential. The mapping with the pose is only done by cropping the person within the input RGB images. The spatial attention computed through the 3D joint coordinates does not correspond to the part of the image (no pixel to pixel correspondence), although it is crucial for recognizing fine-grained actions. To correlate both modalities, an embedding technique inspired from image captioning task <ref type="bibr" target="#b56">[55]</ref>, <ref type="bibr" target="#b57">[56]</ref> is used to build an accurate RGB-Pose embedding in order to enable the poses to represent the visual content of the actions. Thus, the embedding is performed by propagating a normalized euclidean loss between the visual features and a spatial attention vector (z 1 obtained from STC). Both the visual feature and spatial attention vectors are obtained by  Finally, VPN is plugged into the 3D ConvNet for an end-to-end training with a regularized loss L which is a convex combination of entropy loss, embedding loss and an attention regularization loss (refer to <ref type="bibr" target="#b30">[30]</ref> for details).</p><p>Aiming at learning similar video-pose embeddings by hallucinating discriminative pose-level features, we propose an extension of VPN, namely VPN++. VPN++ effectively makes use of the pose features at training time and eliminates its reliance over Poses at inference time. In <ref type="figure" target="#fig_3">fig. 5</ref>, we provide a schematic diagram of VPN and our proposed distillation models to illustrate the disparities among them. VPN++ with only feature-level distillation is denoted as VPN-F and VPN++ with only attention-level distillation is denoted as VPN-A. Below, we elaborate the two levels of distillation in VPN++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VPN-F (Feature-level distillation)</head><p>VPN++ involves knowledge distillation among modalities and thus, we have a teacher-student structure. This is referred to as the feature-level distillation in our model to infuse Pose stream into RGB stream. This is an attempt analogous to the spatial embedding in VPN. In order to perform this distillation, the Pose stream is considered as the Teacher Network T F , whereas the RGB stream as the Student Network S. But unlike previous teacher-student networks <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b50">[49]</ref>, here the teacher network occasionally provides irrelevant features, especially for actions where appearance information is important. For instance, using only Poses cannot discriminate actions like wearing a shoe or taking off a shoe but they can provide salient information about the localization of the action. For disambiguating these actions with similar appearance, we have to go beyond just mimicking the Pose stream to capture discriminative information. Consequently, we use the concept of contrastive learning to learn a representation for which the positive pairs are close to each other and negative pairs are pushed apart in some metric space. Most related to our work, Contrastive Representation Distillation <ref type="bibr" target="#b53">[52]</ref> (CRD) involves learning an unsupervised representation through knowledge distillation followed by a downstream training on the same set of training samples. In contrast, we focus specifically on video domain (with RGB and 3D poses) and formulate a supervised training strategy. This strategy includes jointly optimizing the student network with the class labels? in addition to distilling the knowledge from Pose stream to RGB. This enables the actions with similar appearance to move apart in the feature space due to their dissimilar distillation through pose embeddings. We call the model with only this feature-level distillation as VPN-F. At training time, we learn the VPN-F representation in two steps. Let V i be a video (stack of RGB frames) and P i be the   Here, the teacher VPN* is the video-pose network <ref type="bibr" target="#b30">[30]</ref> without the spatial embedding (SE). Also, A T and T A (V i , P i ) can be referred to as the attention weights (A) and modulated feature map (f ) of VPN (see <ref type="figure" target="#fig_1">fig. 3</ref>). Teacher network VPN* is trained collaboratively with the student RGB backbone.</p><p>corresponding 3D Poses for the i th sample in the training set. In the first step, the teacher network T is trained with the 3D poses for classifying V i into C action classes and its weights are then frozen.</p><p>In the second step, our goal is to learn a latent space where semantically related RGB frames and Poses are close to each other and far away otherwise. We achieve this by imposing a supervised contrastive distillation (SCD) loss between the teacher and the student at the feature level as illustrated in <ref type="figure" target="#fig_4">fig. 6</ref> (B). Inspired from audio-video <ref type="bibr" target="#b58">[57]</ref> and text-video analysis <ref type="bibr" target="#b59">[58]</ref>, we assign a set of candidate positive pairs (V i , P i ), thus the RGB frames and 3D Poses are extracted from the same video labeled action C k ? C. On the other hand, the negative pairs are some randomly associated data (V i , P j ) where P j is randomly chosen from the subset C \ C k as shown in <ref type="figure" target="#fig_4">fig. 6 (A)</ref>. For distillation, the SCD loss is imposed between the features at the output of the layer immediately before the final fully-connected layer of the teacher network and the features of the visual embedding obtained from the RGB student network. This visual embedding E F (V i ) is a linear projection of f S , where spatio-temporal feature map f S is computed by the RGB backbone S(V i ). We denote the fea-tures from the teacher network as T F (P j ). We maximize the mutual information between Pose teacher and RGB student representations by jointly optimizing the student network at the same time as we learn a video-pose embedding</p><formula xml:id="formula_0">[T F (P j ), E F (V i )].</formula><p>Thus, our distillation loss over a batch of data (B) is formulated as the log likelihood of the data under this model:</p><formula xml:id="formula_1">L SCD = 1 |B ? N | i log[T F (P i ), E F (V i )]+ j =i log(1 ? [T F (P j ), E F (V i )]) where [T F (P j ), E F (V i )] = e T F (Pj ) E F (Vi) e T F (Pj ) E F (Vi) + M<label>(1)</label></formula><p>Here, [T F (P j ), E F (V i )] ? (0, 1) corresponds to the videopose embedding and constant M is determined by the ratio of the number of negatives N to the cardinality of the dataset. Thus for the positive pairs, L SCD enforces the video student representation E F (V i ) to project along the Pose teacher representation T F (P i ). Conversely for the negative pairs, the student representation is projected perpendicular to the teacher representation in feature space. This feature modulation (at student network) due to the distillation loss is accompanied by cross-entropy loss L S C to optimize the RGB student network for predicting the action labels Y S . This joint optimization induces a selective infusion of the pose features in the RGB space with respect to the action class. Note that the student network is always fed with the ground-truth? corresponding to the RGB input. So, a video sample with corresponding ground-truth is repeated twice in a mini-batch while training VPN-F with SCD loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VPN-A (Attention-level distillation)</head><p>Next, we aim at learning RGB representation benefiting from attention mechanism. Attention mechanisms focusing on salient image region across time have become instrumental for discriminative visual representation. For RGB based ADL recognition, VPN <ref type="bibr" target="#b30">[30]</ref> has shown that pose driven attention mechanism is more accurate and effective compared to the ones using self-attention mechanisms through RGB itself. Therefore, we develop a second-level of distillation for transferring pose driven attention knowledge to RGB stream. For the sake of simplicity, we first explain the model with only attention-level distillation, dubbed as VPN-A. For this distillation, we chose our Video-Pose Network <ref type="bibr" target="#b30">[30]</ref> as a Teacher network T A . This Video-Pose Network (VPN*) is implemented following <ref type="bibr" target="#b30">[30]</ref> with no spatial embedding since we find that the feature-level distillation could hallucinate the features learned through spatial embedding in VPN. The student network S is a video backbone, similar to the one used in VPN-F. The challenge is to transfer the knowledge of attention weights learned by the teacher to the RGB student network. Therefore, a self-attention block similar to <ref type="bibr" target="#b8">[9]</ref> is invoked in the RGB based network which could learn the attention weights from the teacher. However, a feature-level distillation in this case does not activate the relevant neurons at the student network. We empirically support this claim in the experimental analysis. Moreover, learning attention weights is an evolutionary mechanism where a model learns the salient regions in the spatio-temporal space with every batch of iteration over the training data. So, for this level of distillation, we opt for online distillation, where the teacher VPN* and the student RGB backbone along with the self-attention block collaboratively optimize their respective entropy loss as illustrated in <ref type="figure" target="#fig_4">fig. 6 (C)</ref>. Such a distillation encourages the RGB student to produce similar attention weights as the VPN* teacher, intuitively paying attention to similar parts of the video as the teacher. VPN-A is trained in a single step. On one hand, VPN* teacher network is trained with action labels to learn pose driven attention weights A T to modulate its RGB feature map. Note that these attention weights corresponds to A from STC of VPN discussed in section 3.1. On the other hand, the student network intakes only the RGB frames. The self-attention block projects the RGB feature f S to a query (Q) and memory (key and value, K &amp; V ) embedding using linear projections (1 ? 1 ? 1 Conv), where typically the query and keys are of lower dimension (see <ref type="figure" target="#fig_5">fig. 7</ref> for a zoom into the self-attention block). The output for the query, i.e. the modulated feature map f S , is computed as an attention weighted sum of values V , with the attention weights A S obtained from the product of the query Q with keys K. The </p><formula xml:id="formula_2">L D = ||A + T ? E A (A S )|| 2<label>(2)</label></formula><p>The VPN* backbone, i.e. T A (V i , P i ) classifies its modulated feature map using the entropy loss L T C between the true class labels? and the predicted class labels Y T . Besides, the modulated feature map f S at the student network is classified simultaneously using the entropy loss L S C between the same true class labels and the predicted class labels Y S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">VPN++: Integrating VPN-F &amp; VPN-A</head><p>Finally, we aim at learning a unified RGB representation that can emulate both Pose based features and pose driven attention weights. This objective also encourages the model to jointly optimize the two levels of distillation loss along with the cross entropy loss to learn the class labels. Thus, we integrate the two levels of distillation into a single model -we call VPN++. The training methodology of VPN++ involves contrastive learning for the feature-level distillation and collaborative learning for the attention-level distillation. In <ref type="figure" target="#fig_5">fig. 7</ref>, we show the VPN++ model with two levels of distillation. Here the RGB student network includes the RGB backbone and the self-attention block whereas there are two teacher networks -a pre-trained Pose backbone for infusing the pose features to the RGB stream, and VPN* for transferring the pose driven attention knowledge to the self-attention block of the student network. In order to incorporate the contrastive and collaborative learning strategies both in the same model, a batch of samples with (positive, negative) pairs for the feature-level distillation and (positive, positive) pairs for the attention-level distillation is fed to the model. Note that the Pose teacher network for feature-level distillation is frozen. Thus, the RGB student network is jointly optimized with the following linear combination of the distillation losses and the entropy losses:</p><formula xml:id="formula_3">L = L S C (Y S ,? ) + L T C (Y T ,? ) ? ?L SCD + ?L D<label>(3)</label></formula><p>where ? and ? are the weighting factors of the distillation losses. Thus, VPN++ not only learns to distill the pose knowledge into RGB but also learn discriminative representation through pose driven attention distillation. While testing VPN++ (the RGB student network), we only use RGB frames as input to compute the action class scores, avoiding the requirement of 3D Poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the effectiveness of VPN++ and its corresponding components for action classification on four datasets popular for ADL: a real-world dataset -Toyota-Smarthome <ref type="bibr" target="#b14">[15]</ref>, a large scale human activity dataset -NTU RGB+D-60 <ref type="bibr" target="#b15">[16]</ref>, the super-set of NTU-60 dataset -NTU RGB+D-120 <ref type="bibr" target="#b16">[17]</ref>, and a relatively small scale human-object interaction dataset -Northwestern-UCLA <ref type="bibr" target="#b60">[59]</ref>. Toyota-Smarthome (Smarthome or SH) is a recent ADL dataset recorded in an apartment where 18 older subjects carry out tasks of daily living during a day. The dataset contains 16.1k video clips, 7 different camera views and 31 complex activities performed in a natural way without strong prior instructions. This dataset provides RGB data and 3D skeletons which are extracted from LCRNet <ref type="bibr" target="#b6">[7]</ref>. For evaluation on this dataset, we follow cross-subject (CS) and cross-view (CV 2 ) protocols proposed in <ref type="bibr" target="#b14">[15]</ref>. We ignore protocol CV 1 due to limited training samples. NTU RGB+D (NTU-60 &amp; NTU-120): NTU-60 is acquired with a Kinect v2 camera and consists of 56880 video samples with 60 activity classes. The activities were performed by 40 subjects and recorded from 80 viewpoints. For each frame, the dataset provides RGB, depth and a 25-joint skeleton of each subject in the frame. For evaluation, we follow the two protocols proposed in <ref type="bibr" target="#b15">[16]</ref>: cross-subject (CS) and cross-view (CV). NTU-120 is a super-set of NTU-60 adding a lot of new similar actions. NTU-120 dataset contains 114k video clips of 106 distinct subjects performing 120 actions in a laboratory environment with 155 camera views. For evaluation, we follow a cross-subject (CS 1 ) protocol and a cross-setting (CS 2 ) protocol proposed in <ref type="bibr" target="#b16">[17]</ref>. Northwestern-UCLA Multiview activity 3D Dataset (N-UCLA) is acquired simultaneously by three Kinect v1 cameras. The dataset consists of 1194 video samples with 10 activity classes. The activities were performed by 10 subjects, and recorded from three viewpoints. We performed experiments on N-UCLA using the cross-view (CV) protocol proposed in <ref type="bibr" target="#b60">[59]</ref>: we trained our model on samples from two camera views and tested on the samples from the remaining view. For instance, the notation V 3 1,2 indicates that we trained on samples from view 1 and 2, and tested on samples from view 3.   <ref type="bibr" target="#b19">[20]</ref>. For attention-level distillation, the Teacher Network (VPN*) is adapted with a 2 layer AGCN as Pose backbone and no spatial embedding. The Student network is I3D <ref type="bibr" target="#b2">[3]</ref> RGB backbone pre-trained on ImageNet <ref type="bibr" target="#b61">[60]</ref> and Kinetics-400 <ref type="bibr" target="#b4">[5]</ref>. It takes 64 RGB frames as input. The self-attention block is implemented with an additional Non-Local block <ref type="bibr" target="#b8">[9]</ref> placed on top of the I3D (Mixed_5c layer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Training. For training the teacher networks of VPN++ with categorical cross entropy loss, we follow the steps as in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b30">[30]</ref>. For training the student network, a dropout <ref type="bibr" target="#b62">[61]</ref> of 0.3 and a softmax layer are added at the end of the selfattention block for class prediction. VPN++ is trained with a 4-GPU machine where each GPU has 4 video clips in a mini-batch. It is trained with SGD optimizer having initial learning rate of 0.01, momentum of 0.9, and a weight decay rate of 0.1 after every 10 epochs. While training VPN++, we chose ? = ? = 50. For feature-level distillation, each batch consists of 8 positives and 8 negatives.</p><p>Inference. At test time, we perform fully convolutional inference in space as in <ref type="bibr" target="#b8">[9]</ref>. The final classification is obtained by max-pooling the softmax scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyper-parameter sensitivity</head><p>VPN-F and VPN-A distillation models are trained by a linear combination of two losses: cross-entropy loss between the logits and the ground-truth targets, and the distillation loss between the video-pose features. In <ref type="figure" target="#fig_6">fig. 8</ref>, we report the accuracy of VPN-F and VPN-A on Smarthome and NTU-60 datasets using different values of ? and ? respectively. We observe that a non-zero value of ? or ? increases the action classification accuracy compared to the baseline RGB stream. This shows the importance of the distillation from Pose stream to RGB in both models. We also observe that by increasing the weighting factor of the distillation loss, we reach a peak accuracy for ? = ? = 50 as shown in <ref type="figure" target="#fig_6">fig. 8</ref>. This   shows that our distillation models effectively leverage both RGB and Pose streams to classify the action when combined in a strategic manner. Further increase in the values of ? or ? influences the distillation loss to dominate the student RGB network while training. This causes the resultant student network to mimic the Pose stream rather than exploiting both streams. For SCD loss, the choice of number of negatives for each positive input (video-pose pair) is flexible. Conventionally, more negatives for each Positive in contrastive learning yields higher accuracy. This observation is not noted in our case due to a supervised strategy of using the contrastive loss. Thus, we take one negative for each positive to train VPN-F. We utilize the above observation for hyperparameters while training VPN++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>In this section, we analyze the impact of proposed distillation methods w.r.t. previous methods. We also quantify the robustness of VPN-F and VPN-A. Which loss is better for feature-level distillation? In this ablation study <ref type="table" target="#tab_3">(Table 1)</ref>, we compare different distillation losses for transferring knowledge from pose features to RGB features. The training strategy for all these losses are different but are applied between the video-pose features T F (P j ) and E F (V i ). The mechanism of learning visual representation with the concept of contrastive learning between the positive and negative samples (CRD <ref type="bibr" target="#b53">[52]</ref> and SCD) outperform the classical way to distillate knowledge using MSE loss <ref type="bibr" target="#b51">[50]</ref>. We also note that our SCD outperforms CRD significantly on Smarthome, whereas the margin of improvement on NTU is comparatively low. This indicates that CRD is effective for scenarios where high quality Poses are available and SCD is consistently effective even for low quality Poses. Why do we need to emulate pose driven attention? We know that attention mechanisms are crucial for understanding ADL <ref type="bibr" target="#b14">[15]</ref>. But attention weights obtained using RGB based self-attention mechanism like Non-Local blocks <ref type="bibr" target="#b8">[9]</ref> rely too much on variation of intensities in spatio-temporal feature maps, hence lacks semantics. In contrast, 3D poses capture the semantics in the videos and significantly improve the action recognition performance as shown in <ref type="table">Ta</ref>   on Smarthome dataset. It is worth noting that the improvement is significant for Smarthome compared to NTU-60 as it contains many fine-grained actions with videos captured by fixed cameras in an unconstrained Field of View. Thus, enforcing the embedding loss enhances the spatial precision during inference. Therefore, we chose to mimic pose driven attention for a second-level of distillation in VPN++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which loss is better for attention-level distillation?</head><p>For VPN-A, we propose to distill knowledge at attention-level than at feature-level due to its more effectiveness as supported by the experiments in <ref type="table" target="#tab_6">Table 3</ref>. Note that the featurelevel distillation in the former experiment is performed between the output of the modulated feature map of VPN*, i.e. T A (V i , P i ) and the modulated feature map of the student network f S . We investigate the effectiveness of collaborative training the teacher-student network for transferring attention-level features. In these experiments in <ref type="table" target="#tab_6">Table 3</ref>, the VPN* teacher network is pre-trained and frozen when collaborative training is not performed. We also compare the performance of supervised contrastive distillation loss (most effective loss in <ref type="table" target="#tab_3">Table 1</ref>) with MSE loss at attention-level. However, MSE loss with collaborative training strategy to distill attention weights from VPN* Teacher network to RGB based Non-Local student network outperforms the baselines by up to 7.9% on Smarthome dataset. This shows that reducing MSE between the attention weights of video and pose embeddings is a better strategy to distill attention weights than contrastive learning. This is coherent with the fact that distillation of attention weights do not correspond to positives and negatives w.r.t. video samples whereas distillation at feature level represents entities that could have positives and negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to combine VPN-F &amp; VPN-A?</head><p>In <ref type="table" target="#tab_7">Table 4</ref>, we observe a performance drop when both the training strategies of VPN-F &amp; VPN-A are combined in a sequential manner, one after the other. The cause for this performance drop is the difficulty for the second distillation to significantly modify the RGB feature map (at student's network) once the first distillation has modified it. In contrast to these fusion strate-   gies, the score level fusion of both student networks significantly outperforms the above two end-to-end strategies. Similar improvement is noted for our multi-teacher network trained with contrastive and collaborative strategy. Due to the lower final model complexity and lower inference time, the multi-teacher network is superior than the one with late fusion. This performance improvement highlights the complementary optimizations which are well preserved while training jointly in a single model in VPN++. Finally, we also observe that spatial embedding in the teacher network of attention-level distillation do not contribute to the classification accuracy and hence can be ignored. This shows that the feature-level distillation could hallucinate the pose features performed by the spatial embedding in VPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of distilled models with RGB &amp; Pose streams.</head><p>In <ref type="table" target="#tab_9">Table 5</ref>, we compare our distillation models -VPN-F &amp; VPN-A with uni-modal models and their combinations. RGB and 3D Poses are modeled using I3D <ref type="bibr" target="#b2">[3]</ref> and AGCN-J <ref type="bibr" target="#b19">[20]</ref> networks. Following the state-of-the-art trends, RGB and Poses are combined using score level fusion (Late Fusion) and attention mechanism (VPN  <ref type="table" target="#tab_10">Table 6</ref>, we dig deeper into this problem by investigating the influence of Pose quality on the performance for different models. First, we describe the experimental setup to obtain the different levels of Pose quality. NTU-60 dataset was recorded in a laboratory, so we can have hight-quality 3D Poses captured by the Microsoft Kinect v2 sensor. For low-quality 3D Poses, we downscaled the original videos by reducing their resolution to 320 ? 180 and randomly invoke partial occlusions to fabricate the dataset similar to real-world settings. Then, we extract the 3D Poses using LCRNet++ <ref type="bibr" target="#b6">[7]</ref>. In Smarthome, Poses are obtained from RGB rather than using depthmap. For medium-quality 3D Poses, we apply Selective Spatio-Temporal Aggregation based Pose Refinement System (SSTA-PRS) <ref type="bibr" target="#b63">[62]</ref> which aims at improving the performance of pose estimation by integrating the advantages of several state-of-the-art pose estimation systems (eg. LCR-Net++ <ref type="bibr" target="#b6">[7]</ref>, OpenPose <ref type="bibr" target="#b64">[63]</ref> and AlphaPose <ref type="bibr" target="#b65">[64]</ref>) to extract 2D Poses. Then, we apply VideoPose3D <ref type="bibr" target="#b7">[8]</ref> to obtain 3D Poses over 2D Poses. For the low-quality 3D Poses, we only use LCRNet++ <ref type="bibr" target="#b6">[7]</ref>. The two baselines compared with VPN++ in <ref type="table" target="#tab_10">Table 6</ref> include skeleton based model: AGCN-J <ref type="bibr" target="#b19">[20]</ref> and RGB+Pose based attention model: VPN. We observe that VPN++ is less sensitive to the quality of Poses with a deterioration of classification accuracy by <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUALITATIVE VISUALIZATION</head><p>In <ref type="figure" target="#fig_7">fig. 9</ref>, we present a visualization of class activation maps of RGB, VPN-F, VPN-A, and VPN++ using Grad-CAM <ref type="bibr" target="#b66">[65]</ref>. These maps enable us to visualize discriminative regions specific to each action class. VPN-F, for actions like reach into pocket, VPN-A for actions like clean dishes, and both VPN-F &amp; VPN-A for actions with subtle motion like stirring, focus sharply around the hands grasping objects providing contextual information worth modeling the actions. The activation map of RGB stream either focuses only on irrelevant  motion patterns (see <ref type="figure" target="#fig_7">fig. 9</ref>). Moreover, the class activation maps obtained for VPN++ select the ones (VPN-F or VPN-A) that is effective for an action class. Thus, our combining strategy of the two levels of distillation in VPN++ can take advantage of the complementary features learned via both distillation mechanisms. This qualitative visualization shows that our distillation mechanisms learn discriminative representation that exploits the contextual information in the scene which is crucial for ADL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">QUANTITATIVE ANALYSIS</head><p>In this section, we present an analysis of Top-10 classwise performance of an RGB based approach, a Pose based approach, &amp; our distillation model VPN++ (see <ref type="figure" target="#fig_8">fig. 10</ref>). First, (a) we present the performance of <ref type="bibr">Top</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">COMPARISON TO THE THE STATE-OF-THE-ART</head><p>We compare VPN++ to the State-of-the-Art (SoA) on Smarthome, NTU-60, NTU-120, and N-UCLA in <ref type="table" target="#tab_14">Tables 7,  8</ref>, 9, and 10.</p><p>For smarthome dataset, we present the SoA categorized into RGB and RGB+Pose based methods in <ref type="table" target="#tab_14">Table 7</ref>. We provide the evaluation results on old Poses and new Poses (referred to as low &amp; medium levels of Pose quality in <ref type="table" target="#tab_10">Table 6</ref>). VPN++ outperforms all the SoA methods by up to <ref type="bibr" target="#b8">9</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>In this paper, we extend our previous framework Video-Pose network (VPN) to explore new mechanisms for combining video and Poses in order to classify action. Consequently, we have proposed two levels of distillation that can be adapted to different real-world application settings for recognizing actions. We summarize in table 11, the appropriate choice of distillation model or fusion mechanisms that could be exploited based on the requirements of a practitioner. Along with providing the appropriate choice of models, we also present the inference time, number of parameters of the resultant model, and action classification accuracy on relevant datasets. The choice of a model is based on factors (i.e. application requirements or network settings) concerning its applicability like (A) inference time, (B) quality of poses, (C) model size, and (D) amount of training data. From this experimental analysis, we conclude that our variants of distillation model (i.e., VPN++ and VPN-F) are useful when the end-user wants real-time predictions (e.g., low inference time), whereas the late fusion of VPN++ and Poses is preferred for offline action recognition. We notice that VPN-F is an effective model if further speed-up is required compared to VPN++ under the constraints of bad quality of Poses or less available training data. Interestingly, these lighter models are more accurate than models with similar training modalities <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b77">[76]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">SCOPE BEYOND RGB AND POSES</head><p>In this section, we go beyond utilizing video-pose embedding by combining video with other modalities like Optical Flow through distillation. To this end, we investigate the applicability of using the distillation mechanisms involved in VPN++ for combining RGB and Optical Flow (OF). In our experiments, following the attempts towards distillation at feature space level <ref type="bibr" target="#b51">[50]</ref>, we use supervised contrastive distillation loss (SCD) between the features of RGB and OF streams. We dub this new Flow Augmented RGB stream as VFN++. Note that the Flow backbone for this experiment is an I3D flow stream. Experimentally, we find that attention level distillation is not effective while using optical flow as a Teacher. This might be due to high dimensionality of the flow features that hampers the attention network   to learn relevant attention weights. Moreover, the flow features are not view-adaptive and do not consider the human anatomy while learning the attention weights. We present a comparative study with VFN++ in <ref type="table" target="#tab_3">Table 12</ref>. We observe that VFN++ outperforms MARS+RGB due to the supervised contrastive learning mechanism. On availability of OF at inference time, the performance shoots up significantly. However this accuracy is lower than the one obtained with 3D Poses, substantiating that 3D Poses are superior than OF for ADL with subtle actions. In <ref type="table" target="#tab_3">Table 13</ref>, we take a step forward towards combining VPN++ and VFN++. We call this resultant model Video-Pose-Flow Net-work++ (VPFN++). The combination is performed by the late fusion of VPN++ and VFN++ prediction scores. The minor performance improvement (+0.7% for Smarthome &amp; +0.4% for NTU-60) in VPFN++ compared to our distillation model (VPN++) is attributed to OF distillation. So, for ADL, OF does not contribute much when 3D Poses are already well infused in RGB. With availability of Poses and OF at test time, VPFN++ + 3D Poses + OF supersedes the SoA models. Thus, our proposed framework could be extended for combining privileged modalities which is a possible perspective of this work. However, it is to be introspected that the appropriate distillation mechanism may depend on the given modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>In this paper, we have extended our proposed videopose embedding for video understanding and presented a different perspective for combining RGB and 3D Poses through knowledge distillation. In an attempt to rethink combining RGB and Poses via feature fusion and attention mechanism, we propose two levels of distillation by infusing Poses at training time -feature-level and attentionlevel. Consequently, VPN++ does not rely anymore on the availability of 3D poses at inference time resulting in high speed up and high resiliency to noisy Poses. In addition to this, VPN++ learns a discriminative representation for classifying ADL. We show that VPN++ when combined with 3D Poses, if available, outperforms the state-of-theart methods on 4 ADL datasets. Then, we study different strategies of combining modalities for video understanding which could be exploited by the community based on their needs. Preliminary results show also that VPN++ can be extended to optical flow. Future work will explore towards an end-toend framework, infusing several modalities simultaneously into a RGB stream.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Accuracy vs Time plot on Toyota Smarthome dataset for RGB and Pose modalities. 3D Poses are estimated using LCRNet++</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>VPN takes as input RGB images with their corresponding 3D poses. The RGB images are processed by a visual backbone which generates a spatio-temporal feature map (f ). The proposed VPN takes as input the feature map (f ) and the 3D poses (P ). VPN consists of two components: an attention network and a spatial embedding (SE). The attention network further consists of a Pose Backbone and STC (spatio-temporal Coupler). VPN computes a modulated feature map f . This modulated feature map f is then used for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>STC: spatio-temporal Coupler to generate spatiotemporal attention weights A from the latent pose based feature h * . linear projection of the video content f and the 3D poses into a common dimensional embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>A schematic diagram of our models -VPN, VPN-F, VPN-A, and VPN++ reflecting their variation for providing video-pose embeddings. The inputs to each model at training are the RGB and Poses. STC indicates the spatio-temporal coupler learning the attention weights. SE indicates the spatial embedding module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Positive vs Negative samples (B) VPN-F (Feature-level distillation) (C) VPN-A (Attention-level distillation) (A) The positive &amp; negative video-pose pairs (at the left) are input to the teacher-student network. (B) VPN-F: VPN++ distillation model with only feature-level distillation. Here, the Pose Teacher network is pre-trained for action classification. Supervised Contrastive Distillation (SCD) is applied between the RGB and Pose features. (C) VPN-A: VPN++ distillation model with only attention-level distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>VPN++: The proposed distillation model when both VPN-F and VPN-A are integrated into a single model. The student network consists of a RGB backbone and a self-attention bock. At training, the model is trained in a contrastive manner for the feature-level distillation, and collaborative manner for the attention-level distillation. Note that Video-Pose attention model VPN* does not have the spatial embedding module. attention weights A S have to be learned from the evolution of 3D Poses. So, we invoke a distillation loss between the self-attention and VPN attention weights (A S &amp; A T ). The distillation loss is a classical Mean Squared Error (MSE) loss between the attention weight embeddings (E A (A S ) &amp; A + T ) from the teacher-student network. The projection of the attention weights (A S &amp; A T ) is necessary since they differ in terms of their dimensionality. This projection is performed by linearly transforming them into the same dimension and further normalizing them through L-2 norm. The distillation loss L D is formulated as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Accuracy of VPN-F (on left) and VPN-A (on right) for different values of ? &amp; ? respectively on Smarthome (CS) and NTU-60 (CS) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative visualization of class activation maps of RGB, VPN-F, VPN-A, and VPN++ using Grad-CAM [65]. The red bounding box refers to the precised Region of Interest relevant to classifying the action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>10 Actions misclassified by both RGB &amp; Poses Action classification accuracy (in %) of top-10 actions (a) for which Pose stream outperforms RGB stream, and (b) which are mis-classified by both RGB and Poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>RGB backbone Pose backbone Pose backbone Pose backbone RGB Pose RGB Pose Pose backbone Pose backbone RGB backbone RGB backbone RGB backbone</head><label></label><figDesc></figDesc><table><row><cell>Training</cell><cell>Training + Testing</cell><cell cols="2">Distillation</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pose</cell><cell></cell></row><row><cell></cell><cell>RGB</cell><cell>Self-</cell><cell>RGB</cell><cell>Self-</cell></row><row><cell></cell><cell></cell><cell>attention</cell><cell></cell><cell>attention</cell></row><row><cell>SE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STC</cell><cell>Pose</cell><cell>STC</cell><cell>Pose</cell><cell>STC</cell></row><row><cell>VPN</cell><cell>VPN-F</cell><cell>VPN-A</cell><cell>VPN++</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>For the input at training time, the 3D Poses are provided for NTU and N-UCLA dataset. For Smarthome dataset, two sets of 3D Poses, namely old and new Poses, are provided which are eventually extracted from RGB. Note that the new 3D Poses are of higher quality compared to the older ones.</figDesc><table /><note>For VPN++, the Teacher network for feature-level distilla- tion is AGCN-J [20] Pose backbone. Thus, we follow the pre- processing step on the 3D Poses as in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Ablation for choice of distillation loss in VPN-F.</figDesc><table><row><cell>Model</cell><cell>Poses</cell><cell>SH</cell><cell cols="2">NTU-60 NTU-60</cell></row><row><cell></cell><cell></cell><cell>(CS)</cell><cell>(CS)</cell><cell>(CV)</cell></row><row><cell>I3D w/o attention (backbone)</cell><cell>?</cell><cell>53.4</cell><cell>85.5</cell><cell>87.3</cell></row><row><cell>I3D w NL attention (self-attn)</cell><cell>?</cell><cell>53.6</cell><cell>88.4</cell><cell>87.1</cell></row><row><cell>I3D w pose attention (VPN)</cell><cell></cell><cell>65.2</cell><cell>93.5</cell><cell>96.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Impact of pose driven attention (VPN) compared to RGB based Non Local (NL) attention mechanism.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison of VPN-A with other strategies to distill pose driven attention.</figDesc><table><row><cell>Combining strategy</cell><cell>L e</cell><cell>Test</cell><cell cols="3">SH NTU-60 NTU-60</cell></row><row><cell></cell><cell></cell><cell>time (s)</cell><cell>CS</cell><cell>CS</cell><cell>CV</cell></row><row><cell>VPN-F (1 st ) + VPN-A (2 nd )</cell><cell>?</cell><cell>0.4</cell><cell>62.3</cell><cell>90.5</cell><cell>93.2</cell></row><row><cell>VPN-A (1 st ) + VPN-F (2 nd )</cell><cell>?</cell><cell>0.4</cell><cell>63.9</cell><cell>90.6</cell><cell>93.4</cell></row><row><cell cols="2">VPN-F + VPN-A (Late Fusion) ?</cell><cell>0.7</cell><cell>68.7</cell><cell>91.7</cell><cell>94.8</cell></row><row><cell>VPN++ (multi-teacher)</cell><cell></cell><cell>0.4</cell><cell>68.9</cell><cell>91.9</cell><cell>94.8</cell></row><row><cell>VPN++ (multi-teacher)</cell><cell>?</cell><cell>0.4</cell><cell>69.0</cell><cell>91.9</cell><cell>94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Comparison of different strategies to combine</cell></row><row><cell>VPN-F &amp; VPN-A. L e represents the spatial embedding in</cell></row><row><cell>VPN* (teacher of VPN-A).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">Top-1 accuracy of RGB, 3D Poses, VPN-F, VPN-A,</cell></row><row><cell cols="2">and VPN++ on 4 datasets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Pose</cell><cell cols="3">AGCN-J VPN VPN++</cell></row><row><cell></cell><cell>Quality</cell><cell>[20]</cell><cell>[30]</cell><cell></cell></row><row><cell>SH (CS)</cell><cell>Medium</cell><cell>54.0</cell><cell>65.2</cell><cell>69.0</cell></row><row><cell>SH (CS)</cell><cell>Low</cell><cell>49.1</cell><cell>62.1</cell><cell>66.8</cell></row><row><cell>NTU-60 (CS)</cell><cell>High</cell><cell>85.8</cell><cell>93.5</cell><cell>91.9</cell></row><row><cell>NTU-60 (CS)</cell><cell>Low</cell><cell>44.4</cell><cell>90.1</cell><cell>91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6 :</head><label>6</label><figDesc>Performance of several methods with different levels of pose quality.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>As shown in fig. 2, VPN++ does not require Poses at test time which substantially reduces the model inference time. Thus, the bad quality of Poses at inference time does not hamper the performance of these models. But what if the Poses are shoddy for the entire data distribution (even at training)? In</figDesc><table><row><cell>in terms of mimicking the Pose stream. However, their</cell></row><row><cell>superior performances compared to all prior techniques</cell></row><row><cell>of combining RGB and 3D Poses show the discriminative</cell></row><row><cell>representation learned by our VPN++.</cell></row><row><cell>What happens when the Pose quality degrades?</cell></row><row><cell>). Both VPN-F &amp; VPN-</cell></row><row><cell>A significantly outperform the individual modalities. VPN-</cell></row><row><cell>F with contrastive distillation outperforms the late fusion</cell></row><row><cell>strategy of combining RGB and Poses on all the datasets</cell></row><row><cell>except NTU-60 (CV protocol). This exception is coming</cell></row><row><cell>from the high action classification performance (93.8%) with</cell></row><row><cell>view-invariant 3D poses for cross-view protocol of NTU-60,</cell></row><row><cell>where high quality 3D Poses are available. On the other</cell></row><row><cell>hand, VPN-A is an attention based model and requires</cell></row><row><cell>subsequently large amount of data for learning salient atten-</cell></row><row><cell>tion weights. This is corroborated by its lower classification</cell></row><row><cell>accuracy for NTU-60 in contrast to NTU-120 (up to 0.4%</cell></row><row><cell>higher than even VPN-F) where the number of training</cell></row><row><cell>samples is two times that of NTU-60. The combination of</cell></row><row><cell>VPN-F &amp; VPN-A in VPN++ further boosts the classification</cell></row><row><cell>accuracy by up to 2.8% relatively on Smarthome. Further</cell></row><row><cell>improvement in action classification when combined with</cell></row><row><cell>3D Poses indicates that our distilled models still lacks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 7 :</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Results on Smarthome dataset with cross-subject</cell></row><row><cell cols="4">(CS) and cross-view (CV 2 ) settings (accuracies in %); Att</cell></row><row><cell cols="4">indicates attention mechanism, ? indicates that the modality</cell></row><row><cell cols="2">has been used only in training.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Methods</cell><cell cols="2">Pose RGB Att CS CV</cell></row><row><cell></cell><cell>2s-AGCN [20]</cell><cell></cell><cell>? ? 88.5 95.1</cell></row><row><cell></cell><cell>DGNN [19]</cell><cell></cell><cell>? ? 89.9 96.1</cell></row><row><cell></cell><cell>MS-G3D Net [68]</cell><cell></cell><cell>? ? 91.5 96.2</cell></row><row><cell></cell><cell>PEM [69]</cell><cell></cell><cell>? 91.7 95.2</cell></row><row><cell></cell><cell>Separable STA [15]</cell><cell></cell><cell>92.2 94.6</cell></row><row><cell>I3D</cell><cell>P-I3D [28] VPN [30]</cell><cell></cell><cell>93.0 95.4 93.5 96.2</cell></row><row><cell></cell><cell>VPN++</cell><cell>?</cell><cell>91.9 94.9</cell></row><row><cell></cell><cell>VPN++ + 3D Poses</cell><cell></cell><cell>94.9 98.1</cell></row><row><cell>RNX3D</cell><cell>VPN (RNX3D101) [30] RNX3D101+MS-AAGCN [21] VPN++ VPN++ + 3D Poses</cell><cell>?</cell><cell>95.5 98.0 ? 96.1 99.0 93.5 96.1 96.6 99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 8 :</head><label>8</label><figDesc>Results on NTU-60 dataset with cross-subject (CS) and cross-view (CV) settings (accuracies in %).10 actions mis-classified by both RGB and Pose streams. Interestingly, VPN++ improves the performance of RGB stream for actions which are mostly mis-classified owing to two challenges -(i) similarity in appearance like taking off a shoe (+5%) or wearing a shoe, clapping (+4%) or rubbing two hands, and (ii) subtle motion while performing the actions like reading (+13%), writing (+1%), and headache (+2%). Thus, VPN++ confirms empirically its potential to mitigate the drawbacks of SoA approaches by effectively providing an appropriate combination of the modalities (RGB and Poses) through distillation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>.8% &amp; 5.8% relatively with old and new Poses respectively. This significant improvement on this dataset can be</figDesc><table><row><cell>Methods</cell><cell cols="2">Pose RGB Att CS 1 CS 2</cell></row><row><cell>2s-Att LSTM [70]</cell><cell>?</cell><cell>61.2 63.3</cell></row><row><cell>Multi-Task CNN [71]</cell><cell cols="2">? ? 62.2 61.8</cell></row><row><cell>PEM [69]</cell><cell>?</cell><cell>64.6 66.9</cell></row><row><cell>2s-AGCN [20]</cell><cell>?</cell><cell>82.9 84.9</cell></row><row><cell>MS-G3D Net [68]</cell><cell cols="2">? ? 86.9 88.4</cell></row><row><cell>Two-streams [1]</cell><cell>?</cell><cell>? 58.5 54.8</cell></row><row><cell>I3D  *  [3]</cell><cell>?</cell><cell>? 77.0 80.1</cell></row><row><cell>Two-streams + ST-LSTM [17]</cell><cell></cell><cell>? 61.2 63.1</cell></row><row><cell>Separable STA  *  [15]</cell><cell></cell><cell>83.8 82.5</cell></row><row><cell>VPN [30]</cell><cell></cell><cell>86.3 87.8</cell></row><row><cell>VPN++</cell><cell>?</cell><cell>86.7 89.3</cell></row><row><cell>VPN++ + 3D Poses</cell><cell></cell><cell>90.7 92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">Results on NTU-120 dataset with cross-subject</cell></row><row><cell cols="3">(CS 1 ) and cross-setup (CS 2 ) settings (accuracies in %); Att</cell></row><row><cell cols="2">indicates attention mechanism.</cell><cell></cell></row><row><cell>Methods</cell><cell>Data</cell><cell>Att V 3 1,2</cell></row><row><cell>HPM+TM [72]</cell><cell>Depth</cell><cell>? 91.9</cell></row><row><cell>Ensemble TS-LSTM [73]</cell><cell>Pose</cell><cell>? 89.2</cell></row><row><cell>SGN [74]</cell><cell>Pose</cell><cell>? 92.5</cell></row><row><cell>NKTM [75]</cell><cell>RGB</cell><cell>? 85.6</cell></row><row><cell>I3D  *  [3]</cell><cell>RGB</cell><cell>? 86.0</cell></row><row><cell>Glimpse Cloud [14]</cell><cell>RGB+ P ose</cell><cell>90.1</cell></row><row><cell>Separable STA [15]</cell><cell>RGB+Pose</cell><cell>92.4</cell></row><row><cell>P-I3D [28]</cell><cell>RGB+Pose</cell><cell>93.1</cell></row><row><cell>Global Model [76]</cell><cell>RGB+Pose</cell><cell>93.5</cell></row><row><cell>VPN [30]</cell><cell>RGB+Pose</cell><cell>93.5</cell></row><row><cell>VPN++</cell><cell>RGB+ P ose</cell><cell>91.9</cell></row><row><cell>VPN++ + 3D Poses</cell><cell>RGB+Pose</cell><cell>93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 10 :</head><label>10</label><figDesc>Results on N-UCLA dataset with cross-view V 3 As discussed earlier, often low quality Poses are obtained in real-world scenarios with occlusions and low subject resolution. Thanks to the distillation mechanisms, it encourages the classification model to selectively infuse the relevant Pose information into the RGB stream. by providing a discriminative video-pose embedding. For NTU-60 dataset, VPN++ achieves accuracy close to the methods requiring Poses at test time whereas for NTU-120, VPN++ outperforms the later. We observe that the skeleton based action recognition methods perform better compared to the RGB based methods on NTU dataset. But this is due to the high quality of Poses (with no occlusion) which makes the dataset apt for Pose only methods. On the contrary, in real-world dataset like Smarthome (seeTable 7), the Pose only methods substantially under-perform compared to the RGB based methods. Another limitation of Pose only methods includes their lack of appearance encoding. However, VPN++ when combined with 3D Poses outperforms SoA on both NTU datasets. We confirm the robustness of VPN++ by evaluating it with 3D ResNext-101<ref type="bibr" target="#b34">[34]</ref> as a video backbone on NTU-60. Similar observations can also be done on N-UCLA dataset inTable 10hinting that VPN++ generalizes over small scale datasets too.</figDesc><table><row><cell>1,2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 11 :</head><label>11</label><figDesc>Choice of models to the practitioners. ? indicates High and ? indicates Low with (A) inference time, (B) quality of poses, (C) model size, and (D) amount of training data. Accuracy is provided for different Datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 12 :</head><label>12</label><figDesc>Effectiveness of Video-Flow Network++ (VFN++) representation using our SCD loss.</figDesc><table><row><cell>Fusion</cell><cell>SH</cell><cell cols="2">NTU-60 NTU-60</cell></row><row><cell></cell><cell>(CS)</cell><cell>(CS)</cell><cell>(CV)</cell></row><row><cell>RGB + OF + 3D Poses</cell><cell>64.4</cell><cell>90.2</cell><cell>95.9</cell></row><row><cell>VPN++</cell><cell>69.0</cell><cell>91.9</cell><cell>94.9</cell></row><row><cell>VPFN++</cell><cell>69.7</cell><cell>92.1</cell><cell>95.5</cell></row><row><cell>VPFN++ + 3D Poses</cell><cell>71.7</cell><cell>95.1</cell><cell>98.2</cell></row><row><cell cols="2">VPFN++ + 3D Poses + OF 72.9</cell><cell>96.7</cell><cell>99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 13 :</head><label>13</label><figDesc>Combination of RGB, 3D Poses and Flow modalities into a single model. Here VPFN++ is VPN++ + VFN++.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We are grateful to INRIA Sophia Antipolis -Mediterranean "NEF" computation cluster for providing resources and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<ptr target="http://arxiv.org/abs/1212.0402" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multiperson 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semisupervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1812.02707</idno>
		<ptr target="http://arxiv.org/abs/1812.02707" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno>abs/1812.01289</idno>
		<ptr target="http://arxiv.org/abs/1812.01289" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toyota smarthome: Real-world activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hanqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9532" to="9545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-modal feature fusion for action recognition in rgb-d sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 6th International Symposium on Communications, Control and Signal Processing</title>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Action recognition based on 3d skeleton and rgb frame fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning action recognition model from depth and skeleton videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5833" to="5842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mfas: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human activity recognition with pose-driven attention to rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition: Posebased attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision WorkshopS</title>
		<meeting>the IEEE International Conference on Computer Vision WorkshopS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Where to focus on for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph distillation for action detection with privileged modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Vpn: Learning video-pose embedding for activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What makes training multi-modal networks hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.12681" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.510</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2015.510" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ser. ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ser. ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On geometric features for skeletonbased action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015-11" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0031320317300936" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJgMK64Ywr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Assemblenet++: Assembling modality representations via attention connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kangaspunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A new hybrid architecture for human activity recognition from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakhalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Br?mond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francesca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling. MMM 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, ser. NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, ser. NIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="826" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-modal adaptation for rgb-d detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5032" to="5039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modality distillation with multiple stream networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="106" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MARS: Motion-Augmented RGB Stream for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Online knowledge distillation via collaborative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno>abs/1804.02516</idno>
		<ptr target="http://arxiv.org/abs/1804.02516" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Audio-visual scene analysis with selfsupervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Ima-geNet: A Large-Scale Hierarchical Image Database</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2670313" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Selective spatio-temporal aggregation based pose refinement system: Towards understanding human activities in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">RMPE: Regional multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/inria-00583818/en" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Regularizing long short term memory with 3d human-skeleton sequences for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3054" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3671" to="3680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Looking deeper into time for activities of daily living recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
