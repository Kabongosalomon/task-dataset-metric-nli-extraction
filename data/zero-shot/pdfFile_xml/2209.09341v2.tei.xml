<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgy</forename><surname>Ponimatkin</surname></persName>
							<email>georgy.ponimatkin@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Valeo.ai</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code and supplementary material: https://ponimatkin.github.io/ssl-vos</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple, yet powerful approach for unsupervised object segmentation in videos. We introduce an objective function whose minimum represents the mask of the main salient object over the input sequence. It only relies on independent image features and optical flows, which can be obtained using off-the-shelf self-supervised methods. It scales with the length of the sequence with no need for superpixels or sparsification, and it generalizes to different datasets without any specific training. This objective function can actually be derived from a form of spectral clustering applied to the entire video. Our method achieves on-par performance with the state of the art on standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually and practically much simpler.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While the two research communities working on unsupervised video object segmentation <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b48">49]</ref> and on object discovery <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b87">87,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b66">67]</ref> often remain separated, they share the goal of segmenting objects in visual data without depending on manual labels for these objects. This ability is essential to autonomous systems for evolving and interacting in open world. It is also a fundamental problem for Computer Vision as humans have the ability to learn about new objects without guidance.</p><p>Object appearance and motion are important cues to achieve this task. However, many challenges remain: Objects can share similar appearance with the background, their visual appearance may not be uniform, and different parts can move in different directions. As a result, many methods still rely on some sort of supervision, at least for learning to extract visual features.</p><p>We present here a simple approach to unsupervised video object segmentation. It leverages recent progress on unsupervised learning in a simple but powerful, novel optimization scheme over the objects' masks. We start from <ref type="bibr">(a)</ref> (b) (c) <ref type="figure">Figure 1</ref>: (a) Segmentation obtained by our spectral clustering formulation on the self-supervised image features from DINO <ref type="bibr" target="#b8">[9]</ref> in a single frame. (b) Segmentation obtained using the same self-supervised image features and optical flow from ARFlow <ref type="bibr" target="#b45">[46]</ref>, but still from a single frame. (c) Final segmentation obtained with our complete method, after optimization on the full frame sequence, using the same features and optical flow estimated by ARFlow.</p><p>self-supervised features such as DINO <ref type="bibr" target="#b8">[9]</ref>, MoCo-v3 <ref type="bibr" target="#b13">[14]</ref>, SWAV <ref type="bibr" target="#b7">[8]</ref> or Barlow Twins <ref type="bibr" target="#b94">[94]</ref> as the appearance cue, and the optical flow from methods such as RAFT <ref type="bibr" target="#b72">[73]</ref> or ARFlow <ref type="bibr" target="#b45">[46]</ref>. DINO, MoCo-v3, SWAV, Barlow Twins and ARFlow methods are "self-supervised" or "unsupervised", in the sense that they do not use any manual annotations, making our method also entirely unsupervised. <ref type="bibr" target="#b0">1</ref> Used alone, the DINO features can already provide a surprisingly good segmentation of the objects, but still below the state of the art, in particular for videos. Yet, with our optimization scheme, we can reach a performance that is on par or even better than much more sophisticated methods.</p><p>Our optimization starts from an initial estimate for the object's mask in each frame of the input sequence. It then optimizes the masks over the whole video sequence. We show that our optimization function can be derived from spectral clustering over the video sequence. Yet, spectral clustering is difficult to make tractable for long sequences as it requires computing one of the eigenvectors of a huge matrix <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b51">52]</ref>. To obtain a tractable problem, previous methods based on spectral clustering rely on superpixels <ref type="bibr" target="#b26">[27]</ref> or graph sparsification <ref type="bibr" target="#b38">[39]</ref>. However, superpixels may in- troduce artefacts at object boundaries, and both superpixels and sparsification require careful parameter tuning. Besides, even using fast methods to extract relevant eigenvectors, such as Power Iteration Clustering (PIC) <ref type="bibr" target="#b43">[44]</ref>, the complexity of these formulations is a priori quadratic in the length of the sequence (depending however on sparsity). In contrast, our method only needs eigenvectors computed for each frame independently, based on image features and optical flow for the frame, thus essentially scaling linearly with the number of frames. In short, our approach yields a tractable approximation of spectral clustering over the entire sequence. Concretely, on videos from the DAVIS2016 dataset <ref type="bibr" target="#b59">[60]</ref>, our method is on average ?170? faster than our full spectral clustering counterpart, i.e., TokenCut <ref type="bibr" target="#b82">[82]</ref>. Importantly, for practical applications, our method can be applied to different datasets without retraining. This is in contrast with learning-based segmentation approaches. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, they do not generalize well to new data.</p><p>To summarize, our contribution is the derivation of a novel objective function from spectral clustering, which results in a simple and efficient optimization method for object discovery and segmentation in videos. Moreover, our method is arguably the first that relies purely on selfsupervised features, without the need of any manual annotation. Still, it outperforms previous methods on several standard challenging datasets, including some methods that only rely on supervised training. Last, it can directly be applied to different datasets, without any training or tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We discuss here works on object discovery and unsupervised video object segmentation, as both are closely related to our work. We also discuss works on unsupervised feature learning, which we rely on for image feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Discovery</head><p>Object discovery aims to localize the objects in images or videos. Some works rely on a collection of images containing objects of the same class and localize these objects using clustering <ref type="bibr" target="#b23">[24]</ref>, image matching <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b15">16]</ref>, topic discovery <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b67">68]</ref> or optimization for selecting region proposals <ref type="bibr" target="#b75">[76]</ref>. One limitation of these works is that such a collection could be difficult to obtain, and its quality would heavily impact performance. Du et al. <ref type="bibr" target="#b21">[22]</ref> proposes to discover and segment objects from unseen classes based on instance masks predicted by models trained only on seen classes, while our method does not need any supervision.</p><p>Recently, some works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref> adopt a bottom-up approach and segment the object in the images by exploiting the similarity among pixel colors or patch features. These works have only demonstrated their performance on synthetic images and can be easily affected by image texture or colors. In contrast, our approach aims to discover objects using unlabeled in-the-wild videos without any supervision. <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b40">41]</ref> and <ref type="bibr" target="#b29">[30]</ref> can also be considered as object discovery methods, as their goal is to segment the primary objects given some video sequences. <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b40">41]</ref> propose to use an attention-based network to compute the trajectory embeddings from optical flow only. One major limitation of <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b40">41]</ref> is that optical flow is not sufficient to segment static objects. Besides, even though <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b40">41]</ref> do not require any labeled data, they still have to train their network with different strategies on different datasets, which makes it difficult to generalize, as show in <ref type="figure" target="#fig_0">Figure 2</ref>. As we will show in Section 4, our method does not need any training and can achieve good results on all the benchmarks. IKE <ref type="bibr" target="#b29">[30]</ref> proposes an iterative refinement approach: In the first stage, the videos are first fed into a graph module for mask initialization. Then, in the second stage, the initialized masks are used to train a segmentation network, whose predictions are used as initialization for the graph module in the first stage. In the first stage, optical flow is used to generate long-term space-time trajectories. Optical flow can be very noisy and thus it may be difficult to obtain long trajectories reliably in general. We will show in the Experiments section that our method can achieve better results compared to <ref type="bibr" target="#b29">[30]</ref> while at the same time being simple. Some methods <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b90">90]</ref> are also presented as self-supervised or unsupervised while they actually rely on supervised optical flow methods such as RAFT <ref type="bibr" target="#b72">[73]</ref>. In comparison, our method is able to achieve state-of-the-art performance while only using self-supervised optical flow learned from videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unsupervised Video Object Segmentation</head><p>Given a video sequence, Unsupervised Video Object Segmentation (UVOS) aims to consistently segment and track the most salient objects in the video without any human intervention. The approaches for UVOS can be divided into two categories depending on whether they need labeled data or not. Methods such as <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b69">70]</ref> use either labeled images or labeled videos to train their networks for video segmentation. In addition, some works <ref type="bibr" target="#b96">[96,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b89">89,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b95">95,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b74">75]</ref> also exploit low-level cues such as object boundaries to get better mask predictions. One main limitation of these methods is that they rely heavily on their large-scale wellannotated training data, which can be hard to get in practice. In contrast, our proposed method is entirely self-supervised and does not need any labeled data.</p><p>There are also several works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b87">87,</ref><ref type="bibr" target="#b92">92]</ref> that can segment the primary objects in the video without any labeled data. One major limitation of all these works is that they all consider that the pixels on the objects in the video share similar motion patterns, thus, their methods can fail when the objects are static or move with the same speed as the background, while our proposed method combines the appearance cues and the motion cues and can largely alleviate this issue.</p><p>Currently, DyStaB <ref type="bibr" target="#b89">[89]</ref> is the best performing unsupervised method for video object segmentation. The method consists of three parts: A static model and a dynamic model (both based on DeepLab backend <ref type="bibr" target="#b9">[10]</ref>), together with an inpainter network based on <ref type="bibr" target="#b90">[90]</ref>. The three networks are jointly trained via an adversarial loss in an iterative fashion to obtain final segmentation results and and significant post-processing via CRF is utilized. In comparison, our approach does not require training-besides for the image features and optical flow that can be obtained out-of-the-shelf-while an adversarial loss can be difficult to train. More importantly, our approach is much simpler, while achieving a similar performance to DyStaB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-Supervised Learning</head><p>Recent self-supervised approaches <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref> propose to train a feature extraction network using an instance classification pipeline, which treats each image as a single class and trains the network to distinguish images cropped from a large image collection without any manual annotation. In particular, DINO <ref type="bibr" target="#b8">[9]</ref> introduced an approach which makes a network trained in a self-supervised manner learn class-specific features.</p><p>Motivated by this ability, discovering objects in images using self-supervised features recently gained attention <ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b66">67]</ref>. LOST <ref type="bibr" target="#b66">[67]</ref> utilizes self-supervised features within a seed selection and expansion strategy and localizes the main object given an image. Closely related to LOST, TokenCut <ref type="bibr" target="#b82">[82]</ref> and Deep Spectral Methods (DSM) <ref type="bibr" target="#b52">[53]</ref> propose graph-based methods that use selfsupervised transformer features to discover and segment salient objects. TokenCut <ref type="bibr" target="#b82">[82]</ref> builds a graph where visual tokens are nodes and similarity scores between tokens are edges of a weighted graph. They formulate the segmentation problem as a normalized graph cut and solve it using spectral clustering with eigendecomposition. Similar to TokenCut <ref type="bibr" target="#b82">[82]</ref>, inspired from traditional spectral segmentation methods, DSM <ref type="bibr" target="#b52">[53]</ref> first constructs a Laplacian matrix which is a combination of color information and selfsupervised transformer features. Next, the image is decomposed using the eigenvectors of the Laplacian matrix.</p><p>Our method is similar to these methods as we also aim to detect and segment salient objects based on a graph formulation. There are two key differences: (i) We aim to discovery objects in videos rather than still frames and in order to ensure the temporal consistency, we extend the affinity matrix with optical flow to establish inter-frame connectivity; (ii) We introduce a novel method for optimizing on the entire video efficiently. Moreover, we show that using power iteration clustering <ref type="bibr" target="#b43">[44]</ref> instead of full eigenvector decomposition for spectral clustering as we do results in significantly faster run time (0.1s/frame vs. 17s/frame).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>We consider a sequence of N video frames of size H ? W , where H and W are the number of rows and columns of a frame. The segmentation of the main salient object, in each frame p, is modeled as a soft, vectorized image mask x p ? R HW + . The actual image mask at frame p is recovered by splitting the pixels into two clusters based on x p , separating the object of interest from the background.</p><p>We assume that we are given a rough mask estimate x p in each frame p, as well as functions w q p (x p ) warping a mask x p in frame p to frame q using the optical flow from p to q. Then, the objective function we minimize is:</p><formula xml:id="formula_0">L({x p } p ) = p ? CE( x p , x p ) + CE(x p+1 , w p+1 p (x p )) + CE(x p , w p p+1 (x p+1 )) ,<label>(1)</label></formula><p>where CE(?) denotes the cross-entropy. Intuitively, CE( x p , x p ) expresses the deviation between a mask x p and the initial mask estimate x p ; CE(x p+1 , w p+1 p (x p )) expresses the deviation between mask x p+1 and the warping of mask x p by flow w p+1 p , and reciprocally for CE(x p , w p p+1 (x p+1 )); finally, ? is a constant weight to balance the significance of both kind of deviations (difference from initial mask vs flow discrepancy).</p><p>As illustrated in <ref type="figure">Figure 3</ref>, our objective function can be interpreted easily: The optimization starts from a first estimate of the mask for each frame of the sequence; it encourages the mask in frame p to align with the mask in frame p + 1 after being warped by the optical flow from frame p to frame p + 1, and vice versa, while keeping the masks close to their initialization.</p><p>This approach can in fact be derived from a formulation of the problem in terms of spectral clustering, which also provides a way to compute initial estimates x p . We present this formulation and the derivation of our approach below.  <ref type="figure">Figure 3</ref>: Overview of our approach. Given a video sequence, starting from first estimates for the object masks obtained by spectral clustering on each image independently, we optimize the masks so that they remain close to the first estimates while being consistent with the optical flow. The objective function we optimize to retrieve the masks can be derived from spectral clustering applied to the video sequence. Our method can rely on self-supervised visual features only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vanilla Spectral Clustering for Segmentation</head><p>We first briefly present spectral clustering, in the context of image segmentation as done in <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b51">52]</ref>, for example. We then discuss how it can be extended to video segmentation. The reader interested in more details about spectral clustering can refer to the tutorial in <ref type="bibr" target="#b77">[78]</ref>.</p><p>For image segmentation with spectral clustering, one considers an affinity matrix we will denote A. Each row and each column of A corresponds to an image location. The coefficient A ij ? 0 on row i and column j should express how likely image location for row i and image location for column j belong to the same cluster. In the following, we will identify the row and column indices i and j with the corresponding image locations for simplicity.</p><p>According to spectral clustering theory, a good segmentation mask can be obtained from the second largest eigenvector X 2 of the normalized affinity matrix W defined as</p><formula xml:id="formula_1">W = D ?1 A ,<label>(2)</label></formula><formula xml:id="formula_2">where D = diag({ j A ij } i )</formula><p>is the degree matrix. The coefficients of X 2 correspond to image locations. By thresholding them, one obtains a binary mask for the segment.</p><p>To extend this approach to object segmentation in videos, we also start from an affinity matrix A. Each row of A, and each column, corresponds to an image location in a video frame. We will denote the coefficients of A with A (pi)(qj) : (pi) is a notation for the index of row corresponding to image location i in frame p; similarly (qj) is the index for column corresponding to image location j in frame q. Coefficients A (pi)(qj) should express how likely image location i in frame p and image location j in frame q are to correspond to the same object. For this, we will use the similarity between their local image features and optical flow.</p><p>However, this results in a very large A matrix, and thus also a very large W matrix. of size N HW ? N HW , which is in the order of 10 9 ? 10 9 for typical sequences in standard benchmarks. This is clearly too large in practice both for storage and for eigenvector computation. Some methods exploit superpixels <ref type="bibr" target="#b26">[27]</ref> or graph sparsification <ref type="bibr" target="#b38">[39]</ref> to decrease the computational complexity, but it complicates the approach; our method is more direct and more scalable. Another approach <ref type="bibr" target="#b51">[52]</ref> is to retrieve the second largest eigenvector X 2 with the classical problem:</p><formula xml:id="formula_3">X 2 = arg max X X WX such that X 2 = 1 , (3)</formula><p>which could be approximately but efficiently computed using Power Iteration Clustering (PIC) <ref type="bibr" target="#b43">[44]</ref>. However, it still cannot be scaled for common video lengths.</p><p>Nevertheless, as described below, we draw on this approach: Instead of building W explicitly and struggling to compute its second largest eigenvector X 2 , we compute eigenvectors x p by PIC for each frame p independently, for scalability. We use data that include inter-frame information to reinforce temporal consistency which leads to greater accuracy. This makes our initialization scheme similar to To-kenCut <ref type="bibr" target="#b82">[82]</ref>, except that we utilize approximate eigenvector extraction instead of full spectral decomposition as well as inclusion of optical flow connectivity between adjacent frames. In <ref type="table" target="#tab_1">Table 1</ref>(b), we show differences between using the baseline TokenCut method and our approximation without optical flow. These initial masks are refined using additional inter-frame consistency constraints, resulting in an optimization problem that is much more tractable than the originating spectral clustering problem.</p><p>In the following, we introduce a suitable affinity matrix A, that we use in two ways: to efficiently compute good initial estimates x p , and to derive the formulation in Eq. (1) as a simplification of term X WX we wish to maximize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Affinity Matrix for Video Object Segmentation</head><p>Our affinity matrix A relies both on object appearance features and on optical flow features: Appearance features. For these, we use an image feature extractor that generates an appearance feature vector ? i p of the object at location i of frame p. The idea is that the features at different locations of the same object should tend to look alike, while differing from the features located on other objects or on the background. Such appearance features can typically be learned using a dataset of annotated objects or, as in our case, using a self-supervised method such as DINO <ref type="bibr" target="#b8">[9]</ref> or MoCo <ref type="bibr" target="#b13">[14]</ref>.</p><p>Optical flow features. We use an image flow extractor that yields the optical flow ? i p,q ? R 2 at location i between frame p and frame q: What is seen at location i in frame p is also seen at location j = i + ? i p,q in frame q. Such an optical flow can typically be obtained using various gradient-based formulations <ref type="bibr" target="#b25">[26]</ref>, or learned using a dataset of annotated flows <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>. It can also be obtained, as in our case, using a self-supervised method such as ARFlow <ref type="bibr" target="#b45">[46]</ref>.</p><p>Affinity matrix. We define our affinity matrix as follows:</p><formula xml:id="formula_4">A = ? ? ? ? ? ? ? ? ? A 1 F 1 0 . . . 0 F 1 A 2 F 2 . . . . . . 0 F 2 A 3 . . . 0 . . . . . . . . . . . . F N ?1 0 . . . 0 F N ?1 A N ? ? ? ? ? ? ? ? ? .<label>(4)</label></formula><p>Each block is a HW ?HW matrix. It contains the affinities between image locations in a frame and image locations in the same frame or in another one. We detail below matrices A p , F p and F p , and justify the use of 0 matrices.</p><p>Matrices A p . Each matrix A p on the block diagonal of A contains the affinities between two image locations in the same frame p. It is based not only on object appearance features within frame p, but also on information from the preceding and succeeding frames p ? 1 and p + 1. Concretely, we take the affinity A i,j p between locations i and j in frame p as the following combined similarity of their object appearance features and their forward and backward flows:</p><formula xml:id="formula_5">A i,j p = 1 ? ? + 2? ? g s ? ? ? i p , ? j p + ? ? ? i p,p+1 , ? j p,p+1 + ? i p,p?1 , ? j p,p?1 ,<label>(5)</label></formula><p>where ? ? and ? ? are relative weights, and g s (?) is a thresholding function that zeroes values lower than s. We use s &gt; 0 to guarantee that the coefficients of A p are positive.</p><p>Matrices F p and F p . Each matrix F p stores affinities between image locations in frame p and image locations in frame p + 1. Here, we only consider the optical flow between frames p and p + 1, and ignore the information given appearance features. (Using appearance features as well might improve the results, but it would lead to a much more complex optimization problem.) Two locations i and j in frames p and p + 1 are likely to both belong to the same object or both lie on the background if the optical flow maps one to the other. In that case, their affinity F i,j p should be large, close to 1; otherwise, we set it to 0. We thus take:</p><formula xml:id="formula_6">F i,j p = 1 if i + ? i p,p+1 = j 0 otherwise ,<label>(6)</label></formula><p>where ? is the vector of nearest integers to ?. F p is defined likewise using the optical flow from frames p + 1 to p. If x denotes a 2D mask in vector form, then F p x is "almost" the mask w p+1 p (x) after warping by the optical flow from frame p to frame p + 1, up to the integer discretization in Eq. <ref type="bibr" target="#b5">(6)</ref>. It also applies to F p x and w p p+1 (x). The spectral clustering purist may notice that this expression for F p and F p makes the affinity matrix A not symmetric. However, F p is almost equal to F p ; they are slightly different because (1) optical flow is not exactly a bijection, as some pixels can appear or disappear, and (2) the predicted flow from frame p to p + 1 is not exactly the inverse of the predicted flow from p + 1 to p. In practice, we may consider that the affinity matrix A is "almost" symmetric. Matrices 0. Matrices that do not belong to the tri-diagonal of matrix A contain affinities between image locations in two "distant", i.e., non-consecutive frames. Here, we ignore the information provided by the appearance features and the optical flow, resulting in the zero matrix. Being able to exploit the appearance features would probably slightly improve the final segmentation, but would also make the optimization significantly less scalable and more complex. Ignoring the optical flow between distant frames is a safe thing to do anyway, as its estimation is likely unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deriving the Objective Function</head><p>By expanding the term X WX found in Eq. (3), using the definition of the normalized affinity matrix W in Eq. (2), the notation X for its second eigenvector, and the definition of the affinity matrix A in Eq. (4), we obtain:</p><formula xml:id="formula_7">X WX = p x p D -1 p A p x p + p x p+1 D -1 p+1 F p x p + p x p D -1 p F p x p+1 ,<label>(7)</label></formula><p>where the D p matrices are on the block diagonal of D and are themselves diagonal. Computing this expression seemingly involvesx the products between matrices A p , F p and vectors x p . It would require large amounts of memory and be prohibitively slow as these matrices are very large. However, we show that we do not need to store these matrices, nor do we need to compute these products for estimating masks x p .</p><p>The terms x p D -1 p A p x p , when considered independently, lead to a spectral clustering problem per frame. The second eigenvector x p of matrix W p = D -1 p A p is in practice a good first estimate of x p , thanks to the combination of the object appearance features and the optical flow.</p><p>In the supplementary material, we also show that when x p is close to x p , then:</p><formula xml:id="formula_8">x p D -1 p A p x p ? ( x p ) x p .<label>(8)</label></formula><p>Furthermore, as mentioned in Section 3.3, the terms F p x and F p x are approximations of the warpings of vector x by the optical flow, i.e., F p x ? w p+1 p (x) and F p x ? w p p+1 (x). As matrix D p is diagonal, we can compute efficiently the last two sums in Eq. (7) using</p><formula xml:id="formula_9">x p+1 D -1 p+1 F p x p = x p+1 d p+1 w p+1 p (x p ) , x p D -1 p F p x p+1 = x p d p w p p+1 (x p+1 ) ,<label>(9)</label></formula><p>where d p denotes the vectorized coefficients on the diagonal of matrix D -1 p and is the element-wise product. The terms d p weight the warped masks. We noticed during our experiments that their influence was very limited, and we did not keep them in our objective function for simplicity.</p><p>Moreover, while spectral clustering is a powerful framework, it actually is an approximation as it relaxes the search for binary masks by looking for continuous values for the coefficients of X, instead of binary ones. To encourage the optimization to recover binary values, we use the crossentropy (minimizing it) when measuring the similarity between masks, rather than the dot product (maximizing it).</p><p>From Eqs. (3), (7), (8), (9), we finally obtain the formulation in Eq. (1). In practice, to minimize L({x p } p ), and thus maximize the flow consistency while not deviating too much from initial estimates, we first compute the eigenvectors x p and use them to initialize the vectors x p . Note that since the x p vectors remain close to the x p vectors, the norm of X remain approximately constant during optimization and the unit constraint in Eq. <ref type="formula">(3)</ref> is approximately satisfied. After convergence of the minimization, the soft masks x p are discretized using K-means with K = 2 to separate the object of interest from the background. Implementation details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the implementation details of our method. Then we describe the datasets we use for comparison with other methods and make a comprehensive analysis of each component in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our model on three standard benchmarks in unsupervised video object segmentation: DAVIS2016, SegTrack-v2, and FBMS59. DAVIS2016 [60] is a denselyannotated video object segmentation dataset, featuring 50 sequences and 3455 frames in total, captured in 1080p resolution at 24 FPS with precise annotation of a primary moving object at 480p. SegTrack-v2 <ref type="bibr" target="#b41">[42]</ref> is a denselyannotated dataset of 14 sequences with 976 frames in total. It sometimes features multiple objects in a single video and has multiple challenges such as motion blur, deformations, interactions, and objects being static. FBMS59 <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b3">4]</ref> is a dataset of 59 sequences with every 20-th frame being annotated, which yields 720 annotated frames in total. The dataset may involve multiple objects (some of which can be static), occlusions, and other challenging conditions. Since SegTrack-v2 and FBMS might feature multiple objects in one scene and since our method is interested in the main object segmentation, we merge the segmentation masks of the individual objects into one, similar to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b87">87</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>Jaccard (J ). For all datasets, we evaluate them with the Jaccard metric J , which measures the intersection-overunion between predicted masks and ground-truth masks.</p><p>Contour accuracy (F). For the DAVIS2016 dataset, we also report contour accuracy. This measure treats the mask as a set of closed contours to calculate their precision and recall with respect to the annotation. The contour accuracy is then taken as F = 2PcRc</p><p>Pc+Rc where P c is the contour precision and R c is the contour recall.</p><p>Computational cost. On an Nvidia V100, our initialization and optimization runs on average in ? 0.5 s/frame, where initial eigenvector extraction takes ? 0.1 s/frame and the optimization takes ? 0.4 s/frame.</p><p>Optimization consumes ? 8 GB of VRAM for typical sequences in DAVIS2016, and ? 20 MFLOPS/frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation and Parameter Study</head><p>In order to understand the different factors that contribute to the performance of our method, we conduct a series of ablation studies. <ref type="table" target="#tab_1">Table 1</ref> reports the importance of the different components of our pipeline. Note that we do not apply any post-processing in our ablation experiments to show direct gains by our method. We evaluate the different aspects of our method as detailed below.</p><p>Initial masks x p from appearance only. We study applying spectral clustering to appearance features only. We consider recent self-supervised features: DINO <ref type="bibr" target="#b8">[9]</ref>, MoCo-v3 <ref type="bibr" target="#b13">[14]</ref>, SWAV <ref type="bibr" target="#b7">[8]</ref> and Barlow Twins (BT) <ref type="bibr" target="#b94">[94]</ref>. <ref type="table" target="#tab_1">Table 1</ref>(a) compares how these appearance features affect our results. We obtain the best performance with DINO. The fact that DINO largely outperforms other self-supervised features, as also noted in <ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b52">53]</ref>, remains to be understood, but is out of scope. We also note that our method can exploit any image features. Better feature extractors in the future could even improve our results further.  <ref type="table" target="#tab_1">Table 1</ref>: Ablation experiments on DAVIS2016 for different optical flow and appearance features for the framebased initialization and after optimization. (a) Effects of different self-supervised methods for appearance features.</p><p>(b) Effects of optical flow and different optical flow methods on performance. We obtain all initial results using Spectral Clustering on each frame independently. "+Optimization" denotes the results after optimizing our objective function. No post processing is applied to the results. Our optimization approach consistently improves the segmentation results by a large margin for all appearance features, with or without optical flow for initializing the masks.</p><p>ric by the average value of +5.6% and the F metric by the average value of +16.6%. This ablation also shows one potential application of our method, which could serve as a way to benchmark fine-grained representation capacity of self-supervised features <ref type="bibr" target="#b83">[83,</ref><ref type="bibr" target="#b22">23]</ref>: Given frozen features and strictly predefined optical flow, one can "plug-in" new features and estimate how good the representation capacity is. As it has the most similar (graph-based) formulation to ours, we also compare to TokenCut <ref type="bibr" target="#b82">[82]</ref> applied to each frame independently, using the same DINO features. On David2016, our mask extraction from single frames (i.e., our initial masks x p ) is 1.5 pt behind TokenCut on the J metric, while outperforming it on F by 3.5 pts, which shows that our initialization is better at detecting object boundaries on individual images. More importantly, we are ? 170? faster than TokenCut <ref type="bibr" target="#b82">[82]</ref> (0.1 s/frame vs. 17 s/frame).</p><p>Initial masks x p from both appearance and flow. We apply spectral clustering on the combination of appearance features and optical flow, as in Eq. <ref type="bibr" target="#b4">(5)</ref>. We consider supervised RAFT and self-supervised ARFlow. Effects of adding the optical flow to our approach can be seen in Table 1(b). The addition of optical flow to the appearance features greatly improves single-frame clustering performance and gives us a good initialization for the object masks. (Two successive frames are used for computing the optical flow.) Masks x p optimized from different initial masks x p .  <ref type="table">Table 2</ref>: Results of our approach. We show the performance of our approach on three standard benchmarks for video object segmentation (DAVIS2016, SegTrack-v2, and FBMS59), where our approach achieves state-of-the-art results despite. We also provide a comparison with some supervised methods, where we achieve a performance comparable to some of them without using any supervision. Note that recent unsupervised methods (MoSeg, CIS, DyS-taB and IKE) use supervised optical flow estimators such as RAFT <ref type="bibr" target="#b72">[73]</ref>, PWCNet <ref type="bibr" target="#b70">[71]</ref> or FlowNet <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>We present the effects of our global optimization on different appearance features and optical flows in <ref type="table" target="#tab_1">Table 1</ref>. Our optimization gives an average boost of +4.9% over the single frame clustering, validating our approach in all cases. Number of flow steps. We found (see full results in supp. mat.) that one flow step achieves the best performance, which supports our assumption of using a tridiagonal global affinity matrix: Even in the single flow step case, all frames are tied together via optical flow. Conversely, more steps might complicate the optimization due to the potentially noisier flow estimated between distant frames because of the larger displacements and additional occlusions.</p><p>Cross-entropy vs. dot product. In our objective function, we use the cross-entropy rather than the dot product between the masks. We compared the two options experimentally and found that using the cross-entropy indeed improves the masks that we recover. We provide the full results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to the State of the Art</head><p>Here, we use our best configuration (DINO [ViT] + ARFlow + Optimization). Following CIS <ref type="bibr" target="#b90">[90]</ref> and DyS-taB <ref type="bibr" target="#b89">[89]</ref>, we use a CRF as a post-processing step. <ref type="table">Table 2</ref> presents the performances of our method and several estab-lished state-of-the-art unsupervised and supervised methods. Overall, our method performs on-par with the stateof-the-art methods on DAVIS2016 and it achieves the best performance on the STv2 dataset. Our method also has high contour accuracy F, which shows that our approach achieves high quality boundaries. Among unsupervised methods, our method outperforms all previous methods by achieving 80.2 J and 74.9 J scores on DAVIS2016 and STv2, respectively. Our method outperforms the SOTA method DyStaB albeit slightly, with a much simpler approach. Also note that, in contrast to recent unsupervised methods, our approach does not use any supervised component in the pipeline, including optical flow. <ref type="table">Table 2</ref> shows the generalization ability of our method: Rather than training on a target dataset, we leverage general images features obtained from a network pretrained on a large dataset with no supervision. While it may not be optimal in some contexts due to different data distributions, we achieve an excellent performance on three different benchmarks, without training and reusing the exact same networks for feature extraction and flow computation.</p><p>Comparing ViT and CNN based methods is not straightforward as each of them has their own advantages. Although other methods do not exploit ViTs, the most recent ones do use advanced CNN networks, e.g., DeepLabv3 (used in DyStaB), one of the SOTA architectures for segmentation. Besides, while we rely only on pretrained networks, although possibly on large datasets (e.g., ImageNet for appearance, Sintel for flow), a number of other methods (supervised or unsupervised) are advantaged by the fact they train on the target datasets, hence on the task itself and using a data distribution closer to the test sets. Note that other methods in <ref type="table">Table 2</ref> also use extra data beyond the evaluated dataset, e.g., DyStaB uses ImageNet-pretrained weights to initialize its network, while AMD uses Youtube-VOS (a large video dataset) pretrained weights. <ref type="figure">Figure 5</ref> shows some failure cases of our approach. More examples are provided in the supplementary material. Note that some of those failure cases can be removed by further postprocessing, but we do not use it to keep our approach simple. We show examples of segmentation results by our method for qualitative visual inspection in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our method consists of minimizing an objective function that is intuitive, simple to implement, and can be optimized efficiently. It can be derived from spectral clustering, which gives it solid theoretical ground. It could also be used to evaluate fine-grained capabilities of modern selfsupervised representations, which is still a very active area of research <ref type="bibr" target="#b83">[83,</ref><ref type="bibr" target="#b22">23]</ref>. We hope that the simplicity of our method and its connection to spectral clustering will provide others with insights for future development. <ref type="bibr">DINO</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DINO + DINO + Ground Truth ARFlow</head><p>ARFlow + Opt <ref type="figure">Figure 4</ref>: Qualitative results obtained with our approach. The quality of segmentation improves with each component. In the first row, partially segmented object is recovered; the second and third rows show that we successfully recover the small and occluded objects; in the fourth row our method removes the background residuals and finally in the fifth and sixth rows, we show that our method recovers and segments multiple close objects. <ref type="figure">Figure 5</ref>: Failure cases. Our approach has 3 main failure modes: Oversegmentation which is most often caused by multiple similar objects in the scene, imprecise masks due to occlusion, and undersegmentation in scenes with multiple objects which leads to the parts of the primary object not being segmented. In the last example, note that when we use DINO features, our clusters tend to group a single semantic class, which is the desired behavior for object discovery, rather than segment anything that moves. This is thus a failure case w.r.t. the benchmark, but not to the goal of object discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary material -Overview</head><p>In this supplementary material:</p><p>? we justify design choices and assumptions made in the main paper (Sections B-F);</p><p>? we show implementation details of our approach (Section G);</p><p>? we provide additional failure cases (Section H);</p><p>? we provide more qualitative results on DAVIS2016, SegTrack v2 and FBMS-59 (Section I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Justifying only using the flow between adjacent frames</head><p>Our formulation, in Eq. (1) of the main paper, only involves the optical flow between adjacent frames (forward and backward). As discussed in Section 3 of the main paper, it can be related to the tridiagonal affinity matrix A in Eq. <ref type="bibr" target="#b3">(4)</ref>. We remark at the end of Section 3.3 that we could have used a denser matrix correlating more faraway frames, but that the optical flow between frames that are distant in time is less reliable.</p><p>To validate our choice of only using the optical flow between adjacent frames, we consider here the following variant of our objective function, where we introduce warps between more distant frames (up to some constant T ):  <ref type="table">Table 3</ref>: Study of the distance between frames for flow consistency enforcement. We consider different values of T in Eq. (10) and evaluate on DAVIS2016 using our best configuration (DINO [ViT] + ARFlow + Opt) without CRF post-processing. The best performance is achieved for T = 1, coinciding with the tridiagonal matrix configuration. <ref type="table">Table 3</ref> shows that using a time horizon of a single frame is not only enough but actually better than considering the optical flow between more distant frames. In fact, using the flow regarding only the preceding and the succeeding frames already ties together all frames in the sequence. Additional terms with optical flows between more distant frames may actually introduce noise because of worse estimations due to larger displacements, deformations and occlusions.</p><formula xml:id="formula_10">L({x p } p ) = p ? CE( x p , x p ) + T t=1 CE(x p+t , w p+t p (x p )) + CE(x p , w p p+t (x p+t )) ,<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Using the cross-entropy vs the dot product</head><p>In Section 3.4 of the main paper, we replace the dot products</p><formula xml:id="formula_11">( x p ) T x p , x T p+1 w p+1 p (x p ) , and x T p w p p+1 (x p+1 )</formula><p>by cross-entropies, respectively:</p><formula xml:id="formula_12">CE( x p , x p ), CE(x p+1 , w p+1 p (x p )), and CE(x p , w p p+1 (x p+1 )).</formula><p>This was motivated empirically, as we observed that the cross-entropy was providing a better performance.  <ref type="table" target="#tab_4">Table 4</ref>: Dot product vs cross-entropy. Using the crossentropy between two vectors in our objective function rather than their dot product leads to a significant improvement of +14.6% in J and +16.6% in F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. On the Constant Norm Constraint in Eq. (3)</head><p>We estimate the second largest eigenvector of W via a maximization problem over a vector X under the constraint that X 2 is constant, as stated in Eq. (3) in the main paper. At the end of Section 3.4, we claim that since the x p vectors remain close to the x p vectors, X 2 = p ( x p 2 ) 2 remains approximately constant during optimization, thus satisfying the constraint in Eq. (3) up to a constant factor of ? N . <ref type="table">Table 5</ref> shows empirically that this constraint is indeed approximately met at each stage of the global optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Approximation in Eq. (8)</head><p>In the main paper, we assumed the following approximation:  <ref type="table">Table 5</ref>: Study of the constant norm constraint approximation. We study the average of x p 2 over all frames of all sequences in the DAVIS2016 dataset at each iteration of our global optimization (using L-BFGS). We observe that the deviations to the theoretical norm of 1 are small, which in turns means that our approach of not using any explicit constraint is valid.</p><formula xml:id="formula_13">x p D -1 p A p x p ? x p x p .<label>(11)</label></formula><p>The derivation of this approximation is given below. Since W p = D -1 p A p is a row-normalized stochastic matrix, the largest eigenvalue associated to its first eigenvector is 1. Besides, our initial mask estimate x p is computed as the second largest eigenvector of W p via Power Iteration Clustering (PIC) <ref type="bibr" target="#b43">[44]</ref>. According to <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b43">44]</ref>, if K clusters are well-separated, then the significant eigenvalues of W p , noted ? 1 ? . . . ? ? K , are such that ? i /? 1 ? 1 for all i ? {1, ..., K}. Consequently, if the foreground object is well-separated from the background (K ? 2), we may assume that ? 2 ? ? 1 = 1. As x p approximates the second largest eigenvector of W p , we have:</p><formula xml:id="formula_14">W p x p ? ? 2 x p ? x p .<label>(12)</label></formula><p>Therefore, considering also that x p deviates little from x p , i.e., x p ? x p , we have:</p><formula xml:id="formula_15">x p D -1 p A p x p = x p W p x p ? x p W p x p ? x p x p = x p x p .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Dealing with Inaccurate Optical Flow</head><p>Our method relies on predicted optical flows. As they can be wrong or poor quality, they may introduce noise during the computation of the initial mask estimates and the optimization. In order to reduce the influence of this noise, we eliminate poor quality optical flow predictions. Given a predicted flow ? p,q , we first warp frame q to frame p. Next, we calculate the difference image between frame p and the reconstructed framep. The locations with high response on the difference image correspond to wrong or poor quality optical flow predictions. We use k-th percentile of the difference image as a threshold value to eliminate the poor quality optical flow predictions. The locations under the calculated threshold value indicate where optical flow fails to produce the accurate flows. We exclude these optical flow predictions from both the computation of the initial mask estimates and the optimization. We experimentally set k as 90-th percentile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Implementation Details</head><p>All our experiments are implemented with PyTorch <ref type="bibr" target="#b57">[58]</ref>. We use the L-BFGS <ref type="bibr" target="#b6">[7]</ref> with learning rate of 1 to optimize our objective function. The weight ? in Eq. 1 is set to be 10.</p><p>We use DINO pretrained on ImageNet as appearance features. Due to their low resolution, we upscale the initial eigenvectors to the required resolution and run the full optimization pipeline. The optimized eigenvectors can be later either thresholded or clustered with K-means to obtain the final masks. We choose K-means as it is a more universal method that does not require finding threshold parameters. The final segments are then refined using CRF, as <ref type="bibr" target="#b90">[90,</ref><ref type="bibr" target="#b89">89]</ref>.</p><p>We use the ARFlow model pretrained on the CityScapes <ref type="bibr" target="#b16">[17]</ref> dataset in a self-supervised manner to predict optical flow. In ablation studies, we also use the RAFT model <ref type="bibr" target="#b72">[73]</ref> for comparison, which is trained in a supervised manner with labeled data from the Sintel dataset <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Additional Failure Cases</head><p>In <ref type="figure">Figure 6</ref>, we show more failure examples of our approach. The first row shows another example of oversegmentation, where flowing particles are being segmented as foreground. The second row shows undersegmentation. The last row shows the inability of our approach to capture very fine details, such as the thin cables of the paraglider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. More Qualitative Results</head><p>On the next pages <ref type="figure">(Figures 7-19</ref>), we show more qualitative results of our approach, where we compare to the CIS <ref type="bibr" target="#b90">[90]</ref> method, which is the third best self-supervised video object segmentation (VOS) method after ours, according to <ref type="table">Table 2</ref> in the main paper. (DyStab <ref type="bibr" target="#b89">[89]</ref>, which is the second best self-supervised VOS method, did not release code to rerun these experiments nor mask results).</p><p>Compared to CIS, our method is more successful at segmenting objects as a whole and capturing finer details of object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Use of Existing Datasets and Codes</head><p>For the experiments, we used several datasets that are freely available for research purpose:</p><p>? DAVIS 2016 2 <ref type="bibr" target="#b59">[60]</ref> is under license CC BY-NC 4.0.  <ref type="figure">Figure 6</ref>: Failure cases. Our approach has three main failure modes: over-segmentation in scenes with multiple objects, under-segmentation, and inability to capture very fine details.</p><p>? SegTrack-v2 <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b41">[42]</ref> is under a custom non-commercial, research-only license, courtesy of Georgia Institute of Technology,</p><p>? FBMS-59 4 <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b3">4]</ref> is under a custom non-commercial, research-only license, courtesy of University of Freiburg.</p><p>To compute appearance and flow, we experimented with the following methods, whose code is freely available for research purpose:</p><p>? DINO 5 <ref type="bibr" target="#b8">[9]</ref> is under the Apache License 2.0.</p><p>? MoCov2 6 <ref type="bibr" target="#b11">[12]</ref> is under the CC BY-NC 4.0 license. ? ARFlow 7 <ref type="bibr" target="#b45">[46]</ref> is under the MIT License.</p><p>? RAFT 8 <ref type="bibr" target="#b72">[73]</ref> is under the BSD 3-Clause License.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Societal Impact</head><p>We believe that our approach for the self-supervised discovery and segmentation of objects in videos has very little potential for malicious uses (including disinformation, surveillance, invasion of privacy, endangering security), in any case not more, e.g., than the hundreds of previously published methods on supervised object detection and segmentation. Moreover, we are not bound nor promoting any dataset that would lead to unfairness in any sense. Besides, the use of our method has a very little environmental impact as there is no training phase and as the optimization is relatively fast and in the same order of magnitude as other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIS [90]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ground Truth <ref type="figure">Figure 7</ref>: Segmentation in sample frames from videos in SegTrack v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16">CIS [90]</head><p>Ours Ground Truth <ref type="figure">Figure 8</ref>: Segmentation in sample frames from videos in SegTrack v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIS [90]</head><p>Ours Ground Truth </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Generalization to new datasets. Relying on spectral clustering rather than learning generalizes better to new datasets. (a) The learned method of [87] performs poorly on a new sequence. (b) Spectral clustering on the same optical flow performs significantly better, even without optimization. (c) Result of our complete method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label></label><figDesc>https://davischallenge.org Ours Ground Truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9 : 18 CISFigure 10 : 19 CISFigure 11 : 20 CISFigure 12 : 21 CISFigure 13 : 22 CISFigure 14 :Figure 16 : 25 CISFigure 17 : 26 CISFigure 18 : 27 CISFigure 19 :</head><label>91810191120122113221416251726182719</label><figDesc>Segmentation in sample frames from videos in SegTrack v2. Segmentation in sample frames from videos in DAVIS 2016. Segmentation in sample frames from videos in DAVIS 2016. Segmentation in sample frames from videos in DAVIS 2016. Segmentation in sample frames from videos in DAVIS 2016. Segmentation in sample frames from videos in DAVIS 2016. CIS [90] Ours Ground Truth Segmentation in sample frames from videos in FBMS-59. Segmentation in sample frames from videos in FBMS-59. Segmentation in sample frames from videos in FBMS-59. Segmentation in sample frames from videos in FBMS-59.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>(a) also shows that, in every case our optimization method improves the J met-</figDesc><table><row><cell>(a) Different appearance features</cell><cell>(J ?)</cell><cell>(F ?)</cell></row><row><cell>(BT [RN-50] + ARFlow)</cell><cell>53.5</cell><cell>30.3</cell></row><row><cell>(BT [RN-50] + ARFlow) + Optimization (Eq. (1))</cell><cell cols="2">59.5 (+6.0) 48.2 (+17.9)</cell></row><row><cell>(SWAV [RN-50] + ARFlow)</cell><cell>48.0</cell><cell>27.2</cell></row><row><cell cols="3">(SWAV [RN-50] + ARFlow) + Optimization (Eq. (1)) 53.6 (+5.6) 46.0 (+18.8)</cell></row><row><cell>(MoCo-v3 [ViT] + ARFlow)</cell><cell>58.0</cell><cell>35.3</cell></row><row><cell cols="3">(MoCo-v3 [ViT] + ARFlow) + Optimization (Eq. (1)) 64.2 (+6.2) 61.1 (+25.8)</cell></row><row><cell>(DINO [ViT] + ARFlow)</cell><cell>72.1</cell><cell>72.5</cell></row><row><cell>(DINO [ViT] + ARFlow) + Optimization (Eq. (1))</cell><cell cols="2">76.8 (+4.7) 77.0 (+4.5)</cell></row><row><cell>(b) Different optical flows</cell><cell>(J ?)</cell><cell>(F ?)</cell></row><row><cell>DINO</cell><cell>61.2</cell><cell>65.8</cell></row><row><cell>TokenCut [82]</cell><cell>62.7</cell><cell>62.3</cell></row><row><cell>DINO + Optimization (Eq. (1))</cell><cell cols="2">66.7 (+5.5) 70.4 (+4.6)</cell></row><row><cell>(DINO + RAFT)</cell><cell>70.7</cell><cell>72.9</cell></row><row><cell>(DINO + RAFT) + Optimization (Eq. (1))</cell><cell cols="2">75.3 (+4.6) 76.2 (+3.3)</cell></row><row><cell>(DINO + ARFlow)</cell><cell>72.1</cell><cell>72.5</cell></row><row><cell>(DINO + ARFlow) + Optimization (Eq. (1))</cell><cell cols="2">76.8 (+4.7) 77.0 (+4.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">reports the quantitative results of this experiment.</cell></row><row><cell>Measurement of</cell><cell cols="2">DAVIS2016</cell></row><row><cell>mask deviation</cell><cell>J ?</cell><cell>F ?</cell></row><row><cell>Cross-entropy</cell><cell>76.8</cell><cell>77.0</cell></row><row><cell>Dot product</cell><cell>62.1</cell><cell>60.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>https://web.engr.oregonstate.edu/?lif/ SegTrack2/dataset.html 4 https://lmb.informatik.uni-freiburg.de/ resources/datasets/moseg.en.html</figDesc><table /><note>35 https://github.com/facebookresearch/dino6 https://github.com/facebookresearch/moco7 https://github.com/lliuz/ARFlow8 https://github.com/princeton-vl/RAFT</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The terminology is still fluctuating, as the DINO method is called "self-supervised" in the original paper, while ARFlow is called "unsupervised". Throughout the paper, we will use self-supervised and unsupervised in the sense of "not using manual annotations".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012896 made by GENCI and supported in part by the Chistera IPalm project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-Seg: Spatio-Temporal EMbeddings For Instance Segmentation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">It&apos;s Moving! a Probabilistic Model for Causal Motion Segmentation in Moving Camera Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pia</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object Segmentation by Long Term Analysis of Point Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<title level="m">Monet: Unsupervised Scene Decomposition and Representation. In arXiv Preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Naturalistic Open Source Movie for Optical Flow Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Limited Memory Algorithm for Bound Constrained Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihuang</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciyou</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EMerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved Baselines with Momentum Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv Preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An EMpirical Study of Training Self-Supervised Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv Preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SegFlow: Joint Learning for Video Object Segmentation and Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised Object Discovery and Localization in the Wild: Part-Based Matching with Bottom-Up Region Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Learning from Video to Detect Foreground Objects in Single Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flownet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative Unsupervised Feature Learning with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Better Segment Objects from Unseen Classes with Unlabeled Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How Well Do Self-Supervised Models Transfer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustering by Composition&quot;: Unsupervised Discovery of Image Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video Segmentation by Non-Local Consensus Voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer US</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video Segmentation with Superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Object Representation Learning with Iterative Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative Knowledge Exchange Between Deep Learning and Space-Time Spectral Clustering for Unsupervised Segmentation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuela</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><forename type="middle">Magda</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient Coarse-To-Fine Patchmatch for Large Displacement Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised Video Object Segmentation Using Motion Saliency-Guided Spatio-Temporal Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FusionSeg: Learning to Combine Motion and Appearance for Fully Automatic Segmention of Generic Objects in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to Combine Motion and Appearance for Fully Automatic Segmentation of Generic Objects in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Full-Duplex Strategy for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motion Trajectory Segmentation via Minimum Cost Multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Classifier-Based Graph Construction for Video Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Primary Object Segmentation in Videos Based on Region Augmentation and Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segmenting Invisible Moving Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Lamdouar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video Segmentation by Tracking Many Figure-Ground Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video Object Segmentation with Joint Re-Identification and Attention-Aware Mask Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Power Iteration Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Exploring New Representations and Applications for Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others. Beyond</forename><surname>Pixels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The EMergence of Objectness: Learning Zero-Shot Segmentation from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object-Centric Learning with Slot Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Making a Case for 3D Convolutions for Object Segmentation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hennen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video Object Segmentation Without Temporal Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning Segmentation by Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object Segmentation in Video: A Hierarchical Variational Approach for Turning Point Trajectories into Dense Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segmentation of Moving Objects by Long Term Video Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video Object Segmentation Using Space-Time Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fast Object Segmentation in Unconstrained Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning Video Object Segmentation from Static Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reciprocal Transformations for Unsupervised Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Sucheng Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised Joint Object Discovery and Segmentation in Internet Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Using Multiple Segmentations to Discover Objects and Their Extent in Image Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Video Object Segmentation Using Teacher-Student Adaptation in a Human Robot Interaction (hri) Setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Localizing Objects with Self-Supervised Transformers and No Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Visual Object Class Hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Object Level Grouping for Video Shots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pyramid Dilated Deeper Convlstm for Video Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs For Optical Flow Using Pyramid, Warping, and Cost Volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dense Point Trajectories by Gpu-Accelerated Large Displacement Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">RAFT: Recurrent All-Pairs Field Transforms for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning Motion Patterns in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning Video Object Segmentation with Visual Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unsupervised Image Matching and Object Discovery as Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Online Adaptation of Convolutional Neural Networks for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv Preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Tutorial on Spectral Clustering. Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Saliency-Aware Geodesic Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning Unsupervised Video Object Segmentation through Visual Attention</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Self-Supervised Transformers for Unsupervised Object Discovery Using Normalized Cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Vaufreydaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Do Different Tracking Tasks Require Different Appearance Models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Infromation Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Object Discovery in Videos as Foreground Motion Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Qiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Self-Supervised Video Object Segmentation by Motion Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charig</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Lamdouar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning Motion-Appearance Co-Attention for Zero-Shot Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Unsupervised Moving Object Detection via Contextual Information Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Anchor Diffusion for Unsupervised Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Deformable sprites for unsupervised video decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="2657" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep Transport Network for Unsupervised Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Motion-Attentive Transition for Zero-Shot Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">American Association for Artificial Intelligence Conference (AAAI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
