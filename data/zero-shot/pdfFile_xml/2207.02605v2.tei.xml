<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published in Transactions on Machine Learning Research (09/2022) Reviewed on OpenReview: https: // openreview. net/ forum? id= LSAAlS7Yts</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud semantic segmentation from projected views, such as range-view (RV) and bird'seye-view (BEV), has been intensively investigated. Different views capture different information of point clouds and thus are complementary to each other. However, recent projectionbased methods for point cloud semantic segmentation usually utilize a vanilla late fusion strategy for the predictions of different views, failing to explore the complementary information from a geometric perspective during the representation learning. In this paper, we introduce a geometric flow network (GFNet) to explore the geometric correspondence between different views in an align-before-fuse manner. Specifically, we devise a novel geometric flow module (GFM) to bidirectionally align and propagate the complementary information across different views according to geometric relationships under the end-to-end learning scheme. We perform extensive experiments on two widely used benchmark datasets, SemanticKITTI and nuScenes, to demonstrate the effectiveness of our GFNet for project-based point cloud semantic segmentation. Concretely, GFNet not only significantly boosts the performance of each individual view but also achieves state-of-the-art results over all existing projectionbased models. Code is available at https://github.com/haibo-qiu/GFNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published in Transactions on Machine Learning Research (09/2022)</head><p>With the great success of fully convolutional networks for image-based semantic segmentation <ref type="bibr" target="#b8">(Chen et al., 2017a;</ref><ref type="bibr" target="#b24">Long et al., 2015;</ref><ref type="bibr" target="#b47">Zhao et al., 2017)</ref>, projection-based methods have recently received increasing attention. <ref type="figure">Figure 1</ref> illustrates two widely used projected views, i.e., range-view (RV)  and bird's-eye-view (BEV) <ref type="bibr" target="#b46">(Zhang et al., 2020b)</ref>. Single view based methods can only learn view-specific representations <ref type="bibr" target="#b2">(Alonso et al., 2020;</ref><ref type="bibr" target="#b12">Cortinhal et al., 2020;</ref>, failing to handle those occluded points during projection. For example, the RV in <ref type="figure">Figure 2</ref> shows a occluded tail phenomenon (i.e., the distant occluded points are assigned with the labels of near displayed points) in the red rectangle areas. To address this problem, recent methods resort to multi-view models to incorporate complementary information over different views, which usually deal with RV/BEV in sequence <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b15">Gerdzhev et al., 2021)</ref> or perform a vanilla late fusion (Alnaggar et al., 2021;. However, existing methods fail to probe the intrinsically geometric connections of RV/BEV during the representation learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D point cloud analysis has drawn increasing attention from both academic and industrial communities, since the wide deployments of lidar sensors have made it possible to obtain abundant 3D point cloud data <ref type="bibr" target="#b6">Caesar et al., 2020)</ref>. Compared to 2D images (e.g., RGB images), a point cloud can capture precise structures of objects, thus providing a geometry-accurate perspective representation, intrinsically in line with the 3D real world. Point cloud semantic segmentation, aiming to assign a semantic label to each point, is fundamental to scene understanding, which enables intelligent agents to precisely perceive not only the objects but also the dynamically changing environment. Therefore, point cloud semantic segmentation plays a crucial role, especially in safety-critical applications such as autonomous driving <ref type="bibr" target="#b22">(Li et al., 2020;</ref> and robotics <ref type="bibr" target="#b20">(Li et al., 2019;</ref>.</p><p>Unlike structural pixels in an image, a point cloud is a set of points represented by <ref type="bibr">(x, y, z)</ref> coordinates without a specific order, and extremely sparse for in-the-wild scenes. Hence, it is non-trivial to utilize off-the-shelf deep learning technologies on images for point cloud analysis. Recent point cloud segmentation methods usually address the above-mentioned sparse distributed issue from the perspectives of either voxelization, single/multi-view projections, or novel point-based operations. However, voxel-based methods mainly suffer from heavy computations while point-based operations struggle to efficiently capture the neighbour information, especially when dealing with large-scale outdoor scenes <ref type="bibr" target="#b6">Caesar et al., 2020)</ref>. As we can see from <ref type="figure" target="#fig_0">Figure 1</ref>, to find the geometric correspondence between two views (the dash line), we can utilize the original point cloud as a bridge, e.g., the transformation RV to BEV can be obtained from two transformations (the solid line): 1) from RV to point cloud; and 2) from point cloud to BEV. Inspired by this, we introduce a novel geometric flow network (GFNet) to simultaneously learn view-specific representations and explore the geometric correspondences between RV and BEV in an end-to-end learnable manner. Specifically, we first propose to adopt two branches to process RV and BEV inputs, where each branch follows an encoderdecoder architecture using a ResNet <ref type="bibr" target="#b16">(He et al., 2016)</ref> as the backbone. We then devise a geometric flow module (GFM), which is then applied at multiple levels of feature representations, to bidirectionally align and propagate geometric information across two projection views, aiming to learn more discriminative representations. <ref type="figure">Figure 2</ref> illustrates an example of propagating the information from BEV to RV which benefits handling those occluded points by RV projection. In addition, inspired by <ref type="bibr" target="#b19">Kochanov et al. (2020)</ref>, we also use KPConv <ref type="bibr" target="#b33">(Thomas et al., 2019)</ref> at the top of GFNet to replace a KNN post-processing, thus making it easy to train the overall multi-view point cloud semantic segmentation pipeline in an end-to-end paradigm. The main contributions of this paper are summarized as follows:</p><p>? We introduce a novel GFNet to simultaneously learn and fuse multi-view representations, where the proposed geometric flow module (GFM) enables the geometric correspondence information to flow across different views.</p><p>? We devise two branches for RV and BEV with KNN post-processing replaced by KPConv, making the proposed GFNet end-to-end trainable.</p><p>? Extensive experiments are performed on two popular large-scale point cloud semantic segmentation benchmarks, i.e., SemanticKITTI and nuScenes, to demonstrate the effectiveness of GFNet, which achieves state-of-the-art performance over all existing projection-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review recent point cloud semantic segmentation literature from the perspectives of pointbased, voxel-based, and projection-based methods. In addition, we discuss more recently hybrid methods which simultaneously use multiple formats/modalities. Among all projection-based methods, we mainly focus on the multi-view projection-based methods.  <ref type="figure">Figure 2</ref>: The distant occluded points caused by RV projection are misclassified as the labels of near displayed points in the red rectangle areas, while they are totally captured by BEV. By propagating the information between BEV and RV, this issue can be well addressed by our GFNet.</p><p>Point-based Methods. Recent methods mainly devise novel point operations/architectures to directly learn representations from raw points <ref type="bibr" target="#b17">(Hu et al., 2020;</ref><ref type="bibr" target="#b21">Li et al., 2018;</ref><ref type="bibr" target="#b27">Qi et al., 2017a;</ref><ref type="bibr" target="#b33">Thomas et al., 2019;</ref><ref type="bibr" target="#b35">Wang et al., 2019;</ref><ref type="bibr" target="#b31">Tang et al., 2022)</ref>, including mlp-based <ref type="bibr" target="#b17">(Hu et al., 2020;</ref><ref type="bibr" target="#b27">Qi et al., 2017a;</ref>, cnn-based <ref type="bibr" target="#b21">(Li et al., 2018)</ref>, and graph-based methods <ref type="bibr" target="#b33">(Thomas et al., 2019;</ref><ref type="bibr" target="#b35">Wang et al., 2019)</ref>. Specifically, PointNet (Qi et al., 2017a) is a pioneer that directly processes point cloud with multi-layer perceptron (MLP), which is improved by PointNet++ (Qi et al., 2017b) using a hierarchical neural network to learn local features. PointCNN <ref type="bibr" target="#b21">(Li et al., 2018)</ref> learns a X-transformation from the input points for alignment, followed by typical convolution layers. DGCNN <ref type="bibr" target="#b35">(Wang et al., 2019)</ref> proposes a new graph convolution module called EdgeConv to capture local geometric features. RandLA-Net <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> employs random point sampling with an effective local feature aggregation module to persevere the local information. KPConv <ref type="bibr" target="#b33">(Thomas et al., 2019)</ref> introduces a new point convolution operator named Kernel Point Convolution to directly take neighbouring points as input and processes with spatially located weights. Nevertheless, the irregular and disordered characteristics of point clouds make it inefficient to capture the neighbour information.</p><p>Voxel-based Methods. They <ref type="bibr" target="#b11">(Cheng et al., 2021;</ref><ref type="bibr" target="#b30">Tang et al., 2020;</ref><ref type="bibr" target="#b40">Yan et al., 2020a;</ref> first voxelize point clouds to regular grids and process with 3D convolutions. Cylin-der3D  introduces the cylindrical partition and asymmetrical 3D convolution networks to tackle the issues of sparsity and varying density of point clouds. SPVNAS <ref type="bibr" target="#b30">(Tang et al., 2020)</ref> proposes Sparse Point-Voxel Convolution (SPVConv), which is a lightweight 3D module consisting of the vanilla Sparse Convolution and the high-resolution point-based branch. Furthermore, 3D Neural Architecture Search (3D-NAS) is presented to obtain the efficient and effective architecture for semantic segmentation. AF2S3Net <ref type="bibr" target="#b11">(Cheng et al., 2021)</ref> designs an AF2M to capture the global context and local details and an AFSM to learn interrelationships between channels across multi-scale feature maps from AF2M. However, the distributions of large-scale outdoor scenes (e.g., SemanticKITTI ) are extremely sparse, and the computations grow cubically when increasing the voxel resolution.</p><p>Hybrid Methods. Recent methods <ref type="bibr" target="#b44">Ye et al., 2021;</ref><ref type="bibr">Yan et al., 2022)</ref> usually focus on simultaneously using multiple formats/modalities (e.g., voxel, points or natural images) to learn discriminative representations. DRINet++ <ref type="bibr" target="#b44">Ye et al. (2021)</ref> proposes Sparse Feature Encoder to extract local context information from voxelized grids, and Sparse Geometry Feature Enhancement to enhance the geometric characteristics of sparse points using multi-scale sparse projection and attentive multi-scale fusion. RPVNet  explores multiple and mutual information interactions among three views (i.e., projection, voxel and point), following by a gated fusion module to adaptively merge the three features based on concurrent inputs. 2DPASS <ref type="bibr">Yan et al. (2022)</ref> assists raw points with 2D natural images. It distills richer semantic and structural information from 2D images without strict paired data constraints to the pure 3D point network, by leveraging an auxiliary modal fusion and multi-scale fusion-to-single knowledge distillation (MSFSKD). Finally, grid sampling based on corresponding projection relationships is utilized to get the probability (N ? C) for each point, and the fused prediction F f is obtained by applying kpconv on the concatenation of RV/BEV. Note that the subplot in bottom right corner illustrates how grid sampling works with dimension changing annotation, i.e., sampling</p><formula xml:id="formula_0">F (F r or F b ) with N ? C from M (M r or M b ) with H ? W ? C.</formula><p>Projection-based Methods. Point clouds are first projected to 2D images, e.g., range-view (RV) <ref type="bibr" target="#b2">(Alonso et al., 2020;</ref><ref type="bibr" target="#b12">Cortinhal et al., 2020;</ref>) and bird's-eye-view (BEV) <ref type="bibr" target="#b46">(Zhang et al., 2020b)</ref>, and then processed using 2D convolutions. For example, RangeNet++  adopts a DarkNet <ref type="bibr" target="#b29">(Redmon &amp; Farhadi, 2018)</ref> as the backbone to process RV images, and uses a KNN for post-processing. SqueezeSegV3 , standing on the shoulders of , employs a spatially-adaptive Convolution (SAC) to adopt different filters for different locations according to input RV images. SalsaNext <ref type="bibr" target="#b12">(Cortinhal et al., 2020)</ref> introduces a new context module which consists of a residual dilated convolution stack to fuse receptive fields at various scales. On the other hand, PolarNet <ref type="bibr" target="#b46">(Zhang et al., 2020b</ref>) uses a polar-based birds-eye-view (BEV) instead of the standard 2D Cartesian BEV projections to better model the imbalanced spatial distribution of point clouds.</p><p>Among projection-based methods, applying multi-view projection can leverage rich complementary information <ref type="bibr">(Alnaggar et al., 2021;</ref><ref type="bibr" target="#b7">Chen et al., 2020;</ref><ref type="bibr" target="#b15">Gerdzhev et al., 2021;</ref>, while previous works usually process RV/BEV individually in sequence <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b15">Gerdzhev et al., 2021)</ref> or perform a vanilla late fusion <ref type="bibr">(Alnaggar et al., 2021;</ref>. For example, MVLidarNet <ref type="bibr" target="#b7">(Chen et al., 2020)</ref> first obtains predictions from the RV image, which are then projected to BEV as initial features to learn representation by feature pyramid networks. Differently, TornadoNet <ref type="bibr" target="#b15">(Gerdzhev et al., 2021)</ref> conducts in reverse order by devising a pillar-projection-learning module (PPL) to extract features from BEV, and then placing these features into RV, modeled by an encoder-decoder CNN. On the other side, MPF (Alnaggar et al., 2021) utilizes two different models to separately process RV and BEV, and then combines the predicted softmax probabilities from two branches as final predictions. AMVNet ) takes a further step, i.e., after obtaining the separate predictions from RV and BEV, it adopts a point head <ref type="bibr" target="#b27">(Qi et al., 2017a)</ref> to refine the uncertain predictions, which are defined by the disagreements of two branches. Whereas, our GFNet enables geometric correspondence information to flow between RV/BEV at multi-levels during end-to-end learning, leading to a more discriminative representation and better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first provide an overview of point cloud semantic segmentation and the proposed geometric flow network (GFNet). We then introduce projection-based point cloud segmentation using range-view (RV) and bird's-eye-view (BEV) in detail. After that, we describe the proposed geometric flow module (GFM), including geometric alignment and attention fusion. Lastly, the end-to-end optimization of GFNet is depicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a lidar point cloud with N 3D points P ? R N ?4 , we then have the format of each point as <ref type="bibr">(x, y, z, remission)</ref>, where (x, y, z) is the cartesian coordinate of the point relative to the lidar sensor and remission indicates the intensity of returning laser beam. The goal of point cloud semantic segmentation is to assign all points in P with accurate semantic labels, i.e., Q ? N N . For projection-based point cloud semantic segmentation, we also need to transform the ground truth labels Q to the projected views during training, i.e., Q r for RV and Q b for BEV.</p><p>The overall pipeline of GFNet is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Specifically, a point cloud P is first transformed to range-view (RV) as I r and bird's-eye-view (BEV) as I b using spherical and top-down projections, respectively. We then have two sub-network branches with encoder-decoder architectures to take RV/BEV images as inputs and generate dense predictions, which are referred to as the probability maps for each semantic class. The proposed geometric flow module (GFM) is incorporated into each layer of the decoder, bidirectionally propagating feature information according to the geometric correspondences across two views. After that, we obtain the classification probabilities of all points by applying a grid sampling on the dense probability maps, which is based on the projection relationship between a specific view and the original point cloud, as illustrated in the bottom right corner of <ref type="figure" target="#fig_2">Figure 3</ref>. Inspired by <ref type="bibr" target="#b19">Kochanov et al. (2020)</ref>, we also introduce KPConv <ref type="bibr" target="#b33">(Thomas et al., 2019)</ref> on the top of the proposed GFNet to replace the KNN operation and capture the accurate neighbour information in a learnable way. By doing this, the overall multi-view point cloud semantic segmentation pipeline can be trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-View Projection</head><p>For projection-based methods, a point cloud P ? R N ?4 needs to be transformed to an image I ? R HW ?C first to leverage deep neural networks primarily developed for 2D visual recognition, where H and W indicate the spatial size of projected images and C is the number of channels. Different projections are corresponding to different transformations, i.e., P : R N ?4 ? R HW ?C . In this paper, we adopt two widely-used projected views for point cloud analysis, i.e., range-view (RV) and bird's-eye-view (BEV). As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we aim to learn effective representations from two different views, RV and BEV, using the proposed twobranch networks with an encoder-decoder architecture in each branch. We describe the details of multi-view projection as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range-View (RV).</head><p>To learn effective representations from RV images, spherical projection is required to first project a point cloud P to a 2D RV image . Specifically, we first project a point (x, y, z) from the cartesian space to the spherical space as follows:</p><formula xml:id="formula_1">? ? ? ? r ? ? = ? ? arctan(y, x) arcsin(z/ x 2 + y 2 + z 2 ) x 2 + y 2 + z 2 ? ? ,<label>(1)</label></formula><p>where ?, ?, and r indicate azimuthal angle, polar angle, and radial distance (i.e., the range of each point), respectively. We then have the pixel coordinate of (x, y, z) in the projected 2D range image as</p><formula xml:id="formula_2">u v = (1 ? ?/?)/2 ? W (f up ? ?)/f ? H ,<label>(2)</label></formula><p>where (H, W ) represent the spatial size of range image, and f = f up ? f down is the vertical field-of-view of the lidar sensor. For each projected pixel (u, v) (discretized from ( u, v)), we take the <ref type="bibr">(x, y, z, r, remission)</ref> as its feature, leading to a range image with the size of (H, W, 5). In addition, an improved range-projected method is proposed by Triess et al. <ref type="formula" target="#formula_2">(2020)</ref>, which further unfolds the point clouds following the captured order by the lidar sensor, leading to smoother projected images and a higher valid projection rate 1 . If not otherwise stated, we adopt this improved range projection <ref type="bibr" target="#b34">(Triess et al., 2020)</ref> in all our experiments.</p><p>Bird's-Eye-View (BEV). To learn effective representations from BEV images, top-down orthogonal projection is employed to transform a point cloud into a BEV image <ref type="bibr" target="#b10">(Chen et al., 2017c)</ref>. Furthermore, the polar coordinate system is introduced to replace the cartesian system by <ref type="bibr" target="#b46">Zhang et al. (2020b)</ref>, which can be formulated as follows:</p><formula xml:id="formula_3">u v = x 2 + y 2 cos(arctan(y, x)) x 2 + y 2 sin(arctan(y, x)) = polar(x, y),<label>(3)</label></formula><p>where polar(?) is the coordinate transformation from cartesian system to polar system. Following <ref type="bibr" target="#b46">Zhang et al. (2020b)</ref>, we use nine features to describe each pixel (u, v) (by discretizing <ref type="bibr">( u, v)</ref> to [0, H ? 1] and [0, W ? 1]) in BEV image, including three relative cylindrical distance, three cylindrical coordinates, two cartesian coordinates and one remission, which can be constructed as follows:</p><formula xml:id="formula_4">[?cylindrical(x, y, z), cylindrical(x, y, z), x, y, remission)],<label>(4)</label></formula><p>where cylindrical(x, y, z) = [polar(x, y), z] represents the cylindrical coordinates, ?cylindrical(x, y, z) are the relative distances to the center of the BEV grid, and each BEV image thus has the shape of (H, W, 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Geometric Flow Module</head><p>Intuitively, RV and BEV contain different view information of the original point cloud through different projections, leading to different information loss on different classes. For example, RV is good at those tiny or vertically-extended objects such as motorcycle and person, while BEV is sensitive to those objects with large and discriminative spatial size on the x-y plane. To sufficiently investigate the complementary information from RV/BEV, we explore them from a geometric perspective. Specifically, we devise a geometric flow module (GFM), which is based on the geometric correspondences between RV and BEV, to bidirectionally propagate the complementary information across different views. As illustrated in <ref type="figure">Figure 4</ref>, the first step is referred to as Geometric Alignment, which aligns the feature of source view (RV or BEV) to the target view using their geometric transformation; then the second step is called Attention Fusion, which applies the self-attention and the residual connection to combine the aligned feature representation with the original one. We describe the above-mentioned key steps of the proposed GFM module in detail as follows.</p><p>Geometric Alignment. The key idea lies in the geometric transformation matrices between two views, i.e., T R?B (from RV to BEV) and T B?R (from BEV to RV). To obtain these transformation matrices, we propose to utilize the original point cloud as an intermediary agent. Specifically, from Eq. <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref>, we have the transformation from RV to the point cloud P as follows:</p><formula xml:id="formula_5">T R?P = ? ? ? n 0,0 ? ? ? n 0,Wr?1 . . . . . . . . . n Hr?1,0 ? ? ? n Hr?1,Wr?1 ? ? ? ,<label>(5)</label></formula><p>where T R?P ? Z Hr?Wr , (H r , W r ) are the spatial size of 2D RV image, and {(n i,j )| 0 &lt;= i &lt;= H r ? 1, 0 &lt;= j &lt;= W r ? 1} is the (n i,j ) th point which projects on (i, j) coordinates. Note that if multiple points project to the same pixel, the point with smaller range is kept; If a pixel is not projected by any points, then its n i,j is assigned as ?1. We then have the transformation from P to BEV image according to Eq.(3):  <ref type="figure">Figure 4</ref>: An overview of the proposed geometric flow module (GFM). It contains two main steps, i.e., geometric alignment and attention fusion, which first aligns the feature from the source view (RV or BEV) to the target view using their geometric correspondences, and then applies self-attention and residual connections to combine view-specific features with the flowed information. Note that ? r and ? r share the same architecture but not weights with ? b and ? b , respectively.</p><formula xml:id="formula_6">T P ?B = u 0 ? ? ? u N ?1 T<label>(6)</label></formula><formula xml:id="formula_7">= u 0 ? ? ? u N ?1 v 0 ? ? ? v N ?1 T ,<label>(7)</label></formula><p>where T P ?B ? Z N ?2 , and {u k = (u k , v k )| 0 &lt;= k &lt;= N ? 1} are the projected pixel coordinates of 2D BEV image, corresponding to the k th point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Geometric Flow Module</head><formula xml:id="formula_8">(BEV ? RV) Input: RV feature map M r : [H r , W r , C r ], BEV feature map M b : [H b , W b , C b ], T R?B : [H r , W r , 2]. Output: Fused RV feature map M r f used : [H r , W r , C r ].</formula><p>Step 1: Geometric Alignment</p><formula xml:id="formula_9">? Zero-initializing aligned feature M b?r with the shape of [H r , W r , C b ]; ? foreach (i, j) ? [1 : W r ] ? [1 : H r ] do u = T R?B [i, j] u, v = u M b?r [i, j, :] = M b [u, v, :]</formula><p>Step 2: Attention Fusion Lastly, we calculate the transformed matrix T R?B via T R?P and T P ?B . In particular, for each pixel (i, j) in RV image, we first get its 3D point n i,j = T R?P [i, j], then project n i,j to BEV image to obtain the corresponding pixel T P ?B [n i,j ] = u ni,j . Now, we obtain T R?B ? Z Hr?Wr?2 as:</p><formula xml:id="formula_10">T R?B = ? ? ? u n0,0 ? ? ? u n 0,Wr ?1 . . . . . . . . . u n Hr ?1,0 ? ? ? u n Hr ?1,Wr ?1 ? ? ? ,<label>(8)</label></formula><p>Once obtaining T R?B , we can then align BEV features to RV features as follows: for each location (i, j) in RV image, the (u, v) coordinates in BEV image can be fetched via T R?B <ref type="bibr">[i, j]</ref>, and then we fuse the feature in <ref type="bibr">(u, v)</ref> to (i, j) to get aligned feature F b?r . To align features from RV to BEV, we can operate in a similar way with</p><formula xml:id="formula_11">T B?R ? Z H b ?W b ?2 .</formula><p>Attention Fusion. After the geometric feature alignment, we employ an attention fusion module to obtain the fused feature by concatenating the aligned feature and the target feature, which is followed by two convolution operations ?(?) and ?(?). They have simple architectures "Conv-BN-RELU" and "Conv-BN-Softmax" respectively, where the softmax function in ? is to map values to [0, 1] as attention weights. The attention feature is finally combined with the target feature using a residual connection. We demonstrate the overall process of fusing BEV to RV, including geometric alignment and attention fusion modules, in Algorithm 1. The geometric flow from RV to BEV can be calculated similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>Given Q r as the labels for RV image I r and Q b for BEV image I b , which are projected from the original point cloud label Q, we then have the 2D predictions M r for RV and M b for BEV, respectively. After that, we obtain the 3D predictions via grid sampling and KPConv, i.e., F r for RV and F b for BEV. After fusion, we get the final 3D predictions F f . For simplicity and better illustration, we also highlight all predictions, i.e., M r , M b and F r , F b , F f , in <ref type="figure" target="#fig_2">Figure 3</ref>. To train the proposed GFNet, we first use the loss functions L 2D and L 3D for 2D and 3D predictions, respectively, as follows:</p><formula xml:id="formula_12">L 2D = ? ? L CL (M r , Q r ) + ? ? L CL (M b , Q b ),<label>(9)</label></formula><p>and</p><formula xml:id="formula_13">L 3D = ? ? L CE (F r , Q) + ? ? L CE (F b , Q),<label>(10)</label></formula><p>where L CE indicates the typical cross entropy loss function while L CL is the combination of the cross entropy loss and the Lovasz-Softmax loss <ref type="bibr" target="#b5">(Berman et al., 2018)</ref> with weights 1 : 1. We then apply the cross entropy loss on the final 3D predictions F f , that is, the overall loss function L total can be evaluated as:</p><formula xml:id="formula_14">L total = ? ? L CE (F f , Q) + L 3D + L 2D ,<label>(11)</label></formula><p>where ? . = [?, ?, ?, ?, ?] indicates the weight coefficient of different losses, and we investigate the influences of different loss terms in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the section, we first introduce the adopted SemanticKITTI  and nuScenes <ref type="bibr" target="#b6">(Caesar et al., 2020)</ref> datasets and the mean IoU and accuracy metric for point cloud segmentation. We then provide the implementation details of GFNet, including the network architectures and training settings. After that, we perform extensive experiments to demonstrate the effectiveness of GFM and analyze the influences of different hyper-parameters in GFNet. Lastly, we compare the proposed GFNet with recent state-of-the-art point/projection-based methods to show our superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>SemanticKITTI , derived from the KITTI Vision Benchmark <ref type="bibr" target="#b14">(Geiger et al., 2012)</ref>, provides dense point-wise annotations for semantic segmentation task. The dataset presents 19 challenging classes and contains 43551 lidar scans from 22 sequences collected with a Velodyne HDL-64E lidar, where each scan contains approximately 130k points. Following ; , these 22 sequences are divided into 3 sets, i.e., training set (00 to 10 except 08 with 19130 scans), validation set (08 with 4071 scans) and testing set (11 to 21 with 20351 scans). We perform extensive experiments on the validation set to analyze the proposed method, and also report performance on the test set by submitting the result to the official test server.</p><p>nuScenes <ref type="bibr" target="#b6">(Caesar et al., 2020)</ref> is a large-scale autonomous driving dataset, containing 1000 driving scenes of 20 second length in Boston and Singapore. Specifically, all driving scenes are officially divided into training (850 scenes) and validation set (150 scenes). By merging similar classes and removing rare classes, point cloud semantic segmentation task uses 16 classes, including 10 foreground and 6 background classes. We use the official test server to report the final performance on test set. Evaluation Metrics. Following , we use mean intersection-over-union (mIoU) over all classes as the evaluation metric. Mathematically, the mIoU can be defined as:</p><formula xml:id="formula_15">mIoU = 1 C C c=1 T P c T P c + F P c + F N c ,<label>(12)</label></formula><p>where T P c , F P c , and F N c represent the numbers of true positive, false positive, and false negative predictions for the given class c, respectively, and C is the number of classes. For a comprehensive comparison, we also report the accuracy among all samples, which can be formulated as:</p><formula xml:id="formula_16">Accuracy = T P + T N T P + F P + F N + T N .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For SemanticKITTI, we use two branches to learn representations from RV/BEV in an end-to-end trainable way, where each branch follows an encoder-decoder architecture with a ResNet-34 <ref type="bibr" target="#b16">(He et al., 2016)</ref> as the backbone. The ASPP module <ref type="bibr" target="#b9">(Chen et al., 2017b)</ref> is also used between the encoder and the decoder. The proposed geometric flow module (GFM) is incorporated into each upsampling layer. Note that the elements of T R?B , T B?R fed into GFM are scaled linearly according to the current flowing feature resolution. For RV branch, point clouds are first projected to a range image with the resolution <ref type="bibr">[64,</ref><ref type="bibr">2048]</ref>, which is sequentially upsampled bilinearly to [64 ? 2S, 2048 ? S] where S is a scale factor. During training, a horizontal 1/4 random crop of RV image, i.e., <ref type="bibr">[128S, 512S]</ref>, is used as data augmentation. On the other hand, we adopt polar partition <ref type="bibr" target="#b46">(Zhang et al., 2020b)</ref> for BEV, and use a polar grid size of <ref type="bibr">[480,</ref><ref type="bibr">360,</ref><ref type="bibr">32]</ref> to cover a space of [radius : (3m, 50m), z : (?3m, 1.5m)] relative to the lidar sensor. The grid first goes through a mini PointNet <ref type="bibr" target="#b27">(Qi et al., 2017a)</ref> to obtain the maximum feature activations along the z axis, leading to a reduced resolution <ref type="bibr">[480,</ref><ref type="bibr">360]</ref> for BEV branch. We employ a SGD optimizer with momentum 0.9 and the weight decay 1e ? 4. We use the cosine learning rate schedule <ref type="bibr" target="#b25">(Loshchilov &amp; Hutter, 2016)</ref> with warmup at the first epoch to 0.1. The backbone network is initialized using the pretrained weights from ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. By default, we use ? = [2.0, 2.0, 2.0, 1.0, 1.0] as the loss weight for Eq.11. We train the proposed GFNet for 150 epochs using the batch size 16 on four NVIDIA A100-SXM4-40GB GPUs with AMD EPYC 7742 64-Core Processor.</p><p>For nuScenes, we adopt  to project point clouds to a RV image with the resolution <ref type="bibr">[32,</ref><ref type="bibr">1024]</ref> which is then upsampled bilinearly to <ref type="bibr">[32?3S, 1024?S]</ref> where S = 4 in our experiments. Besides, a polar grid size of <ref type="bibr">[480,</ref><ref type="bibr">360,</ref><ref type="bibr">32]</ref> is used to cover a relative space of [radius : (0m, 50m), z : (?5m, 3m)] for BEV branch. We train the model for total 400 epoch with batch size 56 using 8 NVIDIA A100-SXM4-40GB GPUs under AMD EPYC 7742 64-Core Processor. We adopt cosine learning rate schedule <ref type="bibr" target="#b25">(Loshchilov &amp; Hutter, 2016)</ref> with warmup at the first 10 epoch to 0.2. Other settings are kept the same with SemanticKITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of GFM</head><p>In this part, we show the effectiveness of the proposed geometric flow module (GFM) as well as its influences on each single branch. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we denote the results from F r and F b as RV-Flow and BEV-Flow, respectively, in regard to the information flow between RV and BEV brought by GFM. The predictions from F f (obtained by applying KPConv on the concatenation of F r and F b ) are actually our final results, termed as GFNet. Note that the above results are evaluated using ? = [2, 2, 2, 1, 1] for Eq.11. In addition, we train also each single branch separately without GFM modules, i.e., using ? = [0, 2, 0, 1, 0] and ? = [0, 0, 2, 0, 1] for RV-Single and BEV-Single, respectively.</p><p>We compare the performances of RV/BEV-Single and BEV/BEV-Flow in <ref type="table" target="#tab_1">Table 1</ref>. Specifically, we find that both RV and BEV branches have been improved by a clear margin when incorporating with the proposed GFM module, e.g., 55.7% ? 61.0% for BEV. Intuitively, RV is good at those vertically-extended objects like motorcycle and person, while BEV is sensitive to the classes with large and discriminative spatial size on the x-y plane. For example, RV-Single only achieves 32.4% on truck while BEV-Single obtains 64.8%, which is also illustrated by the first row of <ref type="figure">Figure 5</ref> where RV predicts truck as a mixture of truck, car  <ref type="figure">Figure 5</ref>: Visualization of RV and BEV. The view with the cyan contour helps the one with red. By incorporating both RV and BEV, our GFNet makes more accurate predictions. and other-vehicle, but BEV acts much well. This is partially because truck is more discriminative on x-y plane (captured by BEV) than vertical direction (captured by RV) compared to car, other-vehicle. With the information flow from BEV to RV using GFM, RV-Flow significantly boosts the performance from 32.4% to 69.9%. A similar phenomenon can be observed in the second row of <ref type="figure">Figure 5</ref>, where BEV misclassifies bicyclist as trunk, since both of them are vertically-extended and also very close to each other, while RV predicts precisely. With the help of RV, BEV-Flow dramatically improves the performance from 55.7% to 61.0%. When further applying KPConv on the concatenation of RV/BEV-Flow, the proposed GFNet achieves the best performance 63.0%. These results demonstrate that the proposed GFM can effectively propagate complementary information between RV and BEV to boost the performance of each other, as well as the final performance.   Note that the results of methods with * are obtained from RangeNet++ . From top to down, the methods are grouped into point-based, projection-based and multi-view fusion models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this subsection, we first explore the impacts of attention mechanism in GFM, the loss weights ? defined in Eq.11; and the scale factor S introduced in Sec. 4.2. In the default setting, we use the softmax attention with ? = [2, 2, 2, 1, 1] and S = 3.</p><p>As shown in <ref type="table" target="#tab_2">Table 2a</ref>, without attention mechanism (i.e., no ?(?) and ? in <ref type="figure">Figure 4</ref>), the performance 62.0% is obviously inferior to the counterparts 62.9% and 63.0%, indicating that the attention operation helps to focus on the strengths instead of weaknesses of source view when fusing it into target view. If not otherwise stated, we use the softmax attention in our experiments. We evaluate the influences of ? . <ref type="bibr">= [?, ?, ?, ?, ?]</ref> in <ref type="table" target="#tab_2">Table 2b</ref>, where = 1, = 2, e.g., we have ? . <ref type="bibr">= [?, ?, ?, ?, ?]</ref> = [2.0, 2.0, 2.0, 1.0, 1.0] the configuration d. Specifically, when comparing the configurations a to b and c, we see that that additional supervisions on dense 2D and each branch RV/BEV 3D predictions further improve model performance. When comparing c and d, a large weight on 3D prediction brings a better result. Therefore, if not otherwise stated, we adopt ? = [2.0, 2.0, 2.0, 1.0, 1.0] for remaining experiments. The scale factor S in Sec. 4.2 indicates the resolution of RV image, e.g., when S = 3, we have [128S, 512S] = <ref type="bibr">[384, 1536] and [128S, 2048S] = [383, 6144]</ref> for training and testing, respectively. When comparing d and e in <ref type="table" target="#tab_2">Table 2b</ref>, we find that a higher resolution significantly improves model performance, from 61.7% to 63.0%, further enlarging S from 3 to 4 only brings a slightly better performance. For a better speed-accuracy tradeoff, we use S = 3 in the default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Recent State-of-the-Arts</head><p>SemanticKITTI. For fair comparison with recent methods, we follow the same setting in ; <ref type="bibr" target="#b19">Kochanov et al. (2020)</ref>, i.e., both training and validation splits are used for training when evaluating on the test server. As shown in <ref type="table" target="#tab_3">Table 3</ref>, GFNet achieves the new state-of-the-art performance 65.4% mIoU, significantly surpassing point-based methods (e.g., 58.8% for KPConv <ref type="bibr" target="#b33">(Thomas et al., 2019)</ref>) and single view models (e.g., 59.5% for SalsaNext <ref type="bibr" target="#b12">(Cortinhal et al., 2020)</ref>). For multi-view approaches <ref type="bibr">(Alnaggar et al., 2021;</ref><ref type="bibr" target="#b7">Chen et al., 2020;</ref><ref type="bibr" target="#b15">Gerdzhev et al., 2021;</ref>, GFNet outperforms recent methods <ref type="bibr">Alnaggar et al. (2021)</ref>; <ref type="bibr" target="#b7">Chen et al. (2020);</ref><ref type="bibr" target="#b15">Gerdzhev et al. (2021)</ref> by a large margin. Comparing with AMVNet , GFNet clearly outperforms it on the point-wise accuracy, i.e., 92.4% vs. 91.3%. The superior performance of GFNet shows the effectiveness of bidirectionally aligning and propagating geometric information between RV/BEV. Notably, AMVNet requires to train models for RV/BEV branch as well as their post-processing point head separately, while GFNet is end-to-end trainable. Additionally, since the acquisition frequency of the Velodyne HDL-64E LiDAR sensor (used by SemanticKITTI) is 10 Hz, GFNet can thus run in real-time, i.e., 10 FPS. nuScenes. To evaluate the generalizability of GFNet, we also report the performance on the testset in <ref type="table" target="#tab_5">Table 4</ref> by submitting results to the test server. Similarly, GFNet achieves superior mIoU performance 76.1%, which remarkably outperforms PolarNet <ref type="bibr" target="#b46">(Zhang et al., 2020b)</ref> and tights AMVNet . However, our result 90.4% under Frequency Weighted IoU (FW IoU) beats 89.5% from AMVNet , which is consistent with the accuracy comparison on SemanticKITTI. It also reveals that GFNet performs much better on frequent classes while somewhat struggles on those rare/small classes. Despite the good performance of GFNet, it is also interesting to further improve GFNet by addressing rare/small classes from the perspectives of data sampling/augmentation and loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel geometric flow network (GFNet) to learn effective view representations from RV and BEV. To enable propagating the complementary information across different views, we devise a geometric flow module (GFM) to bidirectionally align and fuse different view representations via geometric correspondences. Additionally, by incorporating grid sampling and KPConv to avoid time-consuming and non-differentiable post-processing, GFNet can be trained in an end-to-end paradigm. Extensive experiments on SemanticKITTI and nuScenes confirm the effectiveness of GFM and demonstrate the new state-of-the-art performance on projection-based point cloud semantic segmentation.</p><p>There are some limitations of the proposed method, since it builds upon two specific point cloud projection methods. Specifically, both RV and BEV may not be applicable to indoor datasets such as S3DIS <ref type="bibr" target="#b3">(Armeni et al., 2016)</ref>. For example, in a indoor scene of the bookcase, common objects such as table and chair are distinguishable and meaningful in the vertical direction, while the height information is missing for BEV. Also, RV image requires a scan cycle by the lidar sensor, which typically appears in outdoor scenarios such as autonomous driving (Please also refer to Appendix. B for more details and figures). Additionally, we apply the proposed geometric flow module in each decoder layer (i.e., the upsampling layers), while popular point cloud object detection frameworks don't have such a decoder structure. Therefore, it is also non-trivial to directly apply the proposed method for object detection, which will be the subject of future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This work is partially supported by ARC project FL-170100117.   <ref type="bibr" target="#b34">Triess et al., 2020)</ref> to generate images of 64 ? 2048 size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RV Projection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train Val Original RV  72.47 72.12 Improved RV (Triess et al., 2020) 83.69 83.51</p><p>We project 3D point cloud P to a 2D RV image with the size of (H, W ): due to the 2D-to-3D ambiguity, there are pixels that are not projected by any points, while multiple points might be projected to the same pixel. We define the valid projection rate as the ratio of valid pixels (i.e., projected by as least one point) comparing to total pixels HW . Specifically, more valid pixels usually result in a smoother projected image, i.e., the higher valid projection rate, the better. We compare two different projection methods <ref type="bibr" target="#b34">Triess et al., 2020)</ref>  In this section, we first described the RV projection (i.e., spherical projection) in detail. We then show the difference between the point clouds collected in outdoor and indoor scenes. Lastly, we discuss why RV projection is not suitable for indoor scenes.</p><p>As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, given a 3D point p, we first obtain the corresponding ?, ? for spherical projection. We then normalize ?, ? to [0, 1] and map them to 2D RV image with size [H, W ] according to Eq. 2. However, point clouds in outdoor and indoor scenes are usually collected in different ways. For example, SemanticKITTI is collected via a Velodyne HDL-64E lidar on the top of the driving car, which launches lasers to all-around (360 ? ) horizontal directions and a certain degree [f down , f up ] vertical directions. When applied RV projection, a meaningful projected cylindrical image can be obtained (please refer to <ref type="figure" target="#fig_0">Figure 1</ref>). But for indoor dataset like S3DIS <ref type="bibr" target="#b3">(Armeni et al., 2016)</ref>, it scans the entire room in any directions with a Matterport <ref type="bibr" target="#b18">(Inc., 2015)</ref> scanner to generate point clouds. In addition, those indoor objects are much more dense than outdoor objects. If we still want to use RV projection, it will lead to a severe distort image. We have also made some attempts using RV projection for S3DIS, but obtained meaningless images as in <ref type="figure" target="#fig_8">Figure 9</ref>. That is also the reason why existing projection-based methods <ref type="bibr" target="#b12">Cortinhal et al., 2020)</ref> only employ RV projection in outdoor lidar-collected point clouds. As for indoor datasets like S3DIS, the mainstream methods <ref type="bibr" target="#b27">(Qi et al., 2017a;</ref><ref type="bibr" target="#b33">Thomas et al., 2019)</ref> usually take raw points as input directly given that their size is much smaller than outdoor scenes.   <ref type="figure" target="#fig_0">Figure 10</ref> illustrates comparisons between GFNet and ground truth on complex scenes, revealing the excellent performance of GFNet. We also provide a GIF image, i.e., figs/vis.gif, at https://github.com/ haibo-qiu/GFNet for more visualizations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Geometric bidirectional transformation diagram between range-view (RV) and bird's-eye-view (BEV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The overview of geometric flow network (GFNet). Point clouds are first projected to range-view (RV) and bird's-eye-view (BEV) using spherical and top-down projections, respectively. Then two branches with the proposed geometric flow module (GFM) handle RV/BEV to generate feature maps (H ? W ? C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Concatenating M r and M b?r along the channel dimension as M concat : [H r , W r , C r + C b ] ? Applying the self-attention module to get M atten = ?(M concat ) ? ?(?(M concat )) with the shape of [H r , W r , C r ] ? Employing residual connection M r f used = M r + M atten</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Range images from original RV and improvedRV (Triess et al., 2020). As we can see,<ref type="bibr" target="#b34">Triess et al. (2020)</ref> obtains smoother projected image than.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>in terms of valid projection rate in 5. As we can observe, Triess et al. (2020) significantly outperforms Milioto et al. (2019) by over 11% in both train and val set. In addition, we visualize the RV images obtained by Milioto et al. (2019); Triess et al. (2020) separately in Figure 6. Obviously, the RV image generated by Triess et al. (2020) is clearly smoother than Milioto et al. (2019). Therefore, we use Triess et al. (2020) in all experiments if not states otherwise.B RV under Indoor and Outdoor Scenesp Range-view projection (i.e., spherical projection). p is a 3D point, ?, ? are the azimuthal angle, polar angle of p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>A simple diagram of the working mechanism when the lidar sensor collects point clouds. The lidar launches lasers to all-around (360 ? ) horizontal directions and a certain degree [f down , f up ] vertical directions. Note that the vertical field of view f = f up ? f down where f down is negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Three visualizations of samples from S3DIS<ref type="bibr" target="#b3">(Armeni et al., 2016)</ref> using RV projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of the predictions from our GFNet comparing to GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons in terms of mIoU to demonstrate the effectiveness of GFM on the validation set of SemanticKITTI.</figDesc><table><row><cell>Method</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell><cell>mIoU</cell></row><row><cell>RV-Single</cell><cell cols="20">93.7 48.7 57.7 32.4 40.5 69.2 79.9 0.0 95.9 53.4 83.9 0.1 89.2 59.0 87.8 66.1 75.3 64.0 45.2 60.1</cell></row><row><cell>RV-Flow</cell><cell cols="20">93.8 45.0 58.8 69.9 31.6 63.6 73.8 0.0 95.6 52.9 83.6 0.3 90.3 62.1 88.0 64.3 75.8 63.2 47.4 61.1</cell></row><row><cell cols="21">BEV-Single 93.6 29.9 42.4 64.8 26.8 48.1 74.0 0.0 94.0 45.9 80.7 1.4 89.2 46.5 86.9 61.4 74.9 56.8 41.6 55.7</cell></row><row><cell>BEV-Flow</cell><cell cols="20">93.7 43.7 61.2 74.0 31.0 61.6 80.6 0.0 95.3 53.1 82.8 0.2 90.8 61.4 88.0 63.1 75.6 58.9 43.1 61.0</cell></row><row><cell>GFNet</cell><cell cols="20">94.2 49.7 63.2 74.9 32.1 69.3 83.2 0.0 95.7 53.8 83.8 0.2 91.2 62.9 88.5 66.1 76.2 64.1 48.3 63.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies of attention in GFM, loss weight coefficient ? and scale factor S on the Se-manticKITTI val set.</figDesc><table><row><cell cols="2">(a) Attention in GFM</cell><cell></cell><cell></cell><cell cols="3">(b) ? and S under</cell><cell cols="2">= 1, = 2</cell><cell></cell></row><row><cell>attention sigmoid softmax</cell><cell>mIoU</cell><cell>cfg a</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>S 3</cell><cell>mIoU 61.7</cell></row><row><cell></cell><cell>62.0</cell><cell>b</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>61.8</cell></row><row><cell></cell><cell>62.9</cell><cell>c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>62.4</cell></row><row><cell></cell><cell>63.0</cell><cell>d</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>63.0</cell></row><row><cell></cell><cell></cell><cell>e</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>61.7</cell></row><row><cell></cell><cell></cell><cell>f</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>63.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons under mIoU, Accuracy and Frame Per Second (FPS) on SemanticKITTI test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on nuScenes (the test set) under mIoU and Frequency Weighted IoU (or FW IoU).</figDesc><table><row><cell>Method</cell><cell>barrier</cell><cell>bicycle</cell><cell>bus</cell><cell>car</cell><cell>const-vehicle</cell><cell>motorcycle</cell><cell>pedestrian</cell><cell>traffic-cone</cell><cell>trailer</cell><cell>truck</cell><cell>dri-surface</cell><cell>other-flat</cell><cell>sidewalk</cell><cell>terrain</cell><cell>manmade</cell><cell>vegetation</cell><cell>mIoU</cell><cell>FW IoU</cell></row><row><cell cols="19">PolarNet (Zhang et al., 2020b) 72.2 16.8 77.0 86.5 51.1 69.7 64.8 54.1 69.7 63.4 96.6 67.1 77.7 72.1 87.1 84.4 69.4 87.4</cell></row><row><cell>AMVNet (Liong et al., 2020)</cell><cell cols="18">79.8 32.4 82.2 86.4 62.5 81.9 75.3 72.3 83.5 65.1 97.4 67.0 78.8 74.6 90.8 87.9 76.1 89.5</cell></row><row><cell>GFNet (ours)</cell><cell cols="18">81.1 31.6 76.0 90.5 60.2 80.7 75.3 71.8 82.5 65.1 97.8 67.0 80.4 76.2 91.8 88.9 76.1 90.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Valid projection rate (%) when using two</cell></row><row><cell>different RV projections</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please refer to Appendix. A for more details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saimir</forename><surname>Eren Erdal Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selcuk</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cavdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="926" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi projection fusion for realtime semantic segmentation of 3d lidar point clouds</title>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<editor>Yara Ali Alnaggar, Mohamed Afifi, Karim Amer, and Mohamed ElHelw</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1800" to="1809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5432" to="5439" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mvlidarnet: Real-time multi-class scene understanding for autonomous driving using multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Oldja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Pehserl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2288" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12547" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing (ISVC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="207" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tornado-net: multiview total variation semantic segmentation with diamond inception module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Gerdzhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Bingbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9543" to="9549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Matterport 3d models of interior spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matterport</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="http://matterport.com/" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2015" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Kprnet: Improving projection-based lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyvid</forename><surname>Kochanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Fatemeh Karimi Nejadasl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Booij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integrate point-cloud segmentation with 3d lidar scan-matching for mobile robot localization and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">237</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for lidar point clouds in autonomous driving: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongpu</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">TNNLS</biblScope>
			<biblScope unit="page" from="3412" to="3432" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi Ngoc Tho</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjai</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang Jie</forename><surname>Chong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive boundary learning for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="8489" to="8499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scan-based semantic segmentation of lidar point clouds: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Larissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Triess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">B</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Marius</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Z?llner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1116" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16024" to="16033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03762</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">2dpass: 2d priors assisted semantic segmentation on lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A novel system for off-line 3d seam extraction and path planning based on point cloud segmentation for arc welding robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzhu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zize</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Computer-Integrated Manufacturing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">101929</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Drinet++: Efficient voxel-as-point point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="644" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9939" to="9948" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
