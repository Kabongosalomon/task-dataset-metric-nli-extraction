<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Rayicer, Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiye</forename><surname>Liu</surname></persName>
							<email>huiyeliu@rayicer.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Rayicer, Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Rayicer, Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Medical Image Segmentation ? Transformers ? Convolutional Neural Networks ? Fusion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation -the prerequisite of numerous clinical needs -has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling operations, leads to redundant deepened networks and loss of localized details. Hence, the segmentation task awaits a better solution to improve the efficiency of modeling global contexts while maintaining a strong grasp of low-level details. In this paper, we propose a novel parallelin-branch architecture, TransFuse, to address this challenge. TransFuse combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner. Besides, a novel fusion technique -BiFusion module is created to efficiently fuse the multi-level features from both branches. Extensive experiments demonstrate that TransFuse achieves the newest state-of-the-art results on both 2D and 3D medical image sets including polyp, skin lesion, hip, and prostate segmentation, with significant parameter decrease and inference speed improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) have attained unparalleled performance in numerous medical image segmentation tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, such as multi-organ segmentation, liver lesion segmentation, brain 3D MRI, etc., as it is proved to be powerful at building hierarchical task-specific feature representation by training the networks end-to-end. Despite the immense success of CNN-based methodologies, its lack of efficiency in capturing global context information remains a challenge. The chance of sensing global information is equaled by the risk of efficiency, because existing works obtain global information by generating very large receptive fields, which requires consecutively down-sampling and stacking These authors contributed equally to this work. arXiv:2102.08005v2 [cs.CV] 10 Jul 2021 convolutional layers until deep enough. This brings several drawbacks: 1) training of very deep nets is affected by the diminishing feature reuse problem <ref type="bibr" target="#b22">[23]</ref>, where low-level features are washed out by consecutive multiplications; 2) local information crucial to dense prediction tasks, e.g., pixel-wise segmentation, is discarded, as the spatial resolution is reduced gradually; 3) training parameterheavy deep nets with small medical image datasets tends to be unstable and easily overfitting. Some studies <ref type="bibr" target="#b28">[29]</ref> use the non-local self-attention mechanism to model global context; however, the computational complexity of these modules typically grows quadratically with respect to spatial size, thus they may only be appropriately applied to low-resolution maps.</p><p>Transformer, originally used to model sequence-to-sequence predictions in NLP tasks <ref type="bibr" target="#b25">[26]</ref>, has recently attracted tremendous interests in the computer vision community. The first purely self-attention based vision transformers (ViT) for image recognition is proposed in <ref type="bibr" target="#b6">[7]</ref>, which obtained competitive results on ImageNet <ref type="bibr" target="#b5">[6]</ref> with the prerequisite of being pretrained on a large external dataset. SETR <ref type="bibr" target="#b31">[32]</ref> replaces the encoders with transformers in the conventional encoderdecoder based networks to successfully achieve state-of-the-art (SOTA) results on the natural image segmentation task. While Transformer is good at modeling global context, it shows limitations in capturing fine-grained details, especially for medical images. We independently find that SETR-like pure transformerbased segmentation network produces unsatisfactory performance, due to lack of spatial inductive-bias in modelling local information (also reported in <ref type="bibr" target="#b3">[4]</ref>).</p><p>To enjoy the benefit of both, efforts have been made on combining CNNs with Transformers, e.g., TransUnet <ref type="bibr" target="#b3">[4]</ref>, which first utilizes CNNs to extract low-level features and then passed through transformers to model global interaction. With skip-connection incorporated, TransUnet sets new records in the CT multi-organ segmentation task. However, past works mainly focus on replacing convolution with transformer layers or stacking the two in a sequential manner. To further unleash the power of CNNs plus Transformers in medical image segmentation, in this paper, we propose a different architecture-TransFuse, which runs shallow CNN-based encoder and transformer-based segmentation network in parallel, followed by our proposed BiFusion module where features from the two branches are fused together to jointly make predictions. TransFuse possesses several advantages: 1) both low-level spatial features and high-level semantic context can be effectively captured; 2) it does not require very deep nets, which alleviates gradient vanishing and feature diminishing reuse problems; 3) it largely improves efficiency on model sizes and inference speed, enabling the deployment at not only cloud but also edge. To the best of our knowledge, TransFuse is the first parallel-in-branch model synthesizing CNN and Transformer. Experiments demonstrate the superior performance against other competing SOTA works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, TransFuse consists of two parallel branches processing information differently: 1) CNN branch, which gradually increases the receptive field and encodes features from local to global; 2) Transformer branch, where it starts with global self-attention and recovers the local details at the end. Features with same resolution extracted from both branches are fed into our proposed BiFusion Module, where self-attention and bilinear Hadamard product are applied to selectively fuse the information. Then, the multi-level fused feature maps are combined to generate the segmentation using gated skip-connection <ref type="bibr" target="#b19">[20]</ref>. There  are two main benefits of the proposed branch-in-parallel approach: firstly, by leveraging the merits of CNNs and Transformers, we argue that TransFuse can capture global information without building very deep nets while preserving sensitivity on low-level context; secondly, our proposed BiFusion module may simultaneously exploit different characteristics of CNNs and Transformers during feature extraction, thus making the fused representation powerful and compact.</p><p>Transformer Branch. The design of Transformer branch follows the typical encoder-decoder architecture. Specifically, the input image x ? R H?W ?3 is first evenly divided into N = H S ? W S patches, where S is typically set to 16. The patches are then flattened and passed into a linear embedding layer with output dimension D 0 , obtaining the raw embedding sequence e ? R N ?D0 . To utilize the spatial prior, a learnable positional embeddings of the same demension is added to e. The resulting embeddings z 0 ? R N ?D0 is the input to Transformer encoder, which contains L layers of multiheaded self-attention (MSA) and Multilayer Perceptron (MLP). We highlight that the self-attention (SA) mechanism, which is the core principal of Transformer, updates the states of each embedded patch by aggregating information globally in every layer:</p><formula xml:id="formula_0">SA(z i ) = softmax q i k T ? D h v,<label>(1)</label></formula><p>where [q, k, v] = zW qkv , W qkv ? R D0?3D h is the projection matrix and vector z i ? R 1?D0 , q i ? R 1?D h are the i th row of z and q, respectively. MSA is an extension of SA that concatenates multiple SAs and projects the latent dimension back to R D0 , and MLP is a stack of dense layers (refer to <ref type="bibr" target="#b6">[7]</ref> for details of MSA and MLP). Layer normalization is applied to the output of the last transformer layer to obtain the encoded sequence z L ? R N ?D0 . For the decoder part, we use progressive upsampling (PUP) method, as in SETR <ref type="bibr" target="#b31">[32]</ref>. Specifically, we first reshape</p><formula xml:id="formula_1">z L back to t 0 ? R H 16 ? W 16 ?D0</formula><p>, which could be viewed as a 2D feature map with D 0 channels. We then use two consecutive standard upsampling-convolution layers to recover the spatial resolution, where we obtain</p><formula xml:id="formula_2">t 1 ? R H 8 ? W 8 ?D1 and t 2 ? R H 4 ? W 4 ?D2</formula><p>, respectively. The feature maps of different scales t 0 , t 1 and t 2 are saved for late fusion with corresponding feature maps of the CNN branch.</p><p>CNN Branch. Traditionally, features are progressively downsampled to H 32 ? W 32 and hundreds of layers are employed in deep CNNs to obtain global context of features, which results in very deep models draining out resources. Considering the benefits brought by Transformers, we remove the last block from the original CNNs pipeline and take advantage of the Transformer branch to obtain global context information instead. This gives us not only a shallower model but also retaining richer local information. For example, ResNet-based models typically have five blocks, each of which downsamples the feature maps by a factor of two. We take the outputs from the 4th</p><formula xml:id="formula_3">(g 0 ? R H 16 ? W 16 ?C0 ), 3rd (g 1 ? R H 8 ? W 8 ?C1 ) and 2nd (g 2 ? R H 4 ? W 4 ?C2</formula><p>) blocks to fuse with the results from Transformer <ref type="figure" target="#fig_1">(Fig. 1</ref>). Moreover, our CNN branch is flexible that any off-the-shelf convolutional network can be applied.</p><p>BiFusion Module. To effectively combine the encoded features from CNNs and Transformers, we propose a new BiFusion module (refer to <ref type="figure" target="#fig_1">Fig. 1</ref>) that incorporates both self-attention and multi-modal fusion mechanisms. Specifically, we obtain the fused feature representation f i , i = 0, 1, 2 by the following operations:</p><formula xml:id="formula_4">t i = ChannelAttn(t i ) b i = Conv(t i W i 1 g i W i 2 )? i = SpatialAttn(g i ) f i = Residual([b i ,t i ,? i ])<label>(2)</label></formula><p>where W i 1 ? R Di?Li , W i 2 ? R Ci?Li , | | is the Hadamard product and Conv is a 3x3 convolution layer. The channel attention is implemented as SE-Block proposed in <ref type="bibr" target="#b9">[10]</ref> to promote global information from the Transformer branch. The spatial attention is adopted from CBAM <ref type="bibr" target="#b29">[30]</ref> block as spatial filters to enhance local details and suppress irrelevant regions, as low-level CNN features could be noisy. The Hadamard product then models the fine-grained interaction between features from the two branches. Finally, the interaction featuresb i and attended featurest i ,? i are concatenated and passed through a Residual block. The resulting feature f i effectively captures both the global and local context for the current spatial resolution. To generate final segmentation, f i s are combined using the attention-gated (AG) skip-connection <ref type="bibr" target="#b19">[20]</ref>, where we havef i+1 = Conv([Up(f i ), AG(f i+1 , Up(f i ))]) andf 0 = f 0 , as in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>Loss Function. The full network is trained end-to-end with the weighted IoU loss and binary cross entropy loss L = L w IoU + L w bce , where boundary pix-els receive larger weights <ref type="bibr" target="#b16">[17]</ref>. Segmentation prediction is generated by a simple head, which directly resizes the input feature maps to the original resolution and applies convolution layers to generate M maps, where M is the number of classes. Following <ref type="bibr" target="#b7">[8]</ref>, We use deep supervision to improve the gradient flow by additionally supervising the transformer branch and the first fusion branch. The final training loss is given by L = ?L G, head(f 2 ) + ?L G, head(t 2 ) + ?L G, head(f 0 ) , where ?, ?, ? are tunnable hyperparameters and G is groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Data Acquisition. To better evaluate the effectiveness of TransFuse, four segmentation tasks with different imaging modalities, disease types, target objects, target sizes, etc. are considered: 1) Polyp Segmentation, where five public polyp datasets are used: Kvasir <ref type="bibr" target="#b13">[14]</ref>, CVC-ClinicDB <ref type="bibr" target="#b1">[2]</ref>, CVC-ColonDB <ref type="bibr" target="#b23">[24]</ref>, EndoScene <ref type="bibr" target="#b26">[27]</ref> and ETIS <ref type="bibr" target="#b20">[21]</ref>. The same split and training setting as described in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>   <ref type="bibr" target="#b21">[22]</ref> are used. The dataset contains multi-modal MRIs from 32 patients, with a median volume shape of 20 ? 320 ? 319. Following the setting in <ref type="bibr" target="#b11">[12]</ref>, we reshape all MRI slices to 320 ? 320, and independently normalize each volume using z-score normalization.</p><p>Implementation Details. TransFuse was built in PyTorch framework <ref type="bibr" target="#b15">[16]</ref> and trained using a single NVIDIA-A100 GPU. The values of ?, ? and ? were set to 0.5, 0.3, 0.2 empirically. Adam optimizer with learning rate of 1e-4 was adopted and all models were trained for 30 epochs as well as batch size of 16, unless otherwise specified. In polyp segmentation experiments, no data augmentation was used except for multi-scale training, as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. For skin lesion and hip segmentation, data augmentation including random rotation, horizontal flip, color jittering, etc. were applied during training. A smaller learning rate of 7e-5 was found useful for skin lesion segmentation. Finally, we follow the nnU-Net framework <ref type="bibr" target="#b11">[12]</ref> to train and evaluate our model on Prostate Segmentation, using the same data augmentation and post-processing scheme. As selected pretrained datasets and branch backbones may affect the performance differently, three variants of TransFuse are provided to 1) better demonstrate the effectiveness as well as flexibility of our approach; 2) conduct fair comparisons with other methods. TransFuse-S is implemented with ResNet-34 (R34) and 8-layer DeiT-Small (DeiT-S) <ref type="bibr" target="#b24">[25]</ref> as backbones of the CNN branch and Transformer branch respectively. Similarly, TransFuse-L is built based on Res2Net-50 and 10-layer DeiT-Base (DeiT-B), while TransFuse-L* uses ResNetV2-50 and ViT-B <ref type="bibr" target="#b6">[7]</ref>. Note that ViTs and DeiTs have the same backbone architecture and they mainly differ in the pre-trained strategy and dataset: the former is trained on ImageNet21k while the latter is trained on ImageNet1k with heavier data augmentation.</p><p>Evaluation Results TransFuse is evaluated on both 2D and 3D datasets to demonstrate the effectiveness. As different medical image segmentation tasks serve different diagnosis or operative purposes, we follow the commonly used evaluation metrics for each of the segmentation tasks to quantitatively analyze the results. Selected visualization results of TransFuse-S are shown in <ref type="figure">Fig. 2</ref>.</p><p>Results of Polyp Segmentation. We first evaluate the performance of our proposed method on polyp segmentation against a variety of SOTA methods, in terms of mean Dice (mDice) and mean Intersection-Over-Union (mIoU). As in Tab. 1, our TransFuse-S/L outperform CNN-based SOTA methods by a large margin. Specifically, TransFuse-S achieves 5.2% average mDice improvement on the unseen datasets (ColonDB, EndoSene and ETIS). Comparing to other transformer-based methods, TransFuse-L* also shows superior learning ability on Kvasir and ClinicDB, observing an increase of 1.3% in mIoU compared to TransUnet. Besides, the efficiency in terms of the number of parameters as well as inference speed is evaluated on an RTX2080Ti with Xeon(R) Gold 5218 CPU. Comparing to prior CNN-based arts, TransFuse-S achieves the best performance while using only 26.3M parameters, about 20% reduction with respect to HarDNet-MSEG (33.3M) and PraNet (32.5M). Moreover, TransFuse-S is able to run at 98.7 FPS, much faster than HarDNet-MSEG (85.3 FPS) and PraNet (63.4 FPS), thanks to our proposed parallel-in-branch design. Similarly, TransFuse-L* not only achieves the best results compared to other Transformerbased methods, but also runs at 45.3 FPS, about 12% faster than TransUnet.</p><p>Results of Skin Lesion Segmentation. The ISBI 2017 challenge ranked methods according to Jaccard Index <ref type="bibr" target="#b4">[5]</ref> on the ISIC 2017 test set. Here, we use Jaccard Index, Dice score and pixel-wise accuracy as evaluation metrics. The <ref type="table">Table 1</ref>: Quantitative results on polyp segmentation datasets compared to previous SOTAs. The results of <ref type="bibr" target="#b3">[4]</ref> is obtained by running the released code and we implement SETR-PUP. '-' means results not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Kvasir ClinicDB ColonDB EndoScene ETIS mDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU U-Net <ref type="bibr" target="#b17">[18]</ref> 0.818 0.746 0.823 0.750 0.512 0.444 0.710 0.627 0.398 0.335 U-Net++ <ref type="bibr" target="#b32">[33]</ref> 0.821 0.743 0.794 0.729 0.483 0.410 0.707 0.624 0.401 0.344 ResUNet++ <ref type="bibr" target="#b12">[13]</ref> 0.813 0.793 0.796 0.796 ------PraNet <ref type="bibr" target="#b7">[8]</ref> 0   comparison results against leading methods are presented in Tab. 2. TransFuse-S is about 1.7% better than the previous SOTA SLSDeep <ref type="bibr" target="#b18">[19]</ref> in Jaccard score, without any pre-or post-processing and converges in less than 1/3 epochs. Besides, our results outperform Unet++ <ref type="bibr" target="#b32">[33]</ref> that employs pretrained R34 as backbone and has comparable number of parameters with TransFuse-S (26.1M vs 26.3M). Again, the results prove the superiority of our proposed architecture.</p><p>Results of Hip Segmentation. Tab. 3 shows our results on hip segmentation task, which involves three human body parts: Pelvis, Left Femur (L-Femur) and Right Femur (R-Femur). Since the contour is more important in dianosis and THA preoperative planning, we use Hausdorff Distance (HD) and Average Surface Distance (ASD) to evaluate the prediction quality. Compared to the two advanced segmentation methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28]</ref>, TransFuse-S performs the best on both metrics and reduces HD significantly (30% compared to HRNetV2 as well as 34% compared to Unet++ on average), indicating that our proposed method is able to capture finer structure and generates more precise contour.</p><p>Results of Prostate Segmentation. We compare TransFuse-S with nnU-Net <ref type="bibr" target="#b11">[12]</ref>, which ranked 1st in the prostate segmentation challenge <ref type="bibr" target="#b21">[22]</ref>. We follow the same preprocessing, training as well as evaluation schemes of the publicly   available nnU-Net framework 5 and report the 5-fold cross validation results in Tab. 4. We can find that TransFuse-S surpasses nnUNet-2d by a large margin (+4.2%) in terms of the mean dice score. Compared to nnUNet-3d, TransFuse-S not only achieves better performance, but also reduces the number of parameters by ?41% and increases the throughput by ?50% (on GTX1080).</p><p>Ablation Study. An ablation study is conducted to evaluate the effectiveness of the parallel-in-branch design as well as BiFusion module by varying design choices of different backbones, compositions and fusion schemes. A seen (Kvasir) and an unseen (ColonDB) datasets from polyp are used, and results are recorded in mean Dice. In Tab. 5, by comparing E.3 against E.1 and E.2, we can see that combining CNN and Transformer leads to better performance. Further, by comparing E.3 against E.5, E.6, we observe that the parallel models perform better than the sequential counterpart. Moreover, we evaluate the performance of a double branch CNN model (E.4) using the same parallel structure and fusion settings with our proposed E.6. We observe that E.6 outperforms E.4 by 2.2% in Kvasir and 18.7% in ColonDB, suggesting that the CNN branch and transformer branch are complementary to each other, leading to better fusion results. Lastly, performance comparison is conducted between another fusion module comprising concatenation followed by a residual block and our proposed BiFusion module (E.5 and E.6). Given the same backbone and composition setting, E.6 with BiFusion achieves better results. Additional experiments conducted on ISIC2017 are presented in Tab. 6 to verify the design choice of BiFusion module, from which we find that each component shows its unique benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polyp Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skin Lesion Segmentation</head><p>Hip Segmentation Prostate Segmentation <ref type="figure">Fig. 2</ref>: Results visualization on all three tasks (best viewed in color). Each row follows the repeating sequence of ground truth (GT) and predictions (Pred).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a novel strategy to combine Transformers and CNNs with late fusion for medical image segmentation. The resulting architecture, TransFuse, leverages the inductive bias of CNNs on modeling spatial correlation and the powerful capability of Transformers on modelling global relationship. TransFuse achieves SOTA performance on a variety of segmentation tasks whilst being highly efficient on both the parameters and inference speed. We hope that this work can bring a new perspective on using transformer-based architecture. In the future, we plan to improve the efficiency of the vanilla transformer layer as well as test TransFuse on other medical-related tasks such as landmark detection and disease classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of TransFuse (best viewed in color): two parallel branches -CNN (bottom right) and transformer (left) fused by our proposed BiFusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.898 0.840 0.899 0.849 0.709 0.640 0.871 0.797 0.628 0.567 HarDNet-MSEG [11] 0.912 0.857 0.932 0.882 0.731 0.660 0.887 0.821 0.677 0.613 TransFuse-S 0.918 0.868 0.918 0.868 0.773 0.696 0.902 0.833 0.733 0.659 TransFuse-L 0.918 0.868 0.934 0.886 0.744 0.676 0.904 0.838 0.737 0.661 SETR-PUP [32] 0.911 0.854 0.934 0.885 0.773 0.690 0.889 0.814 0.726 0.646 TransUnet [4] 0.913 0.857 0.935 0.887 0.781 0.699 0.893 0.824 0.731 0.660 TransFuse-L* 0.920 0.870 0.942 0.897 0.781 0.706 0.894 0.826 0.737 0.663</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on ISIC 2017 test set. Results with backbones use weights pretrained on ImageNet.</figDesc><table><row><cell>Methods</cell><cell cols="3">Backbones Epochs Jaccard Dice Accuracy</cell></row><row><cell>CDNN [31]</cell><cell>-</cell><cell>-</cell><cell>0.765 0.849 0.934</cell></row><row><cell>DDN [15]</cell><cell>ResNet-18</cell><cell>600</cell><cell>0.765 0.866 0.939</cell></row><row><cell>FrCN [1]</cell><cell>VGG16</cell><cell>200</cell><cell>0.771 0.871 0.940</cell></row><row><cell cols="3">DCL-PSI [3] ResNet-101 150</cell><cell>0.777 0.857 0.941</cell></row><row><cell cols="2">SLSDeep [19] ResNet-50</cell><cell>100</cell><cell>0.782 0.878 0.936</cell></row><row><cell cols="2">Unet++ [33] ResNet-34</cell><cell>30</cell><cell>0.775 0.858 0.938</cell></row><row><cell cols="3">TransFuse-S R34+DeiT-S 30</cell><cell>0.795 0.872 0.944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>: Results on in-house hip</cell></row><row><cell cols="2">dataset. All models use pretrained</cell></row><row><cell cols="2">backbones from ImageNet and are</cell></row><row><cell cols="2">of similar size (? 26M). HD and</cell></row><row><cell cols="2">ASD are measured in mm.</cell></row><row><cell>Methods</cell><cell>Pelvis HD ASD HD ASD HD ASD L-Femur R-Femur</cell></row><row><cell cols="2">Unet++ [33] 14.4 1.21 9.33 0.932 5.04 0.813</cell></row><row><cell cols="2">HRNetV2 [28] 14.2 1.13 6.36 0.769 5.98 0.762</cell></row><row><cell cols="2">TransFuse-S 9.81 1.09 4.44 0.767 4.19 0.676</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on prostate MRI segmentation. PZ, TZ stand for the two labeled classes (peripheral and transition zone) and performance (PZ, TZ and mean) is measure by dice score.</figDesc><table><row><cell>Methods</cell><cell>PZ</cell><cell>TZ</cell><cell>Mean Params Throughput</cell></row><row><cell>nnUnet-2d [12]</cell><cell cols="3">0.6285 0.8380 0.7333 29.97M 0.209s/vol</cell></row><row><cell cols="4">nnUnet-3d full[12] 0.6663 0.8410 0.7537 44.80M 0.381s/vol</cell></row><row><cell>TransFuse-S</cell><cell cols="3">0.6738 0.8539 0.7639 26.30M 0.192s/vol</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on parallel-inbranch design. Res: Residual.</figDesc><table><row><cell cols="3">Index Backbones Composition</cell><cell>Fusion</cell><cell>Kvasir ColonDB</cell></row><row><cell>E.1</cell><cell>R34</cell><cell>Sequential</cell><cell>-</cell><cell>0.890 0.645</cell></row><row><cell>E.2</cell><cell>DeiT-S</cell><cell>Sequential</cell><cell>-</cell><cell>0.889 0.727</cell></row><row><cell cols="3">E.3 R34+DeiT-S Sequential</cell><cell>-</cell><cell>0.908 0.749</cell></row><row><cell cols="3">E.4 R34+VGG16 Parallel</cell><cell cols="2">BiFusion 0.896 0.651</cell></row><row><cell cols="2">E.5 R34+DeiT-S</cell><cell cols="3">Parallel Concat+Res 0.912 0.764</cell></row><row><cell cols="2">E.6 R34+DeiT-S</cell><cell>Parallel</cell><cell cols="2">BiFusion 0.918 0.773</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">Ablation study on BiFu-</cell></row><row><cell cols="2">sion module. Res: Residual; TFM:</cell></row><row><cell cols="2">Transformer; Attn: Attention.</cell></row><row><cell>Fusion</cell><cell>Jaccard Dice Accuracy</cell></row><row><cell>Concat+Res</cell><cell>0.778 0.857 0.939</cell></row><row><cell>+CNN Spatial Attn</cell><cell>0.782 0.861 0.941</cell></row><row><cell cols="2">+TFM Channel Attn 0.787 0.865 0.942</cell></row><row><cell>+Dot Product</cell><cell>0.795 0.872 0.944</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Another similar dataset ISIC2018 was not used because of the missing test set annotation, which makes fair comparison between existing works can be hardly achieved. 4 All data are from different patients and with ethics approval, which consists of 267 patients of Avascular Necrosis, 182 patients of Osteoarthritis, 71 patients of Femur Neck Fracture, 33 patients of Pelvis Fracture, 26 patients of Developmental Dysplasia of the Hip and 62 patients of other dieases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/MIC-DKFZ/nnUNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We gratefully thank Weijun Wang, MD, Zhefeng Chen, MD, Chuan He, MD, Zhengyu Xu, Huaikun Xu for serving as our medical advisors on hip segmentation project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation in dermoscopy images via deep full resolution convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Masni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Antari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods and programs in biomedicine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Step-wise integration of deep class-specific learning for dermoscopic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<title level="m">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
		<meeting><address><addrLine>In</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ISBI 2018)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: achievements and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hardnet-mseg: A simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automated design of deep learning methods for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>J?ger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08128</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense deconvolutional network for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Slsdeep: Skin lesion segmentation based on dilated residual and pyramid pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M K</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rashwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09063</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<title level="m">Highway networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of healthcare engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving dermoscopic image segmentation with enhanced convolutional-deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
