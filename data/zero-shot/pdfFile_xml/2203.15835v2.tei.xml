<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACR Loss: Adaptive Coordinate-based Regression Loss for Face Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Pourramezan</forename><surname>Fard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
							<email>mmahoor@du.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACR Loss: Adaptive Coordinate-based Regression Loss for Face Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face Alignment</term>
					<term>Facial Landmark Points detec- tion</term>
					<term>Convolutional Neural Network</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although deep neural networks have achieved reasonable accuracy in solving face alignment, it is still a challenging task, specifically when we deal with facial images, under occlusion, or extreme head poses. Heatmap-based Regression (HBR) and Coordinate-based Regression (CBR) are among the two mainly used methods for face alignment. CBR methods require less computer memory, though their performance is less than HBR methods. In this paper, we propose an Adaptive Coordinatebased Regression (ACR) loss to improve the accuracy of CBR for face alignment. Inspired by the Active Shape Model (ASM), we generate Smooth-Face objects, a set of facial landmark points with less variations compared to the ground truth landmark points. We then introduce a method to estimate the level of difficulty in predicting each landmark point for the network by comparing the distribution of the ground truth landmark points and the corresponding Smooth-Face objects. Our proposed ACR Loss can adaptively modify its curvature and the influence of the loss based on the difficulty level of predicting each landmark point in a face. Accordingly, the ACR Loss guides the network toward challenging points than easier points, which improves the accuracy of the face alignment task. Our extensive evaluation shows the capabilities of the proposed ACR Loss in predicting facial landmark points in various facial images.</p><p>We evaluated our ACR Loss using MobileNetV2, EfficientNet-B0, and EfficientNet-B3 on widely used 300W, and COFW datasets and showed that the performance of face alignment using the ACR Loss is much better than the widely-used L2 loss. Moreover, on the COFW dataset, we achieved state-of-theart accuracy. In addition, on 300W the ACR Loss performance is comparable to the state-of-the-art methods. We also compared the performance of MobileNetV2 trained using the ACR Loss with the lightweight state-of-the-art methods, and we achieved the best accuracy, highlighting the effectiveness of our ACR Loss for face alignment specifically for the lightweight models.</p><p>The code is available on Github</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face alignment, often known as facial landmark points detection, is the process of detecting and estimating the location of predefined key-points in facial images. It is a vital step for many computer vision tasks such as face recognition <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, head pose estimation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, facial expression recognition <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, 3D face reconstruction <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> Despite recent progress in developing algorithms for face alignment, this task has remained challenging especially for real-world applica-</p><formula xml:id="formula_0">Smooth_Face Object 80%</formula><p>Ground-truth Object Face tions, where we deal with face occlusion, scene illumination, and extreme pose variations. Deep face alignment can be categorized into two methods: Coordinate-based Regression (CBR) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>, and Heatmap-based Regression (HBR) <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Although the HBR methods can achieve better accuracy, they are less efficient than CBR methods. To be more specific, while the output of the HBR methods are k heatmap channels (where k is the number of the landmark points), the output of CBR methods are the predicted landmark points, a k?dimensional vector. Thus, while the former requires complex post-processing, no post-processing is needed for the latter method. Moreover, CNNs that can be used as a backbone in CBR methods are mostly more efficient than CNNs that are used for the HBR methods since the models require greater number of model parameters to be capable of creating k heatmap channel accurately.</p><p>Loss functions are an integral part of each deep learning model that can affect the performance of the network dramatically. We proposed a piece-wise loss function called ACR Loss designed to find the most challenging points during the training stage and guide the network to pay more attention to them. Inspired by the Active Shape Model (ASM) <ref type="bibr" target="#b23">[24]</ref>, we model a face object (the ground truth landmark points for a face) using the point-wise mean of all facial landmark points plus the shape-specific variation which is generated using the Eigenvectors of Covariance matrix (see Sec.III). Using more Eigenvectors will decrease the similarity between the mean face object and the ground truth face objects. We use this characteristic and for each face object, we consider the points with fewer similarities to the mean face as the mostchallenging, and the points with the highest similarities as the least-challenging points (see <ref type="figure" target="#fig_0">Fig.1</ref>).</p><p>More clearly, for two different facial landmark points p 1 and p 2 , if the distance between the location of p 1 and its corresponding point in Mean Face object is smaller than the distance between p 2 and its corresponding point in Mean Face object, prediction of the location of p 2 is more challenging than p 1 . Considering this attribute, our proposed ACR Loss adapts its curvature to penalize the network for each landmark point in a face considering the hardness level of its prediction.</p><p>We use our proposed ACR Loss function to train 3 widely used CNNs, MobileNetV2 <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> the challenging 300W dataset <ref type="bibr" target="#b26">[27]</ref>, and the Caltech Occluded Faces in the Wild (COFW) dataset <ref type="bibr" target="#b27">[28]</ref>. Our experimental results show that the accuracy of face alignment using our novel ACR Loss significantly improves the performance of the CBR models compared to the widely used L2 loss. The contributions of this paper are:</p><p>? We propose a method to define the hardness level of each landmark point in a face object. ? We propose ACR Loss, which can adapt its curvature concerning the hardness level of each landmark point in a face. Accordingly, the magnitude and the influence of the loss are defined adaptively according to the hardness level of the landmark points. The remainder of this paper is organized as follows. Sec. II reviews the related work in face alignment. Sec. III describes the proposed ACR Loss and how it improves the accuracy of face alignment task. Sec. IX provides the experimental results, and finally, Sec. VI concludes the paper with some discussions on the proposed method and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Automated facial landmark points detection has been studied extensively by the computer vision community. Templatebased fitting methods such as ASM <ref type="bibr" target="#b23">[24]</ref> and AAM <ref type="bibr" target="#b28">[29]</ref> are among the classical methods, where they aim to constrain the search space by using prior knowledge. Regression-based methods consider a facial image as a vector and use a transformation such as Principle Component Analysis (PCA), Discrete Cosine Transform (DCT) <ref type="bibr" target="#b29">[30]</ref>, or Gabor Wavelet <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> to transform the image into another domain, then a classification algorithm such as Support Vector Machines (SVM) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> or boosted cascade detector <ref type="bibr" target="#b34">[35]</ref> is used to detect facial landmarks.</p><p>CBR methods: Sun et al. <ref type="bibr" target="#b35">[36]</ref> proposed a CNN using the cascaded deep convolutional network for face alignment. A coarse-to-fine auto-encoder networks (CFAN) proposed by Zhang et al. <ref type="bibr" target="#b36">[37]</ref> for real-time face alignment. A cascaded coordinate regression method called the mnemonic descent method (MDM) proposed by Trigeorgis et al. <ref type="bibr" target="#b37">[38]</ref> for facial landmark point detection. Xiao et al. <ref type="bibr" target="#b38">[39]</ref> used a recurrent neural network while fusing the feature extraction and the regression steps and trained the network end-to-end. Two-Stage Re-initialization Deep Regression Model (TSR) <ref type="bibr" target="#b39">[40]</ref> splits a face into several parts to ease the parts variations snd regresses the coordinates of different parts. Valle et al. <ref type="bibr" target="#b40">[41]</ref> proposed a simple CNN for better initialization to Ensemble of Regression Trees (ERT) regressor used for face alignment. Wingloss, introduced by Feng et al. <ref type="bibr" target="#b13">[14]</ref>, is a new loss function capable of overcoming the widely used L2-norm loss in conjunction with a pose-based data balancing (PDB). Zhang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a weakly-supervised framework to detect facial components and landmarks simultaneously. Ning et al. <ref type="bibr" target="#b19">[20]</ref> proposed a new loss function to enhance the convergence of local regions. An attention distillation module proposed by Sadiq et al. <ref type="bibr" target="#b15">[16]</ref> to infer the occlusion probability of each position in high-level features. Zhang et al. <ref type="bibr" target="#b17">[18]</ref> proposed a group-wise face alignment dividing the landmark points into two stages, affine transformations and non-rigid distortions. Fard et al. <ref type="bibr" target="#b5">[6]</ref> proposed a lightweight multi-task network for jointly detecting facial landmark points as well as the estimation face pose. KD-Loss <ref type="bibr" target="#b41">[42]</ref> proposed a knowledge distillation-based architecture for face alignment.</p><p>HBR methods: Yang <ref type="bibr" target="#b42">[43]</ref> proposed a two-part network including a supervised transformation to normalize faces, as well as a stacked hourglass network <ref type="bibr" target="#b43">[44]</ref>, which is designed to predict heatmaps. HRNet <ref type="bibr" target="#b44">[45]</ref>, a high-resolution network applicable in many Computer Vision tasks such as facial landmark detection, achieves a reasonable accuracy. Liu et al. <ref type="bibr" target="#b21">[22]</ref> proposed an attention-guided coarse-to-fine network, which is guided to emphasize key information while suppressing less important information. In addition, Iranmanesh et al. <ref type="bibr" target="#b45">[46]</ref> proposed an approach to provide a robust face alignment algorithm that copes with shape variations by aggregating a set of manipulated images to capture a robust landmark representation. Park et al. <ref type="bibr" target="#b46">[47]</ref> proposed a complementary regression network to combine both global and local regression methods for face alignment. An encoder-decoder CNN et al. <ref type="bibr" target="#b47">[48]</ref> uses a cascade of CNN regressors to increase the accuracy of the face alignment task. Wu et al. <ref type="bibr" target="#b22">[23]</ref> used a lightweight U-Net model and a dynamic optical flow to obtain the optical flow vector of the landmark and uses dynamic routing to improve landmark stabilization. To improve the tolerance of the face alignment toward the occlusion, Park et al. <ref type="bibr" target="#b48">[49]</ref> combined a coordinate regression network and a heatmap regression network with spatial attention.</p><p>Although heatmap-based models achieve better results than coordinate-based models, they perform poorly in terms of inference time, computational complexity, and memory consumption. On the contrary, the coordinate-based models do not require any post-processing operations. Hence, since the efficiency of the model is a key factor in this paper, we train 3 different models one using L2 and one using our proposed ACR Loss. We show in Sec.IX that using ACR Loss improves the performance of face alignment compared to the widely used L2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED ACR LOSS</head><p>In this section, we first discuss the issues with the previously proposed loss functions which are used in order to train the CBR methods, l2, and L1 losses. We then present our proposed ACR Loss and show its effectiveness in improving the performance of CBR face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Issues with the Current CBR Loss Functions</head><p>The loss function and the magnitude of its gradient (aka the influence) of the loss function, are the two attributes that we analyze in our investigation.</p><p>For l2 loss (y = x 2 ), the magnitude of the gradient is |x|. Accordingly, The influence of the loss function depends on the magnitude of the error. Therefore, the influence of the loss function on the network is large for large errors while it is small for small errors. The optimization problem for a single landmark point is solvable using Stochastic Gradient Descent (SGD) or numerical methods. However, when it comes to the optimization of M ? b facial landmark points -M is the number of facial landmark points defined for each input image, and b is the size of the mini-batch-, the accumulative influence of the large errors on the loss function can be more significant than the influence of the small errors. Hence, the loss function might only focus on localizing the landmark points with large errors while ignoring the points with small errors. This characteristic of l2 loss can negatively affect the performance of facial landmark localization and result in an inaccurate prediction.</p><p>For L1 loss (y = |x|), the magnitude of the gradient is 1, indicating that the influence of the loss is agnostic to the magnitude of the error. Although L1 loss does not have the above-mentioned issue of l2 loss, its indifference on the magnitude of the errors affects the optimization problem negatively, meaning that the training of the model requires more steps to converge to the optimal solution.</p><p>To tackle the weaknesses of l2, and L1 loss functions, we propose a piece-wise loss function which is logarithmic (y = ln(|x| 2?? + 1)) for small errors and quadratic for large errors. Moreover, we define ? ? [0, 1] such that it is larger for challenging points compared to points which are easier for the network to be predicted (see Sec.III-B). Accordingly, ? modifies the curvature of the loss function to adapt the loss magnitude and the influence of the loss on the model for each facial landmark point according to its hardness of prediction for the model. Thus, for the challenging points where ? ? 1, the logarithmic piece of the ACR Loss is y ? ln(|x|+1), while for the less challenging ones (? ? 0) it is y = ln(x 2 + 1) (see <ref type="figure" target="#fig_1">Fig 2)</ref>. Typically, for each landmark point that exists in a face, we define the corresponding weight, ?, which is used to adapt the influence of the logarithmic part. Hence, instead of only considering the magnitude of the errors (the distance between the ground truth and the predicted landmark point) to design the loss function, we define a metric to measure how challenging a landmark point is and adapt the loss curvature accordingly.</p><p>The gradient of the logarithmic part for ? ? 1 is y = 1 1+|x| which means the influence of the loss increases as the errors become smaller. Consequently, contrary to l2 loss (where the influence of the loss reduces as the magnitude of the error decreases), the ACR Loss is designed to guide the network to pay more attention to the localization of the challenging points no matter how small the magnitude of the error is. On the contrary, for less challenging landmark points, where ? ? 0, the gradient of the logarithmic part is y = 2|x| 1+x 2 . Therefore, as the error decreases the influence of the loss decreases too. We show in Sec.IX that this characteristic of the ACR Loss enables it to perform more accurately in face alignment tasks compared to the widely used L2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ACR Loss</head><p>Inspired by ASM <ref type="bibr" target="#b28">[29]</ref>, we define a Face object using an M-dimensional vector in the following Eq. 1:</p><formula xml:id="formula_1">Face M?1 ? Mean Face M ?1 + V M ?k b k?1<label>(1)</label></formula><p>where M is the number of facial landmark points defined for each image, Mean Face object is the mean shape object created by calculating the point-wise mean of all facial landmark points in the training set. In addition, V = {v 1 , v 2 , ..., v k } contains k Eigenvectors of the Covariance matrix created from all samples in the training set. In addition, b is a k dimensional vector given by Eq. 2:</p><formula xml:id="formula_2">b k?1 = V k?M (Face M ?1 ? Mean Face M ?1 ) (2)</formula><p>Considering that the statistical variance (i.e., Eigenvalue) of the i th parameter of the vector b is ? i . To make sure the generated shape object after applying ASM (i.e., approximating a face object using Eq. 1) is relatively similar to their corresponding ground truth, the parameter b i of vector b is usually limited to ?3 ? ? i <ref type="bibr" target="#b49">[50]</ref>. Then, we define the Smooth Face object as follows in Eq. 3:</p><formula xml:id="formula_3">Smooth Face M ?1 = Mean Face M ?1 +? M ?k b k?1<label>(3)</label></formula><p>We define l ? {0, 1, ..., k} indicates the number of the Eigenvectors we use to create the Smooth Face object.</p><p>The number of the Eigenvectors that we choose for creating Smooth Face object defines the similarity between the original face object (the ground truth face) and the generated Mean Face object. In other words, if we define the number of the Eigenvectors (or the parameter l in Eq. 1) equal to 0, Smooth Face object will be the same as Mean Face object. Likewise, if we use all the Eigenvectors, the generated Smooth Face object will be the same as the original Face, and thus having the least similarity to Mean Face object.</p><p>Since the variation of Smooth Face object is less than the ground truth face object, -as a rule of thumb-it is easier for the network to learn the distribution of Smooth Face object compared to the distribution of Face. We use this characteristic to propose our ACR Loss. The main idea of the ACR Loss is to find the most challenging landmark points for each input image and penalize the model more by changing the loss curvature adaptively for such points compared to less challenging points. For each input image, we use the distance between each landmark point and the corresponding points in Smooth Face object as a metric that indicates how challenging a landmark point is. For an input image img i , the ground truth face object F ace i , and the corresponding smooth face F ace i , we define ? i,m as follows in Eq. 4:</p><formula xml:id="formula_4">? i,m = |Smooth Face i,m ? Face i,m | max(|Smooth Face i,q ? Face i,q |) ?q ? M<label>(4)</label></formula><p>such that Face i,m is the m th landmark points of the i th sample in the training set. We use ? i,m as a weight indicating how challenging is a landmark point, and accordingly adapt the loss curvature to declare the magnitude and the influence of the loss. Then, we define our ACR Loss in Equations 5, 6, 7:</p><formula xml:id="formula_5">? i,m = |Face i,m ? Pr Face i,m | (5) loss f ace i,m = ? ? ? ? ln(1 + ? 2??i,m i,m ) If : ? i,m ? 1 ? 2 i,m + C If : ? i,m &gt; 1<label>(6)</label></formula><formula xml:id="formula_6">Loss ACR = 1 M N N i=1 M m=1 loss f ace i,m<label>(7)</label></formula><p>where, M and N are the numbers of the landmark points and images in the training set, respectively. P r F ace i,m is the m th landmark point of the i th the predicted face object, and C = ? i,m ln(2) ? 1 is a constant defined to link the quadratic part to the logarithmic part smoothly. In addition, the hyperparameter ? is defined to adjust the ACR Loss curvature (see the ablation study, Sec V-C, for the effect of the different value of ? on the performance of the face alignment task.) As <ref type="figure" target="#fig_1">Fig. 2</ref> depicts, we define the ACR Loss as a piece-wise function to make it cope well with both small and large errors. For the large errors (where ? i,m is greater than 1), we define the ACR Loss to be a quadratic function. Accordingly, the magnitude and the influence of the loss function rely upon the magnitude of the error. For small errors (where ? i,m is lower <ref type="figure">Fig. 3</ref>. Gradient of the ACR Loss function for the small errors where ? ? 1. For ? = 1, the influence of the loss decreases as the error decreases, while for ? = 0, the influence of the loss increases as the error magnitude decreases. than or equal to 1), we modify the loss curvature according to the value of ? i,m , which indicate the hardness level of the prediction of the corresponding point. Accordingly, for the challenging points, the value of ? i,m is close to zero, the magnitude of the loss and its influence decrease as the value of the error decreases. In contrast, for the challenging points, the value of ? i,m is close to 1 and accordingly, the ACR Loss becomes a logarithmic loss function. Consequently, the influence of the loss function increases as the value of the error decreases.</p><p>In addition, in L2 loss the magnitude of the loss value only relies on the prediction error, which is the distance between a ground truth facial landmark point and its corresponding predicted point. Therefore, the greater the prediction error, the greater the loss value. Consequently, L2 loss is less sensitive to small errors compared to the large errors.</p><p>To cope with this issue, for each landmark point P g , we define ? as the distance between the landmark point P s in Smooth Face object and the corresponding landmark point P g in the ground truth set. ? can modify the curvature of the ACR Loss, so the magnitude of the loss is set based on the hardness of the prediction of P g for the network. Consequently, the ACR Loss guides the model to pay more attention to localizing the challenging points which results in accuracy improvement. The number of the Eigenvectors (or the parameter k) defines the similarity between the ground truth face object and the corresponding Smooth Face. In the very first epochs during the training, the predicted face objects are similar to the Mean Face object. Then, gradually the model learns the facespecific features so that the predicted facial landmark point becomes more similar to their corresponding ground-truth points. Accordingly, we increase the number of the Eigenvectors for creating the Smooth Face objects. In other words, as the prediction accuracy of the model increases during the training, we need to add more face-specific variations (see Eq.3) to the generated Smooth Faces to make sure that ? i,m represent the most challenging points. Empirically we choose k to be k = 80% of all the available Eigenvectors from epoch 0 to 15, k = 85% from epoch 16 to 30, k = 90% from epoch 31 to 70, k = 95% from epoch 71 to 100, and k = 97% from epoch 101 to 150. So, in the very first epochs, we update k faster comparing to the last epochs since on the last epochs the network required more effort to predict the facial landmark points more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ACR Loss Optimization</head><p>We use SGD algorithm to optimize our proposed ACR Loss. Based on the Eq. 7 (and as also <ref type="figure" target="#fig_1">Fig. 2 shows)</ref>, ACR Loss is a convex function so that we can use SGD to find the optimal minimum. Therefore, we calculate the gradient of our piecewise ACR Loss in Eq.8:</p><formula xml:id="formula_7">?Loss ACR = ? ? ? ? ?(??2) ? ? +? 2 If : ? ? 1 2 ? If : ? &gt; 1<label>(8)</label></formula><p>Accordingly, for ? &gt; 1 part, the derivative of the loss function is 2 ? which is differentiable on its domain of definition, and SGD can find its optimal minimum. As <ref type="figure">Fig.3</ref> depicts, for ? ? 1 part, the gradient function is ? ?(??2) ? ? +? 2 . Therefore, for ? = 0, the gradient of the loss is ? 2? 1+? 2 , a differentiable function in its domain of definition. Likewise, for ? = 1, the gradient of the loss is ? 1 1+? , which is differentiable in its domain of definition as well. Thus, SGD is capable of finding its optimal minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>For the ease of use, we define MN ACR , EF-0 ACR , and EF-3 ACR which are MobileNetV2 <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and Ground Truth MN base EF0 base EF3 base MN ACR EF0 ACR EF3 ACR <ref type="figure">Fig. 4</ref>. Examples of facial landmark points detection using our proposed ACR loss versus l2 loss on COFW <ref type="bibr" target="#b27">[28]</ref>(first and second rows) and 300W <ref type="bibr" target="#b26">[27]</ref> (third and forth rows) dataset. For each landmark point, if the error rate is more than 0.1, it is considered as a failure and we printed it as red, and otherwise it is green.</p><p>. EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> being trained using our ACR Loss versus the corresponding MN base ,EF-0 base , and EF-3 base being trained using the widely used L2 loss. We uses 300W <ref type="bibr" target="#b26">[27]</ref>, and COFW <ref type="bibr" target="#b27">[28]</ref> dataset for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION DETAILS</head><p>For the training set in each dataset, we cropped all the images and extract the face region. Then the facial images are scaled to 224 ? 224 pixels. We augment the images (in terms of contrast, brightness, and color) to add robustness of data variation to the network. We train networks for about 150 epochs, using the Adam optimizer <ref type="bibr" target="#b50">[51]</ref> with a learning rate of 10 ?3 , ? 1 = 0.9, ? 2 = 0.999, and decay = 10 ?5 on a NVidia 1080Ti GPU. <ref type="table" target="#tab_0">Table I</ref>, for the COFW <ref type="bibr" target="#b27">[28]</ref> dataset, using MobileNetV2 <ref type="bibr" target="#b24">[25]</ref> as the network, the NME and FR reduce 1.15% (from 4.93 to 3.78) and 0.2% (from 0.59% to 0.39%) respectively, and ACU increases about 8.77% from 0.7338 to 0.8215 while we train the model using ACR Loss. Likewise, using EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref> as the backbone network, we see that both NME and FR fell from 4.93% to 4.20% (0.73% reduction), and from 1.18% to 0.59% (0.59% reduction) respectively, and ACU rises about 5.59% from 0.7333 to Method NME FR SFPD <ref type="bibr" target="#b51">[52]</ref> 6.40 -DAC-CSR <ref type="bibr" target="#b52">[53]</ref> 6.03 4.73 FARN <ref type="bibr" target="#b53">[54]</ref> 5.81 -CNN6 (Wing + PDB) <ref type="bibr" target="#b13">[14]</ref> 5.44 3.75 ResNet50 (Wing + PDB) <ref type="bibr" target="#b13">[14]</ref> 5.07 3.16 LAB <ref type="bibr" target="#b18">[19]</ref> 3.92 0.39 ODN <ref type="bibr" target="#b54">[55]</ref> 5.30 -ResNet50-FFLD <ref type="bibr" target="#b16">[17]</ref> 5.32 -ACN <ref type="bibr" target="#b48">[49]</ref> 3.83 -EF-3 ACR (ours) 3.47 0.39 0.7892. As well as that, using EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref>, we also face about 0.24% (from 3.71% to 3.47%) reduction on NME, while AUC increases from 0.8275 to 0.8421, about 1.46%. <ref type="table" target="#tab_0">Table II</ref> shows the results of the state-of-the-art methods as well as EF 3 ACR, the EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> trained using our proposed ACR Loss. As shown in <ref type="table" target="#tab_0">Table II</ref>, the NME of EF 3 ACR is 3.47%, which is the lowest among the state-ofthe-art methods. In addition, the FR of our method is 0.39%, which equals to the FR of LAB <ref type="bibr" target="#b18">[19]</ref> and the lowest as well.  <ref type="table" target="#tab_0">Table I</ref> shows that on the Challenging subset of 300W <ref type="bibr" target="#b26">[27]</ref> dataset, training the networks using ACR Loss results in about 1.16% (from 7.32% to 6.16%), 0.21% (from 6.92% to 6.71%) and 0.65% (from 6.01% to 5.36%) reduction in NME using MobileNetV2 <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> as the network respectively. Similarly, the FR decreases about 11.11% (from 12.59% to 1.48%) for MobileNetV2 <ref type="bibr" target="#b24">[25]</ref>, about 2.96% (from 7.40% to 4.44%) EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, while it remains without any change for EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref>. Moreover, The AUC increase about 11.23% (from 0.4953 to 0.6076), 1.98% (from 0.5228 to 0.5426) and 6.42% (from 0.6196 to 0.6838), using MobileNetV2 <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> as the network respectively and train the models using ACR Loss compared to L2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation on COFW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on 300W</head><p>As <ref type="table" target="#tab_0">Table III</ref> shows, the performance of the EF-3 ACR is comparable to the state-of-the-art methods specifically on the Challenging subset of the 300W <ref type="bibr" target="#b26">[27]</ref> dataset. To be more specific, on the Challenging subset, while the lowest NME, 5.15%, achieved by CHR2c <ref type="bibr" target="#b47">[48]</ref>, EF-3 ACR achieves 5.36%, which is much better than many recently-proposed methods for face alignment. Accordingly, we can conclude that ACR Loss performs much better in localizing the faces under challenging circumstances such as occlusions, extreme pose, and illumination. Moreover, <ref type="table" target="#tab_0">Table IV</ref> presents the NME of the lightweight state-of-the-art models with MN ACR , which is MobileNetV2 <ref type="bibr" target="#b24">[25]</ref> trained using ACR Loss. According to the <ref type="table" target="#tab_0">Table IV,</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>As Fig2 depicts, the parameter ? in Eq 6 can adjust the curvature of the ACR Loss. Typically, by increasing the value of ?, the magnitude of the ACR Loss function increases as well. We conducted 6 experiments to study the effect of the different values of the hyper-parameter ? on the accuracy of the face alignment. In our experiments, we use EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> as the model, and conduct the experiments on 300W <ref type="bibr" target="#b26">[27]</ref> dataset.</p><p>For the Common and Full subsets, defining ? to be in { 1, 2, 3} does not affect the NME of the face alignment too much. To be more detailed, for ? = 1, the NME on the Common subset is 3.59%, while it reduces to 3.51% for ? = 2, and then increases to 3.56% for ? = 3. In contrast, for the Challenging subset, increasing the value of ? from 1 to 4 results in reduction in the NME from 3.56% (for ? = 1) to 5.36% (for ? = 4). Similarly, on the Common and Full subsets the minimum value of the NME achieved by defining ? = 4. Then, we continue increasing the value of ? to be 5 and then 10. As Fig2 shows, the NME goes up on all subsets as we increase the value of ? to be 4. Likewise, increasing ? to 10 worsen the accuracy and the NME goes up to its maximum in the experiment range. Therefore, according to our experiments we define ? = 4 since it results in least NME value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>This paper proposed an adaptive piece-wise loss function called ACR Loss. Comparing the face objects with their corresponding smoothed versions, we define a metric that indicates how challenging the prediction of a landmark point for the CNN is. Thus, for each landmark point, while l2 loss just uses the magnitude of the error to define the magnitude of the loss, ACR Loss adapts its curvature to set the loss NME(%) <ref type="figure">Fig. 5</ref>. Studying the effect of the hyper-parameter ? on the accuracy of the face alignment. We use EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> trained using ACR Loss as the model and 300W <ref type="bibr" target="#b26">[27]</ref> as the dataset. . influence and its magnitude based on the hardness of the landmark points. ACR Loss is capable of guiding the network towards focusing on the more challenging points, having the least similarity to their corresponding Smooth-Face, compared to the less challenging points, having the highest level of similarity to their corresponding Smooth-Face. For further study, we plan to use our proposed loss function in other computer vision tasks such as human body joint tracking.</p><p>VII. SUPPLEMENTARY MATERIALS VIII. DATASETS 300W dataset: We followed the protocol described in <ref type="bibr" target="#b57">[58]</ref> to train our networks on the 300W <ref type="bibr" target="#b26">[27]</ref> dataset. We used 3,148 faces consisting of 2,000 images from the training subset of the HELEN <ref type="bibr" target="#b58">[59]</ref> dataset, 811 images from the training subset of the LFPW <ref type="bibr" target="#b59">[60]</ref> dataset, and 337 images from the full set of the AFW <ref type="bibr" target="#b60">[61]</ref> dataset with a 68-point annotation. For testing, 300W <ref type="bibr" target="#b26">[27]</ref> has three subsets: Common subset with 554 images, Challenging subset with 135 images, and Full subset, including both the Common and Challenging subsets, with 689 images. More specifically, the Challenging subset is the IBUG <ref type="bibr" target="#b26">[27]</ref> dataset, while the Common subset is a combination of the HELEN test subset (330 images) and the LFPW test subset (224 images).</p><p>COFW dataset: This dataset contains facial images with large pose variations as well as heavy occlusions and is known as the most challenging and common issue in the real situation <ref type="bibr" target="#b27">[28]</ref>. The training set contains 1345 faces, while the testing set has 507 faces. Each image in the COFW <ref type="bibr" target="#b27">[28]</ref> dataset has 29 manually annotated landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>We follow the previous work on facial landmark points detection and employ normalized mean error (NME) to measure our model's accuracy (see Equations 9 and 10). We define the normalizing factor, followed by MDM <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b26">[27]</ref> as "inter-ocular" distance (the distance between the outer-eyecorners). We also calculate failure rate (FR), defined as the proportion of failed detected faces, for a maximum error of 0.1. Cumulative Errors Distribution (CED) curvature and the area-under-the-curvature (AUC) <ref type="bibr" target="#b61">[62]</ref> are reported as well.</p><formula xml:id="formula_8">M E j = 1 n n j=1 (P i x , ?G i x ) 2 + (P i y ? G i y ) 2 (9) N M E = 1 m ? d m i=1 M E i<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. ADDITIONAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation on COFW</head><p>In order to study the affect of ACR loss in different models, we provide <ref type="figure">Fig 8 which</ref> depicts the NME of the face alignment using MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> COFW <ref type="bibr" target="#b27">[28]</ref> dataset. According to the <ref type="figure">Fig 8,</ref> on MobileNetV2- <ref type="bibr" target="#b24">[25]</ref> the reduction in the value of the NME (which are 4.93% using L2 loss compared to 3.78% training network using ACR loss) is the greatest, about 1.15%. Using EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, the NME reduction is less, about 0.73%. Likewise, we experience the least reduction on EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, about 0.24%. In fact, we can conclude from <ref type="figure">Fig 8 that</ref> on COFW <ref type="bibr" target="#b27">[28]</ref> dataset, our ACR loss is more effective for a lightweight network such as MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, compared to a more heavyweight network like EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref>.</p><p>Since both NME and FR are sensitive to the outliers, we also depict the CED curvature of the MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> trained once using L2 loss and once using ACR loss. As <ref type="figure">Fig. 6</ref> shows, the generated CED curvature on COFW <ref type="bibr" target="#b27">[28]</ref> indicates that of training the models using ACR loss (MN ACR , EF-0 ACR , and EF-3 ACR ) perform much better than base models (MN ACR , EF-0 ACR , and EF-3 ACR ) indicating the fact that the proposed ACR loss has effectively lead the networks to better learn the face alignment task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on 300W</head><p>According to <ref type="figure">Fig 9,</ref> which depicts the NME of the face alignment using MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> on 300W <ref type="bibr" target="#b26">[27]</ref> dataset, training the lightweight MobileNetV2- <ref type="bibr" target="#b24">[25]</ref> using ACR loss results in the highest reduction in NME compared to the NME reduction taking place for EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref>. This characteristic of ACR loss that it can adaptively change its curvature according to the hardness of the prediction of a landmark point, can be considered more effective for the lightweight models than the heavyweight networks.</p><p>In addition, based on <ref type="figure">Fig. 7</ref> which shows CED curvature of the MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> trained once using L2 loss and once using ACR loss, training the models using ACR loss (MN ACR , EF-0 ACR , and EF-3 ACR ) improves the accuracy of the face alignment task compared to the base models (MN ACR , EF-0 ACR , and EF-3 ACR ), conveying that ACR loss is capable of leading the models to focus more on localizing the challenging points and thus performs much better than the L2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results Evaluation</head><p>In order to investigate the qualitative performance of our proposed ACR loss, we provide some examples of facial landmark detection using MN ACR , EF-0 ACR , EF-3 ACR , and compared to the corresponding networks trained using L2 loss, MN base , EF-0 base , EF-3 base . As both <ref type="figure" target="#fig_0">Fig. 10</ref> (example of face alignment using on 300W <ref type="bibr" target="#b26">[27]</ref> dataset), and <ref type="figure" target="#fig_0">Fig. 11</ref> (examples of facial landmark detection on COFW <ref type="bibr" target="#b27">[28]</ref> dataset) show, the qualitative performance of the models trained with ACR loss are much better than the corresponding models trained with L2 loss, indicating the effectiveness of our proposed ACR loss in improvement of CBR face alignment task. <ref type="figure">Fig. 6</ref>. CED curvature for the MN ACR , EF0 ACR , and EF3 ACR versus MN base , EF0 base , and EF3 base on COFW <ref type="bibr" target="#b27">[28]</ref> dataset . <ref type="figure">Fig. 7</ref>. CED curvature for the MN ACR , EF0 ACR , and EF3 ACR versus MN base , EF0 base , and EF3 base on 300W <ref type="bibr" target="#b26">[27]</ref>   <ref type="figure">Fig. 8</ref>. Comparison of NME (in %) using MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> as the CNN model on COFW <ref type="bibr" target="#b27">[28]</ref> dataset. We train each model two times, once using L2 loss and call the models MN base, EF0 base, and EF3 base, and once using ACR loss and calling the models MN ACR, EF0 ACR, and EF3 ACR.  <ref type="figure">Fig. 9</ref>. Comparison of NME (in %) using MobileNetV2- <ref type="bibr" target="#b24">[25]</ref>, EfficientNet-B0 <ref type="bibr" target="#b25">[26]</ref>, and EfficientNet-B3 <ref type="bibr" target="#b25">[26]</ref> as the CNN model on the 3 subsets of 300W <ref type="bibr" target="#b26">[27]</ref> dataset. We train each model two times, once using L2 loss and call the models MN base, EF0 base, and EF3 base, and once using ACR loss and calling the models MN ACR, EF0 ACR, and EF3 ACR.</p><p>. <ref type="figure" target="#fig_0">Fig. 10</ref>. Examples of facial landmark points detection using our proposed ACR loss versus the L2 loss on 300W <ref type="bibr" target="#b26">[27]</ref> dataset. For each landmark point, if the error rate is more than 0.1, it is considered as a failure and we printed it as red, and otherwise it is green.</p><p>. <ref type="figure" target="#fig_0">Fig. 11</ref>. Examples of facial landmark points detection using our proposed ACR loss versus the L2 loss on COFW <ref type="bibr" target="#b27">[28]</ref> dataset. For each landmark point, if the error rate is more than 0.1, it is considered as a failure and we printed it as red, and otherwise it is green.</p><p>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>We use the difference between the coordinate of a landmark point in Smooth Face and its corresponding Ground truth point to measure how hard is the prediction of that point. Accordingly, the prediction of the Yellow facial landmark point is harder for the model compared to the prediction of the Green one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed ACR Loss for a single landmark point. In each figure, we depicts the ACR Loss curvature for ? as equal to 0, 1, 2. Besides, we depict the ACR Loss curvature for different values of the hyper-parameter ? as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 4 shows some samples of face alignment using ACR Loss compared to l2 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>MN ACR achieves by far the best performance. Fig 4 shows samples of face alignment using ACR Loss comparing to l2 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARING</head><label>I</label><figDesc>NME (IN %), FR (IN %), AND AUC OF LANDMARKS LOCALIZATION USING ACR LOSS AND L2 LOSS ON 300W<ref type="bibr" target="#b26">[27]</ref>, AND COFW<ref type="bibr" target="#b27">[28]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DATASETS.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>NME (?)</cell><cell></cell><cell></cell><cell></cell><cell>FR (?)</cell><cell></cell><cell></cell><cell></cell><cell>AUC (?)</cell><cell></cell></row><row><cell>Method</cell><cell>COFW</cell><cell cols="3">300W Challenging Common Full</cell><cell>COFW</cell><cell cols="3">300W Challenging Common Full</cell><cell>COFW</cell><cell cols="2">300W Challenging Common</cell><cell>Full</cell></row><row><cell>MNbase</cell><cell>4.93</cell><cell>7.32</cell><cell>4.22</cell><cell>4.82</cell><cell>0.59</cell><cell>12.59</cell><cell>0.18</cell><cell cols="2">2.61 0.7338</cell><cell>0.4953</cell><cell>0.7896</cell><cell>0.7319</cell></row><row><cell>MNACR</cell><cell>3.78</cell><cell>6.16</cell><cell>3.81</cell><cell>4.27</cell><cell>0.39</cell><cell>1.48</cell><cell>0.00</cell><cell cols="2">0.29 0.8215</cell><cell>0.6076</cell><cell>0.8197</cell><cell>0.7781</cell></row><row><cell>EF-0base</cell><cell>4.93</cell><cell>6.92</cell><cell>4.67</cell><cell>5.11</cell><cell>1.18</cell><cell>7.40</cell><cell>0.18</cell><cell cols="2">1.59 0.7333</cell><cell>0.5228</cell><cell>0.7535</cell><cell>0.7083</cell></row><row><cell>EF-0ACR</cell><cell>4.20</cell><cell>6.71</cell><cell>4.38</cell><cell>4.83</cell><cell>0.59</cell><cell>4.44</cell><cell>0.18</cell><cell cols="2">1.01 0.7892</cell><cell>0.5426</cell><cell>0.7785</cell><cell>0.7323</cell></row><row><cell>EF-3base</cell><cell>3.71</cell><cell>6.01</cell><cell>3.81</cell><cell>4.24</cell><cell>0.39</cell><cell>1.48</cell><cell>0.00</cell><cell cols="2">0.29 0.8275</cell><cell>0.6196</cell><cell>0.8215</cell><cell>0.7820</cell></row><row><cell>EF-3ACR</cell><cell>3.47</cell><cell>5.36</cell><cell>3.36</cell><cell>3.75</cell><cell>0.39</cell><cell>1.48</cell><cell>0.00</cell><cell cols="2">0.29 0.8421</cell><cell>0.6838</cell><cell>0.8504</cell><cell>0.8177</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II NME</head><label>II</label><figDesc>(IN %) AND FR (IN %) OF 28-POINT LANDMARKS LOCALIZATION ON COFW [28] DATASET.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="5">NME (IN %) OF 68-POINT LANDMARKS LOCALIZATION ON 300W [27]</cell></row><row><cell></cell><cell cols="2">DATASET.</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">NME Common Challenging Fullset</cell></row><row><cell>FARN [54]</cell><cell>4.23</cell><cell></cell><cell>7.53</cell><cell>4.88</cell></row><row><cell>SAN [56]</cell><cell>3.34</cell><cell></cell><cell>6.60</cell><cell>3.98</cell></row><row><cell>LAB [19]</cell><cell>2.98</cell><cell></cell><cell>5.19</cell><cell>3.49</cell></row><row><cell>ODN [55]</cell><cell>3.56</cell><cell></cell><cell>6.67</cell><cell>4.17</cell></row><row><cell>HORNet [57]</cell><cell>3.38</cell><cell></cell><cell>6.36</cell><cell>3.96</cell></row><row><cell>CHR2c [48]</cell><cell>2.85</cell><cell></cell><cell>5.15</cell><cell>3.30</cell></row><row><cell>mnv2KD [42]</cell><cell>3.56</cell><cell></cell><cell>6.13</cell><cell>4.06</cell></row><row><cell>EF-3ACR (ours)</cell><cell>3.36</cell><cell></cell><cell>5.36</cell><cell>3.75</cell></row><row><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="5">COMPARISON OF THE NME (IN %) OF LIGHTWEIGHT MODELS IN</cell></row><row><cell cols="5">LANDMARKS LOCALIZATION ON 300W [27] DATASET.</cell></row><row><cell>Method</cell><cell cols="4">NME Common Challenging Fullset</cell></row><row><cell></cell><cell></cell><cell cols="3">inter-ocular normalization</cell></row><row><cell>res loss [20]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>4.93</cell></row><row><cell>ASMNet [6]</cell><cell></cell><cell>4.82</cell><cell>8.2</cell><cell>5.50</cell></row><row><cell cols="2">MobileNet+ASMLoss [6]</cell><cell>3.88</cell><cell>7.35</cell><cell>4.59</cell></row><row><cell>MNACR(ours)</cell><cell></cell><cell>3.81</cell><cell>6.16</cell><cell>4.27</cell></row><row><cell></cell><cell></cell><cell cols="3">inter-pupil normalization</cell></row><row><cell>DOF [23]</cell><cell></cell><cell>4.86</cell><cell>9.13</cell><cell>5.55</cell></row><row><cell>MNACR(ours)</cell><cell></cell><cell>5.32</cell><cell>8.94</cell><cell>6.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>dataset .</figDesc><table><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NME(%)</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MN_base</cell><cell>MN_ACR</cell><cell>EF0_base</cell><cell>EF0_ACR</cell><cell>EF3_base</cell><cell>EF3_ACR</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="26" to="756" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of local feature methods for 3d face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boufama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition based on facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pintavirooj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 10th Biomedical Engineering International Conference (BMEiCON)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition by distance and slope between facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?zseven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>D?genci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Artificial Intelligence and Data Processing Symposium (IDAP)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Head pose estimation in the wild assisted by facial landmarks based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ieee Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="48" to="470" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Asmnet: A lightweight deep neural network for face alignment and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1521" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial micro-expression recognition using two-dimensional landmark feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="121" to="549" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial expression recognition from 3d facial landmarks reconstructed from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kalapala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kharwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expression recognition using hybrid features of pixel and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="18" to="876" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d face reconstruction with geometry details from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4756" to="4770" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d face reconstruction from single 2d image using distinctive features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="180" to="681" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial componentlandmark detection with weakly-supervised lr-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10" to="263" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facial landmark detection via attention-adaptive deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained facial landmark detection exploiting intermediate feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phutane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Naturel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blanc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page">103036</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phased groupwise face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="62" to="415" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A cpu real-time face alignment for mobile platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="8834" to="8843" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6971" to="6981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-guided coarse-to-fine network for 2d face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="97" to="196" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design of a facial landmark detection system using a dynamic optical flow approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="68" to="737" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Active shape models with invariant optimal features (iof-asm) application to cardiac mri segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boisrobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Cardiology</title>
		<imprint>
			<biblScope unit="page" from="633" to="636" />
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks. arxiv 2019</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Statistical models of appearance for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust facial landmarking for registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales des T?l?communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A face recognition system based on automatically determined facial fiducial points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="432" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully automatic facial feature point detection using gabor feature based boosted classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vukadinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1692" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Independent component analysis and support vector machine for face feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Svm based asm for facial landmarks location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 8th IEEE International Conference on Computer and Information Technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3317" to="3326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A deeplyinitialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial landmark points detection using knowledge distillation-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="page">103316</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via aggregation on geometrically manipulated faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A complementary regression network for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">103883</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cascade of encoderdecoder cnns with learned coordinates regressor for robust facial landmarks detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="326" to="332" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Acn: Occlusion-tolerant face alignment by attentional combination of heterogeneous regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">107761</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An introduction to active shape models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="223" to="248" />
		</imprint>
	</monogr>
	<note>Image processing and analysis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3471" to="3480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2481" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Face alignment recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="448" to="458" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via occlusion-adaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3486" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Heterogenous output regression network for direct face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">107311</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">An empirical study of recent face alignment methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05049</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
