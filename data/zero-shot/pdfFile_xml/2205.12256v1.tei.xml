<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Dynamics for Articulated 3d Human Motion Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>G?rtner</surname></persName>
							<email>erik.gartner@math.lth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Lund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<email>mykhayloa@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
							<email>erwincoumans@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>sminchisescu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Dynamics for Articulated 3d Human Motion Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce DiffPhy, a differentiable physics-based model for articulated 3d human motion reconstruction from video. Applications of physics-based reasoning in human motion analysis have so far been limited, both by the complexity of constructing adequate physical models of articulated human motion, and by the formidable challenges of performing stable and efficient inference with physics in the loop. We jointly address such modeling and inference challenges by proposing an approach that combines a physically plausible body representation with anatomical joint limits, a differentiable physics simulator, and optimization techniques that ensure good performance and robustness to suboptimal local optima. In contrast to several recent methods <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b58">56]</ref>, our approach readily supports full-body contact including interactions with objects in the scene. Most importantly, our model connects endto-end with images, thus supporting direct gradient-based physics optimization by means of image-based loss functions. We validate the model by demonstrating that it can accurately reconstruct physically plausible 3d human motion from monocular video, both on public benchmarks with available 3d ground-truth, and on videos from the internet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We seek to contribute to the development of physicsbased methodology as one of the building blocks in constructing accurate and robust 3d visual human sensing systems. Incorporating the laws of physics into the visual reasoning process is appealing as it promotes the plausibility of estimated motion and facilitates more efficient use of training examples <ref type="bibr" target="#b8">[9]</ref>. We focus on articulated human motion as an epitome of a real-world prediction task that is both well studied and challenging. Existing state-of-the-art approaches demonstrate relatively high accuracy in terms of joint position estimation metrics <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b65">63]</ref>. However, predictions can sometimes be physically implausible, even for simple motions such as walking and running. For instance, estimates can include unreasonably abrupt transitions in world space, or artifacts such as foot skating or non-equilibrium states <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b45">43]</ref>. Many methods are typically trained on large motion capture datasets and encounter difficulties when tested on motions not well represented in those training sets. Arguably, imposing some form of physicsbased generally valid prior on the articulated motion estimates should greatly improve the plausibility of results.</p><p>However, physics-based reasoning comes at the cost of substantial modeling and inference complexity. Typically, physics-based articulated estimation methods rely on rigid body dynamics (RBD) <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b47">45]</ref>, a formulation that introduces many auxiliary variables corresponding to forces acting at the body joints at each time step. Moreover, physical contact results in non-smooth effects where small changes to model parameters might result in substantially different motions. Therefore inferring physics variables given the inherent uncertainty in monocular video, and under contact discontinuities, becomes significantly difficult, algorithmically and computationally. Despite such challenges, a number of recent methods successfully apply physicsbased constraints for articulated human motion estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b62">60]</ref>. One possibility to cope with modeling complexity, explored in recent work, is to simplify the physics and model contacts only between the body and the feet <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b58">56]</ref>. Others use auxiliary external forces applied at the body to compensate for modeling error <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b62">60]</ref>.</p><p>In this paper, we aim to broaden the methodology for physics-based articulated human motion estimation. Specifically, we demonstrate that we can successfully leverage recent progress in differentiable simulation <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b55">53]</ref> in order to incorporate physics-based constraints into the articulated 3d human motion reconstruction. Our approach, DiffPhy, relies on gradient-based optimization, connects end-to-end with images, and does not require simplifying assumptions on contacts or the introduction of external nonphysical residual forces.  <ref type="figure">Figure 1</ref>. Overview of DiffPhy. Given kinematic estimates (described in ?3.1) of a subject's body shape ?, the body's initial pose and velocity s0, and time-varying 3d posesq0:T with detected 2d keypoints, our model reconstructs the motion in physical simulation, by minimizing a differentiable loss L (see ?3.5). DiffPhy optimizes the control trajectoryq0:T containing joint angle targets to PD controllers (cf . <ref type="bibr" target="#b3">(4)</ref>). In turn, the PD controllers compute a torque vector ? , which actuates motors in the joints of the simulated body. DiffPhy integrates a full-featured differentiable simulator, TDS <ref type="bibr" target="#b19">[17]</ref> (described in ?3.2), that supports complex contacts. Each subject is represented by means of a personalised physical model (see ?3. <ref type="bibr" target="#b2">3</ref>). In addition, we optimize the initial state (see ?3.6), which makes DiffPhy robust to low quality initial estimates. The outputs are 3d pose estimates that align with visual evidence and respect physical constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Kinematics-based 3d Human Pose Estimation. The problem of monocular 3d pose estimation is usually addressed through end-to-end <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b33">31,</ref><ref type="bibr" target="#b64">62]</ref>, or two-stage <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">18]</ref> models where neural networks are used to predict 3d joint positions. This is an ill-posed problem due to depth ambiguities and occlusion. The networks are usually trained on vast pose datasets <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b53">51]</ref> which usually supports good performance on poses previously observed during training. Several methods <ref type="bibr" target="#b26">[24,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b65">63]</ref> directly regress the parameters of statistical body models <ref type="bibr" target="#b29">[27,</ref><ref type="bibr" target="#b59">57]</ref> (rather than 3d joint positions), including the subject's body shape as well as kinematic pose. The methods mentioned above take a purely visual inference approach to the problem and do not consider physics-based constraints. As observed by <ref type="bibr" target="#b42">[40]</ref>, this may cause artifacts such as jitter, ground-penetration, foot sliding, or unnatural leaning <ref type="bibr" target="#b45">[43]</ref>.</p><p>Physics-based 3d Human Pose Estimation. Recent work <ref type="bibr">[15, 28, 40-43, 56, 60]</ref> aims to increase realism, by using physics to regularize reconstruction. This aims to enforce physical constraints such as proper contact and dynamic coherence. In <ref type="bibr" target="#b42">[40]</ref> motion is reconstructed through optimization, but the method only accounts for collisions between the feet and the ground. Such simplifications are recurring in current approaches and limit the types of motions that can be reconstructed. In contrast, in this work, we use a full-featured physical simulator which supports contacts between all objects in the scene. PhysCap <ref type="bibr" target="#b45">[43]</ref> is a real-time optimization-based approach, where feet contact is pre-detected based on a neural network. During the physics-based inference, contacts are considered fixed and thherefore cannot be corrected or improved. Moreover, following <ref type="bibr" target="#b61">[59]</ref> the method uses non-physical "residual forces" which improve 3d joint reconstruction metrics at the cost of altered physical plausibility. Since we aim to increase the physicality of reconstructed motions, we avoid using any residual forces. <ref type="bibr" target="#b62">[60]</ref> follows on <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b56">54]</ref> to learn a neural network that estimates torques to drive a model in the fullfeatured physical simulator MuJoCo <ref type="bibr" target="#b50">[48]</ref>. However, Mu-JoCo is non-differentiable, hence the need to resort to expensive training using numerical gradients in a reinforcement learning setting. The method is trained for millions of steps using 3d ground-truth labels from a motion capture dataset, but the method's ability to generalize to in-thewild is not demonstrated. Similarly to <ref type="bibr" target="#b42">[40]</ref>, the method assumes a known ground plane, whereas DiffPhy estimates it. <ref type="bibr" target="#b44">[42]</ref> integrates a simplified physics approach, dubbed "physionical", into a neural network that estimates joint torques and ground-reaction forces. Similarly to <ref type="bibr" target="#b45">[43]</ref> they detect foot contact using a neural network predictor rather than by means of physical simulation. Most recently, <ref type="bibr" target="#b58">[56]</ref> introduced a method relying on a simplified physical formulation that makes it possible to refine 3D pose estimates well enough to train motion synthesis models based on that output. However, the method assumes a known ground plane, models only foot contact, and implements a simplified physical body scaled solely based on the estimated bone length rather than shape estimates. Finally, in our concurrent work <ref type="bibr" target="#b17">[15]</ref>, we perform physics-based human pose reconstruction of complex motions through trajectory optimization based on CMA-ES <ref type="bibr" target="#b18">[16]</ref> in the non-differentiable simulator Bullet <ref type="bibr" target="#b6">[7]</ref>. This general approach uses a mature and full-featured simulator which, while capable, is slow due to costly black-box optimization. The method does not optimize the initial state of the body (see ?3.6) together with the joint control variables, being more vulnerable to unfavorable initialisation. In summary, this work takes the novel approach of tightly integrating physics into the reconstruction process through a full-featured differentiable physics model. As a result, DiffPhy supports complex full-body contacts, connects pixels-to-physics using end-to-end differentiable losses, supports personalised body models, does not resort to residual forces, and is robust to poor initialization. See tab. 1 for an overview of physics-based methods. It is worth mentioning that, aside from physical simulation, there exist many other approaches to grounding the human pose estimates using e.g., inertial estimates from IMUs <ref type="bibr" target="#b60">[58]</ref>, scene constraints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b66">64]</ref>, and motion priors <ref type="bibr" target="#b41">[39]</ref>. Differentiable Physics for Human Modeling. Physical simulation is a mature area with several established simulation engines available <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b50">48]</ref>. These engines implement forward simulation but do not facilitate the computation of derivatives necessary for efficient gradient-based optimization. These simulators are well-suited for training with gradient-free methods such as reinforcement-learning or evolutionary algorithms and have been used for gradientfree optimization of human motion models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b62">60]</ref>. More recently differentiable physics simulators have emerged <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">14,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b55">53]</ref>. Applying these to human motion reconstruction is difficult due to noisy gradients <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b35">33]</ref>, and a non-convex objective function. We present a methodology using gradient-based local search with stochastic global optimization enabling the first use of a full-featured differentiable physics model <ref type="bibr" target="#b19">[17]</ref> for human pose reconstruction from video. Furthermore, we show that our approach is magnitudes faster than a purely sampling-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section presents our approach to reconstructing 3d human shape and motion from video with differentiable physics in the loop. Given a monocular video of a human subject, we use a kinematic neural network to estimate 2d body keypoints, the body shape, and 3d body poses. Since estimating 3d pose from monocular video is ill-posed, due to e.g. depth-ambiguities and occlusion <ref type="bibr" target="#b46">[44]</ref>, the kinematic 3d reconstructions may suffer from self-penetration, inconsistent translation, jitters, floating above the ground, and non-physical leaning <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b45">43]</ref>. We, therefore, reconstruct the motion in physical simulation, by jointly accounting for both visual evidence and the constraints of physical simulation (e.g. collisions, gravity, and Newton's laws of motion). See <ref type="figure">fig. 1</ref> for an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Kinematic Initialization</head><p>Given a sequence of monocular images {I i }, we assume a pinhole camera with intrinsics i = [f x , f y , c x , c y ] and constant camera extrinsics. We obtain the visual evidence used in our optimization objectives following the procedure introduced in <ref type="bibr" target="#b17">[15]</ref>. This relies on HUND <ref type="bibr" target="#b63">[61]</ref>, a 3d pose estimator that produces per-frame 2d keypointsx i with confidence scores c i , 3d body poses ? i , and 3d body shape ? i , where ? and ? are the GHUM <ref type="bibr" target="#b59">[57]</ref> posing and shape parameters, respectively.</p><p>Since HUND is a per-frame estimator, a temporally consistent shape is recovered by selecting the N = 5 highestscoring frames according to keypoint confidences. For these frames, HUND image losses <ref type="bibr" target="#b63">[61]</ref> are minimized using BFGS under the additional constraint of a constant shape, ?, across all frames. In addition, <ref type="bibr" target="#b17">[15]</ref> introduces a final round of optimization where poses are updated under the time-consistent body shape and a temporal smoothness loss to reduce jittering.</p><p>Finally, as the ground plane location is not assumed to be known and HUND produces estimates in camera space k, we estimate the global transform T g ? R 3?4 for the physical scene, with gravity along the y axis, as well as the ground plane at y = 0. This is achieved by minimizing</p><formula xml:id="formula_0">L g (T g ) = N i ? min ?, L y T g [M(?, ? i ), 1] ? 2 , (1)</formula><p>where L y is an operator that extracts the k = 20 smallest signed distances from the mesh vertices M(?, ? i ) after the global transformation. This assumes the body is in ground plane contact for most of the sequence. To allow for frames where the subject is not in contact with the ground, we clip the maximum shortest distance to the ground to ? = 20 cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Differentiable Physics Simulation Model</head><p>We implement our models in the framework of the "Tiny Differentiable Simulator" (TDS) <ref type="bibr" target="#b19">[17]</ref>. This formulates rigid-body dynamics for articulated bodies in terms of reduced coordinates. Elements in the vector q represent the position of each joint, and elements in the vectorq represent joint space velocities, based on revolute and spherical joints. Given the state of the body ? s t = (q t ,q t ) at time t, as well as the vector of joint torques ? t , and external forces f t , the computation shown in <ref type="figure" target="#fig_3">fig. 3</ref> produces a new body state ? s t+?t corresponding to the rigid multi-body dynamics with contacts. To that end, we first run forward kinematics to compute world space positions and velocities, as well as forward dynamics to compute unconstrained acceleration obtained without taking contacts into account. The forward dynamics computes the acceleration by solving the equation of motion for the kinematic tree given by</p><formula xml:id="formula_1">? t = H(q t )q t + C(q t ,q t , f x t ) (2)</formula><p>where H(q) is the joint-space inertia matrix, C the a joint space bias force and f x is the vector of external forces. The forward dynamics is computed by propagation-based Articulated-Body Algorithm (ABA) <ref type="bibr" target="#b12">[11]</ref> that traverses the kinematic chain of the body three times in order to compute quantities necessary to finally obtain the acceleration of each rigid component of the body <ref type="bibr" target="#b0">1</ref> . The joint-space inertia matrix is computed using the Composite Rigid Body Algorithm (CRBA) <ref type="bibr" target="#b12">[11]</ref>. Unconstrained accelerationsq u t+?t are then used to compute unconstrained velocities, which in conjunction with the output of the forward kinematics x t+?t are used to update the contact points between the articulated body and the environment. Contact points with positive (separating) distance are classified as inactive, while contact points with zero or negative distance are active. Active contacts generate a repulsive impulse that needs to be taken into account when computing the new body state. To that end, the forward dynamics computation is phrased as a linear complementarity problem (LCP) at the velocity level <ref type="bibr" target="#b48">[46,</ref><ref type="bibr" target="#b49">47]</ref> </p><formula xml:id="formula_2">J c H ?1 J ? c p + J c? = v (3) v = [v u , v b ] s.t. v ? u p u = 0 v u ? 0 p u ? 0 v b = 0</formula><p>where J c is a contact Jacobian for the positions of contact points computed in the previous step, p is the vector of reaction impulses, and v is the vector of relative velocities. The indices u and b indicate the unilateral and bilateral portion of constraints, respectively. The LCP problem in (3) is then iteratively solved with a projected Gauss-Seidel method following the formulation in <ref type="bibr" target="#b49">[47]</ref>, by relying on a 1 See tab. 7.1 in <ref type="bibr" target="#b12">[11]</ref> for the Articulated-Body Algorithm.</p><p>per-contact LCP <ref type="bibr" target="#b22">[20]</ref>. The final step of the computation is to obtain joint positions q t+?t from joint velocities using semi-implicit Euler integration. </p><formula xml:id="formula_3">U = " &gt; A A A C D X i c b V C 7 S g N B F J 3 1 G e M r a m k z G I W A E H Y F 0 T J o Y x n B P C A b l 9 n Z W R 2 c f T h z V w j D + g F p / A h / w M Z C E V t 7 O 2 t / x N k k h a 8 D F w 7 n 3 M u 9 9 / i p 4 A p s + 8 O a m p 6 Z n Z s v L Z Q X l 5 Z X V i t r 6 2 2 V Z J K y F k 1 E I r s + U U z w m L W A g 2 D d V D I S + Y J 1 / K v j w u / c M K l 4 E p / B I G X 9 i F z E P O S U g J G 8 y r Z 2 I w K X f q j d I E h A X + d 5 7 m n Y d Q M m g G D I z 3 W W e 5 W q X b d H w H + J M y H V R q 3 2 O b x V 9 0 2 v 8 u 4 G C c 0 i F g M V R K m e Y 6 f Q 1 0 Q C p 4 L l Z T d T L C X 0 i l y w n q E x i Z j q 6 9 E 3 O d 4 x S o D D R J q K A Y / U 7 x O a R E o N I t 9 0 F q e r 3 1 4 h / u f 1 M g g P + 5 r H a Q Y s p u N F Y S Y w J L i I B g d c M g p i Y A i h k p t b M b 0 k k l A w A Z Z N C M 7 v l / + S 9 l 7 d 2 a / b p y a N I z R G C W 2 i L V R D D j p A D X S C m q i F K B q i B / S E n q 0 7 6 9 F 6 s V 7 H r V P W Z G Y D / Y D 1 9 g U m A q C N &lt; / l a t e x i t &gt; q u t+ t</formula><p>Compute contact points and contact Jacobian &lt; l a t e x i t s h a 1 _ b a s e </p><formula xml:id="formula_4">6 4 = " g T o 4 O h Y T T u M 8 1 X 8 S t K 3 H Q 4 L 1 Z T A = " &gt; A A A C A n i c b V C 7 S g N B F J 2 N r x h f U S u x G Q 1 C R A i 7 g m g Z t L G M Y B 6 Q X Z b Z y W w y Z P b h z F 0 h L I u N v V 9 h Y 6 G I r Z W f Y O e H 2 D t 5 F J p 4 4 M L h n H u 5 9 x 4 v F l y B a X 4 Z u b n 5 h c W l / H J h Z X V t f a O 4 u d V Q U S I p q 9 N I R L L l E c U E D 1 k d O A j W i i U j g S d Y 0 + t f D P 3 m L Z O K R + E 1 D G L m B K Q b c p 9 T A l p y i z u p H R D o e X 5 6 k 2 V u C k d 2 h w k g G D K 3 W D I r 5 g h 4 l l g T U q r u l b 8 / H u z D m l v 8 t D s R T Q I W A h V E q b Z l x u C k R A K n g m U F O 1 E s J r R P u q y t a U g C p p x 0 9 E K G D 7 T S w X 4 k d Y W A R + r v i Z Q E S g 0 C T 3 c O 7 1 X T 3 l D 8 z 2 s n 4 J 8 5 K Q / j B F h I x 4 v 8 R G C I 8 D A P 3 O G S U R A D T Q i V X N + K a Y 9 I Q k G n V t A h W N M v z 5 L G c c U 6 q Z h X O o 1 z N E Y e 7 a J 9 V E Y W O k V V d I l q q I 4 o u k O P 6 B m 9 G P f G k / F q v I 1 b c 8 Z k Z h v 9 g f H + A 2 + c m x g = &lt; / l a t e x i t &gt; q t+ t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 0 g 5 1 f t 9 9 o t I h V M v t C 2 e c a k 4 E 5 M = " &gt; A A A C A n i c b V C 7 S g N B F J 2 N r x h f q 1 Z i M x q E i B B 2 B d E y a G M Z w T w g u 4 T Z y W w y Z P b B z F 0 x L M H G 3 q + w s V D E 1 s p P s P N D 7 J 1 N U m j i g Q u H c + 7 l 3 n u 8 W H A F l v V l 5 O b m F x a X 8 s u F l d W 1 9 Q 1 z c 6 u u o k R S V q O R i G T T I 4 o J H r I a c B C s G U t G A k + w h t e / y P z G D Z O K R + E 1 D G L m B q Q b c p 9 T A l p q m z t O Q K D n + e n t s J 0 C P s J O h w k g G I Z t s 2 i V r R H w L L E n p F j Z K 3 1 / P D i H 1 b b 5 6 X Q i m g Q s B C q I U i 3 b i s F N i Q R O B R s W n E S x m N A + 6 b K W p i E J m H L T 0 Q t D f K C V D v Y j q S s E P F J / T 6 Q k U G o Q e L o z O 1 h N e 5 n 4 n 9 d K w D 9 z U x 7 G C b C Q j h f 5 i c A Q 4 S w P 3 O G S U R A D T Q i V X N + K a Y 9 I Q k G n V t A h 2 N M v z 5 L 6 c d k + K V t X O o 1 z N E Y e 7 a J 9 V E I 2 O k U V d I m q q I Y o u k O P 6 B m 9 G P f G k / F q v I 1 b c 8 Z k Z h v 9 g f H + A 1 t W m m c = &lt; / l a t e x i t &gt; x t+ t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 g v O 5 f L h d i + L p n 5 B A V P m z 8 D p d v 8 = " &gt; A A A C B 3 i c b V C 7 S g N B F J 2 N r x h f U U t B R o M Q E c K u I F o G b S w j m A d k l 2 V 2 M p s M m X 0 w c 1 c M y 3 Y 2 V v 6 H j Y U i t v o J d n 6 I v Z N H o Y k H B g 7 n 3 H v n 3 u P F g i s w z S 8 j N z e / s L i U X y 6 s r K 6 t b x Q 3 t x o q S i R l d R q J S L Y 8 o p j g I a s D B 8 F a s W Q k 8 A R r e v 2 L o d + 8 Y V L x K L y G Q c y c g H R D 7 n N K Q E t u c T e 1 A w I 9 z 0 / t T g T 4 N s v c F I 7 s D h N A M G R u s W R W z B H w L L E m p F T d K 3 9 / P N i H N b f 4 q e f Q J G A h U E G U a l t m D E 5 K J H A q W F a w E 8 V i Q v u k y 9 q a h i R g y k l H d 2 T 4 Q C s d 7 E d S v x D w S P 3 d k Z J A q U H g 6 c r h 0 m r a G 4 r / e e 0 E / D M n 5 W G c A A v p + C M / E R g i P A w F d 7 h k F M R A E 0 I l 1 7 t i 2 i O S U N D R F X Q I 1 v T J s 6 R x X L F O K u a V T u M c j Z F H O 2 g f l Z G F T l E V X a I a q i O K 7 t A j e k Y v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Physical Human Body Modeling</head><p>In the physical simulation, we model the human body as rigid geometric primitives connected by joints. The model is comprised of 16 joints with a total of 48 degrees of freedom, joining together 26 capsules that represent the various body parts (cf . <ref type="figure">fig. 1</ref>). The shape and mass of the model is automatically adapted for various body shapes by relying on a statistical body model <ref type="bibr" target="#b59">[57]</ref>. Given the 3d mesh M(?, ?) corresponding to a shape estimate ? in rest pose, we infer the dimensions of the geometric primitives following the approach of <ref type="bibr" target="#b1">[2]</ref>. The process is entirely automatic and yields individualized physical models for each subject. As a physical model requires mass, we first estimate the total mass of the body based on a human shape dataset <ref type="bibr" target="#b38">[36]</ref> then distribute the weight according to an anatomical distribution <ref type="bibr" target="#b39">[37]</ref>. Finally, the inertia of each primitive is computed based on its mass and dimensions.</p><p>DiffPhy reconstructs a motion in simulation by actuating torque motors in the joints of the body. Following prior work <ref type="bibr" target="#b0">[1]</ref> we optimize over control targets to proportionalderivative (PD) controllers rather than over the torques directly. We define the body's angular joint positions as q t , and joint velocities asq t , the associated 3d Cartesian coordinates of the joints as x t for the time step t. Given a set of joint targetsq 1:T = {q 1 ,q 2 , . . . ,q t } the PD controllers infer the joint torques as</p><formula xml:id="formula_5">? t = k p (q t ? q t ) ? k dqt ,<label>(4)</label></formula><p>where k p and k d are gain parameters of PD controllers. We may then specify a motion of length T as the initial state s 0 = (q 0 ,q 0 ), the world geometry G defining the position and orientation of the ground plane, and a target trajectory for the jointsq 1:T . Given the loss presented in (5) we reconstruct the motion by minimizing L = L(s 0 , G,q 1:T ) with respect toq 1:T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Gradient-Based Optimization</head><p>Given our loss function L = L(S 0 , G,q 1:T ) we can use any gradient-based optimization method to minimize the  <ref type="table">Table 2</ref>. Comparison of optimization strategies on our Hu-man3.6M validation set. BFGS and Basin-BFGS both use gradients, while CMA-ES is a gradient-free approach. Note that Basin-Hopping together with BFGS (Basin-BFGS) improves the performance of BFGS by combining it with stochastic global optimization. Using only a purely sampling-based approach (CMA-ES) requires magnitudes more function evaluations while still not finding better optima for our loss. BFGS was given a sufficiently large evaluation budget to converge.</p><p>loss with respect toq 1:T . Since the loss function is nonconvex, convergence to suboptimal local minima is possible. Therefore, a global optimization combined with a local gradient-based search is expected to outperform a purely local method. One such method is the global stochastic optimization Basin-Hopping <ref type="bibr" target="#b54">[52]</ref>. It uses a two-stage approach, which alternates between performing gradient-based local search and stochastic global search. Based on an initial candidate, it first performs a local search. It then randomly perturbs the local minimum, performs a local search again on the new candidate, and then either accepts or rejects the new solution based on the Metropolis criterion <ref type="bibr" target="#b34">[32]</ref>. In our model, we use BFGS <ref type="bibr" target="#b15">[13]</ref> for local optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization Objectives</head><p>Reconstructing a motion sequence amounts to finding the control trajectoryq 1:T that minimizes the reconstruction loss L under the constraints of the simulation dynamics. In this work, we formulate L as a weighted combination of loss functions</p><formula xml:id="formula_6">L = w r L r + w j L j + w i L i + w l L l ,<label>(5)</label></formula><p>with the weights w r = 10.0, w j = 0.1, w i = 0.01, and w l = 0.01. The root position loss L r measures errors between the 3d position of the simulated pelvis root joint x root t and the kinematically estimated positionx root</p><formula xml:id="formula_7">t L r (q 1:T ) = 1 T T t ?x root t ? x root t ? 2<label>(6)</label></formula><p>at time t where T is the total length of the sequence. L j computes the rotational distance between the kinematic pose estimate and the simulated body's pose</p><formula xml:id="formula_8">L j (q 1:T ) = 1 T K T t K k arccos(|q k t ?q k t |),<label>(7)</label></formula><p>whereq k t and q k t are rotations expressed as quaternions for joint k at time t for kinematics and the simulated character respectively. Note the difference betweenq and q, where the former are the PD control targets and the latter are the joint angles of the simulated model (cf . (4)). L i computes the 2d projection loss</p><formula xml:id="formula_9">L i (q 1:T ) = 1 T K T t K k c k t ?x k t ? ?(x k t , i)? 2 ,<label>(8)</label></formula><p>where ?(x k t , i) is the perspective operator projecting the simulated model's joint x k t onto the image with camera intrinsics i weighted by the keypoint detection confidence score c k t . Finally, L l is a regularizer that penalizes joints outside of human anatomical limits as present in the statistical body model <ref type="bibr" target="#b59">[57]</ref> L l (q 1:</p><formula xml:id="formula_10">T ) = 1 T K T t K k ? max(z k lower ? q k t , 0) + max(q k t ? z k upper , 0)? 2 ,<label>(9)</label></formula><p>where z k upper and z k lower are upper and lower bounds for joint k respectively.</p><p>Note that in the above definitions, the positions of body joints angles q k t and 3d joint positions x k t are dependent on the control trajectory up until time t, as part of the physics formulation introduced in ?3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Optimized Initialization</head><p>We initialize the pose q 0 in the first time step of the simulation to the kinematically estimated poseq 0 and estimate the velocityq 0 using finite differences between the first two kinematic poses {q 0 ,q 1 }. However, if the initial kinematic pose estimate is poor, this might lead to a low quality starting pose from which the simulation cannot recover. Similarly, jitters in the kinematic poses may cause a significant error in the estimated initial velocity. We address these issues by including the initial pose and velocity as variables to optimize. We experimentally validate how such a relatively straightforward approach significantly impacts the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We quantitatively evaluate DiffPhy on the Hu-man3.6M <ref type="bibr" target="#b23">[21]</ref>, and a subset of the AIST <ref type="bibr" target="#b51">[49]</ref> pose datasets. The former contains a diverse set of motions from a motion capture laboratory, whereas the latter contains dance videos with triangulated 3d joints as pseudo-ground-truth. As only DiffPhy and SimPoe <ref type="bibr" target="#b62">[60]</ref> supports full-body contacts (cf . tab. 1), PhysCap <ref type="bibr" target="#b45">[43]</ref> proposed evaluating on a subset of the Human3.6M. This protocol eliminated all sequences requiring more than foot-floor contacts. Hence to allow for comparison, we use this subset in tab. 3, but note that our method is more general and supports contacts for all body parts. For ablations, we use 100 frames from 20 sequences   <ref type="table">Table 3</ref>. Quantitative evaluation on the Human3.6M and AIST datasets. Our full dynamic model improves over the kinematic estimates used as initialization with respect to standard joint position error metrics as well as reducing motion jitter and unnatural foot skating.</p><p>from a validation subset of Human3.6M. Finally, we quantitatively evaluate our method on real-world internet videos released under creative commons licenses. For additional details, refer to our supplementary material.</p><p>Metrics. We report the standard pose metrics such as mean per-joint position error in millimeter (MPJPE-G), mean Procrustes aligned joint error (MPJPE-PA), per-frame translation aligned error (MPJPE), and 2d mean per-joint error in pixels (MPJPE-2d). Note that many papers do not report global position errors since they consider only rootrelative poses. We, however, are interested in measuring the pose error, including translational errors, since unnatural translation is a common (non-physical) reconstruction artifact. In addition, we also measure foot skating and the total variation in the joint acceleration per frame (TV). We measure foot skating as percentage of frames where a foot moves more than 2cm while in contact with the ground in two adjacent frames. Unlike <ref type="bibr" target="#b42">[40]</ref>, we do not assume foot contact annotations but instead heuristically detect foot contacts based on the distance between the foot mesh and the ground-plane. The total variation in acceleration is computed as 1 T t?T k?K |? k t+1 ?? k t |, for the 3d acceleration? k t of joint k at time t estimated using finite differences. Thus, high TV indicates motion jitter, and high foot skate implies motion that slides along the ground. Implementation Details. We use the Tiny Differentiable Simulator <ref type="bibr" target="#b19">[17]</ref> running at 1, 000 Hz with the gradients computed using the auto differentiation framework CppAD <ref type="bibr" target="#b2">[3]</ref>.</p><p>In addition, we use a Python implementation of Basin-Hopping and BFGS <ref type="bibr" target="#b52">[50]</ref>. Since the length of the optimized trajectory may be great, we follow <ref type="bibr" target="#b0">[1]</ref> and perform optimization in overlapping windows of length N = 960. The simulation steps take ? 5s. For the large datasets in tab. 3, we compute the windows in parallel and stitch them together in order to speed up computation. We initialize the control targetsq 1:T to 3d poses estimated by our kinematics. See our supplementary material for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>We compare DiffPhy against both state-of-the-art kinematic video models (VIBE <ref type="bibr" target="#b26">[24]</ref>) and against physics-based methods. The results are summarized in tab. 3. Since VIBE predicts root-relative poses, we estimate the global translation (required to compute MPJPE-G) by minimizing 2d projection errors using a method similar to the one in <ref type="bibr" target="#b45">[43]</ref>. For VIBE, we use the publicly available implementation. For the other methods, we give numbers presented by the authors. On both Human3.6M and AIST, our model improves with respect to the physical metrics (TV and foot skate) compared to the kinematic initialization. On Hu-man3.6M, foot skating is only 7.4% compared to 47.5% for the kinematic initialization and 27.4% for VIBE. On AIST, foot skating is reduced from 50.9% to 19.6%. We believe that increased skating on AIST is due to actual skating motions performed as part of the hip-hop dances. On total variation, our model similarly improves over kinematics with 0.20 and 0.44 on Human3.6M and AIST, respectively. Fur- thermore, we note that our full model improves the global joint position error (MPJPE-G), a metric that measures pose and translation errors. On Human3.6M, DiffPhy has an error of 139.1 compared to 145.3 and 207.7 mm/joint for kinematics and VIBE, respectively. If we look at the error for foot joints only, we see an even larger improvement by including physics compared to kinematics alone (166.8 vs. 174.1 mm/joint). This result aligns with prior work <ref type="bibr" target="#b42">[40]</ref>, showing that physics improves foot position estimation. Furthermore, our method aligns well with image evidence when comparing 2d error, i.e., <ref type="bibr" target="#b15">13</ref>  <ref type="bibr" target="#b44">[42]</ref> on the other hand, focus only on feetground contacts while DiffPhy supports complex contacts. Unfortunately, this advantage cannot be demonstrated on subsets that exclude sequences with complex contacts. <ref type="figure" target="#fig_5">Fig. 5</ref> presents qualitative results where kinematics fails to estimate the positions of legs due to depth ambiguities. The reconstructed poses align well when projected into the image but are unrealistic since the model skates rather than walks forward. Since DiffPhy reconstructs the motion with physics in the loop, it must propel the model forward through bipedal locomotion, thus inferring feasible leg poses. Similarly, in <ref type="figure" target="#fig_4">fig. 4</ref> kinematics estimates a pose that projects well into the image. However, when viewed from a side, it becomes clear that kinematics estimates a pose that leans unnaturally. Since DiffPhy is constrained by gravity, it must find a pose that is both physically plausible and aligns with 2d evidence. <ref type="figure" target="#fig_4">Fig. 4</ref> also includes examples of object interactions and rolling motions requiring complex contacts. We manually modeled the chair as a box since DiffPhy does not estimate scene geometry. For the rolling motion, the kinematics were too noisy for DiffPhy to converge; hence, we manually corrected the worst kinematic frames before running DiffPhy. Finally, <ref type="figure" target="#fig_1">fig. 2</ref> shows two reconstruction examples for sequences in-the-wild. These videos exhibit poses and activities missing from standard laboratory-captured datasets.</p><p>Ablation studies. In tab. 6 we validate our choice of loss components in <ref type="bibr" target="#b4">(5)</ref>. We note that the 2d projection loss, as expected, plays an important role in aligning the reconstruction with the image evidence (17.1 vs. 12.6 px/joint). Furthermore, since 2d keypoints do not suffer from depth ambiguities, they are generally more reliable than 3d keypoints and thus serve as a strong signal. Therefore removing 2d evidence significantly increases MPJPE-G from 144.9 to 158.5 mm/joint. Removing the root position loss (6) has the largest impact on global position error (165.7 mm/joint) since without it, we do not provide DiffPhy with any supervision with respect to world positioning. This allows for suboptimal reconstructions that align well with the projected image (12.8 px/joint) but do not transition correctly in world space. Without the joint angle loss <ref type="bibr" target="#b6">(7)</ref>, DiffPhy is deprived of the per-frame 3d pose estimates, which, when predicted by neural networks such as HUND or VIBE that are trained on large pose datasets, provide useful guidance as long as their predictions do not contradict any physical constraints. Removing the joint angle limit regularizer (cf . (9)) demonstrates the usefulness of constraining the reconstructed motion to the space of anatomically valid poses even for everyday motions like those in Human3.6M. Finally, we validate the usefulness of optimizing the initial starting pose and velocity (see ?3.6). Without it, the kinematic estimates for the initial frames must be accurate. If not, the simulation may start from an initial state from which DiffPhy may fail to recover, as seen by the largest MPJPE-PA in the ablation of 65.1 mm/joint. Next, results in tab. 2 show that gradient-based methods are vastly more efficient for our physics loss compared to the commonly used gradient-free approach CMA-ES <ref type="bibr" target="#b18">[16]</ref>. BFGS obtains a lower MPJPE-G error (160.1 vs 206.7 mm/joint), and requires a fraction of the computations (122 vs. 80k loss evaluations per windows). Next, We note that BFGS converges to suboptimal minima, but by combining BFGS with Basin-Hopping, we can reduce the errors further to 144.9 mm/joint. As Basin-Hopping can explore infinitely  <ref type="table">Table 5</ref>. Results on the effects of optimization window size. A balance needs to be found between a larger window size which allow for more visual evidence to be taken into account while a smaller reduces the dimensionality of the search space.</p><p>many basins, we set the limit to 5 basin steps, each with 50 BFGS iterations as a trade-off between accuracy and speed.</p><p>In tab. 5 we study the effect of the optimization window size. We find that a window of 960 simulation steps (containing 0.96s of video) is optimal for our setup. A larger window size increases the errors, most likely due to a larger search space combined with a larger gradient variance, as noted in <ref type="bibr" target="#b35">[33]</ref>. On the other hand, smaller windows provide scarcer visual evidence and are sensitive to a few occluded frames, or to noisy estimates. Interestingly, a smaller window size performed better for experiments on groundtruth data (see supplementary material). This indicates that smaller apertures are better for noise-free inputs.</p><p>Several methods (cf . tab. 1) introduce "residual forces" acting on the root link of the physical body. This nonphysical force allows the method to translate and rotate the body to align with visual evidence at the expense of physical realism. tab. 4 confirms that this indeed can be used to lower DiffPhy's joint errors (MPJPE-G from 144.9 to 140.2 mm/joint and 2d error from 12.6 to 11.6 px/joint when applying 50N for each of the six degrees of freedom). Interestingly, applying a too great residual force (100N) increased error, perhaps since it allows the model to circumvent some of the constraints of physical simulation. In this work, we avoid using residual forces, in order to keep all forces realistic, and avoid non-physical artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In order to improve the realism of 3d human sensing, we have introduced DiffPhy -the first differentiable physicsbased model for full-body articulated human motion estimation, that supports complex contacts, does not assume a  <ref type="table">Table 6</ref>. Ablation of the model components introduced in ?3. No root means without root position loss <ref type="bibr" target="#b5">(6)</ref>, No 2d without 2d keypoint loss <ref type="bibr" target="#b7">(8)</ref>, No pose without joint angle loss <ref type="bibr" target="#b6">(7)</ref>, No 3d loss without both root link position loss and joint angles losses, No limit without anatomical joint limits <ref type="bibr" target="#b8">(9)</ref>, and No init. opt. is without optimizing the initial state, cf . ?3.6.</p><p>known ground plane, and avoids reliance on non-physical forces. This has the benefit of a human model with realistic physics interactions, that are constrained end-to-end by visual losses. Furthermore, such a model can provide a valuable non-learning-based component, which is always valid, complementing the statistical kinematic prediction and optimization techniques prevalent in the current state of the art. Visual 3d human motion reconstruction experiments on multiple datasets demonstrate that our methodology is competitive with other state of the art physics-based approaches.</p><p>Limitations and Future Work. An inherent limitation to physics-based approaches is the need to model objects in the scene. We hope to address this challenge in future work by integrating with 3d scene reconstruction techniques <ref type="bibr" target="#b3">[4]</ref>. Ideally, we would be able to jointly optimize the control of the body and the world to match visual evidence. Another limitation is our current assumption of constant camera extrinsics. This limits our technique to videos captured using a static camera but can be easily relaxed. Finally, our reconstructions are limited to a single subject. Reconstructing multiple people interacting is interesting since these scenes are complex, and learning statistical models of interaction between humans is challenging <ref type="bibr" target="#b14">[12]</ref>. A physics-based approach could help infer constraints and affordances. Ethical Considerations. Our construction of physicsbased models is motivated by the breadth of transformative 3d applications that would become possible, including fitness, personal well-being or special effects, or humancomputer interaction, among others. In contrast, applications like visual surveillance and person identification would not be effectively supported, given that the model's output does not provide sufficient detail for these purposes. The same is true for the creation of potentially adverselyimpacting deepfakes, as an appearance model or a joint audio-visual model are not included for photorealistic visual and voice synthesis. While our method is fundamentally applicable to a variety of human body types, we have not evaluated this aspect extensively and consider such a study an important objective for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head><p>Tab. 7 presents an ablation on window size performed using mocap data as initialization and reference trajectory rather than using the kinematic initialization. In this case, we note that a smaller window size of 480 outperforms the larger window size of 960 used in the main paper. We hypothesize that when the reference signal lacks noise, a smaller window is easier to optimize since the dimension of the problem is reduced. However, with noisy observations, a larger window is required for the method to be robust to missing or poor kinematic reconstructions.  <ref type="table">Table 7</ref>. Ablation study of the optimization window size. Experiments were carried out on motion capture rather than the kinematic initialization as input. The experiment was performed on the same Human3.6M sequences as in the ablation in the main paper. Note that when using mocap rather than noisy observations, a smaller window size is better (480 vs. 960 in main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>We evaluate our method on the two established datasets Human3.6M <ref type="bibr" target="#b23">[21]</ref> and AIST <ref type="bibr" target="#b51">[49]</ref>. In addition, we evaluate our method on "real-world" internet videos. Human3.6M. When comparing to the state-of-the-art methods, we evaluate on the Human3.6M Protocol P2 sequences while excluding the same sequences as by Xie et al. <ref type="bibr" target="#b58">[56]</ref>. That leaves the sequences: Directions, Discussions, Greeting, Posing, Purchases, Taking Photos, Waiting, Walking, Walking Dog and Walking Together. We evaluate the motions using only camera 60457274. Similar to <ref type="bibr" target="#b58">[56]</ref>, we down sample the Human3.6M data from 50 FPS to 25 FPS.</p><p>The ablation studies were performed on a smaller subset of four-second clips (frames 400-599) from a random camera, see tab. 8. AIST. AIST provides dynamic dance motions not present in Human3.6M. We evaluate our method using the pseudoground-truth provided by <ref type="bibr" target="#b28">[26]</ref>. We use the first four seconds  (120 frames) using a randomly selected camera from the sequences in tab. 9. Internet Videos. Finally, we perform qualitative evaluation of our method on internet videos made public under creative common licences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Metrics</head><p>Total variation. We compute the total variation of the 3d joint acceleration as a measurement of the jitter in motion. This is given as</p><formula xml:id="formula_11">1 T t?T k?K |? k t+1 ?? k t |,<label>(10)</label></formula><p>where? k t is the 3d joint acceleration of joint k at time t. We estimate the acceleration through finite differences. Foot skating. We track unnatural foot skating artifacts by measuring the percentage of frames where either foot is "skating" along the ground. Our formulation doesn't rely on foot contact annotations but instead heuristically detect when foot contacts occur by measuring the distance between the foot mesh and the ground-plane. A contact is defined as N = 10 foot mesh vertices being within d mm of the ground-plane. For kinematics we use d = 5 mm and for dynamics d = 1 mm to account for the capsule approximation being smaller than the foot mesh. We define skating as a foot moving ? 2 cm between two frames while being in contact with the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Usage of data with human subjects</head><p>In this work, we employ two established pose benchmarks that are commonly used in the field of human pose estimation. Human3.6M <ref type="bibr" target="#b23">[21]</ref> was recorded in a laboratory setting with the permission of the actors, and AIST <ref type="bibr" target="#b51">[49]</ref> contains "a shared database containing original street dance videos with copyright-cleared dance music. This is the first large-scale shared database focusing on street dances to promote academic research regarding Dance Information Processing" 2 . As for the "in-the-wild" videos, these were released under creative common licenses granting express permission to "copy and redistribute the material in any medium or format" and "remix, transform, and build upon the material for any purpose, even commercially". Finally, we do not intend to release these videos as part of a dataset. Instead we only use them to demonstrate our method on videos with poses and motion uncommon in laboratory captured datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence</head><p>Frames gBR_sBM_c06_d06_mBR4_ch06</p><p>1-120 gBR_sBM_c07_d06_mBR4_ch02 1-120 gBR_sBM_c08_d05_mBR1_ch01 1-120 gBR_sFM_c03_d04_mBR0_ch01 1-120 gJB_sBM_c02_d09_mJB3_ch10 1-120 gKR_sBM_c09_d30_mKR5_ch05 1-120 gLH_sBM_c04_d18_mLH5_ch07 1-120 gLH_sBM_c07_d18_mLH4_ch03 1-120 gLH_sBM_c09_d17_mLH1_ch02 1-120 gLH_sFM_c03_d18_mLH0_ch15 1-120 gLO_sBM_c05_d14_mLO4_ch07 1-120 gLO_sBM_c07_d15_mLO4_ch09 1-120 gLO_sFM_c02_d15_mLO4_ch21 1-120 gMH_sBM_c01_d24_mMH3_ch02 1-120 gMH_sBM_c05_d24_mMH4_ch07 1-120 <ref type="table">Table 9</ref>. AIST <ref type="bibr" target="#b51">[49]</ref> sequences used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Differentiable Physics for Human Motion</head><p>Tiny Differentiable Simulator (TDS) <ref type="bibr" target="#b19">[17]</ref> is a C++ simulator where the data type is templetized. In our experiments, we use the scalar from the automatic differentiation (AD) framework CppAD <ref type="bibr" target="#b2">[3]</ref> to compute the simulation gradients. That is, we compute the gradients of the loss with respect to the input control variables at each time step: </p><p>where L is objective function of the trajectory optimization, q 1:T are the simulated body's joint positions, andq 1:T 2 https://aistdancedb.ongaaccel.jp/ are the per-timestep control signal to the PD controllers in the body joints.</p><p>To speed up the optimization we implement our simulation as a fixed computational graph of the simulation rollout for a fixed number of steps and then repeatedly use it to compute the values of the gradients in <ref type="bibr" target="#b12">(11)</ref>. This greatly speeds up the optimization since the automatic differentiation framework doesn't need to setup the computational graph for each backward pass. To that end, we make the following adaptations to TDS to make it support a fixed graph. Differentiation and contact points. Since at the time of graph construction it is not known in advance which contact points will be active for particular inputs we always include all contact points into the LCP formulation. This increases the graph size based on the number of contacts considered. The issue of large graph can be address by e.g. "checkpointing" the computation as described in <ref type="bibr" target="#b40">[38]</ref>. Dealing with exploding gradients. As noted in <ref type="bibr" target="#b35">[33]</ref>, gradients from differentiable simulators may explode or vanishing when the window size is large. In this work, we experimentally found it possible to mitigate the issue by setting the LCP solver iterations to K = 1 without noticeable degradation of reconstruction quality. Implementation Details In our experiments we run TDS with a step size of 1ms. This is partly due to the simpler PD controller, which requires smaller simulation steps to allow for stable control. We set the ground-plane friction to 0.8 and the controller gains to k p = 200 and k d = 5. Evaluating our loss function and computing the gradients for a window of 960 simulation steps takes approximately ? 5 seconds on a standard desktop computer with only feet contacts enabled. Enabling more contacts or simulating multiple objects increases memory and computation time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Qualitative results on two in-the-wild sequences. Sports and dynamic activities are rarely found in motion capture datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " O z R j I w w i O O J l 3 6 8 / n N H 7 C X X J p Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>x r 3 x Z L w a b + P S n D H p 2 U Z / Y L z / A B N u n R Q = &lt; / l a t e x i t &gt;? t+ t Solve LCP and compute constrained velocity &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o i w W 1 n P l h 8 S x P Z h 9 p J Q y a r R 3 F 7 c = " &gt; A A A C C H i c b V C 7 S g N B F J 2 N r x h f U U s L R 4 M Q E c K u I F o G b S w j m A d k l 2 V 2 M p s M m X 0 4 c 1 c I y 5 Y 2 N n 6 I j Y U i t v k E O z / E 3 s m j 0 M Q D A 4 d z 7 r 1 z 7 / F i w R W Y 5 p e R W 1 h c W l 7 J r x b W 1 j c 2 t 4 r b O w 0 V J Z K y O o 1 E J F s e U U z w k N W B g 2 C t W D I S e I I 1 v f 7 V y G / e M 6 l 4 F N 7 C I G Z O Q L o h 9 z k l o C W 3 u J / a A Y G e 5 6 d 2 J 4 L 0 L s s y N 4 U T u 8 M E E A y Z W y y Z F X M M P E + s K S l V D 8 r f w y f 7 u O Y W P / U g m g Q s B C q I U m 3 L j M F J i Q R O B c s K d q J Y T G i f d F l b 0 5 A E T D n p + J A M H 2 m l g / 1 I 6 h c C H q u / O 1 I S K D U I P F 0 5 2 l r N e i P x P 6 + d g H / h p D y M E 2 A h n X z k J w J D h E e p 4 A 6 X j I I Y a E K o 5 H p X T H t E E g o 6 u 4 I O w Z o 9 e Z 4 0 T i v W W c W 8 0 W l c o g n y a A 8 d o j K y 0 D m q o m t U Q 3 V E 0 Q N 6 R q / o z X g 0 X o x 3 4 2 N S m j O m P b v o D 4 z h D 4 h r n e 8 = &lt; / l a t e x i t &gt;q t+ t s t = (q t , ? q t ) Overview of the simulation step of the physics model that updates the current state St to a new state after time step ?t. For each computational block we include the output quantities used in the subsequent block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative examples on the AIST dataset (left) and of complex contacts (right). The AIST example shows that both kinematics and DiffPhy projects well into the image. However, when rendered from another viewpoint (cam #2) it becomes clear that kinematics exhibits unrealistic leaning while the physical constraints corrects the pose to keep the body in balance. See tiny.cc/diffphy for more.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative examples on Human3.6M. DiffPhy infers plausible leg motion while kinematics skates unrealistically forward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>MethodBody Cont. DP Trained T g No RF Rempe et al. Feature comparison against other physics-based methods. Body compares the type of physical body representation where "adapt" means individually constructed based on shape estimate, Cont. column compares what type of contacts are supported, DP whether the method uses a differentiable physical formulation,</figDesc><table><row><cell>[40]</cell><cell>Fixed</cell><cell>Feet</cell><cell>?</cell><cell>Contacts</cell><cell>?</cell><cell>?</cell></row><row><cell>PhysCap [43]</cell><cell>Fixed</cell><cell>Feet</cell><cell cols="3">? Contacts ?</cell><cell>?</cell></row><row><cell>SimPoE [60]</cell><cell>Adapt</cell><cell>Full</cell><cell>?</cell><cell>Yes</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">Shimada et al. [42] Fixed</cell><cell>Feet</cell><cell>?</cell><cell>Yes</cell><cell>?</cell><cell>?</cell></row><row><cell>Xie et al. [56]</cell><cell>Fixed</cell><cell>Feet</cell><cell>?</cell><cell>No</cell><cell>?</cell><cell>?</cell></row><row><cell>Dynamics [15]</cell><cell>Adapt</cell><cell>Full</cell><cell>?</cell><cell>Prior</cell><cell>?</cell><cell>?</cell></row><row><cell>DiffPhy</cell><cell>Adapt</cell><cell>Full</cell><cell>?</cell><cell>No</cell><cell>?</cell><cell>?</cell></row><row><cell cols="7">Training if the physical inference requires training, Tg compares</cell></row><row><cell cols="7">if the ground plane is estimated (as opposed to assumed known),</cell></row><row><cell cols="7">and No RF if the method avoid non-physical residual forces. Only</cell></row><row><cell cols="7">our method does not require any additional training and uses a</cell></row><row><cell cols="5">full-featured differentiable physics formulation.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.1 px/joint vs. VIBE's 16.4 px/joint on Human3.6M. In terms of joint error including translation error (MPJPE), SimPoE [60], Xie et al. [56], Shimada et al. [42] outperform DiffPhy (56.7 vs. 68.1 vs. 76.5 vs. 81.7 mm/joint respectively), though in the case of SimPoE and Xie et al. this might stem from initializing from the already strong VIBE predictor (68.6 mm/joint). Furthermore, SimPoE is a neural network requiring extensive training using the 3d ground-truth from Human3.6M, whereas DiffPhy is a general method that requires no additional training (cf . tab. 1). Xie et al., PhysCap [43], Shimada et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results on experiments on the effects of residual force. We note using a residual force decreases the error metrics, but we refrain from using it to avoid unexplained non-physical forces.</figDesc><table><row><cell cols="5">RF MPJPE-G MPJPE MPJPE-PA MPJPE-2d</cell></row><row><cell>0</cell><cell>144.9</cell><cell>84.6</cell><cell>61.1</cell><cell>12.6</cell></row><row><cell>5</cell><cell>141.4</cell><cell>82.2</cell><cell>60.7</cell><cell>11.8</cell></row><row><cell>10</cell><cell>140.1</cell><cell>79.9</cell><cell>60.2</cell><cell>11.7</cell></row><row><cell>25</cell><cell>146.3</cell><cell>81.9</cell><cell>60.0</cell><cell>12.7</cell></row><row><cell>50</cell><cell>140.2</cell><cell>79.4</cell><cell>60.3</cell><cell>11.6</cell></row><row><cell>100</cell><cell>154.0</cell><cell>87.7</cell><cell>61.5</cell><cell>14.4</cell></row><row><cell cols="5">Window MPJPE-G MPJPE MPJPE-PA MPJPE-2d</cell></row><row><cell>240</cell><cell>390.1</cell><cell>224.1</cell><cell>96.6</cell><cell>40.3</cell></row><row><cell>480</cell><cell>165.6</cell><cell>97.2</cell><cell>63.8</cell><cell>13.2</cell></row><row><cell>720</cell><cell>148.9</cell><cell>87.2</cell><cell>61.8</cell><cell>12.6</cell></row><row><cell>960</cell><cell>144.9</cell><cell>84.6</cell><cell>61.1</cell><cell>12.6</cell></row><row><cell>1440</cell><cell>155.6</cell><cell>92.5</cell><cell>65.7</cell><cell>15.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Human3.6M<ref type="bibr" target="#b23">[21]</ref> sequences used for ablation studies. Note that we downsampled the sequences from 50 FPS to 25 FPS.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This supplement presents additional results ( ?A), a description of the datasets used ( ?B) together with a description of the usage of data with human subjects ( ?B.2), and additional details of the simulation setup ( ?C). Please refer to our video for qualitative results at tiny.cc/diffphy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trajectory optimization for full-body movements with complex contacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Mazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Borno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>De Lasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust Physics-based Motion Retargeting with Realistic Body Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Mazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Borno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Righetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cppad: a package for c++ algorithmic differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Behave: Dataset and method for tracking human object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghui</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="387" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pinocchio c++ library -a fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilhem</forename><surname>Saurel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Buondonno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Mirabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Lamiraux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Stasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mansard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on System Integrations (SII)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pybullet, a python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://pybullet.org" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end differentiable physics for learning and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>De Avila Belbute-Peres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rigid Body Dynamics Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Featherstone</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer-Verlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rigid Body Dynamics Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Featherstone</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer-Verlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Remips: Physically consistent 3d reconstruction of multiple interacting people under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teodor</forename><surname>Szente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Practical Methods of Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Brax -a differentiable physics engine for large scale rigid body simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertan</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trajectory optimization for physics-based reconstruction of 3d human pose from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The CMA Evolution Strategy: A Comparing Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NeuralSim: Augmenting differentiable simulators with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Heiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Millard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav S</forename><surname>Sukhatme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Difftaichi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00935</idno>
		<title level="m">Differentiable programming for physical simulation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Percontact iteration method for solving contact dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jemin</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="895" to="902" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dart: Dynamic animation and robotics toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">X</forename><surname>Grey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sehoon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Stilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Karen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">500</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learn to dance with aist++: Music conditioned 3d dance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno>2021. 12</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamics-regulated kinematic policy for egocentric pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Hachiuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5442" to="5451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Equation of state calculations by fast computing machines. The journal of chemical physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusta</forename><forename type="middle">H</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gradients are not all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Daniel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
		<idno>143:1-143:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Building statistical shape spaces for 3d human modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Helten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Christian Theobalt, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Anatomical data for analyzing human motion. Research quarterly for exercise and sport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Plagenhoef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaynor</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Abdelnour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient differentiable simulation of articulated bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming C</forename><surname>Lin</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Humor: 3d human motion model for robust pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contact and human dynamics from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Patrick P&amp;apos;erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Physics-Based Animation of Articulated Rigid Body Systems for Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Stepien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Jeff) Trinkle. An implicit timestepping scheme for rigid body dynamics with coulomb friction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Physics-Based Animation of Articulated Rigid Body Systems for Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>St?pie?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Silesian University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Hamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference<address><addrLine>Delft, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">0: Fundamental Algorithms for Scientific Computing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">A</forename><surname>Moore ; E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020" />
			<publisher>Paul van Mulbregt</publisher>
		</imprint>
	</monogr>
	<note>Ian Henriksen. and SciPy 1.0 Contributors. SciPy 1.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Global optimization by basin-hopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P K</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Chemistry A</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="5111" to="5116" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast and feature-complete differentiable physics for articulated rigid bodies with contact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalton</forename><surname>Omens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionnis</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2103.16021</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A scalable approach to control diverse behaviors for physically simulated characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungdam</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Physics-based human motion estimation and synthesis from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Shkurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">GHUM &amp; GHUML: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="6184" to="6193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Transpose: Real-time 3d human translation and pose estimation with six inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Residual force control for agile human behavior imitation and extended motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simpoe: Simulated character control for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Neural descent for visual 3d human pose and shape. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Transformer-based 3d human reconstruction with markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thundr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Perceiving 3d human-object spatial arrangements from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Pepose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
