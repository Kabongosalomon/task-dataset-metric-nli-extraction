<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahdi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miangoleh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yag?z</forename><surname>Aksoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We propose a method that can generate highly detailed high-resolution depth estimations from a single image. Our method is based on optimizing the performance of a pre-trained network by merging estimations in different resolutions and different patches to generate a high-resolution estimate. We show our results above using MiDaS [34] in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Neural networks have shown great abilities in estimating depth from a single image. However, the inferred depth maps are well below one-megapixel resolution and often lack fine-grained details, which limits their practicality. Our method builds on our analysis on how the input resolution and the scene structure affects depth estimation performance. We demonstrate that there is a trade-off between a consistent scene structure and the high-frequency details, and merge low-and high-resolution estimations to take advantage of this duality using a simple depth merging network. We present a double estimation method that improves the whole-image depth estimation and a patch selection method that adds local details to the final result. We demonstrate that by merging estimations at different resolutions with changing context, we can generate multimegapixel depth maps with a high level of detail using a pre-trained model.</p><p>( * ) denotes equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular or single-image depth estimation aims to extract the structure of the scene from a single image. Unlike in settings where raw depth information is available from depth sensors or multi-view data with geometric constraints, monocular depth estimation has to rely on highlevel monocular depth cues such as occlusion boundaries and perspective. Data-driven techniques based on deep neural networks have thus become the standard solutions in modern monocular depth estimation methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. Despite recent developments in the field including in network design <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>, incorporation of highlevel constraints <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref>, and supervision strategies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>, achieving high-resolution depth estimates with good boundary accuracy and a consistent scene structure remains a challenge. State-of-the-art methods are based on fully-convolutional architectures which in principle can handle inputs of arbitrary sizes. However, practical constraints such as available GPU memory, lack of diverse high-resolution datasets, and the receptive field size of CNN's limit the potential of current methods. <ref type="figure">Figure 2</ref>: The pipeline of our method: (b) We first start with feeding the image in low-and high-resolution to the network, here shown results with MiDaS <ref type="bibr" target="#b33">[34]</ref>, and merge them to get a base estimate with a consistent structure with good boundary localization. (c) We then determine different patches in the image. We show a subset of selected patches with their depth estimates. (d) We merge the patch estimates onto our base estimate from (b) to get our final high-resolution result.</p><p>We present a method that utilizes a pre-trained monocular depth estimation model to achieve high-resolution results with high boundary accuracy. Our main insight comes from the observation that the output characteristics of monocular depth estimation networks change with the resolution of the input image. In low resolutions close to the training resolution, the estimations have a consistent structure while lacking high-frequency details. When the same image is fed to the network in higher resolutions, the high-frequency details are captured much better while the structural consistency of the estimate gradually degrades. We claim following our analysis in Section 3 that this duality stems from the limits in the capacity and the receptive field size of a given model. We propose a double-estimation framework that merges two depth estimations for the same image at different resolutions adaptive to the image content to generate a result with high-frequency details while maintaining the structural consistency.</p><p>Our second observation is on the relationship between the output characteristics and the amount and distribution of high-level depth cues in the input. We demonstrate that the models start generating structurally inconsistent results when the depth cues are further apart than the receptive field size. This means that the right resolution to input the image to the network changes locally from region to region. We make use of this observation by selecting patches from the input image and feeding them to the model in resolutions adaptive to the local depth cue density. We merge these estimates onto a structurally consistent base estimate to achieve a highly detailed high-resolution depth estimation.</p><p>By exploiting the characteristics of monocular depth estimation models, we achieve results that exceed the state-ofthe-art in terms of resolution and boundary accuracy without retraining the original networks. We present our results and analysis using two state-of-the-art monocular depth estimation methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>. Our double-estimation framework alone improves the performance considerably without too much computational overhead while our full pipeline shown in <ref type="figure">Figure 2</ref> can generate highly detailed results even for very complex scenes as <ref type="figure">Figure 1</ref> demonstrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early works on monocular depth estimation rely on hand-crafted features designed to encode pictorial depth cues such as object size, texture density, or linear perspective <ref type="bibr" target="#b35">[36]</ref>. Recent works leverage deep neural networks to learn depth-related priors directly from training data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">57]</ref>. In recent years, impressive depth estimation performance has been achieved thanks to the availability of large-scale depth datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> and several technical breakthroughs including innovative architecture designs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>, effective incorporation of geometric and semantics constraints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref>, novel loss functions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>, and supervision strategies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. In this work, rather than developing a new depth estimation method, we show that by merging estimations from different resolutions and patches, existing depth estimation models can be adapted to generate higher-quality results.</p><p>While impressive performance has been achieved across depth estimation benchmarks, most existing methods are trained to perform on relatively small input resolution, impeding their use in applications for which high-resolution depth maps are desirable <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Several works propose refinement methods for low-resolution depth estimates using guided upsampling alone <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref> or in combination with residual training <ref type="bibr" target="#b58">[59]</ref>. Our approach instead focuses on generating the high-frequency details by changing the input of the network and merging multiple estimations.</p><p>Our patch-based framework shares similarities with patch-based image editing, matting, and synthesis techniques where local results are generated from image patches and blended into global results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. While related, existing patch-based editing techniques are not directly applicable to our scenario because of problemspecific challenges in monocular depth estimation. These challenges include varying range of depth values in patch estimates, strong dependency on context present in the image patch, and characteristic low-frequency artifacts that arise in high-resolution depth estimates. <ref type="figure">Figure 3</ref>: At small input resolutions, the network <ref type="bibr" target="#b33">[34]</ref> can estimate the overall structure of the scene successfully but often misses the details in the image, notice the missing birds in the bottom image. As the resolution gets higher, the performance around boundaries gets much better. However, the network starts losing the overall structure of the scene and generates low-frequency artifacts in the estimate. The resolution at which these artifacts start appearing depends on the distribution of contextual cues in the image. <ref type="figure">Figure 4</ref>: Since the model is fixed, changing the image resolution affects how much of the scene the receptive field can "see". As the resolution increases, depth cues get farther apart, starving the network of information, which progressively degrades the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Observations on Model Behavior</head><p>Monocular depth estimation, with the lack of geometric cues that multi-camera systems exploit, has to rely on highlevel depth cues present in the image. In their analysis, Hu et al. <ref type="bibr" target="#b16">[17]</ref> show that monocular depth estimation models indeed make use of monocular depth cues that the human visual system utilizes such as occlusions and perspectiverelated cues <ref type="bibr" target="#b39">[40]</ref> that we will refer to as contextual cues or more broadly as context. In this section, we present our observations on how the context or more importantly how the context density in the input affects the network performance. We present examples from MiDaS <ref type="bibr" target="#b33">[34]</ref> and show similar results from <ref type="bibr" target="#b47">[48]</ref> in the supplementary material. Most depth estimation methods follow the common practice of training with a pre-defined and relatively low input resolution but the models themselves are fully convolutional, which in principle can handle arbitrary input sizes. When we feed an image into the same model with different resolutions, however, we see a specific trend in the result characteristics. <ref type="figure">Figure 3</ref> demonstrates that in smaller resolutions the estimations lack many high-frequency details while generating a consistent overall structure of the scene. As the input resolution gets higher, more details are gen- erated in the result but we see inconsistencies in the scene structure characterized by gradual shifts in depth between image regions. We explain this duality through the limited capacity and the limited receptive field size of the network.</p><p>The receptive field size of a network depends mainly on the architecture as well as the training resolution. It can be defined as the region around a pixel that contributes to the output at that pixel <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>. As monocular depth estimation relies on contextual cues, when these cues in the image gets further apart than the receptive field, the network is not able to generate a coherent depth estimation around pixels that do not receive enough information. We demonstrate this behavior with a simple scene in <ref type="figure">Figure 4</ref>. MiDaS <ref type="bibr" target="#b33">[34]</ref> with its receptive field size of 384 ? 384 starts to generate inconsistencies between image regions as the input gets larger and the contextual cues concentrated at the edges of the image get further apart than 384 pixels. The inconsistent results for the flat wall in <ref type="figure">Figure 3</ref> (top) also support this observation.</p><p>Convolutional neural networks have an inherently limited capacity that provides an upper bound to the amount of information they can store and generate <ref type="bibr" target="#b2">[3]</ref>. As the network can only see as much as its receptive field size at once, the limit in capacity applies to what the network can generate inside its receptive field. We attribute the lack of high-frequency details in low-resolution estimates to this limit. When there are many contextual cues present in the input, the network is able to reason about the larger structures in the scene much better and is hence able to generate a consistent structure. However, this results in the network not being able to generate high-frequency details at the same time due to the limited amount of information that can be generated in a single forward pass. We show a simple experiment in <ref type="figure" target="#fig_0">Figure 5</ref>. We use an original input image of 192 ? 192 pixels and simply upsample it to generate higher resolution results. This way, the amount of high-frequency information remains the same in the input but we still see an increase in the high-resolution details in the result, demonstrating a limit in the network capacity. We hence claim that the network gets overwhelmed with the amount of contextual cues concentrated in a small image and is only able to generate an overall structure of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method Preliminaries</head><p>Following our observations in Section 3, our goal is to generate multiple depth estimations of a single image to be merged to achieve a result that has high-frequency details with a consistent overall structure. This requires (i) retrieving the distribution of contextual cues in the image that we will use to determine the inputs to the network, and (ii) a merging operation to transfer the high-frequency details from one estimate to another with structural consistency. Before going into the details of our pipeline in Sections 5 and 6, we present our approach to these preliminaries.</p><p>Estimating Contextual Cues Determining the contextual cues in the image is not a straightforward task. Hu et al. <ref type="bibr" target="#b16">[17]</ref> focus on this problem by identifying the most relevant pixels for monocular depth estimation in an image. While they provide a comparative analysis of contextual cues used by different models during inference, we were not able to generate high-resolution estimates we need for cue distribution for MiDaS <ref type="bibr" target="#b33">[34]</ref> using their method. Instead, following their observation that image edges are reasonably correlated with the contextual cues, we use an approximate edge map of the image obtained by thresholding the RGB gradients as a proxy.</p><p>Merging Monocular Depth Estimates In our problem formulation, we have two depth estimations that we would like to merge: (i) a low-resolution map obtained with a smaller-resolution input to the network and (ii) a higherresolution depth map of the same image (Sec. 5) or patch (Sec. 6) that has better accuracy around depth discontinuities but suffers from low-frequency artifacts. Our goal is to embed the high-frequency details of the second input into the first input which provides a consistent structure and a fixed range of depths for the full image.</p><p>While this problem resembles gradient transfer methods such as Poisson blending <ref type="bibr" target="#b30">[31]</ref>, due to the low-frequency artifacts in the high-resolution estimate, such low-level approaches do not perform well for our purposes. Instead, we utilize a standard network and adopt the Pix2Pix architecture <ref type="bibr" target="#b18">[19]</ref> with a 10-layer U-net <ref type="bibr" target="#b34">[35]</ref> as the generator. Our selection of a 10-layer U-net instead of the default 6-layer aims to increase the training and inference resolution to 1024 ? 1024, as we will use this merging network for a wide range of input resolutions. We train the network to transfer the fine-grained details from the highresolution input to the low-resolution input. For this purpose, we generate input/output pairs by choosing patches from depth estimates of a selected set of images from Mid-dlebury2014 <ref type="bibr" target="#b36">[37]</ref> and Ibims-1 <ref type="bibr" target="#b23">[24]</ref>. While creating the lowand high-resolution inputs is not a problem, consistent and high-resolution ground truth cannot be generated natively. Note that we also can not directly make use of the original ground-truth data because we are training the network only for the low-level merging operation and the desired output depends on the range of depth values in the low-resolution estimate. Instead, we empirically pick 672*672 pixels as input resolution to the network which maximizes the number of artifact-free estimations we can obtain over both datasets. To ensure that the ground truth and higher-resolution patch estimation have the same amount of fine-grained details, we apply a guided filter on the patch estimation using the ground truth estimation as guidance. These modified highresolution patches serve as proxy ground truth for a seamlessly merged version of low-and high-resolution estimations. <ref type="figure">Figures 6 and 7</ref> demonstrate our merging operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Double Estimation</head><p>We show the trade-off between a consistent scene structure and the high-frequency details in the estimates in Section 3 and <ref type="figure">Figure 3</ref> with changing input resolution. We also show in <ref type="figure">Figures 3 and 4</ref> that the network starts to produce structurally inconsistent results when the contextual cues in the image are further apart than the receptive field size. The maximum resolution at which the network will be able to generate a consistent structure depends on the distribution of the contextual cues in the image. Using an edge map as the proxy for contextual cues, we can determine this maximum resolution by making sure that no pixel is further apart from contextual cues than half of the receptive field size. For this purpose, we apply binary dilation to the edge map with a receptive-field-sized kernel in different resolutions. Then, the resolution for which the dilated edge map stops to produce all-one results is the maximum resolution where every pixel will receive context information in a forward pass. We refer to this resolution that is adaptive to the image content as R 0 . We will refer to resolutions above R 0 as R x where x represents the percentage of pixels that do <ref type="figure">Figure 6</ref>: We show the depth estimates obtained at different resolutions, (a) at the training resolution of MiDaS <ref type="bibr" target="#b33">[34]</ref> at 384 ? 384, (b) at the selected resolution with edges separated at most by 384 pixels, and (c) at a higher resolution that leaves 20% of the pixels without nearby edges. Although the increasing resolution provides sharper results, beyond (c), the estimates become unstable in terms of the overall structure, visible through incorrect depth range for the bench in the background and unrealistic depth gradients around the tires. (d) Our merging network is able to fuse the fine-grain details in (c) into the consistent structure in (a) to get the best of two worlds. not receive any contextual information at a given resolution. Estimations with resolutions above R 0 will lose structural consistency but they will have richer high-frequency content in the result.</p><p>Following these observations, we propose an algorithm that we call double estimation: to get the best of two worlds, we feed the image to the network in two different resolutions and merge the estimates to get a consistent result with high-frequency details. Our low-resolution estimation is set to the receptive field size of the network that will determine the overall structure in the image. Resolutions below the receptive field size do not improve the structure and in fact reduce the performance as the full capacity of the network is not utilized. We determined through experimental analysis in Section 7.2 that our merging network can successfully merge the high-frequency details onto the low-resolution estimate's structure up to R 20 . The low-resolution artifacts in estimations beyond R 20 start to damage the merged results. Note that R 20 may be higher than the original resolution. <ref type="figure">Figure 6</ref> demonstrates that we can preserve the structure in the low-resolution estimation (a) while integrating the details in the high-resolution estimation (c) successfully into our result (d). Through merging, we can generate consistent results beyond R 0 (b), which is the limit set by the receptive field size of the network, at the cost of a second forward-pass through the base network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Patch Estimates for Local Boosting</head><p>We determine the estimation resolution for the whole image based on the number of pixels that do not have any contextual cues nearby. These regions with the lowest contextual cue density are dictating the maximum resolution we can use for an image. Regions with higher contextual cue density, however, would still benefit from higherresolution estimations to generate more high-frequency details. We present a patch-selection method to generate depth estimates at different resolutions for different regions in the image that are merged together for a consistent full result.</p><p>Ideally, the patch selection process should be guided with high-level information that determines the local resolution optimum for estimation. This requires a data-driven approach that can evaluate the high-resolution performance of the network and an accurate high-resolution estimation of the contextual cues. However, the resolution of the currently available datasets are not enough to train such a system. As a result, we present a simple patch selection method where we make cautious design decisions to arrive at a reliable high-resolution depth estimation pipeline without requiring an additional dataset or training.</p><p>Base estimate We first generate a base estimate using the double estimation in Section 5 for the whole image. The resolution of this base estimate is fixed as R 20 for most images. Only for a subset of images we increase this resolution as detailed at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch selection</head><p>We start the patch selection process by tiling the image at the base resolution with a tile size equal to the receptive field size and a 1/3 overlap. Each of these tiles serves as a candidate patch. We ensure each patch receives enough context to generate meaningful depth estimates by comparing the density of the edges in the patch to the density of the edges in the whole image. If a tile has less edge density than the image, it is discarded. If a tile has a <ref type="figure">Figure 7</ref>: Input patches are shown in our base estimate, patch-estimate pasted onto the base estimate, and our result after merging. The image is picked from <ref type="bibr" target="#b7">[8]</ref>.</p><p>higher edge density, the size of the patch is increased until the edge density matches the original image. This makes sure that each patch estimate has a stable structure.</p><p>Patch estimates We generate depth estimates for patches using another double estimation scheme. Since the patches are selected with respect to the edge density, we do not adjust the estimation resolution further. Instead, we fix the high-resolution estimation size to double the receptive field size. The generated patch-estimates are then merged onto this base estimate one by one to generate a more detailed depth map as shown in <ref type="figure">Figure 2</ref>. Note that the range of depth values in the patch estimates differs from the base estimate since monocular depth estimation networks do not provide a metric depth. Rather their results represent the ordinal depth relationship between image regions. Our merging network is designed to handle such challenges and can successfully merge the high-frequency details in the patch estimate onto the base estimate as <ref type="figure">Figure 7</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base resolution adjustment</head><p>We observe that when the edge density in the image varies a lot, especially when a large portion of the image lacks any edges, our patch selection process ends up selecting too small patches due to the small R 20 . We solve this issue for such cases by upsampling the base estimate to a higher resolution before patch selection. For this, we first determine a maximum base size R max = 3000?3000 from the R 20 value of a 36-megapixel outdoors image with a lot of high-frequency content. Then we define a simple multiplier for the base estimate size as max (1, R max /(4KR 20 )) where K is the percentage of pixels in the image that are close to edges determined by dilating the edge map with a kernel of quarter of the size of the receptive field. The max operation makes sure that we never decrease the base size. This multiplier makes sure that we can select small high-density areas within an overall low-density image with small patches when we define the minimum patch size as the receptive field size in the base resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results and Discussion</head><p>We evaluate our method on two different datasets, Middleburry 2014 <ref type="bibr" target="#b36">[37]</ref> for which high-resolution inputs and ground-truth depth maps are available, and IBMS-1 <ref type="bibr" target="#b23">[24]</ref>. We evaluate using a set of standard depth evaluation metrics as suggested in recent work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>, including root mean squared error in disparity space (RMSE), percentage of pixels with ? = max( zi <ref type="figure" target="#fig_0">25 (? 1.25</ref> ), and ordinal error (ORD) from <ref type="bibr" target="#b47">[48]</ref> in depth space. Additionally, we propose a variation of ordinal relation error <ref type="bibr" target="#b47">[48]</ref> that we call depth discontinuity disagreement ratio (D 3 R) to measure the quality of high frequencies in depth estimates. Instead of using random points as in <ref type="bibr" target="#b47">[48]</ref> for ordinal comparison, we use the centers of superpixels <ref type="bibr" target="#b0">[1]</ref> computed using the ground truth depth and compare neighboring superpixel centroids across depth discontinuities. This metric hence focuses on boundary accuracy. We provide a more detailed description of our metric in the supplementary material.</p><formula xml:id="formula_0">z * i , z * i zi ) &gt; 1.</formula><p>Our merging network is light-weight and the time it takes to do a forward pass is magnitudes smaller than the monocular depth estimation networks. The running time of our method mainly depends on how many times we use the base network in our pipeline. The resolution at which the base estimation is computed, R 20 , and the number of patches we merge onto the base estimate is adaptive to the image content. Our method ended up selecting 74.82 patches per image on average with an average R 20 = 2145 ? 1501 for the Middleburry 2014 <ref type="bibr" target="#b36">[37]</ref> dataset and 12.17 patches per image with an average R 20 = 1443 ? 1082 for IBMS-1 <ref type="bibr" target="#b23">[24]</ref>. The difference between these numbers comes from the different scene structures present in the two datasets. Also note that the original image resolution of IBMS-1 <ref type="bibr" target="#b23">[24]</ref> is 640 ? 480. As we demonstrate in Section 3, upscaling low-resolution images does help in generating more high-frequency details. Hence, our estimation resolution depends mainly on the image content and not on the original input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Boosting Monocular Depth Estimation Models</head><p>We evaluate how much our method can improve upon pre-trained monocular depth estimation models using Mi-DaS <ref type="bibr" target="#b33">[34]</ref> and SGR <ref type="bibr" target="#b47">[48]</ref> as well as the depth refinement method by Niklaus et al. <ref type="bibr" target="#b29">[30]</ref> and a baseline where we refine the original method's results using a bilateral filter after bilinear upsampling. The quantitative results in <ref type="table">Table 1</ref> show that for the majority of the metrics, our full pipeline improves the numerical performance considerably and our double-estimation method already provides a good improvement at a small computational overhead. Our content-adaptive boosting framework consistently improves the depth estimation accuracy over the baselines on both datasets in terms of ORD and D 3 R metrics, indicating accurate depth ordering and better-preserved boundaries. Our <ref type="table">Table 1</ref>: Quantitative evaluation of our method using two base networks on two different datasets. Lower is better.</p><p>Middleburry2014 <ref type="bibr" target="#b36">[37]</ref> Ibims-1 <ref type="bibr" target="#b23">[24]</ref> MiDaS <ref type="bibr" target="#b33">[34]</ref> SGR <ref type="bibr" target="#b47">[48]</ref> MiDaS <ref type="bibr" target="#b33">[34]</ref> SGR <ref type="bibr" target="#b47">[48]</ref> ORD Input MiDaS <ref type="bibr" target="#b33">[34]</ref> Ours using MiDaS SGR <ref type="bibr" target="#b47">[48]</ref> Ours using SGR <ref type="figure">Figure 8</ref>: Additional results using MiDaS <ref type="bibr" target="#b33">[34]</ref> and the Structure-Guided Ranking Loss method <ref type="bibr" target="#b47">[48]</ref> compared to the original methods run at their default size.</p><formula xml:id="formula_1">D 3 R RMSE ? 1.25 ORD D 3 R RMSE ? 1.25 ORD D 3 R RMSE ? 1.25 ORD D 3 R RMSE ? 1.25</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>method also performs comparably in terms of RMSE and ? <ref type="bibr">1.25</ref> . We also observe that simply adjusting the input resolution adaptively to R 0 meaningfully increases the performance.</p><p>The performance improvement provided by our method is much more significant in qualitative comparisons shown in <ref type="figure">Figure 8</ref>. We can drastically increase the number of highfrequency details and the boundary localization when compared to the original networks.</p><p>We do not see a large improvement when depth refinement methods are used in <ref type="table">Table 1</ref> and also in the qualitative examples in <ref type="figure">Figure 9</ref>. This difference comes from the fact that we utilize the network multiple times to generate richer information while the refinement methods are limited by the details available in the base estimation results. Qualitative examples show that the refinement methods are not able to generate additional details that were missed in the base estimate such as small objects or sharp depth discontinuities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Double Estimation and R x</head><p>We chose R 20 as the high-resolution estimation in our double-estimation framework. This number is chosen based on the quantitative results in <ref type="table" target="#tab_0">Table 2</ref>, where we show that using a higher resolution R 30 results in a decrease in per-Input MiDaS <ref type="bibr" target="#b33">[34]</ref> MiDaS + Bilat. Up. MiDaS + <ref type="bibr" target="#b29">[30]</ref> Ours using MiDaS GT <ref type="figure">Figure 9</ref>: We compare out method to bilateral upsampling and the refinement method proposed by Niklaus et al. <ref type="bibr" target="#b29">[30]</ref> as applied to MiDaS <ref type="bibr" target="#b33">[34]</ref> output. Refinement methods fail to add any details that do not exist in the original estimation. With our patch-based merging framework, we are able to generate sharp details in the image.  formance. This is due to the high-resolution results having heavy artifacts as the number of pixels in the image without contextual information increases. <ref type="table" target="#tab_0">Table 2</ref> also demonstrates that our double estimation framework outperforms fixed input resolutions which are the common practice, as well as estimations at R 0 which represents the maximum resolution an image can be fed to the networks without creating structural inconsistencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Limitations</head><p>Since our method is built upon monocular depth estimation, it suffers from its inherent limitations and therefore generates relative, ordinal depth estimates but not absolute depth values. We also observed that the performance of the base models degrade with noise and our method is not able to provide meaningful improvement for noisy images. We address this in an analysis on the NYUv2 <ref type="bibr" target="#b38">[39]</ref> dataset in the supplementary material. The high-frequency estimates suffer from low-magnitude white noise which is not always filtered out by our merging network and may result in flat surfaces appearing noisy in our results.</p><p>We utilize RGB edges as a proxy for monocular depth cues and make some ad-hoc choices in our patch selection process. While we are able to significantly boost base mod- els with our current formulation, we believe research on contextual cues and the patch selection process will be beneficial to reach the full potential of pre-trained monocular depth estimation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have demonstrated an algorithm to infer a highresolution depth map from a single image using pre-trained models. While previous work is limited to sub-megapixel resolutions, our technique can process the multi-megapixel images captured by modern cameras. High-quality highresolution monocular depth estimation enables many application scenarios such as image segmentation. We show a simple segmentation by thresholding the depth values in <ref type="figure" target="#fig_2">Figure 10</ref> which also demonstrates our boundary localization. Our work is based on a careful characterization of the abilities of existing depth-estimation networks and the factors that influence them. We hope that our approach will stimulate more work on high-resolution depth estimation and pave the way for compelling applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>The original image with resolution 192 ? 192 gains additional details in the depth estimate when fed to the network after upsampling to 500 ? 500 (right) instead of its original resolution (middle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Method 0.3840 0.3343 0.1708 0.7649 0.4087 0.3889 0.2123 0.7989 0.4002 0.3698 0.1596 0.6345 0.5555 0.4736 0.1956 0.7513 Refine-Bilateral 0.3806 0.3366 0.1707 0.7627 0.4078 0.3904 0.2122 0.7990 0.3982 0.3768 0.1596 0.6350 0.5551 0.4750 0.1956 0.7501 Refine-with [30] 0.3826 0.3377 0.1704 0.7622 0.4081 0.3880 0.2115 0.7993 0.4006 0.3761 0.1600 0.6351 0.5488 0.4780 0.1953 0.7482 Single-est (R 0 ) 0.3554 0.2504 0.1481 0.7161 0.4312 0.3131 0.1999 0.7841 0.4504 0.3269 0.1687 0.6633 0.6343 0.4901 0.2146 0.7856 Double-est (R 20 ) 0.3496 0.1709 0.1563 0.7364 0.3944 0.2540 0.1983 0.7931 0.4112 0.3272 0.1597 0.6386 0.5591 0.4829 0.1967 0.7473 OURS 0.3467 0.1578 0.1557 0.7406 0.3879 0.2324 0.1973 0.7891 0.3938 0.3222 0.1598 0.6390 0.5538 0.4671 0.1965 0.7460</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ORD 0 .</head><label>0</label><figDesc>384 0.371 0.426 0.478 0.355 0.457 0.505 0.361 0.349 0.349 0.352 D 3 R 0.334 0.217 0.187 0.189 0.250 0.197 0.199 0.258 0.183 0.170 0.171 RMSE 0.170 0.152 0.165 0.186 0.148 0.183 0.198 0.164 0.157 0.156 0.156 ?1.25 0.764 0.745 0.740 0.793 0.716 0.788 0.803 0.749 0.730 0.736 0.745</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 :</head><label>10</label><figDesc>Our boundary accuracy is better visible in this example where we apply a threshold to the estimated depth values of MiDaS [34] (a) and ours (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Whole image estimation performance of Mi-DaS<ref type="bibr" target="#b33">[34]</ref> with changing resolution and double estimation on the Middlebury dataset<ref type="bibr" target="#b36">[37]</ref>. Lower is better.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Single estimation</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Double estimation</cell><cell></cell></row><row><cell></cell><cell cols="2">Fixed size (pixels)</cell><cell></cell><cell></cell><cell cols="3">Context-adaptive</cell><cell></cell><cell></cell></row><row><cell>384</cell><cell>768</cell><cell>1152 1536</cell><cell>R0</cell><cell>R10</cell><cell>R20</cell><cell>R0</cell><cell>R10</cell><cell>R20</cell><cell>R30</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Computing receptive fields of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The capacity of feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="288" to="311" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving monocular depth estimation by leveraging structural awareness and complementary datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">OASIS: A large-scale dataset for single image 3D in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriyuki</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Raise: A raw images dataset for digital image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Pasquini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Conotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Systems Conference</title>
		<meeting>ACM Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image melding: Combining inconsistent images using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map upsampling and refinement for FTV systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Dziembowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grzelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Mieloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgierd</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Doma?ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSES</title>
		<meeting>ICSES</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards good practice for CNN-based monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualization of convolutional neural networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Michael Maire Greg Shakhnarovich, and Erik Learned-Miller. Selfsupervised relative depth learning for urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving patch-based synthesis by learning patch masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCP</title>
		<meeting>ICCP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of CNN-Based Single-Image Depth Estimation Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>K?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07049</idno>
		<title level="m">What are the receptive, effective receptive, and projective fields of neurons in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-loss rebalancing algorithm for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoRL</title>
		<meeting>CoRL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D Ken Burns effect from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting sharp and accurate occlusion boundaries in monocular depth estimation using displacement fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Make3d: Depth perception from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Nesic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GCPR</title>
		<meeting>GCPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3D photography using context-aware layered depth inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Li</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Chapter 3: Visual Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Sternberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sternberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Psychology</title>
		<imprint>
			<publisher>Wadsworth Publishing</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="84" to="134" />
		</imprint>
	</monogr>
	<note>6th Edition</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Synthetic depth-of-field with a single-camera mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">E</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepLens: Shallow depth of field from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SDC-Depth: Semantic divide-and-conquer network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CLIFFNet for monocular depth estimation with hierarchical embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structure-guided ranking loss for single image depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multiscale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">D3VO: Deep depth, deep pose and deep uncertainty for monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lukas Von Stumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Daniel Cohen-Or, and Olga Sorkine-Hornung. Patch-based progressive 3D point set upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">High-resolution deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The Edge of Depth: Explicit constraints between segmentation and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Frequency-dependent depth map enhancement via iterative depth-guided affine transformation and intensityguided refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwu</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
