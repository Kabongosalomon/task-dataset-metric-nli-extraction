<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Ching</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSIE</orgName>
								<orgName type="institution">National Taiwan University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AI Lab</orgName>
								<address>
									<country>Trend Micro</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Jinn</forename><surname>Horng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSIE</orgName>
								<orgName type="institution">National Taiwan University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a Classification Confidence Network (CLCNet) that can determine whether the classification model classifies input samples correctly. The proposed model can take a classification result in the form of vector in any dimension, and return a confidence score as output, which represents the probability of an instance being classified correctly. We can utilize CLCNet in a simple cascade structure system consisting of several SOTA (state-of-the-art) classification models, and our experiments show that the system can achieve the following advantages: 1. The system can customize the average computation requirement (FLOPs) per image while inference. 2. Under the same computation requirement, the performance of the system can exceed any model that has identical structure with the model in the system, but different in size. In fact, we consider our cascade structure system as a new type of ensemble modeling. Like general ensemble modeling, it can achieve higher performance than single classification model, yet our system requires much less computation than general ensemble modeling. We have uploaded our code to a github repository: https://github.com/yaoching0/CLCNet-Rethinking-of-Ensemble-Modeling. arXiv:2205.09612v5 [cs.LG] 23 Oct 2022 Classification Model A (Shallow) Classification Model B (Deep) Input images CLCNet If CLCNet-score &gt; threshold Accept CLCNet Max() output the result with the max CLCNet-score Classifiction result Classifiction result If CLCNet-score &lt; threshold 2 Related work ConfNet [8]  has tried to give a confidence for the classification result to represent whether the classification is correct. It directly adds a fully connected layer at the end of the classification model. When the classification model finishes classifying an input sample, the output of the classification model is sorted from largest to smallest to remove the category information, and then input the sorted classification result to the fully connected layer with sigmoid activation function, the network will eventually return a value ranged from 0 to 1, which represents the confidence of this classification.</p><p>ConfNet is an end-to-end network, it will output a classification result and corresponding confidence at the same time. During training the ConfNet, it will use the Confidence Loss [8] and this loss will be calculated based on both classification result and confidence score. Its purpose is to constrain network to give a lower confidence when cross-entropy loss of a classification output is larger, and vice versa.</p><p>This end-to-end model has a distinct disadvantage. Because the classification model and the subsequent fully connected layer are trained at the same time, when doing gradient descent, the previous</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In deep learning, classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> has always been a popular task. And many SOTA classification models have been proposed in different size variants with the same structure, such as EfficientNet-B0 to B7 <ref type="bibr" target="#b4">[5]</ref>. The larger the number following B, the greater number of parameters the model has, along with better performance. Based on this observation, we want to achieve the following purposes:</p><p>? For a classification model with variants in different sizes, we expect to propose a method that can combine variants of the model, which can achieve higher accuracy than the original model with the same requirement of computation (FLOPs).</p><p>? The above method can achieve the performance of general ensemble modeling, but the demand for computation is lower.</p><p>In order to achieve the above purposes, firstly, we propose a network that can predict whether the classification model classifies correctly, called Classification Confidence Network (CLCNet). Given the output result of a classification model in the form of vector and pass it to CLCNet, CLCNet will return a confidence score representing the correctness of the classification result. The higher the score, * corresponding author  the higher the probability that CLCNet thinks the classification is correct, and vice versa. In particular, in CLCNet, we adopt a special mapping mechanism, so that CLCNet can accept classification results of any dimension. When transferring CLCNet from one classification task to another classification task with a completely different number of categories, it can even be used immediately without retraining.</p><p>Then we can apply CLCNet in a cascade structure system, in which we stack two classification models as shown in <ref type="figure" target="#fig_1">Fig.1</ref>. This system let the simple input samples to be classified by the shallow (lightweight) model first, and only a small number of difficult samples with low confidence are further classified by the deep model. It can greatly save the amount of computation, and at the same time ensure that the accuracy is almost not reduced, or even better. Many studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> have demonstrated the effectiveness of the cascaded structure system.</p><p>In fact, we can also directly use the highest probability of the classification result as the confidence score of the classification. For example, in the classification result of the five-class classification problem (0.6, 0.1, 0.1, 0.1, 0.1), directly use 0.6 as the confidence score. However, it is not able to distinguish such a case. If there is another classification result (0.6, 0.39, 0.01, 0.0, 0.0), it is obvious that the highest probability of these two classification results is 0.6, but the second highest probabilities are 0.1 and 0.39, respectively. Intuitively, the classification result with the second highest probability of 0.39 is more likely to be misclassified, so it should have a slightly lower confidence score. In order to be able to distinguish this situation and evaluate it better, we propose CLCNet, expecting it to make a prediction based on the numerical distribution of a classification result.</p><p>We will evaluate this cascade structure system on the dataset ImageNet <ref type="bibr" target="#b8">[9]</ref>, and the results will show that for the single models (such as shallow or deep model) in the system, the system can outperform them with the same amount of computation. Or achieve the same accuracy, while the system required less computation than that of single models. We will also show that the system can also reach the performance of general ensemble modeling, that is, to obtain higher accuracy than any single model in the system, but with less computation than general ensemble modeling. classification model must consider not only to get correct classification, but also when the crossentropy loss is low, its classification output needs to "look like" to be a correct classification for the subsequent fully connected layer to recognize. In contrast, when the cross-entropy loss is high, the classification model must make its classification result "look like" a misclassification result, which may damage the performance of the classification model itself. Moreover, because the shape of the final fully connected layer is fixed and can only accept fixed-dimensional input, whenever the classification task changes, the entire classification model and the final fully connected layer need to be retrained.</p><p>The above problems incurred in ConfNet <ref type="bibr" target="#b7">[8]</ref> will not appear in CLCNet, CLCNet and the classification model are trained separately, so the performance of the classification model will not be damaged. And it adopts a special mapping mechanism, which can accept a classification result of arbitrary dimension as input. The training set of CLCNet can contain the outputs of multiple different classification models to enhance its robustness. After CLCNet is trained, it can be used directly without fine-tuning when transferring to new tasks in the future, and good performance can still be achieved. Of course, fine-tuning CLCNet on new datasets may achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>In this chapter, we first introduce network architecture and technical details of CLCNet. Then, we will describe the features of the cascade structure system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CLCNet (Classification Confidence Network)</head><p>When we get a classification result, we want to know whether the classification is correct. We will input the classification result to CLCNet which is shown on the left side of <ref type="figure" target="#fig_3">Fig.2</ref>. First CLCNet will sort the class probability of the classification result from largest to smallest to remove its category information, in this way, it can focus on the numerical distribution of the classification result. Then proceed input the sorted result into proposed Restricted Self-Attention module, the purpose of which is to map a classification result of any dimension to an equivalent m-dimension vector, where m is a hyperparameter. In other words, we want to "simulate" an equivalent numerical distribution of the classification result in m-classification task. When the equivalent m-dimensional vector is obtained, we sort it again and give it to TabNet <ref type="bibr" target="#b9">[10]</ref>, TabNet will return a confidence score for this mapping vector, representing the probability that the classification is correct.</p><p>Restricted Self-Attention The architecture of Restricted Self-Attention is shown on the right side of <ref type="figure" target="#fig_3">Fig.2</ref>, its input 2 ? ? i=1 R i , and its output ? R m , m is a hyperparameter, we set it to 100 by default. It is a variant of Self-Attention in Transformer <ref type="bibr" target="#b10">[11]</ref>, so some parts of the inference process are the same. Given a sorted classification result input as a ? R n , which is (a 1 , a 2 , . . . , a n ), first we use a learnable matrix W q ? R m?1 to get q 1 = W q ? a 1 . Next, multiply a x with another learnable matrix W k ? R m?1 , where x ? {1, 2, 3, . . . , n}, to get k x = W k ? a x . Then we use Eq.1 to obtain the attention score att ? R n?1 , which has the same length as the input a (att x is the attention score of a x ).</p><formula xml:id="formula_0">att = sof tmax k 1 , k 2 , . . . , k n T ? q 1<label>(1)</label></formula><p>[ ?, ? ] means to concatenate the vectors horizontally. Next, we define a rule-based matrix M G ? R m?n . Obviously, the matrix consists of n (m ? 1)-dimensional column vectors, and we use C x ? R m?1 to represent one of the column vectors of the matrix, so</p><formula xml:id="formula_1">M G = [C 1 , C 2 , . . . , C n ].</formula><p>Further, the value in C x satisfies the following equation:</p><formula xml:id="formula_2">C x (i) = a x ? e ? ( i m?1 ? x?1 n ) 2 2? 2 , i ? {0, 1, 2, . . . , m ? 1} (2)</formula><p>C x is a vector composed of the function values of m points on Eq.2. In fact, the function (Eq.2) is a simplified version of the Gaussian probability density function, which is also a bell shaped function, i represents the position in the C x , n is the dimension of the input classification result, x?1 n is the position of the maximum value of the bell shaped function (i.e. mean value (?) of the distribution represented by the simplified probability density function), so the function corresponding to different </p><formula xml:id="formula_3">1 i = a 1 ? ? m?1 ? 1?1 2 2 2 2 i = a 2 ? ? m?1 ? 2?1 2 2 2 3 i = a 3 ? ? m?1 ? 3?1 2 2 2</formula><p>Attention scores  x will have different position of the maximum value, ? is a hyperparameter that can control the shape of the function (i.e. the standard deviation of the distribution represented by the simplified probability density function), function with a larger ? will have a flatter shape, and vice versa. We set ? to 0.01 by default, and we draw examples of C 1 , C 2 , and C 3 assuming n being set to 5 in <ref type="figure" target="#fig_3">Fig.2</ref>. With Eq.2 we can obtain all C x and concatenate them by column to form matrix M G . Now, we can use the already obtained att and C x to calculate the mapping vector output:</p><formula xml:id="formula_4">output = n x=1 att x ? C x<label>(3)</label></formula><p>Pragmatically we will also use matrix operations directly to speed up:</p><formula xml:id="formula_5">output = M G ? att<label>(4)</label></formula><p>As mentioned at the beginning, output ? R m , and we sort it again to get the final mapping vector. Finally, we show an example where 5-dimensional classification result is mapped to a 100-dimensional vector in <ref type="figure" target="#fig_4">Fig.3</ref>.</p><p>How does this mapping work First of all, the role of the learnable matrices W q and W k is to give different a x attention scores to appropriately adjust their proportions in the final mapping vector. Specifically, although n x=1 att x = 1 and n x=1 a x = 1, att x does not need to be equal to a x , and a x /a y is not necessarily equal to att x /att y .</p><p>We choose the bell shaped function as the function of C x because its numerical distribution is very similar to that of a general classification result vector. A bell shaped function can be divided into two  distinct parts, as shown in <ref type="figure" target="#fig_7">Fig.5</ref>, one is a sharp region close to the maximum ( x?1 n in C x ), which is small in scope but large in function values. The other part is the flat areas on the left and right away from the maximum value, where there are a large number of function values approaching zero. And according to the observation, the sorted classification vectors in real-world cases are mostly in the form of (0.8, 0.1, 0.00034, 0.00022, 0.00013, ...), that is, those vectors have a very small amounts of large probability values, and have a very large amounts of small probability values close to zero, which is very similar to the distribution of C x , and the large probability values correspond to the sharp region of C x , the small probability values correspond to the flat region of C x . So if we sort C x from largest to smallest, it will be very similar to a sorted general classification vector, as shown in <ref type="figure" target="#fig_6">Fig.4</ref>, and because of this, we can use the sorted C x to "simulate" a sorted classification vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bell shaped function</head><p>Example: y = ? ?0 2 2(0.1 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sharp region</head><p>Flat region Flat region</p><p>The function values are relatively large but small in scope.</p><p>There are a large number of function values approaching 0. We can regard each C x as a mapping vector of a x on the m-classification task, and C x will be multiplied by att x (Eq.3 and <ref type="figure" target="#fig_3">Fig.2</ref>). The att x is generated by considering the relative size of a x in a, and is responsible for adjusting the overall value of C x . The maximum value of att x ? C x will be att x ? a x (according to Eq.2, the maximum value of C x is a x ). Next, we add all att x ? C x , it can be said that each a x participates in the generation of the final mapping vector. And because the positions of the maxima of different C x are different (i.e.</p><p>x?1 n ), when we finally sort the n x=1 att x ? C x , the sharp regions from each att x ? C x can be partially preserved (as shown in <ref type="figure" target="#fig_4">Fig.3</ref>, the sharp areas are staggered from each other). In this way, the more large probability values in a, the more and denser the sharp areas in n x=1 att x ? C x , and the more large probability values will be in the final mapping vector. Finally, we sort the n x=1 att x ? C x , as mentioned before, the sorted C x is very similar to a sorted general classification vector, and so is the sorted n x=1 att x ? C x .</p><p>TabNet After mapping and sorting, we get an m-dimensional vector, and we will continue to input it to TabNet <ref type="bibr" target="#b9">[10]</ref> to get a confidence score. We can treat the m-dimensional vector as a piece of tabular data. TabNet can approximate the performance of tree-based models (such as LGBM <ref type="bibr" target="#b11">[12]</ref> and XGBOOST <ref type="bibr" target="#b12">[13]</ref>) on regression task of tabular data without requiring additional feature engineering. The structure of TabNet is shown in <ref type="figure" target="#fig_10">Fig.6</ref>. TabNet takes many steps which consist of the same structure while inference. Given a set of input features (an m-dimensional vector can be regarded as m features), when starting a step, a mask will be generated by the attentive transformer <ref type="bibr" target="#b9">[10]</ref> (see <ref type="figure" target="#fig_10">Fig.6</ref>). The mask indicates which features will be selected in this step. Next, continue to input the features selected by the mask into the feature transformer <ref type="bibr" target="#b9">[10]</ref> (see <ref type="figure" target="#fig_10">Fig.6</ref>), and its output will be split into two parts, the first part is responsible for determining the confidence score, and the second part is inputted to the attentive transformer to determine the mask shape of the next step. When all steps are executed, the first part of feature transformers' output in each step is summed up, and after a fully connected layer, the final confidence score can be obtained.</p><p>It should be noted that we have modified the original TabNet by adding a Self-Attention <ref type="bibr" target="#b10">[11]</ref> module shared across different steps before the first layer FC of the feature transformer, which can alleviate the instability caused by the sparsity of the input features of the feature transformer. The sparsity is cause by the last layer of the attentive transformer that determines the mask, which uses Sparsemax <ref type="bibr" target="#b13">[14]</ref> as activation function, therefore tends to output a sparse mask. . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Step dependent FC BN Sparsemax . . .    We can use CLCNet in a simple cascade structure system, in which we stack two (or several) classification models, as shown in <ref type="figure" target="#fig_1">Fig.1</ref>. The model with fewer computations is called shallow model. The system will use it to classify first, and then input the classification result to CLCNet to predict whether it is classified correctly. If the confidence score output by CLCNet is higher than threshold, then we will directly accept the classification result and will not continue to the next steps, the threshold is a hyperparameter. The model with higher computation cost will be called deep model. When CLCNet's confidence to the shallow model's classification result is less than the threshold, the input sample will continue to be classified by the deep model, and the classification result will be also input to CLCNet for evaluation and give another confidence score. Finally, we compare the confidence scores of two models' classification results, and accept the result with the higher confidence.</p><p>Obviously, the higher the threshold we set, the more samples will be further handled by the deep model for classification, and the average FLOPs per sample of the system will be higher, but the accuracy will also be improved. By adjusting the threshold value, we can customize the accuracy or average FLOPs of the system.</p><p>It should be noted that the upper limit of the accuracy of the system is not the accuracy of the deep model. We use A to represent the entire testing set, assuming that S C is a subset of samples classified correctly by the shallow model, where S C ? A. Likewise, suppose D C is the subset of samples correctly classified by the deep model, where D C ? A, we show their relationship in <ref type="figure" target="#fig_12">Fig.7</ref>. It can be seen that although S C and D C mostly overlap, but S C ? D C , this means that there are still a small number of samples that the shallow model classifies correctly, but the deep model classifies incorrectly, that is, the green area in <ref type="figure" target="#fig_12">Fig.7</ref>. In an ideal situation, CLCNet can correctly find that these samples have been classified correctly in the shallow model, and will not continue to input to the deep model for classification, or CLCNet gives these correct shallow model classification results higher confidence scores than incorrect deep model classification results, thus preventing these samples from being misclassified by the system. Therefore, the accuracy upper limit of the system will be greater than the accuracy of the deep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and evaluation</head><p>We first select many SOTA classification models that have been pre-trained on ImageNet-1k <ref type="bibr" target="#b8">[9]</ref>, and then we combined these classification models in pairs and used them in the cascade structure system as shallow model and deep model for testing.</p><p>For the training step of CLCNet, in order to test the robustness of CLCNet, we only use EfficientNet-B4 <ref type="bibr" target="#b4">[5]</ref> to classify 50,000 images in the ImagNet-1k validation set, and use the classification results as the dataset of CLCNet. If EfficientNet-B4 classifies a image correctly, the label corresponding to the classification result is 1. Otherwise, the label is 0. So the size of the dataset is also 50,000, and each sample is a 1000-dimensional vector. For fairness, we use 5-fold cross-validation to train CLCNet and evaluate the cascade structure system. Different from regular cross-validation, at each fold, we divide the dataset into two parts of 40,000 samples and 10,000 samples, the training and validation sets of CLCNet are only provided by the part of 40,000 samples (80% of the 40,000 samples are used for training and 20% are used for validation and testing).</p><p>We use common and simple training methods, including using standard regression loss function (mean squared error) and Adam <ref type="bibr" target="#b14">[15]</ref> (learning rate of 0.002) as the optimizer, and train until convergence. When we have completed the training of CLCNet in these 40,000 samples, we will use this CLCNet in the cascade structure system and evaluate the classification accuracy of the system in 10,000 images (images of the ImageNet-1k validation set correspond to the part of the previously divided 10,000 samples). After repeating five folds, we can get the average accuracy of the cascade structure system on all 50,000 images of the ImageNet-1k validation set. Finally, we will use grid search on the threshold to get the performance of the cascaded structure system at the desired accuracy or computational cost for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with models of the same structure of different sizes</head><p>We first choose the classic EfficientNet <ref type="bibr" target="#b4">[5]</ref> series variants for testing. We use EfficientNet-B0/B4 as the shallow model of the cascade structure system and EfficientNet-B4/B7 as the deep model. For brevity, we will use CLCNet to denote the cascade structure system using CLCNet in following texts (If it represents the original CLCNet, it will be marked with <ref type="figure" target="#fig_3">(Fig.2)</ref>.). We set the accuracy or FLOPs of variants of different sizes of EfficientNet to the desired accuracy or computational cost, and then use grid search on the threshold to make the system just reach the requirement. In Tab.1 we compare CLCNet and EfficientNet variants of different sizes under the same FLOPs or the same Top-1 accuracy on ImageNet-1k. Besides, we also draw the comparison in <ref type="figure" target="#fig_14">Fig.8(a)</ref>.</p><p>Obviously, for intermediate size variants of EfficientNet, we found that under the same FLOPs, CLCNet has higher accuracy, and under the same accuracy, CLCNet only needs lower FLOPs. Further, at the bottom of Tab.1, the highest accuracy of CLCNet even slightly exceeds the accuracy of the deep model B7, and only half of its average FLOPs is required, which is also consistent with <ref type="figure" target="#fig_12">Fig.7</ref>.</p><p>Next, we want to test the robustness of CLCNet. We directly combine the CLCNet <ref type="figure" target="#fig_3">(Fig.2)</ref> trained by the dataset generated by EfficientNet-B4 with the VOLO-D1/D5 <ref type="bibr" target="#b0">[1]</ref> without retraining CLCNet <ref type="figure" target="#fig_3">(Fig.2)</ref>, and also compare the system with VOLO variants of different sizes. The results are listed in Tab.2, and <ref type="figure" target="#fig_14">Fig.8</ref> It should be noted that the FLOPs of our proposed CLCNet <ref type="figure" target="#fig_3">(Fig.2)</ref> is 2.7M, which is almost negligible with the Bilion level of the compared classification models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with general ensemble modeling</head><p>In addition to using different size variants of a model, we can use models with completely different structures to put into CLCNet for testing, and we will compare with the general ensemble modeling.</p><p>In order to simulate the real usage scenarios of general ensemble modeling, we try to select SOTA classification models with similar accuracy and large structural differences.</p><p>Retrain CLCNet with shallow model and deep model We also tried to use the current shallow model and deep model to retrain CLCNet <ref type="figure" target="#fig_3">(Fig.2)</ref> for better performance. Specifically, we use the current shallow model and deep model to classify 50,000 images of the ImageNet validation set, respectively, and combine the classification results of the two models as a dataset of 100,000 samples. Then use the same cross-validation as in Sec.4.1, that is, use four-fifths of the data to train and validate CLCNet <ref type="figure" target="#fig_3">(Fig.2</ref>) each time (80,000 samples, 40,000 ImageNet validation set images are classified by two models), and get a CLCNet <ref type="figure" target="#fig_3">(Fig.2)</ref> weight. Use this weight to evaluate the accuracy of the remaining one-fifth of the data on the cascade structure system (the remaining 10,000 ImageNet validation set images), and repeat five times to cover the entire ImageNet validation set. We use (retrain) as a marker in the Tab.3 for distinction.</p><p>We compare the performance of single models, CLCNet and general ensemble modeling on ImageNet-1k in Tab.3. And we use classical ensemble averaging as a general ensemble modeling method for comparison, which means that two models infer an input sample at the same time, and add the outputs of the two models as the output result. For CLCNet, we used multiple pairwise model combinations.</p><p>For example, we first used the base variant of Vision Transformer (ViT-B/16) <ref type="bibr" target="#b15">[16]</ref> as the shallow model and the EfficientNet-B7 trained by Noisy Student <ref type="bibr" target="#b16">[17]</ref> as the deep model, because B7 has higher FLOPs. In addition to the above two models, we also used other model combinations and have been listed in Tab.3.</p><p>We found that CLCNet can achieve competitive performance to general ensemble modeling, surpassing the accuracy of the single models, but only using much less average FLOPs than general ensemble modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a CLCNet that can predict confidence scores for the classification results in arbitrary dimension, and the CLCNet can be used in a simple cascade structure system, which is able to approximate or even exceed the performance of general ensemble modeling, while required much less computation than general ensemble modeling. And the models in the system are replaceable, new SOTA models can be replaced for better results. Further, by adjusting the threshold of the system, the average FLOPs of the system inference can be specified. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Work supported by MOST of Taiwan under contract numbers 111-2218-E-011-011-MBK and 111-2221-E-011-134-, and also by the "Center for Cyber-physical System Innovation" from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE) in Taiwan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The illustration of cascade structure system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.5, 0.2, 0.15, 0.1, 0.05 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of CLCNet (left) and the schematic of Restricted Self-Attention (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>An example of mapping a 5-dimensional classification result to an equivalent 100dimensional vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>x generated by Eq.2 A certain classification result vector Sorting A certain classification result vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The shapes of a sorted C x and a sorted classification vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Schematic diagram of bell shaped function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Architecture of TabNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3. 2</head><label>2</label><figDesc>Cascade structure system improved samples : All input sample space : Deep model correctly classified samples : Shallow model correctly classified samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Diagram of the relationship of the samples in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(b) also draws their comparison. The results show that although CLCNet does not exceed the accuracy of the deep model, it still achieves better performance compared to the intermediate size variants of VOLO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of CLCNet and EfficientNet/VOLO on ImageNet-1K. The red and blue curves are obtained by setting FLOPs and accuracy as coordinates, and connecting those coordinates that CLCNet gets under different thresholds, S and D stand for shallow model and deep model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of CLCNet and EfficientNet on ImageNet-1k. Bold indicates the value of the metric is close to EfficientNet variant, S and D stand for shallow model and deep model respectively, and we use CLCNet to denote the cascade structure system using CLCNet.</figDesc><table><row><cell>Model</cell><cell>Top-1 Acc.</cell><cell>Threshold</cell><cell>FLOPs per image</cell></row><row><cell>EfficientNet-B0</cell><cell>75.40%</cell><cell>##</cell><cell>0.39B</cell></row><row><cell>EfficientNet-B1</cell><cell>77.64%</cell><cell>##</cell><cell>0.70B</cell></row><row><cell>CLCNet (S:B0+D:B4)</cell><cell>77.74%</cell><cell>0.19</cell><cell>0.74B</cell></row><row><cell>EfficientNet-B2</cell><cell>78.73%</cell><cell>##</cell><cell>1.0B</cell></row><row><cell>CLCNet (S:B0+D:B4)</cell><cell>79.06%</cell><cell>0.27</cell><cell>0.996B</cell></row><row><cell>CLCNet (S:B0+D:B4)</cell><cell>78.71%</cell><cell>0.25</cell><cell>0.933B</cell></row><row><cell>EfficientNet-B3</cell><cell>80.52%</cell><cell>##</cell><cell>1.8B</cell></row><row><cell>CLCNet (S:B0+D:B4)</cell><cell>81.19%</cell><cell>0.43</cell><cell>1.77B</cell></row><row><cell>CLCNet (S:B0+D:B4)</cell><cell>80.50%</cell><cell>0.39</cell><cell>1.42B</cell></row><row><cell>EfficientNet-B4</cell><cell>82.00%</cell><cell>##</cell><cell>4.2B</cell></row><row><cell>CLCNet (S:B4+D:B7)</cell><cell>82.02%</cell><cell>0.05</cell><cell>4.27B</cell></row><row><cell>EfficientNet-B5</cell><cell>82.72%</cell><cell>##</cell><cell>9.9B</cell></row><row><cell>CLCNet (S:B4+D:B7)</cell><cell>83.59%</cell><cell>0.45</cell><cell>9.94B</cell></row><row><cell>CLCNet (S:B4+D:B7)</cell><cell>82.75%</cell><cell>0.27</cell><cell>6.1B</cell></row><row><cell>EfficientNet-B6</cell><cell>83.30%</cell><cell>##</cell><cell>19B</cell></row><row><cell>CLCNet (S:B4+D:B7)</cell><cell>83.88%</cell><cell>0.83</cell><cell>18.58B</cell></row><row><cell>CLCNet (S:B4+D:B7)</cell><cell>83.42%</cell><cell>0.39</cell><cell>8.95B</cell></row><row><cell>EfficientNet-B7</cell><cell>83.80%</cell><cell>##</cell><cell>37B</cell></row><row><cell>CLCNet (S:B4+D:B7)</cell><cell>83.88%</cell><cell>0.83</cell><cell>18.58B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of CLCNet and VOLO on ImageNet-1k. Bold indicates the value of the metric is close to VOLO variant, S and D stand for shallow model and deep model respectively.</figDesc><table><row><cell>Model</cell><cell>Top-1 Acc.</cell><cell>Threshold</cell><cell>FLOPs per image</cell></row><row><cell>VOLO-D1</cell><cell>83.66%</cell><cell>##</cell><cell>6.8B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>83.69%</cell><cell>0.15</cell><cell>6.89B</cell></row><row><cell>VOLO-D2</cell><cell>84.34%</cell><cell>##</cell><cell>14.1B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>84.75%</cell><cell>0.47</cell><cell>13.53B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>84.31%</cell><cell>0.39</cell><cell>10.28B</cell></row><row><cell>VOLO-D3</cell><cell>84.81%</cell><cell>##</cell><cell>20.6B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>85.126%</cell><cell>0.71</cell><cell>20.14B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>84.87%</cell><cell>0.51</cell><cell>14.64B</cell></row><row><cell>VOLO-D4</cell><cell>85.01%</cell><cell>##</cell><cell>43.8B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>85.27%</cell><cell>0.94</cell><cell>41.21B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>85.06%</cell><cell>0.63</cell><cell>18.30B</cell></row><row><cell>VOLO-D5</cell><cell>85.43%</cell><cell>##</cell><cell>69.0B</cell></row><row><cell>CLCNet (S:D1+D:D5)</cell><cell>85.28%</cell><cell>0.95</cell><cell>47.43B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of CLCNet and general ensemble modeling (GEM) on ImageNet-1k. Bold indicates fewer computations or higher accuracy. Metrics for the same model may be inconsistent with other tables due to different training methods or input image sizes. (retrain) indicates that CLCNet(Fig.2)is retrained with the current shallow model and deep model.</figDesc><table><row><cell>Model</cell><cell cols="3">Top-1 Acc. Threshold FLOPs per image</cell></row><row><cell>EfficientNet-B7 (Noisy Student) [17]</cell><cell>85.60%</cell><cell>##</cell><cell>37B</cell></row><row><cell>ViT-B/16 [16]</cell><cell>85.22%</cell><cell>##</cell><cell>33.03B</cell></row><row><cell>ConvNeXt-L [2]</cell><cell>85.04%</cell><cell>##</cell><cell>34.4B</cell></row><row><cell>VOLO-D3 [1]</cell><cell>85.71%</cell><cell>##</cell><cell>67.9B</cell></row><row><cell>CLCNet (S:ViT+D:EffNet-B7)</cell><cell>86.42%</cell><cell>0.51</cell><cell>40.75B</cell></row><row><cell>CLCNet (S:ViT+D:EffNet-B7) (retrain)</cell><cell>86.61%</cell><cell>0.96</cell><cell>51.93B</cell></row><row><cell>ViT+EffNet-B7 (GEM)</cell><cell>86.55%</cell><cell>##</cell><cell>70.03B</cell></row><row><cell>CLCNet (S:ConvNeXt-L+D:EffNet-B7)</cell><cell>86.39%</cell><cell>0.83</cell><cell>46.12B</cell></row><row><cell>CLCNet (S:ConvNeXt-L+D:EffNet-B7) (retrain)</cell><cell>86.42%</cell><cell>0.73</cell><cell>45.43B</cell></row><row><cell>ConvNeXt-L+EffNet-B7 (GEM)</cell><cell>86.42%</cell><cell>##</cell><cell>71.4B</cell></row><row><cell>CLCNet (S:ViT+D:VOLO-D3)</cell><cell>86.28%</cell><cell>0.75</cell><cell>56.55B</cell></row><row><cell>CLCNet (S:ViT+D:VOLO-D3) (retrain)</cell><cell>86.46%</cell><cell>0.85</cell><cell>57.46B</cell></row><row><cell>ViT+VOLO-D3 (GEM)</cell><cell>86.44%</cell><cell>##</cell><cell>100.93B</cell></row><row><cell>CLCNet (S:ViT+D:ConvNeXt-L)</cell><cell>86.00%</cell><cell>0.75</cell><cell>44.95B</cell></row><row><cell>CLCNet (S:ViT+D:ConvNeXt-L) (retrain)</cell><cell>86.02%</cell><cell>0.97</cell><cell>51.66B</cell></row><row><cell>ViT+ConvNeXt-L (GEM)</cell><cell>86.18%</cell><cell>##</cell><cell>67.43B</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We denote vectors or matrices in bold.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m">Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00885</idno>
		<title level="m">Idk cascades: Fast deep learning by learning not to overthink</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to cascade: Confidence calibration for improving the accuracy and computational cost of cascade inference systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Enomoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeharu</forename><surname>Eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7331" to="7339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Confnet: predict with confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-Yu</forename><surname>Sheng Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2921" to="2925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tabnet: Attentive interpretable tabular learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6679" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Xgboost: extreme gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Khotilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>R package version 0.4-2</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
