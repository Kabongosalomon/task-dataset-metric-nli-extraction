<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Bifurcated Backbone Strategy for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Ling</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Junwei</forename><forename type="middle">Han</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Bifurcated Backbone Strategy for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multimodal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. In particular, first, we propose to regroup the multilevel features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efficient, and backbone-independent. Extensive experiments show that BBS-Net significantly outperforms 18 SOTA models on 8 challenging datasets under 5 evaluation measures, demonstrating the superiority of our approach (?4% improvement in S-measure vs. the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.</p><p>Index Terms-RGB-D salient object detection, bifurcated backbone strategy, multi-level features, cascaded refinement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE goal of salient object detection (SOD) is to find and segment the most visually prominent object(s) in an image <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Over the last decade, SOD has attracted significant attention due to its widespread applications in object recognition <ref type="bibr" target="#b3">[4]</ref>, content-based image retrieval <ref type="bibr" target="#b4">[5]</ref>, image segmentation <ref type="bibr" target="#b5">[6]</ref>, image editing <ref type="bibr" target="#b6">[7]</ref>, video analysis <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and visual tracking <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Traditional SOD algorithms <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> are typically based on handcrafted features and fall short in capturing high-level semantic information (see also <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>). Recently, convolutional neural networks (CNNs) have been used for RGB SOD <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, achieving better performance compared to the traditional methods.</p><p>However, the performance of RGB SOD models tends to drastically decrease in certain complex scenarios (e.g., <ref type="figure">Fig. 1</ref>. Saliency maps of state-of-the-art (SOTA) CNN-based methods (i.e., DMRA <ref type="bibr" target="#b18">[19]</ref>, CPFP <ref type="bibr" target="#b20">[21]</ref>, TANet <ref type="bibr" target="#b17">[18]</ref>, PCF <ref type="bibr" target="#b21">[22]</ref> and Ours) and methods based on handcrafted features (i.e., SE <ref type="bibr" target="#b24">[25]</ref> and LBE <ref type="bibr" target="#b25">[26]</ref>). Our method generates higher-quality saliency maps and suppresses background distractors in challenging scenarios (top: complex background; bottom: depth with noise). cluttered backgrounds, multiple objects, varying illuminations, transparent objects, etc) <ref type="bibr" target="#b17">[18]</ref>. One of the most important reasons behind these failure cases may be the lack of depth information, which is critical for saliency prediction. For example, an object with less texture but closer to the camera is usually salient than an object with more texture but farther away. Depth maps contain abundant spatial structure and layout information <ref type="bibr" target="#b18">[19]</ref>, providing geometrical cues for improving the performance of SOD. Besides, depth information can be easily obtained using popular devices, e.g., stereo cameras, Kinect and smartphones, which are becoming increasingly more ubiquitous. Therefore, various algorithms (e.g., <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>) have been proposed to solve the SOD problem by combining RGB and depth information (i.e., RGB-D SOD).</p><p>To efficiently integrate RGB and depth cues for SOD, researchers have explored different but complementary multimodal and multi-level strategies <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> and have achieved encouraging results. However, existing RGB-D SOD methods still have to solve the following challenges:</p><p>(1) Effectively aggregating multi-level features. As discussed in <ref type="bibr" target="#b15">[16]</ref>, teacher features contain rich semantic macro information and can serve as strong guidance for locating salient objects, while student features provide affluent micro details that are beneficial for refining object edges. Therefore, arXiv:2007.02713v3 [cs.CV] 18 Aug 2021 current RGB-D SOD methods use either a dedicated aggregation strategy <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> or a progressive merging process <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> to leverage multi-level features. However, because they directly fuse multi-level features without considering levelspecific characteristics, these operations suffer from the inherent problem of noisy low-level features <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>. As a result, several methods are easily confused by the background (e.g., first and second rows in <ref type="figure">Fig. 1</ref>).</p><p>(2) Excavating informative cues from the depth modality. Previous algorithms usually regard the depth map as a fourthchannel input <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> of the original three-channel RGB image, or fuse RGB and depth features by simple summation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and multiplication <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, these methods treat depth and RGB information from the same perspective and ignore the fact that RGB images capture color and texture, whereas depth maps capture the spatial relations among objects. Due to this modality difference, the above-mentioned simple combination methods are not very efficient. Further, depth maps often have low quality, which introduces randomly distributed errors and redundancy into the network <ref type="bibr" target="#b36">[37]</ref>. For example, the depth map in the last row of <ref type="figure">Fig. 1</ref> is blurry and noisy. As a result, many methods (e.g., the top-ranked model DMRA <ref type="bibr" target="#b18">[19]</ref>) fail to detect the full extent of the salient object.</p><p>To address the above issues, we propose a novel Bifurcated Backbone Strategy Network (BBS-Net) for RGB-D SOD. The proposed method exploits multi-level features in a cascaded refinement way to suppress distractors in the lower layers. This strategy is based on the observation that teacher features provide discriminative semantic information without redundant details <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which may contribute significantly to eliminating the lower-layer distractors. As shown in <ref type="figure">Fig. 2</ref> (b), BBS-Net contains two cascaded decoder stages: (1) Crossmodal teacher features are integrated by the first cascaded decoder CD1 to predict an initial saliency map S 1 . (2) Student features are refined by an element-wise multiplication with the initial saliency map S 1 and are then aggregated by another cascaded decoder CD2 to produce the final saliency map S 2 . To fully capture the informative cues in the depth map and improve the compatibility of RGB and depth features, we further introduce a depth-enhanced module (DEM). This module exploits the inter-channel and spatial relations of the depth features and discovers informative depth cues.</p><p>Additionally, to obtain reasonable performance in real-world scenarios, not only an efficient model is needed but also a dataset with great generalization ability is required to train such model. There are several large-scale RGB-D datasets, e.g., NJU2K <ref type="bibr" target="#b37">[38]</ref>, NLPR <ref type="bibr" target="#b31">[32]</ref>, STERE <ref type="bibr" target="#b38">[39]</ref>, SIP <ref type="bibr" target="#b36">[37]</ref> and DUT <ref type="bibr" target="#b18">[19]</ref> with more than 1, 000 image pairs. However, researchers have often trained RGB-D models on the fixed training set (i.e., 1, 485 images from NJU2K and 700 images from NLPR). This limits the model's generation ability in various scenarios. Further, they have not studied the generalization ability of different datasets and have not proposed powerful training sets. In this paper, one of our goals is to study this problem in detail.</p><p>Our main contributions are summarized as follows:</p><p>? We propose a powerful Bifurcated Backbone Strategy Network (BBS-Net) to deal with multiple complicated real-world scenarios in RGB-D SOD. To address the long-overlooked problem of noise in low-level features decreasing the performance of saliency models, we carefully explore the characteristics of multi-level features in a bifurcated backbone strategy (BBS), i.e., features are split into two groups, as shown in <ref type="figure">Fig. 2</ref> (b). In this way, noise in student features can be eliminated effectively by the saliency map generated from teacher features. ? We further introduce a depth-enhanced module (DEM) in BBS-Net to enhance the depth features before merging them with the RGB features. The DEM module concentrates on the most informative parts of depth maps by two sequential attention operations. We leverage the attention mechanism to excavate important cues from the depth features of multiple side-out layers. This module is simple but has proven effective for fusing RGB and depth modalities in a complementary way. ? We conduct a comprehensive comparison with 18 SOTA methods using various metrics (e.g., max Fmeasure, MAE, S-measure, max E-measure, and PR curves). Experimental results show that BBS-Net outperforms all of these methods on eight public datasets, by a large margin. In terms of the predicted saliency maps, BBS-Net generates maps with sharper edges and fewer background distractors compared to existing models. <ref type="bibr">?</ref> We conduct a number of cross-dataset experiments to evaluate the quality of current popular RGB-D datasets and introduce a training set with high generalization ability for fair comparison and future research. Current RGB-D methods train their networks using the fixed training-test splits of different datasets, without exploring the difficulties of those datasets. To the best of our knowledge, we are the first to investigate this important but overlooked problem in the area of RGB-D SOD. This work is based on our previous conference paper <ref type="bibr" target="#b0">[1]</ref> and extends it significantly in five ways: 1) We further extend the approach by designing a depth adapter module, which makes the model contain around 50 percent parameters of the previous version but with similar performance. 2) We provide more details and experiments regarding our BBS-Net model, including motivation, feature visualizations, experimental settings, etc. 3) We investigate several previously unexplored issues, including cross-dataset generalization ability, post-processing methods, failure cases analysis, etc. 4) To further demonstrate our model performance, we conduct several comprehensive experiments over the recently released dataset, DUT <ref type="bibr" target="#b18">[19]</ref>. 5) We perform in-depth analyses and draw several novel conclusions which are critical in developing more powerful models in the future. We are hopeful that our study will provide deep insights into the underlying design mechanisms of RGB-D SOD, and will spark novel ideas. The complete algorithm, benchmark results, and post-processing toolbox are publicly available at https://github.com/zyjwuyan/BBS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Salient Object Detection</head><p>Over the past several decades, SOD <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> has garnered significant research interest due to its diverse applica-  <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>. (b) In this paper, we adopt a bifurcated backbone strategy (BBS) to split the multi-level features into student and teacher features. The initial saliency map S 1 is utilized to refine the student features to effectively suppress distractors. Then, the refined features are passed to another cascaded decoder to generate the final saliency map S 2 .</p><p>tions <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>. In early years, SOD methods were primarily based on intrinsic prior knowledge such as center-surround color contrast <ref type="bibr" target="#b45">[46]</ref>, global region contrast <ref type="bibr" target="#b11">[12]</ref>, background prior <ref type="bibr" target="#b46">[47]</ref> and appearance similarity <ref type="bibr" target="#b47">[48]</ref>. However, these methods heavily rely on heuristic saliency cues and low-level handcrafted features, thus lacking the guidance of high-level semantic information.</p><p>Recently, to solve this problem, deep learning based methods <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b52">[53]</ref> have been explored, exceeding handcrafted feature-based methods in complex scenarios. These deep methods <ref type="bibr" target="#b53">[54]</ref> usually leverage CNNs to extract multi-level multiscale features from RGB images and then aggregate them to predict the final saliency map. Such multi-level multiscale features <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> can help the model better understand the contextual and semantic information to generate highquality saliency maps. Besides, since image-based SOD may be limited in some real-world applications such as video captioning <ref type="bibr" target="#b56">[57]</ref>, autonomous driving <ref type="bibr" target="#b57">[58]</ref> and robotic interaction <ref type="bibr" target="#b58">[59]</ref>, SOD algorithms <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have also been explored for video analysis.</p><p>To further overcome the limits of deep models, researchers have also proposed to excavate edge information <ref type="bibr" target="#b59">[60]</ref> to guide prediction. These methods use an auxiliary boundary loss to improve the training and representative ability of segmentation tasks <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b62">[63]</ref>. With the auxiliary guidance from the edge information, deep models can predict maps with finer and sharper edges. In addition to edge guidance, another useful type of auxiliary information are depth maps, which capture the spatial distance information. These are the main focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-D Salient Object Detection</head><p>? Traditional Models.</p><p>Previous algorithms for RGB-D SOD mainly rely on extracting handcrafted features <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> from RGB and depth images. Contrast-based cues, including edge, color, texture and region, are largely utilized by these methods to compute the saliency of a local region. For example, Desingh et al. <ref type="bibr" target="#b63">[64]</ref> adopted the region-based contrast to calculate contrast strengths for the segmented regions. Ciptadi et al. <ref type="bibr" target="#b64">[65]</ref> used surface normals and color contrast to compute saliency. However, the local contrast methods are easily disturbed by high-frequency content <ref type="bibr" target="#b65">[66]</ref>, since they mainly rely on the boundaries of salient objects. Therefore, some algorithms, such as spatial prior <ref type="bibr" target="#b34">[35]</ref>, global contrast <ref type="bibr" target="#b66">[67]</ref>, and background prior <ref type="bibr" target="#b67">[68]</ref>, proposed to compute saliency by combining both local and global information.</p><p>To combine saliency cues from RGB and depth modalities more effectively, researchers have explored multiple fusion strategies. Some methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> process RGB and depth images together by regarding depth maps as fourth-channel inputs (early fusion). This operation is simple but does not achieve reliable results, since it disregards the differences between the RGB and depth modalities. Therefore, some algorithms <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref> extract the saliency information from the two modalities separately by first leveraging two backbones to predict saliency maps and then fusing the saliency results (late fusion). Besides, to enable the RGB and depth modalities to share benefits, other methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b37">[38]</ref> fuse RGB and depth features in a middle stage and then produce the corresponding saliency maps (middle fusion). Deep models also use the above three fusion strategies, and our method falls under the middle fusion category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Deep Models.</head><p>Early deep methods <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b67">[68]</ref> compute saliency confidence scores by first extracting handcrafted features, and then feeding them to CNNs. However, these algorithms need the low-level handcrafted features to be manually designed as input, and thus cannot be trained in an end-to-end manner. More recently, researchers have begun to extract deep RGB and depth features using CNNs in a bottomup fashion <ref type="bibr" target="#b69">[70]</ref>. Unlike handcrafted features, deep features contain a lot of contextual and semantic information, and can thus better capture representations of the RGB and depth modalities. These methods have achieved encouraging results, which can be attributed to two important aspects of feature fusion. One is their extraction and fusion of multi-level and multi-scale features from different layers, while the other is the mechanism by which the two different modalities (RGB and depth) are combined.</p><p>Various architectures have been designed to effectively integrate the multi-scale features. For example, Liu et al. <ref type="bibr" target="#b26">[27]</ref> obtained saliency map outputs from each side-out features by feeding a four-channel RGB-D image into a single backbone (single stream). Chen et al. <ref type="bibr" target="#b21">[22]</ref> leveraged two independent networks to extract RGB and depth features respectively, and then combined them in a progressive merging way (double stream). Furthermore, to learn supplementary features, <ref type="bibr" target="#b17">[18]</ref> designed a three-stream network consisting of two modalityspecific streams and a parallel cross-modal distillation stream to exploit complementary cross-modal information in the </p><formula xml:id="formula_0">F CD2 BConv3 C BConv3 BConv3 BConv3 BConv3 C UP?4 (a) F CD1 Cascaded Decoder UP?2 UP?2 UP?2 UP?2 BConv3 Conv3?3 11?11?2048 22?22?1024 44?44?512 UP?8 88?88?64 88?88?256 UP?4 UP?2 T 1 (b) T 2 PTM Conv1?1 TransB Conv1?1 TransB Conv1?1 88?88?32 S 1 S 2 G 352?352?1 352?352?3</formula><p>Depth RGB  bottom-up feature extraction process (three streams). Depth maps are sometimes low-quality and may thus contain significant noise or misleading information, which greatly decreases the performance of SOD models. To address this issue, Zhao et al. <ref type="bibr" target="#b20">[21]</ref> proposed a contrast-enhanced network to improve the quality of depth maps using the contrast prior. Fan et al. <ref type="bibr" target="#b36">[37]</ref> designed a depth depurator unit to evaluate the quality of depth maps and filter out the low-quality ones automatically. Three recent works have explored uncertainty <ref type="bibr" target="#b70">[71]</ref>, depth prediction <ref type="bibr" target="#b71">[72]</ref> and a joint learning strategy <ref type="bibr" target="#b72">[73]</ref> for saliency detection and achieved reasonable performance. There were also some concurrent works published in recent top conferences (e.g., ECCV [74]- <ref type="bibr" target="#b75">[76]</ref>). Discussing these works in detail is beyond the scope of this article. Please refer to the online benchmark (http://dpfan.net/d3netbenchmark/) and the latest survey <ref type="bibr" target="#b76">[77]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Current popular RGB-D SOD models directly integrate multi-level features using a single decoder ( <ref type="figure">Fig. 2 (a)</ref>). In contrast, the network flow of the proposed BBS-Net ( <ref type="figure">Fig.  3</ref>) explores a bifurcated backbone strategy. In ? III-B, we first detail the proposed bifurcated backbone strategy with the cascaded refinement mechanism. Then, to fully excavate informative cues from the depth map, we introduce a new depth-enhanced module in ? III-C. Additionally, we design a depth adapter module to further improve the efficiency of the model in ? III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bifurcated Backbone Strategy (BBS)</head><p>Our cascaded refinement mechanism leverages the rich semantic information in high-level cross-modal features to suppress background distractors. To support such a feat, we devise a bifurcated backbone strategy (BBS). It divides the multi-level cross-modal features into two groups, i.e.,</p><formula xml:id="formula_1">G 1 = {Conv1, Conv2, Conv3} and G 2 ={Conv3, Conv4, Conv5},</formula><p>where Conv3 is the split point. The original multi-scale information is well preserved by each group.</p><p>? Cascaded Refinement Mechanism.</p><p>To effectively leverage the characteristics of the features in the two groups' features, we train the network using a cascaded refinement mechanism. This mechanism first generates an initial saliency map with three cross-modal teacher features (i.e., G 2 ) and then enhances the details of the initial saliency map S 1 with three cross-modal student features (i.e., G 1 ), which are refined by the initial saliency map. This is based on the observation that high-level features contain rich semantic information that helps locate salient objects, while low-level features provide microlevel details that are beneficial for refining the boundaries. In other words, by exploring the characteristics of the multi-level features, this strategy can efficiently suppress noise in lowlevel cross-modal features, and can produce the final saliency map through a progressive refinement.</p><p>Specifically, we first merge RGB and depth features processed by the DEM to obtain the cross-modal features {f cm i ; i = 1, 2, ..., 5}. In stage one, the three cross-modality teacher features (i.e., f cm 3 , f cm 4 , f cm 5 ) are aggregated by the first cascaded decoder, which is denoted as:</p><formula xml:id="formula_2">S 1 = T 1 F CD1 (f cm 3 , f cm 4 , f cm 5 ) ,<label>(1)</label></formula><p>where F CD1 is the first cascaded decoder, S 1 is the initial saliency map, and T 1 represents two simple convolutional layers that transform the channel number from 32 to 1. In stage two, we leverage the initial saliency map S 1 to refine the three cross-modal student features, which is defined as:</p><formula xml:id="formula_3">f cm i = f cm i + f cm i S 1 ,<label>(2)</label></formula><p>where f cm i (i ? {1, 2, 3}) represents the refined features and denotes the element-wise multiplication. After that, the three refined student features are aggregated by another decoder followed by a progressively transposed module (PTM), which is formulated as:</p><formula xml:id="formula_4">S 2 = T 2 F CD2 (f cm 1 , f cm 2 , f cm 3 ) ,<label>(3)</label></formula><p>where F CD2 is the second cascaded decoder, S 2 denotes the final saliency map, and T 2 represents the PTM module.</p><p>? Cascaded Decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>After computing the two groups of multi-level cross-modal features ({f</head><formula xml:id="formula_5">cm i , f cm i+1 , f cm i+2 }, i ? {1, 3}),</formula><p>which are a fusion of the RGB and depth features from multiple layers, we need to efficiently leverage the multi-scale multi-level information in each group to carry out the cascaded refinement. Therefore, we introduce a light-weight cascaded decoder <ref type="bibr" target="#b28">[29]</ref> to integrate the two groups of multi-level crossmodal features. As shown in <ref type="figure">Fig. 3 (a)</ref>, the cascaded decoder consists of three global context modules (GCM) and a simple feature aggregation strategy. The GCM is refined from the RFB module <ref type="bibr" target="#b77">[78]</ref>. Specifically, it contains an additional branch to enlarge the receptive field and a residual connection <ref type="bibr" target="#b68">[69]</ref> to preserve the information. The GCM module thus includes four parallel branches. For all of these branches, a 1?1 convolution is first applied to reduce the channel size to 32. Then, for the k th (k ? {2, 3, 4}) branch, a convolution operation with a kernel size of 2k ? 1 and dilation rate of 1 is applied. This is followed by another 3 ? 3 convolution operation with the dilation rate of 2k ? 1. We aim to excavate the global contextual information from the cross-modal features. Next, the outputs of the four branches are concatenated together and a 3?3 convolution operation is then applied to reduce the channel number to 32. Finally, the concatenated features form a residual connection with the input features. The GCM module operation in the two cascaded decoders is denoted by:</p><formula xml:id="formula_6">f gcm i = F GCM (f i ).</formula><p>(4)</p><p>To further improve the representations of cross-modal features, we leverage a pyramid multiplication and concatenation feature aggregation strategy to aggregate the cross-modal features</p><formula xml:id="formula_7">({f gcm i , f gcm i+1 , f gcm i+2 }, i ? {1, 3}).</formula><p>As illustrated in <ref type="figure">Fig. 3 (a)</ref>, first, each refined feature f gcm i is updated by multiplying it with all higher-level features:</p><formula xml:id="formula_8">f gcm i = f gcm i ? kmax k=i+1 Conv F U P (f gcm k ) ,<label>(5)</label></formula><formula xml:id="formula_9">in which i ? {1, 2, 3}, k max = 3 or i ? {3, 4, 5}, k max = 5.</formula><p>F U P represents the upsampling operation if the features are not of the same scale. represents the element-wise multiplication, and Conv(?) represents the standard 3?3 convolution operation. Then, the updated features are integrated by a progressive concatenation strategy to produce the output:</p><formula xml:id="formula_10">S = T f gcm k ; Conv F U P f gcm k+1 ; Conv F U P (f gcm k+2 ) , (6)</formula><p>where S is the predicted saliency map, [x; y] denotes the concatenation operation of x and y, and k ? {1, 3}. In the first stage, T denotes two sequential convolutional layers (i.e., T 1 ), while, for the second stage, it represents the PTM module (i.e., T 2 ). The scale of the output of the second decoder is 88?88, which is 1/4 of the ground-truth (352?352), so directly upsampling the output to the size of the ground-truth will lose some details. To address this issue, we propose a simple yet effective progressively transposed module (PTM, <ref type="figure">Fig. 3</ref> (b)) to generate the final predicted map (S 2 ) in a progressive upsampling way. It consists of two residual-based transposed blocks <ref type="bibr" target="#b78">[79]</ref> and three sequential 1 ? 1 convolutions. Each residual-based transposed block contains a 3 ? 3 convolution and a residual-based transposed convolution.</p><p>Note that the proposed cascaded refinement mechanism is different from the recent refinement strategies CRN <ref type="bibr" target="#b79">[80]</ref>, SRM <ref type="bibr" target="#b80">[81]</ref>, R3Net <ref type="bibr" target="#b81">[82]</ref>, and RFCN <ref type="bibr" target="#b16">[17]</ref> in its usage of the initial map and multi-level features. The obvious difference and advantage of the proposed design is that our model only requires one round of saliency refinement to produce a good saliency map, while CRN, SRM, R3Net, and RFCN all need more iterations, which increases both the training time and computational resources. Besides, the proposed cascaded mechanism is also different from CPD <ref type="bibr" target="#b28">[29]</ref> in that it exploits both the details in student features and the semantic information in teacher features, while suppressing the noise in the student features at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth-Enhanced Module (DEM)</head><p>To effectively fuse the RGB and depth features, two main problems need to be solved: a) the compatibility of RGB and depth features needs to be improved due to the intrinsic modality difference, and b) the redundancy and noise in lowquality depth maps must be reduced. Inspired by <ref type="bibr" target="#b82">[83]</ref>, we design a depth-enhanced module (DEM) to address the issues by improving the compatibility of multi-modal features and excavating informative cues from the depth features.</p><p>Specifically, let f rgb i , f d i represent the feature maps of the i th (i ? 1, 2, ..., 5) side-out layer from the RGB and depth branches, respectively. As shown in <ref type="figure">Fig. 3</ref>, each DEM is added before each side-out feature map from the depth branch to enhance the compatibility of the depth features. This sideout process improves the saliency representation of depth features and, at the same time, preserves the multi-level multiscale information. The fusion process of the two modalities is depicted as:</p><formula xml:id="formula_11">f cm i = f rgb i + F DEM (f d i ),<label>(7)</label></formula><p>where f cm i denotes the cross-modal features of the i th layer. The DEM module contains a sequential channel attention op- eration and a spatial attention operation, which are formulated as:</p><formula xml:id="formula_12">BConv3 - BConv3 RGB Depth Depth Output BConv3 Element-wise Multiplication Element-wise Summation -Element-wise Subtraction BConv3 Conv3?3+BN+ReLU</formula><formula xml:id="formula_13">F DEM (f d i ) = S att C att (f d i ) ,<label>(8)</label></formula><p>in which C att (?) and S att (?) represent the spatial and channel attention operations, respectively. More specifically, the channel attention is implemented as:</p><formula xml:id="formula_14">C att (f ) = M P max (f ) ? f,<label>(9)</label></formula><p>where P max (?) denotes the global max pooling operation for each feature map, M(?) represents a multi-layer (twolayer) perceptron, f denotes the input feature map, and ? is the multiplication by the dimension broadcast. The spatial attention is denoted as:</p><formula xml:id="formula_15">S att (f ) = Conv R max (f ) f,<label>(10)</label></formula><p>where R max (?) is the global max pooling operation for each point in the feature map along the channel axis. The proposed depth enhanced module is different from previous RGB-D algorithms, which fuse the multi-level cross-modal features by direct concatenation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, enhance the multilevel depth features by a simple convolutional layer <ref type="bibr" target="#b18">[19]</ref> or improve the depth map by contrast prior <ref type="bibr" target="#b20">[21]</ref>. To the best of our knowledge, we are the first to introduce the attention mechanism to excavate informative cues from depth features in multiple side-out layers. Our experiments (see Tab. VI and <ref type="figure">Fig.  8</ref>) demonstrate the effectiveness of our approach in improving the compatibility of multi-modal features. Besides, the spatial and channel attention mechanisms are different from the operation proposed in <ref type="bibr" target="#b82">[83]</ref>. Based on the fact that SOD aims at finding the most prominent objects in an image, we only leverage a single global max pooling <ref type="bibr" target="#b83">[84]</ref> to excavate the most critical cues in depth features, which reduces the complexity of the module.</p><p>D. Improve the efficiency of BBS-Net.</p><p>Note that the above proposed BBS-Net leverages two backbones, without sharing weights, to extract RGB features and depth features. Such a design can make the model extract discriminative RGB features and depth features, respectively, but also introduces more parameters, leading to a suboptimal solution for lightweight applications. However, making the two branches share weights can cause a big degradation of the performance. It may be because the RGB image and the depth image are two different modalities, i.e., RGB image contains color, structure, and semantic information while the depth image includes the spatial distance information. Thus a naive sharing-weight mechanism of the two-branch backbones cannot be suitable to extract the multi-modal features. To solve this problem, we design a depth adapter module (DAM) to consider the modality difference of the RGB image and depth image. The same backbone can be suitable to extract twomodality features without decreasing much performance.</p><p>The whole architecture of the DAM is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Let I rgb and I depth denote the input RGB and depth image pair, respectively. We first calculate the modality difference I dif by,</p><formula xml:id="formula_16">I dif = Conv(I rgb ? I depth ),<label>(11)</label></formula><p>where I depth is broadcast to the same dimension as I rgb .</p><p>Such an operation can make the model understand the explicit difference between the depth image and the RGB image. Then the adapted depth output is computed by:</p><formula xml:id="formula_17">I dif = Conv Conv(I depth ) + Conv(I depth ) * I dif .<label>(12)</label></formula><p>In the efficient version of BBS-Net, the backbones of the two branches share parameters. When calculating the depth features, the depth image is first fed to the DAM module to obtain the adapted depth information and is then fed to the backbone to extract features. To further reduce model parameters, we also remove the last progressively transposed module (which makes negligible performance degradation) in the efficient version of BBS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>? Training Loss. Let H and W denote the height and width of the input images. Given the input RGB image X ? R H?W?3 and its corresponding depth map D ? R H?W?1 , our model predicts an initial saliency map S 1 ? [0, 1] H?W?1 and a final saliency map S 2 ? [0, 1] H?W?1 . Let G ? {0, 1} H?W?1 denote the binary ground-truth saliency map. We jointly optimize the two cascaded stages by defining the total loss:</p><formula xml:id="formula_18">L = ? ce (S 1 , G) + (1 ? ?) ce (S 2 , G),<label>(13)</label></formula><p>in which ce represents the binary cross entropy loss <ref type="bibr" target="#b20">[21]</ref> and ? ? [0, 1] controls the trade-off between the two parts of the losses. The ce is computed as:</p><formula xml:id="formula_19">ce (S, G) = G log S + (1 ? G) log(1 ? S),<label>(14)</label></formula><p>where S is the predicted saliency map.</p><p>? Training and Test Protocol. We use PyTorch <ref type="bibr" target="#b84">[85]</ref> to implement our model on a single 1080Ti GPU. Parameters of the backbone network (ResNet-50 <ref type="bibr" target="#b68">[69]</ref>) are initialized from the model pre-trained on ImageNet <ref type="bibr" target="#b85">[86]</ref>. Other parameters are initialized using the default PyTorch settings. We discard the last pooling and fully connected layers of ResNet-50 and leverage each middle output of the five convolutional blocks as the side-out feature maps. The two branches do not share weights and the only difference between them is that the depth branch has the input channel number set to one. Note for the efficient version BBS-Net , the two branches share weights, and the depth images are first fed to the depth adapter module to reduce the modality difference. The Adam algorithm <ref type="bibr" target="#b86">[87]</ref> is used to optimize our model, the betas are set to 0.9 and 0.99, and the weight decay is set to 0. We set the initial learning rate to 1e-4 and divide it by 10 every 60 epochs. The gradients are clipped into [?0.5, 0.5] to make the training stable. The input RGB and depth images are resized to 352 ? 352 for both the training and test phases. We augment all the training images using multiple strategies (i.e., random flipping, rotating, and border clipping). It takes about ten hours to train the model with a mini-batch size of 10 for 150 epochs. Our experiments show that the model is robust to the hyper-parameter ?. Thus, we set ? to 0.5 (i.e., same importance for the two losses). In the test phase, the predicted maps are upsampled to the same dimension of ground truth by the bilinear interpolation and are then normalized to [0,1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>? Datasets.</p><p>We conduct our experiments on eight challenging RGB-D SOD benchmark datasets: NJU2K <ref type="bibr" target="#b37">[38]</ref>, NLPR <ref type="bibr" target="#b31">[32]</ref>, STERE <ref type="bibr" target="#b38">[39]</ref>, DES <ref type="bibr" target="#b34">[35]</ref>, LFSD <ref type="bibr" target="#b87">[88]</ref>, SSD <ref type="bibr" target="#b88">[89]</ref>, SIP <ref type="bibr" target="#b36">[37]</ref> and DUT <ref type="bibr" target="#b18">[19]</ref>. NJU2K <ref type="bibr" target="#b37">[38]</ref> is the largest RGB-D dataset containing 1, 985 image pairs. NLPR   <ref type="bibr" target="#b18">[19]</ref>.</p><formula xml:id="formula_20"># Method Dataset DUT [19] S? ? max F ? ? max E ? ?</formula><p>Handcrafted MB <ref type="bibr" target="#b89">[90]</ref> .607 .577 .691 LHM <ref type="bibr" target="#b31">[32]</ref> .568 .659 .767 DESM <ref type="bibr" target="#b34">[35]</ref> .659 .668 .733 DCMC <ref type="bibr" target="#b90">[91]</ref> .499 .406 .712 CDCP <ref type="bibr" target="#b35">[36]</ref> .687 .633 .794</p><p>Deep-based DMRA <ref type="bibr" target="#b18">[19]</ref> .888 .883 .927 A2dele <ref type="bibr" target="#b91">[92]</ref> .886 .892 .929 SSF <ref type="bibr" target="#b92">[93]</ref> .916 .924 .951 BBS-Net (ours)</p><p>.920 .927 .955 DUT <ref type="bibr" target="#b18">[19]</ref> includes 1200 images from multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds and low-intensity environments).</p><p>? Training/Testing. We follow the same settings as <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref> for fair comparison. In particular, the training set contains 1, 485 samples from the NJU2K dataset and 700 samples from the NLPR dataset. The test set consists of the remaining images from NJU2K (500) and NLPR (300), and the whole of STERE (1, 000), DES, LFSD, SSD and SIP. As for the recent proposde DUT <ref type="bibr" target="#b18">[19]</ref> dataset, following <ref type="bibr" target="#b18">[19]</ref>, we adopt the same training data of DUT, NJU2K, and NLPR to train the compared deep models (i.e., DMRA <ref type="bibr" target="#b18">[19]</ref>, A2dele <ref type="bibr" target="#b91">[92]</ref>, SSF <ref type="bibr" target="#b92">[93]</ref>, and our BBS-Net) and test the performance on the test set of DUT. Please refer to Tab. I for more details.</p><p>? Evaluation Metrics. We employ five widely used metrics, including S-measure (S ? ) <ref type="bibr" target="#b96">[97]</ref>, E-measure (E ? ) <ref type="bibr" target="#b97">[98]</ref>, Fmeasure (F ? ) <ref type="bibr" target="#b98">[99]</ref>, mean absolute error (MAE), and precisionrecall (PR) curves to evaluate various methods. Evaluation code: http://dpfan.net/d3netbenchmark/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with SOTAs</head><p>? Contenders. We compare the proposed BBS-Net with ten algorithms based on handcrafted features <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b93">[94]</ref>- <ref type="bibr" target="#b95">[96]</ref> and eight methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b69">[70]</ref> that use deep learning. We train and test these methods using their default settings. For the methods without released source codes, we compare with their reported results.</p><p>? Quantitative Results.</p><p>As shown in Tab. I, Tab. III, our method outperforms all algorithms based on handcrafted features as well as SOTA CNN-based methods by a large margin, in terms of all four evaluation metrics (i.e., S-measure (S ? ), F-measure (F ? ), E-measure (E ? ) and MAE (M )). Performance gains over the best compared algorithms (ICCV'19 DMRA <ref type="bibr" target="#b18">[19]</ref> and CVPR'19 CPFP <ref type="bibr" target="#b20">[21]</ref>) are (2.5% ? 3.5%, 0.7% ? 3.9%, 0.8% ? 2.3%, 0.009 ? 0.016) for the metrics (S ? , maxF ? , maxE ? , M ) on seven challenging datasets. The PR curves of different methods on various datasets are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. It can be easily deduced from the PR curves that our method (i.e., solid red lines) outperforms all the SOTA algorithms.</p><p>In terms of speed, BBS-Net achieves 24.32 fps on a single GTX 1080Ti GPU (batch size of one), as shown in Tab. II, which is suitable for real-time applications. In terms of parameters, BBS-Net contains only around 50 percent parameters of the BBS-Net (i.e., 25.96M vs. 49.77M), but its performance is similar to the BBS-Net and also superior to other compared methods (as shown in the last two columns in Tab. III). It means that BBS-Net can process more images in the same time (with a larger batch size) for real-world applications.</p><p>There are three popular backbone models used in deep RGB-D models (i.e., VGG-16 <ref type="bibr" target="#b99">[100]</ref>, VGG-19 <ref type="bibr" target="#b99">[100]</ref> and ResNet-50 <ref type="bibr" target="#b68">[69]</ref>). To further validate the effectiveness of the proposed method, we provide performance comparisons using different backbones in Tab. IV. We find that ResNet-50 performs best among the three backbones, and VGG-19 and VGG-16 have similar performances. Besides, the proposed method exceeds the SOTA methods (e.g., TANet <ref type="bibr" target="#b17">[18]</ref>, CPFP <ref type="bibr" target="#b20">[21]</ref>, and DMRA <ref type="bibr" target="#b18">[19]</ref>) with any of the backbones.</p><p>? Visual Comparison. <ref type="figure" target="#fig_5">Fig. 6</ref> provides examples of maps predicted by our method and several SOTA algorithms. Visualizations cover simple scenes (a) and various challenging scenarios, including small objects (b), multiple objects (c), complex backgrounds (d), and low contrast scenes (e).</p><p>First, the first row of (a) shows an easy example. The flower in the foreground is evident in the original RGB image, but the depth map is of low quality and contains some misleading information. The SOTA algorithms, such as DMRA and CPFP, fail to predict the whole extent of the salient object due to the interference from the depth map. Our method can eliminate the side-effects of the depth map by utilizing the complementary depth information more effectively. Second, two examples of small objects are shown in (b). Despite the handle of the teapot in the first row being tiny, our method can accurately detect it. Third, we show two examples with multiple objects in an    <ref type="bibr" target="#b17">[18]</ref> . image in (c). Our method locates all salient objects in the image. It segments the objects more accurately and generates sharper edges compared to other algorithms. Even though the depth map in the first row of (c) lacks clear information, our algorithm predicts the salient objects correctly. Fourth, (d) shows two examples with complex backgrounds. Here, our method produces reliable results, while other algorithms confuse the background as a salient object. Finally, (e) presents two examples in which the contrast between the object and the background is low. Many algorithms fail to detect and segment the entire extent of the salient object. Our method produces satisfactory results by suppressing background distractors and exploring the informative cues from the depth map.</p><formula xml:id="formula_21">S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? TANet (VGG-16)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>? Analysis of Different Aggregation Strategies. To validate the effectiveness of our cascaded refinement mechanism, we conduct several experiments to explore different aggregation strategies. Results are shown in Tab. V and <ref type="figure">Fig.   7</ref>. 'Low3' means that we only integrate the low-level features (Conv1?3) using the decoder without the refinement from the initial map. Low-level features contain abundant details that are beneficial for refining the object edges, but at the same time introduce a lot of background distraction. Integrating only low-level features produces inadequate results and generates many distractors (e.g., the example in <ref type="figure">Fig. 7)</ref>. 'High3' only integrates the high-level features (Conv3?5) to predict the saliency map. Compared with low-level features, high-level features contain more semantic information. As a result, they help locate the salient objects and preserve edge information. Thus, integrating high-level features leads to better results. 'All5' aggregates features from all five levels (Conv1?5) directly, using a single decoder for training and testing. It achieves comparable results with the 'High3' but may include background noise introduced by the low-level features (see column 'All5' in <ref type="figure">Fig. 7)</ref>. 'BBS-NoRF' indicates that we directly remove the refinement flow of our model. This leads to poor performance. 'BBS-RH' is a reverse refinement strategy to our cascaded refinement mechanism, where teacher features  <ref type="table" target="#tab_2">(Conv1?3) AND ARE THEN INTEGRATED TO GENERATE THE FINAL SALIENCY MAP, AND 6: OUR CASCADED REFINEMENT MECHANISM.</ref> # Settings NJU2K <ref type="bibr" target="#b37">[38]</ref> NLPR <ref type="bibr" target="#b31">[32]</ref> STERE <ref type="bibr" target="#b38">[39]</ref> DES <ref type="bibr" target="#b34">[35]</ref> LFSD <ref type="bibr" target="#b87">[88]</ref> SSD <ref type="bibr" target="#b88">[89]</ref> SIP <ref type="bibr" target="#b36">[37]</ref>   </p><formula xml:id="formula_22">S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? 1</formula><formula xml:id="formula_23">BM CA SA PTM S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? S? ? M ? 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Low3 High3</head><p>All5 BBS-RH GT BBS-RL (Conv3?5) are first refined by the initial map aggregated by low-level features (Conv1?3) and are then integrated to generate the final saliency map. It performs worse than the proposed mechanism (BBS-RL), because noise in lowlevel features cannot be effectively suppressed in this reverse refinement strategy. Besides, compared to 'All5', our method fully utilizes the features at different levels, and thus achieves significant performance improvement (i.e., the last row in Tab. V) with fewer background distractors and sharper edges.</p><p>? Impact of Different Modules.</p><p>To validate the effectiveness of the different modules in the proposed BBS-Net, we conduct various experiments, as shown in Tab. VI and <ref type="figure">Fig. 8</ref>. The base model (BM) is our BBS-Net without additional modules (i.e., CA, SA, and PTM). Note that the BM alone performs better than the SOTA methods over almost all datasets, as shown in Tab. III and Tab. VI. Adding the channel attention (CA) and spatial attention (SA) modules enhances the performance on most of the datasets (see the results shown in the second and third rows of Tab. VI). When we combine the two modules (the fourth row in Tab. VI), the performance is greatly improved on all datasets, compared to the BM. We can easily conclude from the '#2', '#3' and '#4' columns in <ref type="figure">Fig.  8</ref> that the spatial attention and channel attention mechanisms in DEM allow the model to focus on the informative parts of the depth features, which results in better suppression of background clutter. Finally, we add a progressively transposed block before the second decoder to gradually upsample the feature map to the same resolution as the ground truth. The results in the fifth row of Tab. VI and the '#5' column of <ref type="figure">Fig.  8</ref> show that the 'PTM' achieves impressive performance gains on all datasets and generates sharper edges with finer details.</p><p>To further analyze the effectiveness of the cascaded decoder, we experiment with changing it to an element-wise summation mechanism. That is to say, we first change the features from different layers to the same dimension using 1 ? 1 convolution and upsampling operation and then fuse them by elementwise summation. Experimental results in Tab. VII show that the cascaded decoder achieves comparable results on SIP, and outperforms the element-wise sum on the other six datasets, which demonstrates its effectiveness.</p><p>? Hyper-parameter Analysis.</p><p>We conduct an experiment to discuss the settings of ?. As shown in Tab. VIII, the performance (S ? and MAE) is about the same for different values of ?, thus we simply set it to 0.5 to balance the weight between the losses of the initial map and the final map.</p><p>? Effectiveness Analysis of the Depth Adapter Module.</p><p>To demonstrate the effectiveness of the proposed depth adapter module (DAM), we conduct an experiment in Tab. IX. As shown in the table, BBS-Net (w/ DAM) performs better than BBS-Net (w/o DAM) on seven datasets, especially on the dataset of NJU2K, LFSD, and SIP. The DAM can model the modality difference between the RGB image and depth image, reduces the gap between them. Thus the same backbone is more suitable to extract two different modality features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Utility of Depth Information</head><p>To explore whether depth information can really contribute to the performance of SOD, we conduct two experiments, results of which are shown in Tab. X. On the one hand, we compare the proposed method with five SOTA RGB SOD methods (i.e., PiCANet <ref type="bibr" target="#b100">[101]</ref>, PAGRN <ref type="bibr" target="#b49">[50]</ref>, R3Net <ref type="bibr" target="#b81">[82]</ref>, CPD <ref type="bibr" target="#b28">[29]</ref>, and PoolNet <ref type="bibr" target="#b15">[16]</ref>) by neglecting the depth information. We train and test CPD and PoolNet using the same training and test sets as our model. For other methods, we use the published results from <ref type="bibr" target="#b18">[19]</ref>. It is clear that the proposed methods (i.e., BBS-Net (w/ depth)) can significantly exceed SOTA RGB SOD methods thanks to depth information. On the other hand, we train and test the proposed method without using the depth information by setting the inputs of the depth branch to zero (i.e., BBSNet (w/o depth)). Comparing the results of the last two rows in the table, we find that depth information effectively improves the performance of the proposed model (especially over the small datasets, i.e., DES, LFSD, and SSD).</p><p>The two experiments together demonstrate the benefits of the depth information for SOD. Depth map serves as prior knowledge and provides spatial distance information and contour guidance to detect salient objects. For example, in   <ref type="figure">Fig. 10</ref>. Visual effects of different post-processing methods. We explore three methods, including the adaptive threshold cut ('ADP' in the paper), Ostu's method and the popular algorithm of conditional random fields (CRF). 9, depth feature (b) has high activation on the object border. Thus, cross-modal feature (c) has clearer borders compared with the original RGB feature (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of Post-processing Methods</head><p>According to <ref type="bibr" target="#b101">[102]</ref>- <ref type="bibr" target="#b103">[104]</ref>, the predicted saliency maps can be further refined by post-processing methods. This may be useful to sharpen the salient edges and suppress the background response. We conduct several experiments to study the effects of various post-processing methods, including the adaptive threshold cut (i.e., the threshold is defined as the double of the mean value of the saliency map), Ostu's method <ref type="bibr" target="#b104">[105]</ref>, and conditional random field (CRF) <ref type="bibr" target="#b105">[106]</ref>. The performance comparisons of the post-processing methods in terms of MAE are shown in Tab. XI, while a visual comparison is provided in <ref type="figure">Fig. 10</ref>.</p><p>From the results, we draw the following conclusions. First, the three post-processing methods all make the salient edges sharper, as shown in the fourth to sixth columns in <ref type="figure">Fig. 10</ref>. Second, both Ostu and CRF help reduce the MAE effectively, as shown in Tab. XI. This is possibly because they can suppress the background noise. As shown in <ref type="figure">Fig. 10</ref>, Ostu and CRF can significantly reduce the background noise, while the adaptive threshold operation further expands the background blur from the original results of BBS-Net. Further, in terms of overall results, CRF performs the best, while the adaptive threshold algorithm is the worst. Ostu performs worse than CRF, because it cannot always fully eliminate the background noise (e.g., the fifth and sixth columns in <ref type="figure">Fig. 10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Failure Case Analysis</head><p>We illustrate six representative failure cases in <ref type="figure" target="#fig_8">Fig. 11</ref>.</p><p>The failure examples are divided into four categories. In the first category, the model either misses the salient object or detects it imperfectly. For example, in column (a), our model fails to detect the salient object even when the depth map has clear boundaries. This is because the salient object has the same texture and content layout as the background in the RGB image. Thus, the model cannot find the salient object based only on the borders. In column (b), our method cannot fully segment the transparent salient objects, since the background has low contrast, and the depth map lacks useful information. The second situation is that the model identifies the background as the salient part. For example, the lanterns in column (c) have a similar color to the background wallpaper, which confuses the model into thinking that the wallpaper is the salient object. Besides, the background of the RGB image in column (d) is complex and thus our model does not detect the complete salient objects. The third type of failure case is when an image contains several separate salient objects. In this case, our model may not detect them all. As shown in column (e), with five salient objects in the RGB images, the model fails to detect the two objects that are far from the camera. This may be because the model tends to consider the objects that are closer to the camera more salient. The final case is when salient objects are occluded by non-salient ones. Note that in column (f), the car is occluded by two ropes in front of the camera. Here our model predicts the ropes as salient objects.</p><p>Most of these failure cases can be attributed to interference information from the background (e.g., color, contrast, and content). We propose some ideas that may be useful for solving these failure cases. The first is to introduce some humandesigned prior knowledge, such as providing a boundary that can approximately distinguish the foreground from the background. Leveraging such prior knowledge, the model may better capture the characteristics of the background and salient objects. This strategy may contribute significantly to solving the failure cases especially for columns (a) and (b). Besides, the depth map can also be seen as a type of prior knowledge for this task. Thus, some failure cases (i.e., (b), (c), and (e)) may be solved when a high-quality depth map is available. Second, we find that in the current RGB-D datasets, the image pairs for challenging scenarios (e.g., complex backgrounds, lowcontrast backgrounds, transparent objects, multiple objects, shielded objects, and small objects) constitute a small fraction of the whole dataset. Therefore, adding more difficult examples to the training data could help mitigate the failure cases. Finally, depth maps may sometimes introduce misleading information, such as in column (d). Considering how to exploit salient cues from the RGB image to suppress the noise in the depth map could be a promising solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-Dataset Generalization Analysis</head><p>For a deep model to obtain reasonable performance in real-world scenarios, it not only requires an efficient design but must also be trained on a high-quality dataset with a great generalization power. A good dataset usually contains sufficient images, with all types of variations that occur in reality, so that deep models trained on it can generalize well to the real world. In the area of RGB-D SOD, there are several large-scale datasets (i.e., NJU2K, NLPR, STERE, SIP, and DUT), with around 1, 000 training images.</p><p>? Single Dataset Generalization Analysis.</p><p>Here, we conduct cross-dataset generalization experiments on the abovementioned five datasets to measure their generalization ability. To make fair comparisons among multiple datasets, we balance the datasets with equal number of training samples. Specifically, we randomly choose 700 image pairs in each dataset for training, and the remaining images are used for testing. We then retrain the proposed model on a single training set, and test it on all four test sets. The results are summarized in Tab. XII. 'Self' represents the results of training and testing on the same dataset. 'Mean Others' indicates the average performance on all test sets except 'self'. 'Drop' means the (percent) drop from 'Self' to 'Mean Others'. First, it can be seen from the table that NJU2K and DUT are the hardest datasets since their 'Mean Others' of column 'NJU2K' and 'DUT' are significantly lower than the other three datasets. This may be because the two datasets include multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds, etc). Second, STERE has the best generalization ability, because the average drop of S ? and F ? is lowest among all five datasets. Besides, SIP generalizes worst (i.e., the drop is the largest among all five datasets), since it mainly focuses on a single person or multiple persons in the wild. We also notice that the score of the SIP column ('Mean Others') is the highest. This is likely because the quality of the depth maps captured by the Huawei Mate10 is higher than that produced by traditional devices. Finally, none of the models trained with a single dataset perform best over all test sets. Thus, we further explore training on different combinations of datasets with the aim of building a dataset with a strong generalization ability for future research.</p><p>? Dataset Combination for Generalization Improvement. According to the results in Tab. XII, the model trained on the SIP dataset does not generalize well to other datasets, so we discard it. We thus select four relatively large-scale datasets, i.e., NJU2K, NLPR, STERE, and DUT, to conduct our multi-dataset training experiments. As shown in Tab. XIII, we consider all possible training combinations of these four datasets and test the models on all available test sets. From  <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">400</ref> the results in the table, we draw the following conclusions. First, more training examples do not necessarily lead to better performance on some test sets. For example, although 'NJ+NL+ST', 'NJ+NL+DU' and 'NJ+NL+ST+DU' contain external training sets, unlike 'NJ+NL', they perform similarly with 'NJ+NL' on the test set of 'NL'. Second, including the NJU2K dataset is important for the model to generalize well to small datasets (i.e., LFSD, SSD). The model trained using the combinations without NJU2K (i.e., 'NL+ST' 'NL+DU', 'ST+DU' and 'NL+ST+DU') all obtain low F-measure values (less than 0.8) on the LFSD and SSD test sets. In contrast, including 'NJ' in the training sets increases the F-measures on the LFSD and SSD datasets by over 0.05. Finally, including more examples in the training sets can improve the stability of the model, as it allows diverse scenarios to be taken into consideration. Thus, the model trained on 'NJ+NL+ST+DU', which has the most examples, obtains the best, or are very close to the best, performance. Due to the limited size of current RGB-D datasets, it is hard for a model trained using a single dataset to perform well under various scenarios. Thus, we recommend training a model using a combination of datasets with diverse examples to avoid model over-fitting issues. To promote the development of RGB-D SOD, we hope more challenging RGB-D datasets with diverse examples and high-quality depth maps can be proposed in the future.</p><formula xml:id="formula_24">S? ? F ? ? M ? S? ? F ? ? M ? S? ? F ? ? M ? S? ? F ? ? M ? S? ? F ? ? M ? S? ? F ? ? M ? S? ? F ? ? M ? S? ? F ? ? M ? NJ+NL</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we present a Bifurcated Backbone Strategy Network (BBS-Net) for the RGB-D SOD. To effectively suppress the intrinsic distractors in low-level cross-modal features, we propose to leverage the characteristics of multi-level crossmodal features in a cascaded refinement way: low-level features are refined by the initial saliency map that is produced by the high-level cross-modal features. Besides, we introduce a depth-enhanced module to excavate the informative cues from the depth features in the channel and spatial views, in order to improve the cross-modal compatibility when merging RGB and depth features. Experiments on eight challenging datasets demonstrate that BBS-Net outperforms 18 SOTA models, by a large margin, under multiple evaluation metrics. Finally, we conduct a comprehensive analysis of the existing RGB-D datasets and introduce a powerful training set with a strong generalization ability for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 . 1 ? f rgb 5 ) 3 ? f cm 5 ) 1 ? f cm 3 )</head><label>3153513</label><figDesc>Architecture of our BBS-Net. Feature Extraction: 'Conv1'?'Conv5' denote different layers from ResNet-50<ref type="bibr" target="#b68">[69]</ref>. Multi-level features (f d 1 ? f d 5 ) from the depth branch are enhanced by the DEM and are then fused with features (i.e., f rgb from the RGB branch. Stage 1: cross-modal teacher features (f cm are first aggregated by the cascaded decoder (a) to produce the initial saliency map S 1 . Stage 2: Then, student features (f cm are refined by the initial saliency map S 1 and are integrated by another cascaded decoder to predict the final saliency map S 2 . See ? III for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Architecture of the depth adapter module (DAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>PR Curves of the proposed model and 18 SOTA algorithms over six datasets. Dots on the curves represent the value of precision and recall at the maximum F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b31">[32]</ref> consists of 1, 000 image pairs captured by a standard Microsoft Kinect with a resolution of 640?480. STERE<ref type="bibr" target="#b38">[39]</ref> is the first stereoscopic photo collection, containing 1, 000 images downloaded from the Internet. DES<ref type="bibr" target="#b34">[35]</ref> is a small-scale RGB-D dataset that includes 135 indoor image pairs. LFSD<ref type="bibr" target="#b87">[88]</ref> contains 60 image pairs from indoor scenes and 40 image pairs from outdoor scenes. SSD<ref type="bibr" target="#b88">[89]</ref> includes 80 images picked from three stereo movies with both indoor and outdoor scenes. The collected images have a high resolution of 960 ? 1, 080.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative visual comparison of our model versus eight SOTA models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Visual comparison of aggregation strategies. 'Low3' only integrates low-level features (Conv1?3), while 'High3' aggregates high-level features (Conv3?5) for predicting the saliency map. 'All5' combines all fivelevel features directly for prediction. 'BBS-RH/BBS-RL' denotes that highlevel/low-level features are first refined by the initial map aggregated by the low-level/high-level features and are then integrated to predict the final map. Analysis of gradually adding various modules. The first two columns are the RGB and ground-truth images, respectively. '#' denotes the corresponding row of Tab. VI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Feature visualization. Here, (a), (b), and (c) are the average RGB feature, depth feature and cross-modal feature of the Conv3 layer. To visualize them, we average the feature maps along their channel axis to obtain the visualization map. 'Ours' refers to the BBS-Net (w/ depth).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Some representative failure cases of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF DIFFERENT MODELS ON THE DUT<ref type="bibr" target="#b18">[19]</ref> DATASET.</figDesc><table /><note>MODELS ARE TRAINED AND TESTED ON THE DUT USING THE PROPOSED TRAINING AND TEST SETS SPLIT FROM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II MULTIPLE</head><label>II</label><figDesc>COMPARISONS OF BBS-NET AND BBS-NET . THE EFFICIENT VERSION BBS-NET HAS ONLY AROUND 50 PERCENT PARAMETERS OF BBS-NET.</figDesc><table><row><cell>#</cell><cell>Parameters (M)</cell><cell>FLOPs (G)</cell><cell>fps</cell></row><row><cell>BBS-Net</cell><cell>49.77</cell><cell>31.40</cell><cell>24.32</cell></row><row><cell>BBS-Net</cell><cell>25.96</cell><cell>25.26</cell><cell>25.54</cell></row></table><note>SIP [37] consists of 1, 000 image pairs captured by a smart phone with a resolution of 992 ? 744, using a dual camera.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON OF MODELS USING S-MEASURE (S?), MAX F-MEASURE (maxF ? ), MAX E-MEASURE (maxE ? ) AND MAE (M ) SCORES ON SEVEN PUBLIC DATASETS. ? (?) DENOTES THAT THE HIGHER (LOWER) THE SCORE, THE BETTER. DENOTES THE EFFICIENT VERSION OF BBS-Net. Net LHM CDB DESM GP CDCP ACSD LBE DCMC MDSF SE DF AFNet CTMF MMCI PCF TANet CPFP DMRA Ours Ours S? ? .514 .624 .665 .527 .669 .699 .695 .686 .748 .664 .763 .772 .849 .858 .877 .878 .879 .886 .916 .921 maxF ? ? .632 .648 .717 .647 .621 .711 .748 .715 .775 .748 .650 .775 .845 .852 .872 .874 .877 .886 .918 .920 maxE ? ? .724 .742 .791 .703 .741 .803 .803 .799 .838 .813 .696 .853 .913 .915 .924 .925 .926 .927 .948 .949 M ? .205 .203 .283 .211 .180 .202 .153 .172 .157 .169 .141 .100 .085 .079 .059 .060 .053 .051 .038 .035 NLPR S? ? .630 .629 .572 .654 .727 .673 .762 .724 .805 .756 .802 .799 .860 .856 .874 .886 .888 .899 .925 .930 maxF ? ? .622 .618 .640 .611 .645 .607 .745 .648 .793 .713 .778 .771 .825 .815 .841 .863 .867 .879 .909 .918 maxE ? ? .766 .791 .805 .723 .820 .780 .855 .793 .885 .847 .880 .879 .929 .913 .925 .941 .932 .947 .959 .961 M ? .108 .114 .312 .146 .112 .179 .081 .117 .095 .091 .085 .058 .056 .059 .044 .041 .036 .031 .026 .023 STERE S? ? .562 .615 .642 .588 .713 .692 .660 .731 .728 .708 .757 .825 .848 .873 .875 .871 .879 .835 .905 .908 maxF ? ? .683 .717 .700 .671 .664 .669 .633 .740 .719 .755 .757 .823 .831 .863 .860 .861 .874 .847 .898 .903 maxE ? ? .771 .823 .811 .743 .786 .806 .787 .819 .809 .846 .847 .887 .912 .927 .925 .923 .925 .911 .940 .942 M ? .172 .166 .295 .182 .149 .200 .250 .148 .176 .143 .141 .075 .086 .068 .064 .060 .051 .066 .043 .041 DES S? ? .578 .645 .622 .636 .709 .728 .703 .707 .741 .741 .752 .770 .863 .848 .842 .858 .872 .900 .930 .933 maxF ? ? .511 .723 .765 .597 .631 .756 .788 .666 .746 .741 .766 .728 .844 .822 .804 .827 .846 .888 .921 .927 maxE ? ? .653 .830 .868 .670 .811 .850 .890 .773 .851 .856 .870 .881 .932 .928 .893 .910 .923 .943 .965 .966 M ? .114 .100 .299 .168 .115 .169 .208 .111 .122 .090 .093 .068 .055 .065 .049 .046 .038 .030 .022 .021 LFSD S? ? .553 .515 .716 .635 .712 .727 .729 .753 .694 .692 .783 .738 .788 .787 .786 .801 .828 .839 .859 .864 maxF ? ? .708 .677 .762 .783 .702 .763 .722 .817 .779 .786 .813 .744 .787 .771 .775 .796 .826 .852 .855 .858 maxE ? ? .763 .871 .811 .824 .780 .829 .797 .856 .819 .832 .857 .815 .857 .839 .827 .847 .863 .893 .896 .901 M ? .218 .225 .253 .190 .172 .195 .214 .155 .197 .174 .145 .133 .127 .132 .119 .111 .088 .083 .076 .072 SSD S? ? .566 .562 .602 .615 .603 .675 .621 .704 .673 .675 .747 .714 .776 .813 .841 .839 .807 .857 .858 .882 maxF ? ? .568 .592 .680 .740 .535 .682 .619 .711 .703 .710 .735 .687 .729 .781 .807 .810 .766 .844 .827 .859 maxE ? ? .717 .698 .769 .782 .700 .785 .736 .786 .779 .800 .828 .807 .865 .882 .894 .897 .852 .906 .894 .919 M ? .195 .196 .308 .180 .214 .203 .278 .169 .192 .165 .142 .118 .099 .082 .062 .063 .082 .058 .058 .044 SIP S? ? .511 .557 .616 .588 .595 .732 .727 .683 .717 .628 .653 .720 .716 .833 .842 .835 .850 .806 .876 .879 maxF ? ? .574 .620 .669 .687 .505 .763 .751 .618 .698 .661 .657 .712 .694 .818 .838 .830 .851 .821 .880 .883 maxE ? ? .716 .737 .770 .768 .721 .838 .853 .743 .798 .771 .759 .819 .829 .897 .901 .895 .903 .875 .919 .922 M ? .184 .192 .298 .173 .224 .172 .200 .186 .167 .164 .185 .118 .139 .086 .071 .075 .064 .085 .056 .055</figDesc><table><row><cell>Data</cell><cell>Metric</cell><cell>Hand-crafted-features-Based Models BBS-[32] [94] [35] [95] [36] [38] [26] [91] CNNs-Based Models [96] [25] [66] [30] [70] [23] [22] [18] [21] [19]</cell></row><row><cell>NJU2K</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON USING DIFFERENT BACKBONE MODELS. WE EXPERIMENT WITH MULTIPLE POPULAR BACKBONE MODELS USED IN RGB-D SOD, INCLUDING VGG-16 [100], VGG-19 [100] AND RESNET-50 [69].</figDesc><table><row><cell>Models</cell><cell>NJU2K [38]</cell><cell>NLPR [32]</cell><cell>STERE [39]</cell><cell>DES [35]</cell><cell>LFSD [88]</cell><cell>SSD [89]</cell><cell>SIP [37]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF FEATURE AGGREGATION STRATEGIES. 1: ONLY AGGREGATING THE LOW-LEVEL FEATURES (Conv1?3), 2: ONLY AGGREGATING THE HIGH-LEVEL FEATURES (Conv3?5), 3: DIRECTLY INTEGRATING ALL FIVE-LEVEL FEATURES (Conv1?5) BY A SINGLE DECODER, 4: OUR MODEL WITHOUT THE REFINEMENT FLOW, 5: HIGH-LEVEL FEATURES (Conv3?5) ARE FIRST REFINED BY THE INITIAL MAP AGGREGATED BY LOW-LEVEL FEATURES</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>BBS-Net. 'BM' = BASE MODEL. 'CA' = CHANNEL ATTENTIO. 'SA' = SPATIAL ATTENTION. 'PTM' = PROGRESSIVELY</figDesc><table><row><cell>2</cell><cell>Low 3 levels High 3 levels</cell><cell>.881 .902</cell><cell>.051 .042</cell><cell>.882 .911</cell><cell>.038 .029</cell><cell>.832 .886</cell><cell>.070 .048</cell><cell>.853 .912</cell><cell>.044 .026</cell><cell>.779 .845</cell><cell>.110 .080</cell><cell>.805 .850</cell><cell>.080 .058</cell><cell>.760 .833</cell><cell>.108 .073</cell></row><row><cell>3 4 5</cell><cell>All 5 levels BBS-NoRF BBS-RH</cell><cell>.905 .893 .913</cell><cell>.042 .050 .040</cell><cell>.915 .904 .922</cell><cell>.027 .035 .028</cell><cell>.891 .843 .881</cell><cell>.045 .072 .054</cell><cell>.901 .886 .919</cell><cell>.028 .039 .027</cell><cell>.845 .804 .833</cell><cell>.082 .105 .085</cell><cell>.848 .839 .872</cell><cell>.060 .069 .053</cell><cell>.839 .843 .866</cell><cell>.071 .076 .063</cell></row><row><cell>6</cell><cell>BBS-RL (ours)</cell><cell>.921</cell><cell>.035</cell><cell>.930</cell><cell>.023</cell><cell>.908</cell><cell>.041</cell><cell>.933</cell><cell>.021</cell><cell>.864</cell><cell>.072</cell><cell>.882</cell><cell>.044</cell><cell>.879</cell><cell>.055</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">ABLATION ANALYSIS OF OUR TRANSPOSED MODULE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#</cell><cell>Settings</cell><cell></cell><cell cols="2">NJU2K [38]</cell><cell cols="2">NLPR [32]</cell><cell cols="2">STERE [39]</cell><cell>DES [35]</cell><cell></cell><cell>LFSD [88]</cell><cell cols="2">SSD [89]</cell><cell cols="2">SIP [37]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII EFFECTIVENESS</head><label>VII</label><figDesc>ANALYSIS ON THE NJU2K DATASET. WE DO NOT REPORT THE RESULT FOR ? = 1, BECAUSE ITS LOSS OF THE FINAL PREDICTED MAP IS 0. S?) .918 .925 .923 .919 .924 .923 .920 .923 .922 .924 NJU2K (MAE) .037 .034 .034 .036 .033 .034 .035 .034 .035 .033</figDesc><table><row><cell cols="5">ANALYSIS OF THE CASCADED DECODER IN TERMS OF</cell></row><row><cell cols="5">THE S-MEASURE (S?) ON SEVEN DATASETS.</cell></row><row><cell cols="5">Methods NJU2K NLPR STERE DES SSD LFSD SIP [38] [32] [39] [35] [89] [88] [37]</cell></row><row><cell>Element-wise sum</cell><cell>.915</cell><cell>.925</cell><cell>.897</cell><cell>.925 .868 .856 .880</cell></row><row><cell>Cascaded decoder</cell><cell>.921</cell><cell>.930</cell><cell>.908</cell><cell>.933 .882 .864 .879</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell></row><row><cell cols="5">HYPER-PARAMETER ? ? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell></row><row><cell>NJU2K (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE IX EFFECTIVENESS</head><label>IX</label><figDesc>ANALYSIS OF THE DEPTH ADAPTER MODULE IN TERMS OF THE S-MEASURE (S?) ON SEVEN DATASETS. REPRESENTS THE EFFICIENT VERSION OF BBS-Net, WHERE THE TWO BACKBONES SHARE PARAMETERS. S?) COMPARISON WITH SOTA RGB SOD METHODS. 'W/O DEPTH' AND 'W/ DEPTH' REPRESENT TRAINING AND TESTING THE PROPOSED METHOD WITHOUT/WITH THE DEPTH INFORMATION (i.e., THE INPUTS OF THE DEPTH BRANCH ARE OR ARE NOT SET TO ZEROS).</figDesc><table><row><cell cols="3">Settings NJU2K NLPR STERE DES SSD LFSD SIP [38] [32] [39] [35] [89] [88] [37]</cell></row><row><cell>BBS-Net (w/o DAM) .905</cell><cell>.922</cell><cell>.899 .928 .856 .841 .849</cell></row><row><cell>BBS-Net (w/ DAM) .916</cell><cell>.925</cell><cell>.905 .930 .858 .859 .876</cell></row><row><cell></cell><cell cols="2">TABLE X</cell></row><row><cell cols="3">S-MEASURE (Methods NJU2K NLPR STERE DES LFSD SSD SIP [38] [32] [39] [35] [88] [89] [37]</cell></row><row><cell>PiCANet [101] .847 PAGRN [50] .829 R3Net [82] .837 CPD [29] .894 PoolNet [16] .887</cell><cell>.834 .844 .798 .915 .900</cell><cell>.868 .854 .761 .832 .851 .858 .779 .793 .855 .847 .797 .815 .902 .897 .815 .839 .859 ---.880 .873 .787 .773 .861</cell></row><row><cell>BBS-Net (w/o depth) .914 BBS-Net (w/ depth) .921</cell><cell>.925 .930</cell><cell>.915 .912 .836 .855 .875 .908 .933 .864 .882 .879</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XI PERFORMANCE</head><label>XI</label><figDesc>COMPARISON (MAE) OF DIFFERENT POST-PROCESSING STRATEGIES ON SEVEN DATASETS. THE LAST COLUMN IS THE TIME FOR THE POST-PROCESSING METHODS TO OPTIMIZE EACH IMAGE. SEE ? V-B</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FOR DETAILS.</cell></row><row><cell>Strategy</cell><cell cols="3">NJU2K NLPR STERE DES LFSD SSD SIP time [38] [32] [39] [35] [88] [89] [37] ms</cell></row><row><cell cols="2">BBS-Net BBS-Net+ADP .050 .035 BBS-Net+Ostu .030</cell><cell>.023 .024 .020</cell><cell>.041 .021 .072 .044 .055 .049 .018 .072 .053 .055 1.46 -.036 .018 .066 .039 .051 0.99</cell></row><row><cell cols="2">BBS-Net+CRF .030</cell><cell>.020</cell><cell>.035 .019 .065 .038 .051 450.8</cell></row><row><cell>RGB</cell><cell>GT</cell><cell>BBS-Net</cell><cell>BBS-Net+ADP BBS-Net+Ostu BBS-Net+CRF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XII PERFORMANCETABLE XIII PERFORMANCE</head><label>XIIXIII</label><figDesc>COMPARISON WHEN TRAINING WITH DIFFERENT DATASETS. THE NUMBER IN PARENTHESES DENOTES THE NUMBER OF THE CORRESPONDING TRAINING AND TEST IMAGES. SEE ? V-D FOR DETAILS. S? ? F ? ? S? ? F ? ? S? ? F ? ? S? ? F ? ? S? ? F ? ? S? ? F ? ? COMPARISONWHEN TRAINING WITH DIFFERENT COMBINATIONS OF MULTIPLE DATASETS. 'NJ', 'NL', 'ST', 'SI' AND 'DU' REPRESENT NJU2K, NLPR, STERE, SIP AND DUT, RESPECTIVELY. THE NUMBER IN PARENTHESES DENOTES THE NUMBER OF CORRESPONDING TRAINING AND TEST IMAGES. THE NUMBER OF TRAINING IMAGES FOR EACH DATASET IS 700. THE TRAINING AND TEST SETS WILL BE AVAILABLE AT: HTTPS://DRIVE.GOOGLE.COM/DRIVE/FOLDERS/1UYGYG50-0Y7I21TWREVMCBPATQPFPSR0?USP=SHARING.</figDesc><table><row><cell>Train</cell><cell cols="4">Test NJU2K (1285) S? ? F ? ?</cell><cell cols="2">NLPR (300)</cell><cell cols="2">STERE (300)</cell><cell cols="2">SIP (229)</cell><cell cols="2">DUT (500)</cell><cell>Self</cell><cell></cell><cell cols="2">Mean Others</cell><cell>Drop ? S? F ?</cell></row><row><cell cols="3">NJU2K (700) NLPR (700) STERE (700) SIP (700) DUT (700)</cell><cell>.902 .712 .779 .436 .751</cell><cell>.894 .689 .741 .325 .777</cell><cell>.834 .919 .897 .618 .808</cell><cell>.795 .903 .868 .528 .761</cell><cell>.864 .876 .915 .534 .736</cell><cell>.846 .882 .913 .479 .764</cell><cell>.802 .883 .900 .963 .801</cell><cell>.782 .881 .900 .972 .802</cell><cell>.741 .795 .724 .423 .887</cell><cell>.691 .779 .731 .303 .877</cell><cell>.902 .919 .915 .963 .887</cell><cell>.894 .903 .913 .972 .877</cell><cell>.810 .817 .825 .503 .774</cell><cell>.779 .808 .810 .409 .776</cell><cell>10.2% 12.9% 11.2% 10.5% 9.8% 11.3% 47.8% 57.9% 12.7% 11.5%</cell></row><row><cell cols="3">Mean Others</cell><cell>.670</cell><cell>.633</cell><cell>.789</cell><cell>.738</cell><cell>.753</cell><cell>.743</cell><cell>.847</cell><cell>.841</cell><cell>.671</cell><cell>.626</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Train</cell><cell>Test</cell><cell></cell><cell cols="2">NJ (1285)</cell><cell cols="2">NL (300)</cell><cell>ST (300)</cell><cell></cell><cell>DES (135)</cell><cell></cell><cell>LFSD (80)</cell><cell cols="2">SSD (80)</cell><cell></cell><cell>SI (229)</cell><cell>DU (500)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>) .911 .905 .039 .926 .916 .025 .899 .898 .044 .934 .932 .019 .865 .862 .070 .861 .836 .054 .890 .893 .048 .799 .748 .095 NJ+ST (1,400) .913 .909 .038 .885 .859 .040 .916 .912 .035 .927 .910 .022 .853 .836 .078 .869 .848 .054 .885 .882 .052 .729 .719 .118 NJ+DU (1,400) .906 .893 .043 .852 .802 .050 .875 .854 .053 .884 .859 .037 .861 .854 .070 .862 .839 .053 .834 .825 .075 .905 .903 .041 NL+ST (1,400) .781 .748 .097 .930 .919 .024 .919 .920 .032 .942 .938 .019 .672 .645 .161 .774 .722 .090 .895 .894 .047 .836 .819 .070 NL+DU (1,400) .777 .771 .104 .923 .908 .023 .878 .882 .052 .940 .936 .019 .717 .728 .135 .801 .774 .091 .886 .888 .054 .905 .903 .040 ST+DU (1,400) .821 .794 .082 .893 .863 .036 .917 .914 .034 .940 .935 .020 .762 .734 .125 .777 .736 .092 .907 .910 .039 .913 .914 .037 NJ+NL+ST (2,100) .913 .910 .038 .923 .904 .027 .922 .924 .033 .943 .939 .018 .865 .858 .072 .853 .818 .056 .902 .905 .043 .816 .780 .088 NJ+NL+DU (2,100) .911 .905 .041 .924 .909 .027 .902 .901 .043 .942 .939 .018 .865 .856 .067 .866 .838 .051 .894 .897 .048 .916 .915 .036 NJ+ST+DU (2,100) .910 .903 .041 .890 .867 .039 .923 .923 .031 .932 .918 .021 .859 .851 .073 .863 .838 .055 .896 .899 .046 .917 .916 .035 NL+ST+DU (2,100) .825 .808 .079 .924 .911 .026 .919 .920 .033 .946 .944 .017 .751 .732 .125 .797 .758 .082 .901 .905 .043 .916 .911 .036 NJ+NL+ST+DU (2,800) .912 .905 .039 .932 .917 .024 .921 .920 .033 .946 .942 .018 .864 .856 .070 .858 .829 .054 .903 .905 .042 .917 .913 .037</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BBS-Net: RGB-D Salient Object Detection with a Bifurcated Backbone Strategy Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="275" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BING: binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intelligent visual media processing: When graphics meets vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCST</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Repfinder: Finding approximately repeated scene elements for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semisupervised video salient object detection using pseudo-labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7284" to="7293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adaptive object tracking by learning background context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a selfpaced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency prediction in the deep learning era: Successes and limitations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="679" to="700" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salient object detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1746" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1605" to="1616" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Contrast prior and fluid pyramid integration for RGBD salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3051" to="3060" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multiscale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYBERNETICS</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-Modal Weighting Network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="665" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image by single stream recurrent convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pdnet: Prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive fusion for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55" to="277" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HSCS: Hierarchical sparsity based co-saliency detection for RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1660" to="1671" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RGBD salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient region detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DSP</title>
		<imprint>
			<biblScope unit="page" from="454" to="458" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2625" to="2636" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIMCS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="1509" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="page" from="2075" to="2089" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1115" to="1119" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="186" to="202" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1529" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reverse attentionbased residual network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3763" to="3776" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3799" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A multistage refinement network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3534" to="3545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Progressive feature polishing network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">F3net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="123" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Video captioning with transferred semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="984" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Boundary-guided feature aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1800" to="1804" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">EGNet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Depth really matters: Improving visual salient region detection with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An in depth view of saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Going from RGB to RGBD saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYBERNETICS</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Learning RGB-D salient object detection using background enclosure, depth contrast, and top-down features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shigematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="2749" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">CNNs-Based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYBERNETICS</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8582" to="8591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Synergistic saliency and depth prediction for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cascade Graph Neural Networks for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="346" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="646" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="225" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">RGB-D Salient Object Detection: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="404" to="419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">ACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1440" to="1444" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">R3Net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="684" to="690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="3008" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A multilayer backpropagation saliency detection algorithm based on depth mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAIP</title>
		<imprint>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9060" to="9069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Select, Supplement and Focus for RGB-D Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3472" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guided-background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Exploiting global priors for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Structuremeasure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="698" to="704" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">PiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3089" to="3098" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="576" to="588" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7223" to="7233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SMC</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
