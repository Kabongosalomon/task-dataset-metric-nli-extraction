<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 21-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtu</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tencent</forename><surname>Shanghai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><forename type="middle">Zheng</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<email>yanweifu@fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Youtu Lab, Tencent Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Data Science and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Taipei, Taiwan</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 21-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3460426.3463588</idno>
					<note>ACM Reference Format: 2021. NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedes-trian Detection. In Proceedings of the 2021 International Conference on Multi-media Retrieval (ICMR &apos;21), August 21-24, 2021, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3460426.3463588 * Yanwei Fu is with the ACM ISBN 978-1-4503-8463-6/21/08. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Object detection KEYWORDS pedestrian detection</term>
					<term>loss function</term>
					<term>Non-Maximum suppression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Non-Maximum Suppression (NMS) is essential for object detection and affects the evaluation results by incorporating False Positives (FP) and False Negatives (FN), especially in crowd occlusion scenes. In this paper, we raise the problem of weak connection between the training targets and the evaluation metrics caused by NMS and propose a novel NMS-Loss making the NMS procedure can be trained end-to-end without any additional network parameters. Our NMS-Loss punishes two cases when FP is not suppressed and FN is wrongly eliminated by NMS. Specifically, we propose a pull loss to pull predictions with the same target close to each other, and a push loss to push predictions with different targets away from each other. Experimental results show that with the help of NMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are both better than state-of-the-art competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pedestrian detection <ref type="bibr" target="#b11">[12]</ref> is an essential computer vision task that has numerous applications such as automatic driving, video surveillance and person re-identification. With the help of deep convolution neural networks (CNNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>, the performance of pedestrian detection has been significantly improved. However, the False Negatives (FN) in crowd occlusion scenes and False Positives (FP) generated for the same person are still the fundamental challenges.</p><p>Existing methods for pedestrian detection can mainly be divided into two categories: hand-crafted feature based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> and deep learning based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. The first one applies the sliding-window way to get different scales of patches, then uses human-designed feature extractor such as Haar <ref type="bibr" target="#b29">[30]</ref> and HoG <ref type="bibr" target="#b8">[9]</ref> to obtain feature representation, last utilizes SVM <ref type="bibr" target="#b7">[8]</ref> classifier to filter background. These hand-crafted feature representations could not handle complex scenes. The second one uses deep convolutional neural networks (CNNs) to obtain highlevel semantic feature representation, which has a discriminative ability to deal with complex scenes for pedestrian detection. To alleviate FN issue in high occlusion scenes, different variants of Non-Maximum Suppression (NMS) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> are proposed to change NMS threshold during inference adaptively. To reduce FP, many works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> jointly predict pedestrian boxes and parts information such as head due to that it is less occluded. However, the objective between training and inference is inconsistent, which may result in sub-optimal performance for pedestrian detection.</p><p>NMS is an essential procedure for object detection tasks. Modern pedestrian detectors rely on NMS to remove duplicate detections for both one-stage and two-stage approaches. The nearby detections around one object will be removed once its interaction over union (IoU) with the object is larger than the pre-defined threshold. During the training process, there is no such process, thus resulting in inconsistency between optimized detection training results and final inference results. To handle the inconsistency problem, NMS process should be incorporated into the training process. To this end, we propose a novel NMS-Loss. There are two components, pull and push losses, in our NMS-Loss. Pull loss aims to raise the precision by pulling FP close to the max score prediction, and push loss focuses on improving recall by pushing predictions away from each other. With the help of NMS-Loss, false predictions on the evaluation metric can be directly reflected on loss functions, and thus be directly optimized.</p><p>The main contribution of this work lies in the following aspects.</p><p>? We firstly raise the problem of weak connection between training targets and evaluation metrics in pedestrian detection and propose a novel NMS-Loss making the NMS procedure can be trained end-to-end, which does not introduce any parameters nor runtime cost. ? We propose finely designed pull and push losses helping the network to boost performance on precision and recall, respectively, which considering both prediction coordinates and confidence. ? With the help of NMS-Loss, in pedestrian detection, our proposed NMS-Ped outperforms SOTA methods on the widely used Caltech and CityPersons datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NMS-LOSS 2.1 Overview of NMS-Loss</head><p>The traditional NMS procedure is shown in Alg. 1 without considering the red texts. Starting with a set of detection boxes B with corresponding scores S, NMS firstly moves the proposal with the maximum score from the set B to the set of final kept detections K. It then removes any box in B and its score in S that has an overlap with the higher than a manually set threshold . This process is repeated for the remaining B set.</p><p>However, no existing approaches take the NMS into the training process to adjust the detection boxes, making the learning targets inconsistent with the evaluation metric, which means FP not suppressed by NMS and FN eliminated by NMS can harm the precision and recall, respectively. To avoid inconsistency, we propose the NMS-Loss taking the NMS procedure into the training process, which adaptively selects the false predictions caused by NMS and uses two well-designed pull and push losses to minimize the FP and FN, respectively. Specifically, our NMS-Loss is defined as:</p><formula xml:id="formula_0">= + ? ? ,<label>(1)</label></formula><p>where is the pull loss to punish the FP not suppressed by NMS and ? is the push loss to punish the FN wrongly eliminated by NMS. Coefficients and ? are the weights for balancing losses. Details of our NMS-Loss are present in Algorithm 1 emphasized with red color. Different from the traditional NMS, we use a set G containing corresponding ground truth indexes of detection boxes, which is used to identify FP and FN. In the NMS-Loss calculating procedure, M is an auxiliary dictionary with the ground truth index as key and corresponding max score detection as value, which is used to record the max score prediction of each ground truth. Our NMS-Loss is naturally merged into the NMS procedure without incorporating any additional training parameters. The runtime cost of NMS-Loss is zero for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pull Loss Definition</head><p>With the objective to reduce FP, we need to find out wrongly kept predictions. To this end, in every iteration, we check whether the current max score prediction is the max score prediction for its corresponding ground truth. If not, it means is an FP not suppressed by NMS, pull loss should be performed between and the max score prediction of the ground truth (see <ref type="figure">Fig. 1</ref>). Formally, our pull loss is calculated as:</p><formula xml:id="formula_1">= ? (1 ? + ( , )) ,<label>(2)</label></formula><p>Algorithm 1: NMS-Loss Calculating Procedure</p><formula xml:id="formula_2">Input: B = [ 1 , . . . , ], S = [ 1 , . . . , ], , G = [ 1 , . . . , ] B</formula><p>is the list of initial detection boxes S contains corresponding detection scores is the NMS threshold G contains corresponding ground truth indexes Auxiliary Variable:</p><p>K ? [ ], M ? (), K is the list to keep final detections after NMS M is a dictionary using the ground truth index as key and corresponding max score detection as value begin <ref type="figure">Figure 1</ref>: Illustration of our NMS-Loss. All boxes , and are predictions as described in Alg. 1, where boxes with the same color have the same target and boxes with the solid line get a higher score than boxes with the dotted line. In (a), is a FP not suppressed by . Our pulls towards . In (b), is a FN wrongly eliminated by . Our ? pushes away from .</p><formula xml:id="formula_3">while B ? do ? S ; if M. () then M [ ] ? ; else ? M [ ]; pull_loss( , ); Eq. (2) end K ? K ? ; B ? B ? ; S ? S ? ; G ? G ? ; for B do if ( , ) ? then if ? then push_loss( , ); Eq. (3) end B ? B ? ; S ? S ? ; G ? G ? ; end end end return K end Pull (a) Push (b)</formula><p>where is the predefined NMS threshold and is the prediction score corresponding to . We note two properties of the pull loss: (1) When the IoU between and is small, pull loss tends to increase, forcing the network to learn to pull toward . The NMS threshold is used to prevent the gradient of outliers influence too much on model learning. Besides, for the NMS procedure, we just need to make the IoU between FP and TP higher than . Using in pull loss to reduce the gradient of outliers can make the network easy to learn. (2) The prediction score of FP can also have a strong effect on pull loss. FP with a higher score has a greater impact on evaluation results and intuitively needs to be paid more attention. Besides, it makes the network learn to fix FP not only just conditioning the box coordinates but also considering lower the prediction scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Push Loss Definition</head><p>In NMS, the current max score prediction eliminates boxes which get an IoU higher than with . If the eliminated box corresponds to different ground truth index with , will be a FN and reduce recall (see <ref type="figure">Fig. 1</ref>). To avoid from being wrongly eliminated, we propose a push loss to penalize FN:</p><formula xml:id="formula_4">? = ? (1 ? ( , )) ,<label>(3)</label></formula><p>where is the prediction score corresponding to . Different from pull loss, as ( , ) ? 1, the push loss goes higher and the model learns to push away from . To avoid the model tending to reduce the push loss by lowering the score of FN, we use the only for reweighting losses without back propagating gradient.</p><p>For crowded scenes, especially in the CityPersons dataset, the ground truths of bounding boxes are overlapped with each other. It is unreasonable to push their predictions away from each other with an IoU equals to zero. To handle this problem, we only calculate ? on prediction whose IoU is higher than the IoU of its corresponding ground truth boxes.</p><p>Our pull and push loss are performed on predictions. When the pull/push loss is activated, the network tries to pull/push both predictions close to/away from each other, respectively. Since high score predictions generally get a more accurate location, it is unreasonable to move an accurate prediction based on an inaccurate one. To handle this, we stop the gradient backward propagation of high score predictions, leading the network to focus on false predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Experimental Setup</head><p>Datasets and Evaluation metrics. We evaluate our method on two challenging pedestrian datasets: Caltech <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and CityPersons <ref type="bibr" target="#b35">[36]</ref>. We report performance using standard average-log MR between [10 ?2 , 10 0 ] of False Positive per Image (FPPI). A minimum IoU threshold of 0.5 is required for detected box to match with a ground truth box. By default, we report the results on Reasonable subsets is a widely used setup where the pedestrian is at least 65% visible and 50 pixels tall. Experimental Settings. As shown in RPN+BF <ref type="bibr" target="#b34">[35]</ref>, small instances are hard to be detected in the low-resolution feature maps provided by RoI-Pooling, which is more severe in pedestrian detection. Therefore, we used Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> as our baseline, but made two adjustments: (1) Inspired by <ref type="bibr" target="#b34">[35]</ref>, we use a separate network to construct the RCNN and put the cropped original image to RCNN for further refinement. This improves the ability of the network to detect small instances, but it is not suitable for instances with large scale changes. (2) There is an additional weak semantic segmentation loss <ref type="bibr" target="#b2">[3]</ref> to boost performance. Note that the baseline has the same settings as our NMS-Ped except that there is no NMS-Loss in baseline. <ref type="figure">Figure 2</ref>: Comparison between the cases with/without using pull/push loss. Green bounding boxes are predicted pedestrians whose score is greater than 0.8 and red bounding boxes are ground truth. Our pull loss effectively suppresses FP in both sparse scenes and crowded scenes (left three columns), yielding higher precision. Our push loss robustly handles occlusions (right two columns), yielding higher recall.  PyTorch <ref type="bibr" target="#b25">[26]</ref> is used to train the NMS-ped for both datasets. We use 8 NVIDIA GPUs with a mini-batch comprises 1 image per GPU. SGD with momentum of 0.9 and weight decay of 1 ? 10 ?4 is adopted for training. Both datasets are trained only using the images with foreground. Random cropping and flipping are used for data augmentation. Detailed settings on Caltech and CityPersons are described as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Baseline Ours:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMS-Ped (a) Pull Loss (b) Push Loss</head><p>Caltech: The learning rate for Caltech is 5?10 ?3 and is dropped by a factor of 10 after 9, 600 iterations and 13, 200 iterations. The images are resized to 1280 ? 960 in our experiments. The weights for pull loss and push loss are both 0.1 getting from experiments.</p><p>CityPersons: The learning rate for CityPersons is 1?10 ?2 and dropped by a factor of 10 after 24, 000 iterations and 33, 000 iterations. We use the original image resolution of 2048?1024 in our experiments. The weights for pull and push loss are 0.1 and 0.001 respectively for the reason that CityPersons contains much more crowded scenes than Caltech and lots of instances are heavily overlapped with others. Giving a relatively lower weight for push loss will reduce the gradient of pushing and make multi-tasks work well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone MR RepLoss <ref type="bibr" target="#b30">[31]</ref> ResNet-50 13.20% OR-CNN <ref type="bibr" target="#b37">[38]</ref> ResNet-50 12.80% Adaptive-NMS <ref type="bibr" target="#b18">[19]</ref> VGG-16 11.90% CSP <ref type="bibr" target="#b20">[21]</ref> ResNet-50 11.00% MGAN <ref type="bibr" target="#b24">[25]</ref> VGG-16 11.50% R 2 NMS <ref type="bibr" target="#b17">[18]</ref> VGG-16 11.10% EMD-RCNN <ref type="bibr" target="#b6">[7]</ref> ResNet-50 10.70%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our baseline</head><p>ResNet-50 11.20%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMS-Ped</head><p>ResNet-50 10.08% <ref type="table">Table 4</ref>: Comparisons on Caltech dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone MR RPN+BF <ref type="bibr" target="#b34">[35]</ref> VGG-16 9.58% F-DNN <ref type="bibr" target="#b13">[14]</ref> ResNet-50 8.65% SDS-RCNN <ref type="bibr" target="#b2">[3]</ref> VGG-16 7.36% MGAN <ref type="bibr" target="#b24">[25]</ref> VGG-16 6.83% AR-Ped <ref type="bibr" target="#b1">[2]</ref> VGG-16 6.45% SSA-CNN <ref type="bibr" target="#b38">[39]</ref> VGG-16 6.27% TFAN+TDEM+PRM <ref type="bibr" target="#b31">[32]</ref> ResNet-101 6.50% W 2 Net <ref type="bibr" target="#b21">[22]</ref> ResNet-50 6.37%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our baseline</head><p>ResNet-50 6.61%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMS-Ped</head><p>ResNet-50 5.92%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>We conduct experiments on CityPersons to evaluate our NMS-Loss for the reason that pedestrian in CityPersons is more crowded and challenging. There are enough complicated scenes to review effectiveness of our approach. Baseline comparison. Tab. 1 shows the performance of our baseline with separate components. When only the pull loss is used, MR can be reduced from 11.20% to 10.58%. <ref type="figure">Fig. 2</ref> shows some results corrected for using pull loss. In both sparse scenes (first column) and crowded scenes (second and third columns), our pull loss will effectively pull predictions targeting on the same ground truth close to each other. The same experiments are conducted on push loss. With the help of push loss, the MR can be reduced from 11.20% to 10.61%. Some visible results are present in <ref type="figure">Fig. 2</ref> showing the corrected predictions for using push loss. In the occlusion scenes (right two columns), push loss trained model performs more robust, even detected the unlabeled instance (fourth column). When we use the complete NMS-Loss, our NMS-Ped can be boosted from both pull loss and push loss, getting an amazing 10.08% MR. Experiments on hyperparameters. Tab. 2 shows our results with different thresholds on NMS-Loss. When is lower than evaluation metric threshold 0.5, push loss will be activated more frequently and pull loss will not be activated making the network produce more FPs that harms precision. In contrast, when is higher than 0.5, more FNs will be produced and lower recall. Our NMS-Loss performs robust with various NMS thresholds, gaining stable improvement. When we use equivalent to the threshold 0.5, our NMS-Loss yields the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons with SOTA methods</head><p>To demonstrate the effectiveness of our NMS-Loss, we compare NMS-Ped with the SOTA methods on CityPersons and Caltech. Tab. 3 presents the performance of NMS-Ped and SOTA methods on the CityPersons dataset. With the help of NMS-Loss, our method improve the MR of baseline from 11.20% to 10.08%, better than the SOTA method EMD-RCNN <ref type="bibr" target="#b6">[7]</ref> (MR of 10.70%). Tab. 4 presents the performance on Caltech, the MR of NMS-Ped is 5.92%, better than SOTA method W 2 Net <ref type="bibr" target="#b21">[22]</ref> (MR of 6.37%). With the help of NMS-Loss, we can obtain more than 10% improvement in NMS-Ped compared with baseline. This demonstrates the effectiveness of our NMS-Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Difference to RepLoss</head><p>We make a detailed comparison between our NMS-Loss and the RepLoss <ref type="bibr" target="#b30">[31]</ref> for the reason that both methods pull and push predictions based on their targets. There are three main differences: <ref type="bibr" target="#b0">(1)</ref> RepLoss is performed on all instances, while NMS-Loss is only performed on instances wrongly processed by NMS, which enables end-to-end training. (2) RepLoss only considers regression, while the score is also used in NMS-Loss to reweight instances. (3) In dense crowd scenarios, RepLoss pushes instances away even if their targets are originally close to each other, making the repulsion loss contradicts with the regression loss. Instead, NMS-Loss pushes instances whose IoU with others is higher than the IoU of its corresponding ground truth boxes, which can eliminates the contradiction of RepLoss. As shown in Tab. 5, our NMS-Loss not only performs better than RepLoss, but also gains higher relative improvement on CityPersons. This demonstrates that our NMS-Loss can achieve stable relative improvement (higher than 10%) on the widely used datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION AND FUTURE WORK</head><p>In this work, we raise the problem of weak connection between training targets and evaluation metrics in the object detection. To address this, we propose the NMS-Loss which contains two components called pull loss and push loss, making the false predictions can be directly reflected on loss functions. With the help of NMS-Loss, the model can be trained with NMS end-to-end and pay more attention to the false predictions caused by NMS. Our NMS-Loss can be easily incorporated into network, which does not introduce any parameters nor runtime cost. NMS-Loss is only suitable for single class object detection, in the future, we will extend our NMS-Loss to other tasks by further considering object classes in generic detections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our NMS-Ped with the baseline on CityPersons.</figDesc><table><row><cell>Method</cell><cell>MR</cell></row><row><cell>baseline</cell><cell>11.20%</cell></row><row><cell>baseline + pull loss</cell><cell>10.58%</cell></row><row><cell>baseline + push loss</cell><cell>10.61%</cell></row><row><cell>NMS-Ped</cell><cell>10.08%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison on different thresholds of NMS-Loss on CityPersons.</figDesc><table><row><cell></cell><cell>0.4</cell><cell>0.45</cell><cell>0.5</cell><cell>0.55</cell></row><row><cell>MR</cell><cell>10.76%</cell><cell>10.66%</cell><cell>10.08%</cell><cell>10.67%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison on CityPersons dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparison between RepLoss and NMS-Loss on the CityPersons. We use , , , to represent the of baseline model, of complete model, of the improvement and relative improvement based on the baseline, respectively. Ped ResNet-50 11.2% 10.08% 1.12% 10.00%</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>RepLoss</cell><cell cols="2">ResNet-50 14.6%</cell><cell>13.2%</cell><cell>1.4%</cell><cell>9.59%</cell></row><row><cell>NMS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Soft-NMS-Improving Object Detection With One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pedestrian Detection with Autoregressive Network Phases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7231" to="7240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Illuminating pedestrians via simultaneous detection &amp; segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4950" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relational learning for joint head and human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10647" to="10654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PedHunter: Occlusion Robust Pedestrian Detector in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10639" to="10646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection in Crowded Scenes: One Proposal, Multiple Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuangeng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12214" to="12223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="953" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2241" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10750" to="10759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive NMS: Refining Pedestrian Detection in a Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Highlevel Semantic Feature Detection: A New Perspective for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Where, What, Whether: Multi-Modal Learning Meets Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14065" to="14073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3127" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mask-Guided Attention Network for Occluded Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5420" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. I-I</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13430" to="13439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation of deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2367" to="2380" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2497" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Is Faster R-CNN doing well for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Occlusionaware R-CNN: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengju</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew-Kei</forename><surname>Lam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09080</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
