<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LETTER-BASED SPEECH RECOGNITION WITH GATED CONVNETS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-19">February 19, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
							<email>vitaliy888@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LETTER-BASED SPEECH RECOGNITION WITH GATED CONVNETS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-19">February 19, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the recent literature, "end-to-end" speech systems often refer to letter-based acoustic models trained in a sequence-to-sequence manner, either via a recurrent model or via a structured output learning approach (such as CTC <ref type="bibr" target="#b14">(Graves et al., 2006)</ref>). In contrast to traditional phone (or senone)-based approaches, these "end-to-end" approaches alleviate the need of word pronunciation modeling, and do not require a "forced alignment" step at training time. Phone-based approaches remain however state of the art on classical benchmarks. In this paper, we propose a letter-based speech recognition system, leveraging a ConvNet acoustic model. Key ingredients of the ConvNet are Gated Linear Units and high dropout. The ConvNet is trained to map audio sequences to their corresponding letter transcriptions, either via a classical CTC approach, or via a recent variant called ASG <ref type="bibr" target="#b8">(Collobert et al., 2016)</ref>. Coupled with a simple decoder at inference time, our system matches the best existing letter-based systems on WSJ (in word error rate), and shows near state of the art performance on LibriSpeech <ref type="bibr" target="#b26">(Panayotov et al., 2015)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State of the art speech recognition systems leverage pronunciation models as well as speaker adaptation techniques involving speaker-specific features. These systems rely on lexicon dictionaries, which decompose words into one or more sequences of phones. Phones themselves are decomposed into smaller sub-word units, called senones. Senones are carefully selected through a procedure involving a phonetic-context-based decision tree built from another GMM/HMM system. In the recent literature, "end-to-end" speech systems attempt to break away from these hardcoded a-priori, the underlying assumption being that with enough data pronunciations should be implicitly inferred by the model, and speaker robustness should be also achieved. A number of works have thus naturally proposed ways how to learn to map audio sequences directly to their corresponding letter sequences. Recurrent models, structured-output learning or combination of both are the main contenders.</p><p>In this paper, we show that simple convolutional neural networks (CNNs) coupled with structured-output learning can outperform existing letter-based solutions. Our CNNs employ Gated Linear Units (GLU). Gated ConvNets have been shown to reduce the vanishing gradient problem, as they provide a linear path for the gradients while retaining non-linear capabilities, leading to state of the art performance both in natural language modeling and machine translation tasks <ref type="bibr" target="#b11">Gehring et al., 2017)</ref>. We train our system with a structured-output learning approach, either with CTC <ref type="bibr" target="#b14">(Graves et al., 2006)</ref> or ASG <ref type="bibr" target="#b8">(Collobert et al., 2016)</ref>. Coupled with a custom-made simple beam-search decoder, we exhibit word error rate (WER) performance matching the best existing letter-based systems, both for the WSJ and LibriSpeech datasets <ref type="bibr" target="#b26">(Panayotov et al., 2015)</ref>. While phone-based systems still lead on WSJ (81h of labeled data), our system is competitive with the existing state of the art systems on LibriSpeech (960h).</p><p>The rest of the paper is structured as follows: the next section goes over the history of the work in the automatic speech recognition area. We then detail the convolutional networks used for acoustic modeling, along with the structured-output learning and decoding approaches. The last section shows experimental results on WSJ and LibriSpeech.  <ref type="figure">Figure 1</ref>: Overview of our acoustic model, which computes log-mel filterbanks (MFSC) that are fed to a Gated ConvNet. The ConvNet outputs one score for each letter in the dictionary, and for each input feature frame. At inference time, these scores are fed to a decoder (see Section 3.3) to form the most likely sequence of words. At training time, the scores are fed to the CTC or ASG criterions (see <ref type="figure" target="#fig_0">Figure 2</ref>) which promote sequences of letters leading to the target transcription sequence (here "c a t").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The historic pipeline for speech recognition requires first training an HMM/GMM model to force align the units on which the final acoustic model operates (most often context-dependent phone or senone states) <ref type="bibr" target="#b44">Woodland and Young (1993)</ref>. The performance improvements brought by deep neural networks (DNNs)  and convolutional neural networks (CNNs) <ref type="bibr" target="#b38">(Sercu et al., 2016;</ref><ref type="bibr" target="#b39">Soltau et al., 2014)</ref> for acoustic modeling only extend this training pipeline. Current state of the art models on LibriSpeech also employ this approach <ref type="bibr" target="#b26">(Panayotov et al., 2015;</ref><ref type="bibr" target="#b29">Peddinti et al., 2015b)</ref>, with an additional step of speaker adaptation <ref type="bibr" target="#b33">(Saon et al., 2013;</ref><ref type="bibr" target="#b28">Peddinti et al., 2015a)</ref>. Departing from this historic pipeline, <ref type="bibr" target="#b37">Senior et al. (2014)</ref> proposed GMM-free training, but the approach still requires to generate a forced alignment. Recently, maximum mutual information (MMI) estimation <ref type="bibr" target="#b2">(Bahl et al., 1986</ref>) was used to train neural network acoustic models <ref type="bibr" target="#b30">(Povey et al., 2016)</ref>. The MMI criterion <ref type="bibr" target="#b2">(Bahl et al., 1986)</ref> maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayes Risk (MBR) criterion <ref type="bibr" target="#b12">(Gibson and Hain, 2006)</ref>, and belongs to segmental discriminative training criterions, although compatible with generative models.</p><p>Even though connectionist approaches <ref type="bibr" target="#b22">(Lee et al., 1995;</ref><ref type="bibr" target="#b21">LeCun et al., 1995)</ref> long coexisted with HMM-based approaches, they had a recent resurgence. A modern work that directly cut ties with the HMM/GMM pipeline used a recurrent neural network (RNN) <ref type="bibr" target="#b15">(Graves et al., 2013)</ref> for phoneme transcription with the connectionist temporal classification (CTC) sequence loss <ref type="bibr" target="#b14">(Graves et al., 2006)</ref>. This approach was then extended to character-based systems <ref type="bibr" target="#b13">(Graves and Jaitly, 2014)</ref> and improved with attention mechanisms <ref type="bibr" target="#b1">(Bahdanau et al., 2016;</ref><ref type="bibr" target="#b6">Chan et al., 2016)</ref>. But the best such systems are often still behind state of the art phone-based (or senone-based) systems. Competitive end-to-end approaches leverage acoustic models (often ConvNet-based) topped with RNN layers as in <ref type="bibr" target="#b16">(Hannun et al., 2014a;</ref><ref type="bibr" target="#b24">Miao et al., 2015;</ref><ref type="bibr" target="#b34">Saon et al., 2015;</ref><ref type="bibr" target="#b0">Amodei et al., 2016;</ref><ref type="bibr" target="#b49">Zhou et al., 2018;</ref><ref type="bibr" target="#b47">Zeyer et al., 2018</ref>) (e.g. a state of the art on WSJ (Chan and Lane, 2015a)), trained with a sequence criterion (the most popular ones being CTC <ref type="bibr" target="#b14">(Graves et al., 2006)</ref> and MMI <ref type="bibr" target="#b2">(Bahl et al., 1986)</ref>). A survey of segmental models can be found in <ref type="bibr" target="#b42">(Tang et al., 2017)</ref>. On conversational speech (that is not the topic of this paper), the state of the art is still held by complex ConvNets+RNNs acoustic models (which are also trained or refined with a sequence criterion), coupled with domain-adapted language models <ref type="bibr" target="#b45">(Xiong et al., 2017;</ref><ref type="bibr">Saon et al., 2017)</ref>.</p><p>3 Architecture</p><p>Our acoustic model (see an overview in <ref type="figure">Figure 1</ref>) is a Convolutional Neural Network (ConvNet) <ref type="bibr" target="#b21">(LeCun et al., 1995)</ref>, with Gated Linear Units (GLUs)  and dropout applied to activations of each layer except the output one. The model is fed with log-mel filterbank features, and is trained with either the Connectionist Temporal Classification (CTC) criterion <ref type="bibr" target="#b14">(Graves et al., 2006)</ref>, or with the ASG criterion: a variant of CTC that does not have blank labels but employs a simple duration model through letter transition scores <ref type="bibr" target="#b8">(Collobert et al., 2016)</ref>. At inference, the acoustic model is coupled with a decoder which performs a beam search, constrained with a count-based language model. We detail each of these components in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gated ConvNets for Acoustic Modeling</head><p>The acoustic model architecture is a 1D Gated Convolutional Neural Network (Gated ConvNet), trained to map a sequence of audio features to its corresponding letter transcription. Given a dictionary of letters L, the ConvNet (which acts as a sliding-approach over the input sequence) outputs one score for each letter in the dictionary, for each input frame. In the transcription, words are separated by a special letter, denoted #.</p><p>1D ConvNets were introduced early in the speech community, and are also referred as Time-Delay Neural Networks (TDNNs) <ref type="bibr" target="#b43">(Waibel, 1989)</ref>. Gated ConvNets  stack 1D convolutions with Gated Linear Units. More formally, given an input sequence X ? R T ?d i with T frames of d-dimensional vectors, the i th layer of our network performs the following computation:</p><formula xml:id="formula_0">h i (X) = (X * W i + b i ) ? ?(X * V i + c i ) ,<label>(1)</label></formula><p>where * is the convolution operator,</p><formula xml:id="formula_1">W i , V i ? R d i+1 ?d i ?k i and b i , c i ? R d i+1</formula><p>are the learned parameters (with convolution kernel size k i ), ?(?) is the sigmoid function and ? is the element-wise product between matrices.</p><p>Gated ConvNets have been shown to reduce the vanishing gradient problem, as they provide a linear path for the gradients while retaining non-linear capabilities, leading to state of the art performance both for natural language modeling and machine translation tasks <ref type="bibr" target="#b11">Gehring et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature Normalization and Zero-Padding</head><p>Each input feature sequence is normalized to have zero mean and unit variance. Given an input sequence X ? R T ?d , a convolution with kernel size k will output T ? k + 1 frames, due to border effects. To compensate those border effects, we pad the log-mel filterbanks X 0 with zeroed frames. To take into account the whole network, the padding size is i (k i ? 1), divided into two equal parts at the beginning and the end of the sequence. We considered two structured-output learning approaches to train our acoustic models: the Connectionist Temporal Classification (CTC), and a variant called AutoSeG (ASG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Acoustic Model Training</head><formula xml:id="formula_2">? ? ? C C C C ? ? ? A A A A ? ? ? T T T T ? ? ? (a) C C C C A A A A T T T T (b) A B ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The CTC Criterion</head><p>CTC <ref type="bibr" target="#b14">(Graves et al., 2006)</ref> efficiently enumerates all possible sequences of sub-word units (e.g. letters) which can lead to the correct transcription, and promotes the score of these sequences. CTC also allows a special "blank" state to be optionally inserted between each sub-word unit. The rationale behind the blank state is two-fold: (i) modeling "garbage" frames which might occur between each letter and (ii) identifying the separation between two identical consecutive sub-word units in a transcription. <ref type="figure" target="#fig_0">Figure 2a</ref> shows the CTC graph describing all the possible sequences of letters leading to the word "cat", over 6 frames. We denote G ctc (?, T ) the CTC acceptance graph over T frames for a given transcription ?, and ? = ? 1 , . . . , ? T ? G ctc (?, T ) a path in this graph representing a (valid) sequence of letters for this transcription. CTC assumes that the network outputs probability scores, normalized at the frame level. At each time step t, each node of the graph is assigned with its corresponding log-probability letter i (that we denote f t i (X)) output by the acoustic model (given an acoustic sequence X). CTC minimizes the Forward score over the graph G ctc (?, T ):</p><formula xml:id="formula_3">CT C(?, T ) = ? logadd ??Gctc(?,T ) T t=1 f t ?t (X) ,<label>(2)</label></formula><p>where the "logadd" operation (also called "log-sum-exp") is defined as logadd(a, b) = log(exp(a) + exp <ref type="formula">(b)</ref>). This overall score can be efficiently computed with the Forward algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">The ASG Criterion</head><p>Blank labels introduce code complexity when decoding letters into words. Indeed, with blank labels "?", a word gets many entries in the sub-word unit transcription dictionary (e.g. the word "cat" can be represented as "c a t", "c ? a t", "c ? a t", "c ? a ? t", etc... -instead of only "c a t"). We replace the blank label by special letters modeling repetitions of preceding letters. For example "caterpillar" can be written as "caterpil1ar", where "1" is a label to represent one repetition of the previous letter.</p><p>The AutoSeG (ASG) criterion <ref type="bibr" target="#b8">(Collobert et al., 2016)</ref> removes the blank labels from the CTC acceptance graph G ctc (?, T ) (shown in <ref type="figure" target="#fig_0">Figure 2a</ref>) leading to a simpler graph that we denote G asg (?, T ) (shown in <ref type="figure" target="#fig_0">Figure 2b)</ref>. In contrast to CTC which assumes per-frame normalization for the acoustic model scores, ASG implements a sequence-level normalization to prevent the model from diverging (the corresponding graph enumerating all possible sequences of letters is denoted G asg (?, T ), as shown in <ref type="figure" target="#fig_0">Figure 2c</ref>). ASG also uses unnormalized transition scores g i,j (?) on each edge of the graph, when moving from label i to label j, that are trained jointly with the acoustic model. This leads to the following criterion::</p><formula xml:id="formula_4">ASG(?, T ) = ? logadd ??Gasg(?,T ) T t=1 (f t ?t (X) + g ?t?1,?t (X)) + logadd ??G f ull (?,T )</formula><p>T t=1 (f t ?t (X) + g ?t?1,?t (X)) .</p><p>( <ref type="formula">3)</ref> The left-hand part in Equation <ref type="formula">(3)</ref> promotes the score of letter sequences leading to the right transcription (as in Equation <ref type="formula" target="#formula_3">(2)</ref> for CTC), and the right-hand part demotes the score of all sequences of letters. As for CTC, these two parts can be efficiently computed with the Forward algorithm.</p><p>When removing transitions in Equation <ref type="formula">(3)</ref>, the sequence-level normalization becomes equivalent to the frame-level normalization found in CTC, and the ASG criterion is mathematically equivalent to CTC with no blank labels. However, in practice, we observed that acoustic models trained with a transition-free ASG criterion had a hard time to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Other Training Considerations</head><p>We apply dropout at the output to all layers of the acoustic model. Dropout retains each output with a probability p, by applying a multiplication with a Bernoulli random variable taking value 1 with probability p and 0 otherwise <ref type="bibr" target="#b40">(Srivastava et al., 2014)</ref>.</p><p>Following the original implementation of Gated ConvNets , we found that using both weight normalization <ref type="bibr" target="#b32">(Salimans and Kingma, 2016)</ref> and gradient clipping <ref type="bibr" target="#b27">(Pascanu et al., 2013)</ref> were speeding up training convergence. The clipping we implemented performs:</p><formula xml:id="formula_5">? ?C = max(||?C||, ) ?C ||?C|| ,<label>(4)</label></formula><p>where C is either the CTC or ASG criterion, and is some hyper-parameter which controls the maximum amplitude of the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Beam-Search Decoder</head><p>We wrote our own one-pass decoder, which performs a simple beam-search with beam thresholding, histogram pruning and language model smearing <ref type="bibr" target="#b41">(Steinbiss et al., 1994)</ref>. We kept the decoder as simple as possible (under 1000 lines of C code). We did not implement any sort of model adaptation before decoding, nor any word graph rescoring. Our decoder relies on KenLM <ref type="bibr" target="#b18">(Heafield et al., 2013)</ref> for the language modeling part. It also accepts unnormalized acoustic scores (transitions and emissions from the acoustic model) as input. The decoder attempts to maximize the following:</p><formula xml:id="formula_6">L(?) = logadd ??G lex (?,T ) T t=1 (f ?t (x) + g ?t?1,?t (x)) + ? log P lm (?) + ?|{i|? i = #}| ,<label>(5)</label></formula><p>where G lex (?, T ) is a graph constrained by lexicon, P lm (?) is the probability of the language model given a transcription ?, ?, ?, and ? are three hyper-parameters which control the weight of the language model, and the silence (#) insertion penalty, respectively. The beam of the decoder tracks paths with highest scores according to Equation <ref type="formula" target="#formula_6">(5)</ref>, by bookkeeping pairs of (language model, lexicon) states, as it goes through time. The language model state corresponds to the (n ? 1)-gram history of the n-gram language model, while the lexicon state is the sub-word unit position in the current word hypothesis. To maintain diversity in the beam, paths with identical (language model, lexicon) states are merged. Note that traditional decoders combine the scores of the merged paths with a max(?) operation (as in a Viterbi beam-search) -which would correspond to a max(?) operation in Equation <ref type="formula" target="#formula_6">(5)</ref> instead of logadd(?). We consider instead the logadd(?) operation (as first suggested by <ref type="bibr" target="#b3">Bottou (1991)</ref>), as it takes into account the contribution of all the paths leading to the same transcription, in the same way we do during training (see Equation <ref type="formula">(3)</ref>). In Section 4.1, we show that this leads to better accuracy in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We benchmarked our system on WSJ (about 81h of labeled audio data) and LibriSpeech <ref type="bibr" target="#b26">(Panayotov et al., 2015)</ref> (about 960h). We kept the original 16 kHz sampling rate. For WSJ, we considered the classical setup SI284 for training, DEV93 for validation, and EVAL92 for evaluation. For LibriSpeech, we considered the two available setups CLEAN and OTHER. All the hyper-parameters of our system were tuned on validation sets. Test sets were used only for the final evaluations.</p><p>The letter vocabulary L contains 30 graphemes: the standard English alphabet plus the apostrophe, silence (#), and two special "repetition" graphemes which encode the duplication (once or twice) of the previous letter (see Section 3.2.2). Decoding is achieved with our own decoder (see Section 3.3). We used standard language models for both datasets, i.e. a 4-gram model (with about 165K words in the dictionary) trained on the provided data for WSJ, and a 4-gram model 1 (about 200K words) for LibriSpeech. In the following, we either report letter-error-rates (LERs) or word-error-rates (WERs).</p><p>Training was performed with stochastic gradient descent on WSJ, and mini-batches of 4 utterances on LibriSpeech. Clipping parameter (see Equation <ref type="formula" target="#formula_5">(4)</ref>) was set to = 0.2. We used a momentum of 0.9. Input features, log-mel filterbanks, were computed with 40 coefficients, a 25 ms sliding window and 10 ms stride.</p><p>We implemented everything using TORCH7 2 . The CTC and ASG criterions, as well as the decoder were implemented in C (and then interfaced into TORCH).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>We tuned our acoustic model architectures by grid search, validating on the dev sets. We consider here two architectures, with low and high amount of dropout (see the parameter p in Section 3.2.3). <ref type="table" target="#tab_1">Table 1</ref> reports the details of our architectures. The amount of dropout, number of hidden units, as well as the convolution kernel width are increased linearly with the depth of the neural network. Note that as we use Gated Linear Units (see Section 3.1), each layer is duplicated as stated in Equation (1). Convolutions are followed by a fully connected layer, before the final layer which outputs 30 scores (one for each letter in the dictionary). Concerning WSJ, the LOW DROPOUT (p = 0.2) architecture has about 17M trainable parameters. For LibriSpeech, architectures have about 130M and 208M of parameters for the LOW DROPOUT (p = 0.2) and HIGH DROPOUT (p = 0.2 ? 0.6) architectures, respectively.   <ref type="figure" target="#fig_1">Figure 3</ref> shows the LER and WER on the LibriSpeech development sets, for the first 40 training epochs of our LOW DROPOUT architecture. LER and WER appear surprisingly well correlated, both on the "clean" and "other" version of the dataset.</p><p>In <ref type="table" target="#tab_2">Table 2b</ref>, we report WERs on the LibriSpeech development sets, both for our LOW DROPOUT and HIGH DROPOUT architectures. Increasing dropout regularize the acoustic model in a way which impacts significantly generalization, the effect being stronger on noisy speech. <ref type="table" target="#tab_2">Table 2a</ref> and <ref type="table" target="#tab_2">Table 2b</ref> also report the WER for the decoder ran with the max(?) operation (instead of logadd(?) for other results) used to aggregate paths in the beam with identical (language model, lexicon) states. It appears advantageous (as there is no code complexity increase in the decoder -one only needs to replace max(?) by logadd(?) in the code) to use the logadd(?) aggregation.   <ref type="bibr" target="#b48">(Zhang et al., 2014)</ref>. We also report extra information (besides word transcriptions) which might be used by each system, including speaker adaptation, or any other domain-specific data.</p><p>Paper Acoustic Model Sub-word Spkr Adapt. Extra Resources <ref type="bibr" target="#b26">Panayotov et al. (2015)</ref> HMM+DNN+pNorm phone fMLLR phone lexicon <ref type="bibr" target="#b29">Peddinti et al. (2015b)</ref> HMM+CNN phone iVectors phone lexicon <ref type="bibr" target="#b30">Povey et al. (2016)</ref> HMM+CNN phone iVectors phone lexicon, phone LM, data augm. <ref type="bibr" target="#b21">Ko et al. (2015)</ref> HMM+CNN+pNorm phone iVectors phone lexicon, data augm. <ref type="bibr" target="#b0">Amodei et al. (2016)</ref> 2D-CNN+RNN letter none 11.9Kh train set, Common Crawl LM <ref type="bibr" target="#b49">Zhou et al. (2018)</ref> CNN+GRU+policy learning letter none data augmentation <ref type="bibr" target="#b47">Zeyer et al. (2018)</ref> RNN+attention letter none LSTM LM this paper GLU-CNN letter none none <ref type="figure" target="#fig_2">Figure 4</ref> depicts alignments of the models with CTC and ASG criterions when forced aligned to a given target. Our analysis shows that the model with CTC criterion exhibits 500 ms delay compared to the model with ASG criterion. Similar observation was also previously noted in <ref type="bibr" target="#b31">Sak et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with other systems</head><p>In <ref type="table" target="#tab_4">Table 4</ref>, we compare our system with existing phone-based and letter-based approaches on WSJ and LibriSpeech. Phone-based acoustic state of the art models are reported as reference. These systems output in general senones; senones are carefully selected through a procedure involving a phonetic-context-based decision tree built from another GMM/HMM system. Phone-based systems also require an additional word lexicon which translates words into a sequence of phones. Most state of the art systems also perform speaker adaptation; iVectors compute a speaker embedding capturing both speaker and environment information <ref type="bibr" target="#b46">(Xue et al., 2014)</ref>, while fMMLR is a two-pass decoder technique which computes a speaker transform in the first pass <ref type="bibr" target="#b10">(Gales and Woodland, 1996)</ref>. Even though <ref type="table" target="#tab_3">Table 3</ref> associates speaker adaptation exclusively with phone-based systems, speaker adaptation can be also applied to letter-based systems.</p><p>State of the art performance for letter-based models on LibriSpeech is held by DEEP SPEECH 2 <ref type="bibr" target="#b0">(Amodei et al., 2016)</ref> and <ref type="bibr" target="#b47">(Zeyer et al., 2018)</ref> on noisy and clean subsets respectively. On WSJ state of the art performance is held by DEEP SPEECH 2. DEEP SPEECH 2 uses an acoustic model composed of a ConvNet and a Recurrent Neural Network (RNN). DEEP SPEECH 2 relies on a lot of extra speech data at training, combined with a very large 5-gram language model at inference time to make the letter-based approach competitive. Our system outperforms DEEP SPEECH 2 on clean data, even though our system has been trained with an order of magnitude less data. Acoustic model in <ref type="bibr" target="#b47">(Zeyer et al., 2018)</ref> is also based on RNNs and in addition employs attention mechanism. With LSTM language model their system shows lower WER than our, but with a simple 4-gram language model our system has slightly lower WER.</p><p>On WSJ the state of the art is a phone-based approach <ref type="bibr" target="#b5">(Chan and Lane, 2015b)</ref> which leverages an acoustic model combining CNNs, bidirectional LSTMs, and deep fully connected neural networks. The system also performs speaker adaptation at inference. We also compare with existing letter-based approaches on WSJ, which are abundant in the literature. They rely on recurrent neural networks, often bi-directional, and in certain cases combined with ConvNet architectures. Our system matches the best reported letter-based WSJ performance. The Gated ConvNet appears to be very strong at modeling complete words as it achieves 6.7% WER on LibriSpeech clean data even with no decoder, i.e. on the raw output of the neural network.</p><p>Concerning LibriSpeech, we summarize existing state of the art systems in <ref type="table" target="#tab_3">Table 3</ref>. We highlighted the acoustic model architectures, as well as the type of underlying sub-word units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced a simple end-to-end automatic speech recognition system, which combines a ConvNet acoustic model with Gated Linear Units, and a simple beam-search decoder. The acoustic model is trained to map audio  <ref type="bibr" target="#b30">(Povey et al., 2016)</ref> 4.3 -- <ref type="bibr" target="#b26">(Panayotov et al., 2015)</ref> 3.9 5.5 14.0 <ref type="bibr" target="#b29">(Peddinti et al., 2015b)</ref> -4.8 - <ref type="bibr" target="#b5">(Chan and Lane, 2015b)</ref> 3.5 -- <ref type="bibr" target="#b21">(Ko et al., 2015)</ref> ? --12.5 <ref type="bibr" target="#b17">(Hannun et al., 2014b)</ref> 14.1 -- <ref type="bibr" target="#b1">(Bahdanau et al., 2016)</ref> 9.3 -- <ref type="bibr" target="#b13">(Graves and Jaitly, 2014)</ref> 8.2 -- <ref type="bibr" target="#b24">(Miao et al., 2015)</ref> 7.3 -- <ref type="bibr" target="#b7">(Chorowski and Jaitly, 2016)</ref> 6.7 --(Liu et al., 2017) ? 6.7 -- <ref type="bibr" target="#b20">(Hori et al., 2017)</ref> 5.6 -- <ref type="bibr" target="#b49">(Zhou et al., 2018</ref> sequences to sequences of characters using a structured-output learning approach based on a variant of CTC. Our system outperforms existing letter-based approaches (which do not use extra data at training time or powerful LSTM language models), both on WSJ and LibriSpeech. Overall phone-based approaches are still holding the state of the art, but our system's performance is competitive on LibriSpeech, suggesting pronunciations is implicitly well modeled with enough training data. Further work should include leveraging speaker identity, training from the raw waveform, data augmentation, training with more data, and better language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) The CTC graph which represents all the acceptable sequences of letters for the transcription "cat" over 6 frames. (b) The same graph used by ASG, where blank labels have been discarded. (c) The fully connected graph describing all possible sequences of letter; this graph is used for normalization purposes in ASG. Un-normalized transitions scores are possible on edges of these graphs. At each time step, nodes are assigned a conditional unnormalized score, output by the Gated ConvNet acoustic model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>LibriSpeech Letter Error Rate (LER) and Word Error Rate (WER) for the first training epochs of our LOW DROPOUT architecture. (a) is on dev-clean, (b) on dev-other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of alignments produced by the models with CTC (top) and ASG (bottom) criterions on audio spectrogram over time (each time frame on X axis corresponds to a 40ms window with 10 ms stride).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Architecture details. "#conv." is the number of convolutional layers. Dropout amplitude, "#hu" (number of output hidden units) and "kw" (convolution kernel width) are provided for the first and last layer (all are linearly increased with layer depth). The size of the final layer is also provided.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Architecture #conv.</cell><cell>dropout</cell><cell>#hu</cell><cell>kw</cell><cell>#hu</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">first/last layer first/last layer first/last layer full connect</cell></row><row><cell>WSJ</cell><cell>Low Dropout</cell><cell>17</cell><cell>0.25/0.25</cell><cell>100/375</cell><cell>3/21</cell><cell>1000</cell></row><row><cell cols="2">LibriSpeech Low Dropout</cell><cell>17</cell><cell>0.25/0.25</cell><cell>200/750</cell><cell>13/27</cell><cell>1500</cell></row><row><cell></cell><cell>High Dropout</cell><cell>19</cell><cell>0.20/0.60</cell><cell>200/1000</cell><cell>13/29</cell><cell>2000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison in LER and WER of variants of our model on (a) WSJ and (b) LibriSpeech. LER is computed with no decoding. Operator max and logadd refer to the aggregation of beam hypotheses (see Section 3.3).</figDesc><table><row><cell>(a) WSJ</cell><cell></cell><cell></cell><cell cols="3">(b) LibriSpeech</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">dev93</cell><cell></cell><cell cols="2">dev-clean</cell><cell cols="2">dev-other</cell></row><row><cell>model</cell><cell>LER</cell><cell>WER</cell><cell>model</cell><cell>LER</cell><cell>WER</cell><cell>LER</cell><cell>WER</cell></row><row><cell>LOW DROP. (ASG, max)</cell><cell>-</cell><cell>10.0</cell><cell>LOW DROP. (ASG, logadd)</cell><cell>2.7</cell><cell>4.8</cell><cell>9.8</cell><cell>15.2</cell></row><row><cell>LOW DROP. (ASG, logadd)</cell><cell>7.2</cell><cell>9.8</cell><cell>HIGH DROP. (CTC)</cell><cell>2.6</cell><cell>4.7</cell><cell>9.5</cell><cell>14.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HIGH DROP. (ASG, max)</cell><cell>-</cell><cell>4.7</cell><cell>-</cell><cell>14.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HIGH DROP. (ASG, logadd)</cell><cell>2.3</cell><cell>4.6</cell><cell>9.0</cell><cell>13.8</cell></row><row><cell>4.1.1 Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different near state of the art ASR systems on LibriSpeech. We report the type of acoustic model used for various systems, as well as the type of sub-word units. HMM stands for Hidden Markov Model, CNN for ConvNet; when not specified, CNNs are 1D. pNorm is a particular non-linearity</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison in WER of our model with other systems on WSJ and LibriSpeech. Systems with or ? use additional data or data augmentation at training, respectively. WSJ eval92 LibriSpeech test-clean LibriSpeech test-other</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.openslr.org/11. 2 http://www.torch.ch.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2 : End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum mutual information estimation of hidden Markov model parameters for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Une Approche th?orique de l&apos;Apprentissage Connexionniste: Applications ? la Reconnaissance de la Parole</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep recurrent neural networks for acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01482</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep recurrent neural networks for acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01482</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mean and variance adaptation within the MLLR framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypothesis spaces for minimum Bayes risk training in large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2406" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2873</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable modified kneser-ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-level language modeling and decoding for open vocabulary end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Khudanpur ; Y. Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
	<note>Audio augmentation for speech recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An rnn based speech recognition system with discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gram-ctc: Automatic unit selection and target decomposition for sequence labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<idno>abs/1703.00096</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eesen: End-to-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="14" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">JHU ASpIRE system: Robust LVCSR with TDNNs, iVector adaptation, and RNN-LMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Purely sequencetrained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2751" to="2755" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acoustic modelling with cd-ctc-smbr lstm rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="604" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using I-Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05899</idno>
		<title level="m">The IBM 2015 english conversational telephone speech recognition system</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02136</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GMM-free DNN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5639" to="5643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint training of convolutional and non-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5572" to="5576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-06" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improvements in beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Steinbiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Spoken Language Processing (ICSLP)</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="2143" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-to-end neural segmental models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1254" to="1264" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modular construction of time-delay neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The HTK tied-state continuous speech recogniser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurospeech</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5255" to="5259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast adaptation of deep neural network based on discriminant codes for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1713" to="1725" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03294</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="215" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving end-to-end speech recognition with policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
