<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WENETSPEECH: A 10000+ HOURS MULTI-DOMAIN MANDARIN CORPUS FOR SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mobvoi Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Lv</surname></persName>
							<email>hanglv@nwpu-aslp.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mobvoi Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
							<email>lxie@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Shell Shell Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Bu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Shell Shell Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mobvoi Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mobvoi Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mobvoi Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mobvoi Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WENETSPEECH: A 10000+ HOURS MULTI-DOMAIN MANDARIN CORPUS FOR SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-automatic speech recognition</term>
					<term>corpus</term>
					<term>multi- domain</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics and noisy conditions. An optical character recognition (OCR) method is introduced to generate the audio/text segmentation candidates for the YouTube data on the corresponding video subtitles, while a high-quality ASR transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with WenetSpeech for evaluation -Dev for cross-validation purpose in training, Test Net, collected from Internet for matched test, and Test Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with WenetSpeech are provided for three popular speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, WenetSpeech is the current largest open-source Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the past decade, the performance of automatic speech recognition (ASR) systems have been significantly improved. On the one hand, the development of neural networks has increased the capacity of models, pushing the dominant framework from the hybrid hidden Markov models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to end-to-end models, like CTC <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, RNN-T <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, and encoder-decoder based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. To simply implement such advanced models and obtain state-of-the-art reproducible results, researchers also release several open source toolkits, including Kaldi <ref type="bibr" target="#b13">[14]</ref>, Sphinx <ref type="bibr" target="#b14">[15]</ref>, Fariseq <ref type="bibr" target="#b15">[16]</ref>, ESPnet <ref type="bibr" target="#b16">[17]</ref>, and recently WeNet <ref type="bibr" target="#b17">[18]</ref>, etc. On the other hand, self-supervised speech representation learning methods are proposed to make better use of a large amount untranscribed data, such as wav2vec <ref type="bibr" target="#b18">[19]</ref>, wav2vec 2.0 <ref type="bibr" target="#b19">[20]</ref>, Hubert <ref type="bibr" target="#b20">[21]</ref>, and wav2vec-U <ref type="bibr" target="#b21">[22]</ref>, etc. In addition to these algorithm-level efforts, the development of open source corpora is also crucial to the research community, especially for academia or small-scale research groups.</p><p>Most of the current open source speech corpora for ASR benchmark in the literature remain small size and lack of domain diversities. For example, the commonly used English speech corpus -Librispeech <ref type="bibr" target="#b22">[23]</ref>, which includes about 1000 hours reading speech from audiobook, currently has a word error rate (WER) of 1.9% on its test-clean benchmark. However, industrial ASR systems are usually trained with tens of thousands of hours of data with acoustic diversity and domain coverage. To close the gap between industrial system and academic research, we notice that several large-scale multi-domain English corpora, including The People's Speech <ref type="bibr" target="#b23">[24]</ref>, MLS <ref type="bibr" target="#b24">[25]</ref> and GigaSpeech <ref type="bibr" target="#b25">[26]</ref>, are made available recently. The representative GigaSpeech consists of 10000 hours of high quality transcribed English speech for supervised ASR training and 40000 hours audio in total for semi-supervised or unsupervised training, contributing to the research community for developing more generalized ASR systems. Comparing with those English corpora, the largest open source Mandarin speech data is AIShell-2 <ref type="bibr" target="#b26">[27]</ref>, including 1000 hours speech recorded in a quiet environment and having a state-of-the-art character error rate of 5.35%. It is too simple to do further research and ASR systems developed based on it may be susceptible to performance degradation in the complex real-world scenarios. In addition, current open source Mandarin corpora are also unable to train a well generalized pre-trained model, since both the Wav2vec 2.0 large model <ref type="bibr" target="#b19">[20]</ref> and the XLSR-53 model <ref type="bibr" target="#b27">[28]</ref> are trained based on more than 50000 hours of English speech data.</p><p>In this work, we release WenetSpeech, a large multi-domain Mandarin corpus licensed for non-commercial usage under CC-BY 4.0. "We" means connection and sharing, while "net" means all of the data are collected from the Internet which is repository for diversity. The key features of WenetSpeech include:</p><p>? Large scale. 10000+ hours labeled data, 2400+ hours weakly labeled data, and about 10000 hours unlabeled data are provided, resulting in 22400+ hours audio in total. ? Diverse. The data are collected from multiple speaking styles, scenarios, domains, topics, and noisy conditions. ? Extensible. An extensible metadata is designed to extend the data in the future. To the best of our knowledge, WenetSpeech is the current largest open source Mandarin speech corpus with domain diversity to satisfy various speech recognition tasks. In Section 2, we first introduce the construction procedure of WenetSpeech with a reliable pipeline to obtain high-quality transcriptions, including OCR-based caption recognition on Youtube video, ASR-based automatic transcription on Podcast audio, as well as a new end-to-end label error detection approach. The corpus composition is described in Section 3 and baseline benchmarks built on Kaldi, ESPnet and WeNet toolkits are introduced in Section 4. We believe our corpus will bring benefit to research community for developing more generalized ASR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CREATION PIPELINE</head><p>In this section, we introduce the detailed creation pipeline of our WenetSpeech corpus, including original audio collection, audio/text segmentation candidate generation, and candidate calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stage 1: Audio Collection</head><p>In the beginning, we manually define the domains into 10 categories, which including audiobook, commentary, documentary, drama, interview, reading, talk, variety, and others. Then, we collected and tagged the audio files from YouTube and Podcast playlists according to our selected categories. Especially, for the YouTube data, the videos are also downloaded for preparing the audio/text segmentation candidates with our OCR system. For Podcast, since the manually transcribed Mandarin data is limited, we only considered the category information and prepared to transcribed it by a high-quality ASR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stage 2: Candidates Generation</head><p>In this part, we introduce the specific pipelines to obtain the audio/text segmentation candidates from YouTube data by an OCR system and Podcast data by a high-quality ASR system. At last, text normalization 1 technique is applied to all the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">YouTube OCR</head><p>As <ref type="figure" target="#fig_2">Figure 1</ref> shown, an OCR-based pipeline is applied for generating candidates from embedded subtitles on YouTube videos.</p><p>1. Text Detection: apply CTPN <ref type="bibr" target="#b28">[29]</ref> based text detection on each frame image in the video. 2. Subtitle Validation: mark frame t as the start point of a specific subtitle phrase, when a phrase of text is detected at the bottom of the screen for subtitles at frame t. 3. Subtitle Change Detection: compute the structural similarity (SSIM) of subtitle area frame by frame until a change is detected at frame t + n. Then, the frame t + n ? 1 is marked as the the end point of this subtitle phrase. 4. Text Recognition: a CRNN-CTC <ref type="bibr" target="#b29">[30]</ref> based text recognition approach is used to recognize the detected subtitle area. 5. Audio/Text Pair: prepare each audio/text pair segmentation candidate with the corresponding (start point, end point, subtitle phrase) tuple.  To verify whether the proposed OCR-based pipeline is reliable, we randomly extracted 5000 subtitle transcriptions from the YouTube data with different themes and manually annotated them as benchmarks by professionals for testing the Text Recognition module. Finally, we obtain 98% accuracy on the test set and confirm the reliability of our OCR-based pipeline for WenetSpeech corpus. the box is in a reasonable subtitle area, it will be marked as red, and then, the subtitle box is further processed by the Text Recognition module. At last, the recognition result is shown above each subtitle box.</p><p>In addition, we find that some annotators prefer to split a long subtitle phrase into many pieces for a video, so that the problem of audio and subtitle asynchronous is introduced. This leads an inaccurate subtitle boundary detection problem to our OCR system. To alleviate it, we merge the consecutive video phrases until the audio is over 8 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Podcast Transcription</head><p>We use a third-party commercial ASR transcription system to transcribe all the Podcast data. The transcription system is one of the best system on the public benchmark platform 2 , and more than 95% accuracy rates have been achieved in most of the testing scenarios, including news, reading, talk show, conversation, education, games, TV, drama, and so on.</p><p>The transcription system first segments the original audio into short segments by a VAD module, and then the audio/text pair segmentation candidates are generated by speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Stage 3: Candidates Validation</head><p>Although the used OCR system and transcription system are highquality enough, the errors from candidate generation, such as subtitle annotation error, timestamp inaccuracy, OCR mistake, transcription word error, and text normalization error, are still unavoidable. To further improve the quality of our WenetSpeech corpus, we apply the following validation approach to classify the YouTube OCR and Podcast transcription candidates according to their confidences and filter out the extremely bad candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Force Alignment Graph</head><p>Here, we propose an novel CTC-based end-to-end force alignment approach to detect the transcription error. The transcription is first segmented by the model unit of CTC, and then a force alignment graph (L) is built for each candidate as shown in <ref type="figure">Figure 3</ref>. The key features of the alignment graph are:</p><p>1. The oracle transcription alignment path is included. 2. Arbitrary deletion operation at any position is allowed through tag ?del? with penalty p1. 3. Arbitrary insertion or substitution at any position is allowed.</p><p>From each reasonable position, a start tag ?is? and a end tag ?/is? are connected to a global filler state. On this filler state, each CTC modeling unit has a corresponding self-loop arc with penalty p2, which is presented by tag ?gbg?. This makes it possible to capture the error between the audio and the corresponding transcription through decoding technique.</p><p>Compared with traditional hybrid validation approach which is used in Librispeech, our proposed approach is a pure end-to-end approach. There is no need for HMM typologies, lexicon, language model components, or careful design of the filler state. So the proposed novel approach simplifies the whole pipeline a lot. The force alignment graph is implemented by the WeNet toolkit, and it is publicly available <ref type="bibr" target="#b2">3</ref>   <ref type="figure">Fig. 3</ref>. An example force alignment graph L of "????" 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label Error Detection</head><p>After defining the force alignment graph L, it is further composed with the CTC topology graph T [31] to build the final force decoding graph F = T ? L for label error detection and validation.</p><p>In addition, we assign confidence for each candidate by it's reference (ref) and the force decoding hypothesis (hyp). The confidence c is computed as</p><formula xml:id="formula_0">c = 1 ? EditDistance(ref, hyp) max(len(ref ), len(hyp)) .</formula><p>With the confidence, we classify the audio/text segmentation candidates of our WenetSpeech corpus into Strong Label and Weak Label sets and even filter out the extremely ones to Others set. <ref type="figure">Figure 4</ref> shows two real examples that we find by applying label error detection. In Case 1, human subtitle error was successfully detected. In Case 2, OCR error was successfully detected. Please note the model used in the transcription system and force alignment system are developed and trained with different model methods and data. p1 is set to 2.3 and p2 is set to 4.6 in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4. Examples of label error detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE WENETSPEECH CORPUS</head><p>In this section, we describe the metadata, audio format, partition by confidence, data diversity, and training and evaluation set design of our WenetSpeech corpus. Instructions and scripts are available at WenetSpeech GitHub repo 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Metadata and Audio Format</head><p>We save all the metadata information to a single JSON file. Local path, original public URL, domain tags, md5, and segments are provided for each audio. And timestamp, label, confidence, subset information are provided for each segment. The design is extensible, and we are planning to add more diverse data in the future. The original audio files are downloaded and converted to 16k sampling rate, single-channel, and 16-bit signed-integer format. Then Opus compression is applied at an output bit rate of 32 kbps to reduce the size of the WenetSpeech corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Size and Confidence</head><p>We assign confidence for each valid segment which measures the label quality, where the confidence in defined in Section 2.3.2. As shown in <ref type="table" target="#tab_1">Table 1</ref>, we select 10005 hours Strong Label data, whose confidence is greater than 0.95, as the supervised training data. The 2478 hours Weak Label data, whose confidence is between 0.60 and 0.95, is reserved in our metadata for semi-supervised or other usage. At last, Others represent all invalid data (i.e. the confidence of data is less than 0.6 or unrecognized) for speech recognition task. In summary, WenetSpeech has 22435 hours of raw audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Data Diversity and Subsets</head><p>We tag all the training data with its source and domain. All of the training data is from Youtube and Podcast. As shown in <ref type="table">Table 2</ref>, we classify the data into 10 groups according to its category. Please note about 4k hours of data is from drama, which is a special domain with a wide range of themes, topics and scenarios, which may cover any kind of other categories.</p><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, we provide 3 training subsets, namely S, M and L for building ASR systems on different data scales. Subsets S and M are sampled from all the training data which have the oracle confidence 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Sets</head><p>We will release the following evaluation datasets associated with WenetSpeech, and the major information is summarized in <ref type="table">Table 4.</ref> 1. Dev, is specifically designed dataset for some speech tools which require cross-validation in training. 2. Test Net, is a matched test set from the internet. Compared with the training data, it also covers several popular and difficult domains like game commentary, live commerce, etc. 3. Test Meeting, is a mismatched test set since it is a far-field, conversational, spontaneous, and meeting speech dataset. It is sampled from 197 real meetings in a variety of rooms. Its </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we introduce the baseline systems and experimental results on three popular speech recognition toolkits, Kaldi <ref type="bibr" target="#b13">[14]</ref>, ESPnet <ref type="bibr" target="#b16">[17]</ref> and WeNet <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Kaldi Benchmark 5</head><p>The Kaldi baseline implements a classical chain model <ref type="bibr" target="#b31">[32]</ref> using various amounts of WeNetSpeech data (i.e. S, M, L). We choose the open source vocabulary, BigCiDian 6 , as our lexicon. And we segment our transcriptions with the open source word segmentation toolkit, jieba <ref type="bibr" target="#b32">[33]</ref>. First, we train a GMM-HMM model to obtain the training alignments. Then, we train a chain model, which stacks 6 convolutional neural network (CNN) layers, 9 factored timedelay neural network (TDNN-F) layers <ref type="bibr" target="#b33">[34]</ref>, 1 time-restricted attention layer <ref type="bibr" target="#b34">[35]</ref> (H = 30), 2 projected long short-term memory (LSTMP) with TDNN-F blocks. We feed the 40 dimensional filterbank (FBank) features and 100 dimensional i-vector features as the input. In order to be consistent with the other systems, we only use SpecAugment <ref type="bibr" target="#b35">[36]</ref> technique and abandon the speed/volume perturbation techniques. The chain model is trained by LF-MMI criterion with cross-entropy loss (10 epochs for subset S, and 4 epochs for subset M and L). A 3-gram language model (LM) is used for decoding and generating the lattice. Finally, a recurrent neural network LM (RNNLM) is further adopted to rescore the lattices. The 3-gram and RNNLM are both trained on all the WenetSpeech transcriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ESPnet Benchmark 7</head><p>The ESPnet baseline employs a Conformer model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref> which is designed to capture the global context with the multi-head selfattention module and learn the local correlations synchronously with the convolution module. Our Conformer model consists of a 12-block Conformer encoder (d ff = 2048, H = 8, d att = 512, CNNkernel = 15) and a 6-block Transformer <ref type="bibr" target="#b37">[38]</ref> decoder (d ff = 2048, H = 8). A set of 5535 Mandarin characters and 26 English letters is used as the modeling units. The objective is a logarithmic linear combination of the CTC (? = 0.3) and attention objectives. Label smoothing is applied to the attention objective. During data preparation, we generate 80-dimensional FBank features with a 32ms window and a 8ms frame shift. SpecAugment with 2 frequency masks (F = 30) and 2 time masks (T = 40), and global CMVN technique are used as data pre-processing. During training, we choose the Adam optimizer with the maximum learning rate of 0.0015. The Noam learning rate scheduler with 30k warm-up steps is used. The model was trained with dynamic batching skill for 30 epochs. At last, the last 10 best checkpoints were averaged to be the final model. For decoding, the ESPNet system follows the joint CTC/Attention beam search strategy <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">WeNet Benchmark 8</head><p>The WeNet baseline implements a U2 model <ref type="bibr" target="#b17">[18]</ref>, which unifies streaming and non-streaming end-to-end (E2E) speech recognition in a single model. The basic setup of our WeNet model is same as the ESPnet model except the following minor points: 1) We prepare 80-dimensional FBank features with a 25ms window and a 10ms frame shift. SpecAugment with 2 frequency masks (F = 30) and 3 time masks (T = 50) and global CMVN technique are applied on the top of our features. 2) The max trainable epoch is 25. Models of the last 10 epochs were averaged to be the final model. The key difference between WeNet and ESPNet is different decoding strategies. Specifically, different from ESPNet's auto-regressive decoding strategy, Wenet generates the N-Best hypotheses by the CTC branch and rescores them by the attention branch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental Results</head><p>We must announce that the results listed here are purely for the purpose of providing a baseline system for each toolkit. They might not reflect the state-of-the-art performance of each toolkit. In <ref type="table" target="#tab_3">Table 5</ref>, we report the experimental results in Mixture Error Rate (MER) <ref type="bibr" target="#b39">[40]</ref>, which considers Mandarin characters and English words as the tokens in the edit distance calculation, on three designed test sets and one well-known, publicly available test set (i.e. AIShell-1 <ref type="bibr" target="#b40">[41]</ref> test) with Kaldi, ESPNet and WeNet toolkits respectively. The good performance on AIShell-1 reflects the diversity and reliability of our WenetSpeech corpus. And the results on our designed test sets reflect they are quite challenging. In <ref type="table">Table 6</ref>, we provide the Kaldi baseline results for difference scale WenetSpeech subsets. As the growth of the data amount, the performance goes up steadily.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Co-first authors, equal contribution. Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>OCR based YouTube data collection pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 Fig. 2 .</head><label>22</label><figDesc>shows 4 typical examples of our OCR-based system. The results of the Text Detection module are marked with boxes. If 1 https://github.com/speechio/chinese text normalization (a) Audiobook (b) Game commentary (c) Drama (d) Lecture Example outputs of the OCR pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">&lt;gbg&gt;/p 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Filler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;is&gt;</cell><cell>&lt;/is&gt;</cell><cell>&lt;is&gt;</cell><cell>&lt;/is&gt;</cell><cell>&lt;is&gt;</cell><cell>&lt;/is&gt;</cell><cell>&lt;is&gt;</cell><cell>&lt;/is&gt;</cell><cell>&lt;is&gt;</cell><cell>&lt;/is&gt;</cell></row><row><cell>0</cell><cell>?</cell><cell>1</cell><cell>?</cell><cell>2</cell><cell>?</cell><cell>3</cell><cell>?</cell><cell>4</cell><cell></cell></row><row><cell></cell><cell>&lt;del&gt;/p 1</cell><cell></cell><cell cols="2">&lt;del&gt;/p 1</cell><cell cols="2">&lt;del&gt;/p 1</cell><cell cols="2">&lt;del&gt;/p 1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>WenetSpeech partition</figDesc><table><row><cell>Set</cell><cell cols="3">Confidence Hours</cell></row><row><cell cols="4">Strong Label [0.95, 1.00] 10005</cell></row><row><cell>Weak Label</cell><cell cols="3">[0.60, 0.95) 2478</cell></row><row><cell>Others</cell><cell>/</cell><cell cols="2">9952</cell></row><row><cell>Total(hrs)</cell><cell>/</cell><cell cols="2">22435</cell></row><row><cell cols="4">Table 2. Training data in different domains with duration (hrs)</cell></row><row><cell>Domain</cell><cell cols="3">Youtube Podcast Total</cell></row><row><cell>audiobook</cell><cell>0</cell><cell>250.9</cell><cell>250.9</cell></row><row><cell>commentary</cell><cell>112.6</cell><cell>135.7</cell><cell>248.3</cell></row><row><cell cols="2">documentary 386.7</cell><cell>90.5</cell><cell>477.2</cell></row><row><cell>drama</cell><cell>4338.2</cell><cell>0</cell><cell>4338.2</cell></row><row><cell>interview</cell><cell>324.2</cell><cell>614</cell><cell>938.2</cell></row><row><cell>news</cell><cell>0</cell><cell>868</cell><cell>868</cell></row><row><cell>reading</cell><cell>0</cell><cell>1110.2</cell><cell>1110.2</cell></row><row><cell>talk</cell><cell>204</cell><cell>90.7</cell><cell>294.7</cell></row><row><cell>variety</cell><cell>603.3</cell><cell>224.5</cell><cell>827.8</cell></row><row><cell>others</cell><cell>144</cell><cell>507.5</cell><cell>651.5</cell></row><row><cell>Total</cell><cell>6113</cell><cell>3892</cell><cell>10005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The training data subsets</figDesc><table><row><cell cols="4">Training Subsets Confidence Hours</cell></row><row><cell>L</cell><cell cols="2">[0.95, 1.0]</cell><cell>10005</cell></row><row><cell>M</cell><cell>1.0</cell><cell></cell><cell>1000</cell></row><row><cell>S</cell><cell>1.0</cell><cell></cell><cell>100</cell></row><row><cell cols="4">Table 4. The WenetSpeech evaluation sets</cell></row><row><cell cols="4">Evaluation Sets Hours Source</cell></row><row><cell>Dev</cell><cell>20</cell><cell cols="2">Internet</cell></row><row><cell>Test Net</cell><cell>23</cell><cell cols="2">Internet</cell></row><row><cell>Test Meeting</cell><cell>15</cell><cell cols="2">Real meeting</cell></row><row><cell cols="4">topics cover education, real estate, finance, house and home,</cell></row><row><cell cols="2">technology, interview and so on.</cell><cell></cell></row><row><cell cols="4">The three evaluation sets are carefully checked by professional an-</cell></row><row><cell cols="3">notators to ensure the transcription quality.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results (MER%) on different test sets for baseline systems trained using WenetSpeech training subset L</figDesc><table><row><cell>Toolkit</cell><cell cols="4">Dev Test Net Test Meeting AIShell-1</cell></row><row><cell>Kaldi</cell><cell>9.07</cell><cell>12.83</cell><cell>24.72</cell><cell>5.41</cell></row><row><cell cols="2">ESPNet 9.70</cell><cell>8.90</cell><cell>15.90</cell><cell>3.90</cell></row><row><cell>WeNet</cell><cell>8.88</cell><cell>9.70</cell><cell>15.59</cell><cell>4.61</cell></row><row><cell cols="5">Table 6. Kaldi baseline results (MER%) for different WenetSpeech</cell></row><row><cell cols="2">training subsets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SubSet</cell><cell>Dev</cell><cell cols="3">Test Net Test Meeting AIShell-1</cell></row><row><cell>L</cell><cell>9.07</cell><cell>12.83</cell><cell>24.72</cell><cell>5.41</cell></row><row><cell>M</cell><cell>9.81</cell><cell>14.19</cell><cell>28.22</cell><cell>5.93</cell></row><row><cell>S</cell><cell>11.70</cell><cell>17.47</cell><cell>37.27</cell><cell>7.66</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/SpeechColab/Leaderboard</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https:github.com/wenet-e2e/wenet/blob/main/runtime/core/bin/ label checker main.cc 4 https://github.com/wenet-e2e/WenetSpeech</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/wenet-e2e/WenetSpeech/tree/main/toolkits/kaldi 6 https://github.com/speechio/BigCiDian 7 https://github.com/wenet-e2e/WenetSpeech/tree/main/toolkits/espnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/wenet-e2e/WenetSpeech/tree/main/toolkits/wenet 9 https://www.mindspore.cn/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENTS</head><p>We thank Jiayu Du and Guoguo Chen for their suggestions on this work. We thank Tencent Ethereal Audio Lab and Xi'an Future AI Innovation Center for providing hosting service for WenetSpeech. We also thank MindSpore for the support of this work, which is a new deep learning computing framwork <ref type="bibr" target="#b8">9</ref> . Our gratitude goes to Lianhui Zhang and Yu Mao for collecting some of the YouTube data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning Representation Learning Workshop (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade rnn-transducer: Syllable based streaming on-device mandarin speech recognition with a syllable-tocharacter converter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient conformer with probsparse attention mechanism for end-to-endspeech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sining</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4578" to="4582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-toend speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech-Transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
	<note>Ruoming Pang. Interspeech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motl??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Silovsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesel?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An overview of the SPHINX speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-W</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ESPnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsubasa</forename><surname>Ochiai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WeNet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4054" to="4058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">HuBERT: How much can a bad teacher benefit ASR pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="6533" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS), 2021</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The people&apos;s speech: A large-scale diverse english speech recognition dataset for commercial usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Manuel Ciro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Felipe</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Cer?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjali</forename><surname>Achorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mls: A large-scale multilingual dataset for speech research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2757" to="2761" />
		</imprint>
	</monogr>
	<note>Ronan Collobert. Interspeech</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gigaspeech: An evolving, multidomain ASR corpus with 10,000 hours of transcribed audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3670" to="3674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Aishell-2: Transforming mandarin ASR research into industrial scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Bu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10583</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2426" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Purely sequencetrained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Jieba chinese word segmentation tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-orthogonal low-rank matrix factorization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Yarmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3743" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A time-restricted self-attention layer for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoyuki</forename><surname>Kamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuekai</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
	<note>Recent developments on ESPnet toolkit boosted by conformer</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint CTC/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The ASRU 2019 mandarin-english codeswitching speech recognition challenge: open datasets, tracks, methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangze</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aishell-1: An opensource mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
