<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Zhao</surname></persName>
							<email>zhaoborui.gm@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
							<email>cui-quan@toki.waseda.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyu</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
							<email>liangjiajun@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupled Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last decades, the computer vision field has been revolutionized by deep neural networks (DNN), which successfully boost various real-scenario tasks, e.g., image classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>, objection detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, and semantic segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref>. However, powerful networks normally benefit from large model capacities, introducing high computational and storage costs. Such costs are not preferable in industrial applications, where lightweight models are widely deployed. In the literature, a potential direction of cutting down the costs is knowledge distillation (KD).  <ref type="figure" target="#fig_1">Figure 1</ref>. Illustration of the classical KD <ref type="bibr" target="#b11">[12]</ref> and our DKD. We reformulate KD into a weighted sum of two parts, i.e., TCKD and NCKD. The first equation shows that KD (1) couples NCKD with p T t (the teacher's confidence on the target class), and (2) couples the importance of two parts. Furthermore, we demonstrate that the first coupling suppresses the effectiveness, and the second limits the flexibility for knowledge transfer. We propose DKD to address these issues, which employs hyper-parameters ? for TCKD and ? for NCKD, killing the two birds with one stone.</p><p>KD represents a series of methods concentrating on transferring knowledge from a heavy model (teacher) to a light one (student), which can improve the light model's performance without introducing extra costs.</p><p>The concept of KD was firstly proposed in <ref type="bibr" target="#b11">[12]</ref> to transfer the knowledge via minimizing the KL-Divergence between prediction logits of teachers and students <ref type="figure" target="#fig_1">(Figure 1a</ref>). Since <ref type="bibr" target="#b27">[28]</ref>, most of the research attention has been drawn to distill knowledge from deep features of intermediate layers. Compared with logits-based methods, the performance of feature distillation is superior on various tasks, so research on logit distillation has been barely touched. However, training costs of feature-based methods are unsatisfactory, because extra computational and storage usage are introduced (e.g., network modules and complex operations) for distilling deep features during training time.</p><p>Logit distillation requires marginal computational and storage costs, but the performance is inferior. Intuitively, logit distillation should achieve comparable performance as feature distillation, since logits are in higher semantic level than deep features. We suppose that the potential of logit distillation is limited by unknown reasons, causing the unsatisfactory performance. To revitalize logits-based methods, we start this work by delving into the mechanism of KD. Firstly, we divide a classification prediction into two levels: (1) a binary prediction for the target class and all the non-target classes and (2) a multi-category prediction for each non-target class. Based on this, we reformulate the classical KD loss <ref type="bibr" target="#b11">[12]</ref> into two parts, as shown in <ref type="figure" target="#fig_1">Figure 1b</ref>. One is a binary logit distillation for the target class and the other is a multi-category logit distillation for non-target classes. For simplification, we respectively name them as target classification knowledge distillation (TCKD) and non-target classification knowledge distillation (NCKD). The reformulation allows us to study the effects of the two parts independently.</p><p>TCKD transfers knowledge via binary logit distillation, which means only the prediction of the target class is provided while the specific prediction of each non-target class is unknown. A reasonable hypothesis is that TCKD transfers knowledge about the "difficulty" of training samples, i.e., the knowledge describes how difficult it is to recognize each training sample. To validate this, we design experiments from three aspects to increase the "difficulty" of training data, i.e., stronger augmentation, noisier label and inherently challenging dataset.</p><p>NCKD only considers the knowledge among non-target logits. Interestingly, we empirically prove that only applying NCKD achieves comparable or even better results than the classical KD, indicating the vital importance of knowledge contained in non-target logits, which could be the prominent "dark knowledge".</p><p>More importantly, our reformulation demonstrates that the classical KD loss is a highly coupled formulation (as shown in <ref type="figure" target="#fig_1">Figure 1b</ref>), which could be the reason why the potential of logit distillation is limited. Firstly, the NCKD loss term is weighted by a coefficient that negatively correlates with the teacher's prediction confidence on the target class. Thus larger prediction scores would lead to smaller weights. The coupling significantly suppresses the effects of NCKD on well-predicted training samples. Such suppression is not preferable since the more confident the teacher is in the training sample, the more reliable and valuable knowledge it could provide. Secondly, the significance of TCKD and NCKD are coupled, i.e., weighting TCKD and NCKD separately is not allowed. Such limitation is not preferable since TCKD and NCKD should be separately considered since their contributions are from different aspects.</p><p>To address these issues, we propose a flexible and efficient logit distillation method named Decoupled Knowledge Distillation (DKD, <ref type="figure" target="#fig_1">Figure 1b</ref>). DKD decouples the NCKD loss from the coefficient negatively correlated with the teacher's confidence by replacing it with a constant value, improving the distillation effectiveness on wellpredicted samples. Meanwhile, NCKD and TCKD are also decoupled so that their importance can be separately considered by adjusting the weight of each part.</p><p>Overall, our contributions are summarized as follows:</p><p>? We provide an insightful view to study logit distillation by dividing the classical KD into TCKD and NCKD. Additionally, the effects of both parts are respectively analyzed and proved. ? We reveal limitations of the classical KD loss caused by its highly coupled formulation. Coupling NCKD with the teacher's confidence suppresses the effectiveness of knowledge transfer. Coupling TCKD with NCKD limits the flexibility to balance the two parts. ? We propose an effective logit distillation method named DKD to overcome these limitations. DKD achieves state-of-the-art performances on various tasks. We also empirically validate the higher training efficiency and better feature transferability of DKD compared with feature-based distillation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The concept of knowledge distillation (KD) was firstly proposed by Hinton et al. in <ref type="bibr" target="#b11">[12]</ref>. KD defines a learning manner where a bigger teacher network is employed to guide the training of a smaller student network for many tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. The "dark knowledge" is transferred to students via soft labels from teachers. For raising the attention on negative logits, the hyper-parameter temperature was introduced. The following works can be divided into two types, distillation from logits <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> and intermediate features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Previous works of logit distillation mainly focus on proposing effective regularization and optimization methods rather than novel methods. DML <ref type="bibr" target="#b43">[44]</ref> proposes a mutual learning manner to train students and teachers simultaneously. TAKD <ref type="bibr" target="#b21">[22]</ref> introduces an intermediate-sized network named "teacher assistant" to bridge the gap between teachers and students. Besides, several works also focus on interpreting the classical KD method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>State-of-the-art methods are mainly based on intermediate features, which can directly transfer representations from the teacher to the student <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref> or transfer the correlation between samples captured in the teacher to the student <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Most of the feature-based methods could achieve preferable performances (significant higher than logits-based methods), yet involving considerably high computational and storage costs.</p><p>This paper focuses on analyzing what limits the potential of logits-based methods and revitalizing logit distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rethinking Knowledge Distillation</head><p>In this section, we delve into the mechanism of knowledge distillation. We reformulate KD loss into a weighted sum of two parts, one is relevant to the target class, and the other is not. We explore the effect of each part in the knowledge distillation framework and reveal some limitations of the classical KD. Inspired by the findings, we further propose a novel logit distillation method, achieving remarkable performance on various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reformulating KD</head><p>Notations. For a training sample from the t-th class, the classification probabilities can be denoted as p = [p 1 , p 2 , ..., p t , ..., p C ] ? R 1?C , where p i is the probability of the i-th class and C is the number of classes. Each element in p can be obtained by the softmax function:</p><formula xml:id="formula_0">p i = exp(z i ) C j=1 exp(z j ) ,<label>(1)</label></formula><p>where z i represents the logit of the i-th class.</p><p>To separate the predictions relevant and irrelevant to the target class, we define the following notations. b = [p t , p \t ] ? R 1?2 represents the binary probabilities of the target class (p t ) and all the other non-target classes (p \t ), which can be calculated by:</p><formula xml:id="formula_1">p t = exp(z t ) C j=1 exp(z j ) , p \t = C k=1,k? =t exp(z k ) C j=1 exp(z j )</formula><p>.</p><p>Meanwhile, we declarep = [p 1 , ...,p t?1 ,p t+1 , ...,p C ] ? R 1?(C?1) to independently model probabilities among nontarget classes (i.e., without considering the t-th class). Each element is calculated by:</p><formula xml:id="formula_2">p i = exp(z i ) C j=1,j? =t exp(z j )</formula><p>.</p><p>(2)</p><p>Reformulation. In this part 1 , we attempt to reformulate KD with the binary probabilities b and the probabilities among non-target classesp. T and S denote the teacher and the student, respectively. The classical KD uses KL-Divergence as the loss function, which can be written as 2 :</p><p>1 More mathematical formulations are in the supplement. <ref type="bibr" target="#b1">2</ref> We omit the temperature (T) in <ref type="bibr" target="#b11">[12]</ref> without loss of generality</p><formula xml:id="formula_3">KD = KL(p T ||p S ) = p T t log( p T t p S t ) + C i=1,i? =t p T i log( p T i p S i ).<label>(3)</label></formula><p>According to Eqn.(1) and Eqn.(2) we havep i = p i /p \t , so we can rewrite Eqn.(3) as:</p><formula xml:id="formula_4">KD = p T t log( p T t p S t ) + p T \t C i=1,i? =tp T i (log(p T ? p S i ) + log( p T \t p S \t )) = p T t log( p T t p S t ) + p T \t log( p T \t p S \t ) KL(b T ||b S ) +p T \t C i=1,i? =tp T i log(p T ? p S i ) KL(p T ||p S ) .<label>(4)</label></formula><p>Then, Eqn.(4) can be rewritten as:</p><formula xml:id="formula_5">KD = KL(b T ||b S ) + (1 ? p T t )KL(p T ||p S )<label>(5)</label></formula><p>As reflected by Eqn. <ref type="formula" target="#formula_5">(5)</ref>, the KD loss is reformulated into a weighted sum of two terms. KL(b T ||b S ) represents the similarity between the teacher's and student's binary probabilities of the target class. Thus, we name it Target Class Knowledge Distillation(TCKD). Meanwhile, KL(p T ||p S ) represents the similarity between the teacher's and student's probabilities among non-target classes, named Non-Target Class Knowledge Distillation(NCKD). Eqn.(5) could be rewritten as:</p><formula xml:id="formula_6">KD = TCKD + (1 ? p T t )NCKD.<label>(6)</label></formula><p>Obviously, the weight of NCKD is coupled with p T t . The reformulation above inspires us to investigate the individual effects of TCKD and NCKD, which will reveal the limitations of the classical coupled formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Effects of TCKD and NCKD</head><p>Performance gain of each part. We individually study the effects of TCKD and NCKD on CIFAR-100 <ref type="bibr" target="#b15">[16]</ref>. ResNet <ref type="bibr" target="#b8">[9]</ref>, WideResNet (WRN) <ref type="bibr" target="#b41">[42]</ref> and ShuffleNet <ref type="bibr" target="#b20">[21]</ref> are selected as training models, among which both the same and different architectures are considered. The experimental results are reported in <ref type="table" target="#tab_0">Table 1</ref>. For each teacher-student pair, we report the results of (1) the student baseline (vanilla training), (2) the classical KD (where TCKD and NCKD are both used), (3) singly TCKD and (4) singly NCKD. The weight of each loss is set as 1.0 (including the default crossentropy loss). Other implementation details are the same as those in Sec 4.</p><p>Intuitively, TCKD concentrates on the knowledge related to the target class since the corresponding loss function considers only binary probabilities. Conversely, NCKD focuses on the knowledge among non-target classes. We notice that student TCKD singly applying TCKD could be unhelpful (e.g., 0.02% and 0.12% gain on ShuffleNet-V1) or even harmful (e.g., 2.30% drop on WRN-16-2 and 3.87% drop on ResNet8?4) for the student. However, the distillation performances of NCKD are comparable and even better than the classical KD (e.g., 1.76% vs. 1.13% on ResNet8?4). The ablation results suggest that the target-class-related knowledge could not be as important as knowledge among non-target classes. To dive into this phenomenon, we provide further analyses presented as follows.</p><p>TCKD transfers the knowledge concerning the "difficulty" of training samples. According to Eqn.(5), TCKD transfers "dark knowledge" via the binary classification task, which could be related to the sample "difficulty". For instance, a training sample with p T t = 0.99 could be "easier" for the student to learn compared with another one with p T t = 0.75. Since TCKD conveys the "difficulty" of training samples, we suppose the effectiveness would be revealed when the training data becomes challenging. However, the CIFAR-100 training set is easy to fit 3 . Thus the knowledge of "difficulty" provided by the teacher is not informative. In this part, experiments from three perspectives are performed to validate: The more difficult the training data is, the more benefits TCKD could provide 4 .</p><p>(1) Applying Strong Augmentation is a straightforward way to increase the difficulty of training data. We train a ResNet32?4 model as the teacher with AutoAugment <ref type="bibr" target="#b4">[5]</ref> on CIFAR-100, achieving 81.29% top-1 validation accu-racy. As for students, we train ResNet8?4 and ShuffleNet-V1 models with/without TCKD. Results in <ref type="table" target="#tab_1">Table 2</ref>   <ref type="table">Table 3</ref>. Accuracy(%) on the CIFAR-100 validation with different noisy ratios on the training set. We set ResNet32?4 as the teacher and ResNet8?4 as the student.</p><p>(2) Noisy Labels can also increase the difficulty of training data. We train ResNet32?4 models as teachers and ResNet8?4 as students on CIFAR-100 with {0.1, 0.2, 0.3} symmetric noisy ratios, following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref>. As reported in <ref type="table">Table 3</ref>, the results indicate that TCKD achieves more performance promotions on noisier training data.</p><p>(3) Challenging Datasets (e.g., ImageNet <ref type="bibr" target="#b28">[29]</ref>) are also considered. It shows that TCKD could bring +0.32% performance gain on ImageNet in <ref type="table">Table 4</ref>.</p><formula xml:id="formula_7">TCKD top-1 ? 70.71 - ?</formula><p>71.03 +0.32 <ref type="table">Table 4</ref>. Accuracy(%) on the ImageNet validation. We set ResNet-34 as the teacher and ResNet-18 as the student.</p><p>Conclusively, we demonstrate the effectiveness of TCKD by experimenting with various strategies to increase the difficulty of training data (e.g. strong augmentation, noisy labels, difficult tasks). The results validate that the knowledge concerning the "difficulty" of training samples could be more beneficial when distilling knowledge on more challenging training data.</p><p>NCKD is the prominent reason why logit distillation works but is greatly suppressed. Interestingly, we notice in <ref type="table" target="#tab_0">Table 1</ref> when only NCKD is applied, the performances are comparable or even better than the classical KD. It shows that the knowledge among non-target classes is of vital importance to logit distillation, which can be the prominent "dark knowledge". However, by reviewing Eqn.(5), we notice that the NCKD loss is coupled with</p><formula xml:id="formula_8">(1 ? p T t ),</formula><p>where p T t represents the teacher's confidence on the target class. Therefore, more confident predictions result in smaller NCKD weights. We suppose that the more confident the teacher is in the training sample, the more reliable and valuable knowledge it could provide. However, the loss weights are highly suppressed by such confident predictions. We suppose that this fact would limit the effectiveness of knowledge transfer, which is firstly investigated thanks to our reformulation of KD in Eqn. <ref type="bibr" target="#b4">(5)</ref>.</p><p>We design an ablation experiment to verify that wellpredicted samples do transfer better knowledge than the others. Firstly we rank the training samples according to p T t , and evenly split them into two sub-sets. For clarity, one sub-set includes samples with top-50% p T t while remaining samples are in the other sub-set. Then we train student networks with NCKD on each subset to compare the performance gain (while the cross-entropy loss is still on the whole set). <ref type="table">Table 5</ref> shows that utilizing NCKD on the top-50% samples achieves better performance, suggesting that the knowledge of well-predicted samples is richer than others. However, the loss weight of well-predicted samples are suppressed by the high confidence of the teacher.  <ref type="table">Table 5</ref>. Accuracy(%) on the CIFAR-100 validation set. We set ResNet32?4 as the teacher and ResNet8?4 as the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoupled Knowledge Distillation</head><p>So far, we have reformulated the classical KD loss into a weighted sum of two independent parts, and further validated the effectiveness of TCKD and revealed the suppression of NCKD. Specifically, TCKD transfers knowledge concerning the "difficulty" of training samples. More significant improvements could be obtained by TCKD on more challenging training data. NCKD transfers knowledge among non-target classes, which would be suppressed in the condition that the weight (1 ? p T t ) is relatively small. Instinctively, both TCKD and NCKD are indispensable and crucial. However, in the classical KD formulation, TCKD and NCKD are coupled from the following aspects:</p><p>? For one thing, NCKD is coupled with (1 ? p T t ), which could suppress NCKD on the well-predicted samples. Since results in <ref type="table">Table 5</ref> show that well-predicted samples could bring more performance gain, the coupled form could limit the effectiveness of NCKD. ? For another, weights of NCKD and TCKD are coupled under the classical KD framework. It's not allowed to change each term's weight to balance the importance. We suppose that TCKD and NCKD should be sepa- Benefiting from our reformulation of KD, we propose a novel logit distillation method named Decoupled Knowledge Distillation(DKD) to address the above issues. Our proposed DKD independently considers TCKD and NCKD in a decoupled formulation, as shown in <ref type="figure" target="#fig_1">Figure 1b</ref>. Specifically, we introduce two hyper-parameters ? and ?, as the weights of TCKD and NCKD, respectively. The loss function of DKD can be written as follows:</p><formula xml:id="formula_9">DKD = ?TCKD + ?NCKD.<label>(7)</label></formula><p>In DKD, (1 ? p T t ), which would suppress NCKD's effectiveness, is replaced by ?. What's more, it's allowed to adjust ? and ? to balance the importance of TCKD and NCKD. Through decoupling NCKD and TCKD, DKD provides an efficient and flexible manner for logit distillation. Algorithm 1 provides the pseudo-code of DKD in a PyTorch-like <ref type="bibr" target="#b23">[24]</ref> style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We mainly experiment on two representative tasks, i.e., image classification and object detection, including:</p><p>CIFAR-100 <ref type="bibr" target="#b15">[16]</ref> is a well-known image classification dataset, containing 32?32 images of 100 categories. Training and validate sets are composed of 50k and 10k images.</p><p>ImageNet <ref type="bibr" target="#b28">[29]</ref> is a large-scale classification dataset that  <ref type="table">Table 6</ref>. Results on the CIFAR-100 validation. Teachers and students are in the same architectures. And ? represents the performance improvement over the classical KD. All results are the average over 5 trials.</p><p>consists of 1000 classes. The training set contains 1.28 million images and the validation set contains 50k images.</p><p>MS-COCO <ref type="bibr" target="#b19">[20]</ref> is an 80-category general object detection dataset. The train2017 split contains 118k images, and the val2017 split contains 5k images.</p><p>All implementation details are attached in supplement due to the page limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>Firstly, we demonstrate the improvements contributed by decoupling (1) NCKD and p T t and (2) NCKD and TCKD, respectively. Then, we benchmark our method on image classification and object detection tasks.</p><p>Ablation: ? and ?. The two tables below report the student accuracy (%) with different ? and ?. ResNet32?4 and ResNet8?4 are set as the teacher and the student, respectively. Firstly, we prove that decoupling (1 ? p T t ) and NCKD can bring reasonable performance gain (73.63% vs. 74.79%) in the first table. Then, we demonstrate that decoupling weights of NCKD and TCKD could contribute to further improvements (74.79% vs. 76.32%). Moreover, the second table indicates that TCKD is indispensable, and the improvements from TCKD are stable with different ? around 1.0 5 . CIFAR-100 image classification. We discuss experimental results on CIFAR-100 to examine our DKD. The validation accuracy is reported in <ref type="table">Table 6 and Table 7</ref>. <ref type="table">Table 6</ref> contains the results where teachers and students are of the same network architectures. <ref type="table">Table 7</ref> shows the results where teachers and students are from different series. <ref type="bibr" target="#b4">5</ref> We fix ? as 1.0 for simplification in the first table, and ? as 8.0 in the second table since it achieves the best performance in the first one.</p><p>Notably, DKD achieves consistent improvements on all teacher-student pairs, compared with the baseline and the classical KD. Our method achieves 1 ? 2% and 2 ? 3% improvements on teacher-student pairs of the same and different series, respectively. It strongly supports the effectiveness of DKD. Furthermore, DKD achieves comparable or even better performances than feature-based distillation methods, significantly improving the trade-off between distillation performance and training efficiency, which will be further discussed in Sec 4.2.</p><p>ImageNet image classification. Top-1 and top-5 accuracies of image classification on ImageNet are reported in <ref type="table">Table 8 and Table 9</ref>. Our DKD achieves a significant improvement. It's worth mentioning that the performance of DKD is better than the most state-of-the-art results of feature distillation methods.</p><p>MS-COCO object detection. As discussed in previous works, the performance of the object detection task greatly depends on the quality of deep features to locate interested objects. This rule also stands in transferring knowledge between detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>, i.e., feature mimicking is of vital importance since logits are not capable of providing knowledge for object localization. As shown in <ref type="table" target="#tab_0">Table 10</ref>, singly applying DKD can hardly achieve outstanding performances, but expectedly surpasses the classical KD. Thus, we introduce the feature-based distillation method ReviewKD <ref type="bibr" target="#b0">[1]</ref> to obtain satisfactory results. It can be observed that our DKD can further boost AP metrics, even the distillation performance of ReviewKD is relatively high. Conclusively, new state-of-the-art results are obtained by combining our DKD with feature-based distillation methods on the object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Extensions</head><p>For a better understanding of DKD, we provide extensions from four perspectives. First of all, we comprehensively compare the training efficiency of DKD with representative state-of-the-art methods. Then, we provide a new  <ref type="table">Table 9</ref>. Top-1 and top-5 accuracy (%) on the ImageNet validation. We set ResNet-50 as the teacher and MobileNet-V1 as the student. KD* represents the result of our implementation. All results are the average over 3 trials. perspective to explain why bigger models are not always better teachers and alleviate this problem by utilizing DKD. Moreover, following <ref type="bibr" target="#b32">[33]</ref>, we examine the transferability of deep features learned by DKD. And we also present some visualizations to validate the superiority of DKD.</p><p>Training efficiency. We assess the training costs of stateof-the-art distillation methods, proving the high training efficiency of DKD. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, our DKD achieves the best trade-off between model performances and training costs (e.g., training time and extra parameters). Since DKD is reformulated from the classical KD, it needs almost the same computational complexity as KD, and of course no extra parameters. However, feature-based distillation methods require extra training time for distillation intermediate layer features, as well as the GPU memory costs.</p><p>Improving performances of big teachers. We provide a new potential explanation on the bigger models are not always better teachers issue. Specifically, bigger teachers are expected but cannot transfer more beneficial knowledge, even achieving worse performances than smaller ones.</p><p>Previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> explained this phenomenon with the large capacity gap between big teachers and small students. However, we suppose that the main reason is the suppression of NCKD, i.e., the (1 ? p T t ) would become smaller with the teacher getting bigger. What's more, related works on this problem also could be explained from this perspec-tive, e.g., ESKD <ref type="bibr" target="#b2">[3]</ref> employs early-stopped teacher models to alleviate this problem, and these teachers would be underconvergence and yield smaller p T t . To validate our conjecture, we perform our DKD on a series of teacher models. Experimental results in <ref type="table" target="#tab_0">Table 11</ref> and <ref type="table" target="#tab_0">Table 12</ref> consistently indicate that our DKD alleviates the bigger models are not always better teachers problem.   <ref type="table" target="#tab_0">Table 12</ref>. Results on CIFAR-100. We set WRN-16-2 as the student and networks from different series as teachers.</p><p>ing <ref type="bibr" target="#b32">[33]</ref>, we use the WRN-16-2 distilled from WRN-40-2 as the feature extractor. Then linear probing tasks are performed on downstream datasets, i.e. STL-10 <ref type="bibr" target="#b3">[4]</ref> and Tiny-ImageNet <ref type="bibr" target="#b5">6</ref> . Results are reported in <ref type="table" target="#tab_0">Table 13</ref>, proving the outstanding transferability of features learned with our DKD. Implementation details are in the supplement.  <ref type="table" target="#tab_0">Table 13</ref>. Comparison with previous methods on transferring features from CIFAR-100 to STL-10 and Tiny-ImageNet (TI).</p><p>Visualizations. We present visualizations from two perspectives (with setting teacher as ResNet32x4 and student as ResNet8x4 on CIFAR-100). (1) The t-SNE ( <ref type="figure" target="#fig_4">Fig. 3)</ref> results show that representations of DKD are more separable than KD, proving that DKD benefits the discriminability of deep features. (2) We also visualize the difference of correlation matrices of student and teacher logits <ref type="figure" target="#fig_5">(Fig. 4)</ref>. Compared with KD, DKD helps the student to output more similar logits with the teacher, i.e., achieving better distillation performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>This paper provides a novel viewpoint to interpret logit distillation by reformulating the classical KD loss into two 6 https://www.kaggle.com/c/tiny-imagenet  parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). The effects of both parts are respectively investigated and proved. More importantly, we reveal that the coupled formulation of KD limits the effectiveness and flexibility of knowledge transfer. To overcome these issues, we propose Decoupled Knowledge Distillation (DKD), which achieves significant improvements on CIFAR-100, ImageNet and MS-COCO datasets for image classification and object detection tasks. Besides, the superiority of DKD in training efficiency and feature transferability is also demonstrated. We hope this paper will contribute to future logit distillation research.</p><p>limitations and future works. Noticeable limitations are discussed as follows. DKD could not outperform stateof-the-art feature-based methods (e.g., ReviewKD [1]) on object detection tasks because logits-based methods cannot transfer knowledge about localization. Besides, we have provided an intuitive guidance on how to adjust ? in our supplement. However, the strict correlation between the distillation performance and ? is not fully investigated, which will be our future research direction.</p><p>We report experimental results on CIFAR-100 <ref type="bibr" target="#b15">[16]</ref> to verify this conjecture. We select ResNet56, WRN-40-2 and ResNet32?4 as teachers and ShuffleNet-V1 as the student, and apply different ?. Both top-1 accuracy (%) and the gap z t ? z max (averaged over all training samples) are reported. As shown in <ref type="table">Table A</ref>.1, the best value of ? could be positively proportional to the gap, which we suppose could be guidance of tuning ? and a direction for further research. Based on this, the value of ? for each teacher in <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref> of the manuscript is set as follows (in <ref type="table">Table A</ref>  <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref> of the manuscript.</p><p>A.4. Implementation: Experiments in Sec 3.2</p><p>In this part, we report the implementation details of the experiments in Sec 3.2 of the manuscript.</p><p>Basic settings. We set the loss term of KD and CE as 1.0 and 1.0, respectively (instead of the default 0.1CE + 0.9KD setting in <ref type="bibr" target="#b32">[33]</ref>). The setting in <ref type="bibr" target="#b32">[33]</ref> follows the loss form proposed by <ref type="bibr" target="#b11">[12]</ref>, which assumes that the sum of all terms' weights should be 1.0. However, the NCKD loss is targetirrelevant, which means the target-relevant loss could be 0.1 if we utilize the original setting when only applying NCKD. Based on this, we set the loss weight of all terms (e.g., KD, TCKD, NCKD and CE) as 1.0 for all experiments in Sec 3.2 of the manuscript.</p><p>Strong augmentation. We employ the AutoAugment <ref type="bibr" target="#b4">[5]</ref> to reveal the effectiveness of TCKD in Sec 3.2 of the manuscript. Specifically, we add the CIFAR AutoAugment policy 7 after applying the default augmentation (random crop and horizontal flip). Then we train the teacher and the student with the same augmentation policy. <ref type="bibr" target="#b6">7</ref> https://github.com/DeepVoltaire/AutoAugment Noisy labels. We also perform experiments on noisy training data to verify that TCKD conveys the knowledge about sample "difficulty". Specifically, we follow the settings of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref>, utilizing the symmetric noise type <ref type="bibr" target="#b7">8</ref> . We train a teacher network on the noisy training data and select the best epoch to distill the student (on the same training data).</p><p>A.5. Explanation about why TCKD brings performance drop in <ref type="table" target="#tab_0">Table 1</ref> In <ref type="table" target="#tab_0">Table 1</ref> of the manuscript, we reveal that singly applying TCKD could bring performance drop sometimes. An explanation for this phenomenon is that the high temperature (T=4) will lead to a great gradient to increase the nontarget classes' logits, which will harm the correctness of the student's prediction. Without NCKD, the information about the class similarity (or the prominent dark knowledge) is not available, so that TCKD's gradient could do no good but lead to performance drop (since TCKD could bring marginal performance gain on easy-fitting training data). To verify that the large temperature is not proper when singly applying TCKD, we perform experiments with different temperatures (T) in the table below. Results in <ref type="table" target="#tab_0">Ta-T  1  2  3</ref> 4 top-1 73.24 73.05 71.69 70.96 <ref type="table">Table A</ref>.3. Accuracy (%) with different temperature(T) when only applying TCKD. The teacher and the student are set as WRN-40-2 and WRN-16-2, respectively. ble A.3 show that the performance is almost the same as the vanilla training baseline (73.26 in <ref type="table" target="#tab_0">Table 1</ref> of the manuscript) when the temperature is set as 1. And the performance drop is positively related to the temperature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. How to employ DKD on detectors</head><p>In this paper, we employ our DKD on the two-stage object detector Faster R-CNN. We only employ our DKD on the R-CNN head. Specifically, given a student network, we utilize the labels assigned to the proposals (generated by the RPN module) as the target class(if IoU(proposal) &lt; 0.5, the target class is set as "background"). Then, we use a teacher network to get the R-CNN prediction logits of the same proposals (locations are the same, while the features are from the teacher's backbone). Thus, we can employ our DKD by minimizing the KL-Divergence (i.e., TCKD and NCKD) between the student's logits and the teacher's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Implementation: Experiments in Sec 4.2</head><p>Training efficiency. We report the training time of each distillation method in <ref type="figure" target="#fig_3">Figure 2</ref> of the manuscript. The</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Decoupled Knowledge Distillation (DKD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Pseudo code of DKD in a PyTorch-like style. # l_stu: student output logits # l_tea: teacher output logits # T: temperature for KD &amp; DKD # t: labels, (N, C), bool type # alpha, beta: hyper-parameters for DKD p_stu = F.softmax(l_stu / T) p_tea = F.softmax(l_tea / T) # pt &amp; pnt: (N, 1), Eqn.(2) pt_stu, pnt_stu = p_stu[t], p_stu[1-t].sum(1) pt_tea, pnt_tea = p_tea[t], p_tea[1-t].sum(1) # pnct: (N, C-1), Eqn.(3) pnct_stu = F.softmax(l_stu[1-t]/T) pnct_tea = F.softmax(l_tea[1-t]/T) # TCKD tckd = kl_div(log(pt_stu), pt_tea) + kl_div(log(pnt_stu), pnt_tea) # NCKD nckd = F.kl_div(log(pnct_stu), pnct_tea) # ori KD # kd_loss = (tckd + pnt_tea * nckd) * T ** 2 # DKD dkd_loss = (alpha * tckd + beta * nckd) * T ** 2 rately considered since their contributions are from different aspects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.30 75.64 76.12 76.32 76.11 75.42</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Training time (per batch)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>t-SNE of features learned by KD (left) and DKD (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Difference of correlation matrices of student and teacher logits. Obviously, DKD (right) leads to a smaller difference (more similar prediction) than KD (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy(%) on the CIFAR-100 validation set. ? represents the performance improvement over the baseline.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NCKD top-1</cell><cell>?</cell></row><row><cell cols="4">ResNet32?4 as the teacher</cell></row><row><cell></cell><cell></cell><cell></cell><cell>72.50</cell><cell>-</cell></row><row><cell>ResNet8?4</cell><cell>? ?</cell><cell>?</cell><cell cols="2">73.63 +1.13 68.63 -3.87</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell cols="2">74.26 +1.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.50</cell><cell>-</cell></row><row><cell>ShuffleNet-V1</cell><cell>? ?</cell><cell>?</cell><cell cols="2">74.29 +3.79 70.52 +0.02</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell cols="2">74.91 +4.41</cell></row><row><cell cols="4">WRN-40-2 as the teacher</cell></row><row><cell></cell><cell></cell><cell></cell><cell>73.26</cell><cell>-</cell></row><row><cell>WRN-16-2</cell><cell>? ?</cell><cell>?</cell><cell cols="2">74.96 +1.70 70.96 -2.30</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell cols="2">74.76 +1.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.50</cell><cell>-</cell></row><row><cell>ShuffleNet-V1</cell><cell>? ?</cell><cell>?</cell><cell cols="2">74.92 +4.42 70.62 +0.12</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell cols="2">75.12 +4.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>reveal that TCKD obtains significant performance gains if strong augmentations are applied. Accuracy(%) on the CIFAR-100 validation. We set ResNet32?4 as the teacher and ResNet8?4 as the student. Both teachers and students are trained with AutoAugment<ref type="bibr" target="#b4">[5]</ref>.</figDesc><table><row><cell>student</cell><cell cols="2">TCKD top-1</cell><cell>?</cell></row><row><cell>ResNet8?4</cell><cell>?</cell><cell cols="2">73.82 75.33 +1.51 -</cell></row><row><cell>ShuffleNet-V1</cell><cell>?</cell><cell cols="2">77.13 77.98 +0.85 -</cell></row><row><cell cols="3">noisy ratio TCKD top-1</cell><cell>?</cell></row><row><cell>0.1</cell><cell>?</cell><cell cols="2">70.99 70.96 -0.03 -</cell></row><row><cell>0.2</cell><cell>?</cell><cell cols="2">67.55 68.03 +0.48 -</cell></row><row><cell>0.3</cell><cell>?</cell><cell cols="2">64.62 65.26 +0.64 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Results on the CIFAR-100 validation. Teachers and students are in different architectures. And ? represents the performance improvement over the classical KD. All results are the average over 5 trials. Top-1 and top-5 accuracy (%) on the ImageNet validation. We set ResNet-34 as the teacher and ResNet-18 as the student. KD* represents the result of our implementation. All results are the average over 3 trials.</figDesc><table><row><cell>distillation</cell><cell cols="2">teacher</cell><cell>ResNet32?4 79.42</cell><cell cols="2">WRN-40-2 75.61</cell><cell>VGG13 74.64</cell><cell>ResNet50 79.34</cell><cell cols="2">ResNet32?4 79.42</cell></row><row><cell>manner</cell><cell cols="2">student</cell><cell cols="7">ShuffleNet-V1 ShuffleNet-V1 MobileNet-V2 MobileNet-V2 ShuffleNet-V2 70.50 70.50 64.60 64.60 71.82</cell></row><row><cell></cell><cell cols="2">FitNet [28]</cell><cell>73.59</cell><cell></cell><cell>73.73</cell><cell>64.14</cell><cell>63.16</cell><cell></cell><cell>73.54</cell></row><row><cell></cell><cell cols="2">RKD [23]</cell><cell>72.28</cell><cell></cell><cell>72.21</cell><cell>64.52</cell><cell>64.43</cell><cell></cell><cell>73.21</cell></row><row><cell>features</cell><cell cols="2">CRD [33]</cell><cell>75.11</cell><cell></cell><cell>76.05</cell><cell>69.73</cell><cell>69.11</cell><cell></cell><cell>75.65</cell></row><row><cell></cell><cell cols="2">OFD [10]</cell><cell>75.98</cell><cell></cell><cell>75.85</cell><cell>69.48</cell><cell>69.04</cell><cell></cell><cell>76.82</cell></row><row><cell></cell><cell cols="2">ReviewKD [1]</cell><cell>77.45</cell><cell></cell><cell>77.14</cell><cell>70.37</cell><cell>69.89</cell><cell></cell><cell>77.78</cell></row><row><cell></cell><cell cols="2">KD [12]</cell><cell>74.07</cell><cell></cell><cell>74.83</cell><cell>67.37</cell><cell>67.35</cell><cell></cell><cell>74.45</cell></row><row><cell>logits</cell><cell cols="2">DKD</cell><cell>76.45</cell><cell></cell><cell>76.70</cell><cell>69.71</cell><cell>70.35</cell><cell></cell><cell>77.07</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>+2.38</cell><cell></cell><cell>+1.87</cell><cell>+2.34</cell><cell>+3.00</cell><cell></cell><cell>+2.62</cell></row><row><cell cols="3">distillation manner</cell><cell></cell><cell></cell><cell>features</cell><cell></cell><cell></cell><cell>logits</cell></row><row><cell></cell><cell cols="8">teacher student AT [43] OFD [10] CRD [33] ReviewKD [1] KD [12] KD*</cell><cell>DKD</cell></row><row><cell>top-1</cell><cell>73.31</cell><cell>69.75</cell><cell>70.69</cell><cell>70.81</cell><cell>71.17</cell><cell>71.61</cell><cell>70.66</cell><cell cols="2">71.03 71.70</cell></row><row><cell>top-5</cell><cell>91.42</cell><cell>89.07</cell><cell>90.01</cell><cell>89.98</cell><cell>90.13</cell><cell>90.51</cell><cell>89.88</cell><cell cols="2">90.05 90.41</cell></row><row><cell cols="3">distillation manner</cell><cell></cell><cell></cell><cell>features</cell><cell></cell><cell></cell><cell>logits</cell></row><row><cell></cell><cell cols="8">teacher student AT [43] OFD [10] CRD [33] ReviewKD [1] KD [12] KD*</cell><cell>DKD</cell></row><row><cell>top-1</cell><cell>76.16</cell><cell>68.87</cell><cell>69.56</cell><cell>71.25</cell><cell>71.37</cell><cell>72.56</cell><cell>68.58</cell><cell cols="2">70.50 72.05</cell></row><row><cell>top-5</cell><cell>92.86</cell><cell>88.76</cell><cell>89.33</cell><cell>90.34</cell><cell>90.41</cell><cell>91.00</cell><cell>88.98</cell><cell cols="2">89.80 91.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>vs. accuracy on CIFAR-100. We set ResNet32?4 as the teacher and ResNet8?4 as the student. The table shows the number of extra parameters for each method. 62.48 45.88 42.04 62.48 45.88 40.22 61.02 43.81 student 33.26 53.61 35.26 37.93 58.84 41.05 29.47 48.87 30.90 KD [12] 33.97 54.66 36.62 38.35 59.41 41.71 30.13 50.28 31.35 FitNet [28] 34.13 54.16 36.71 38.76 59.62 41.80 30.20 49.80 31.69 FGFI [38] 35.44 55.51 38.17 39.44 60.27 43.04 31.16 50.68 32.92 ReviewKD [1] 36.75 56.72 34.00 40.36 60.97 44.08 33.71 53.15 36.13 DKD 35.05 56.60 37.54 39.25 60.90 42.73 32.34 53.77 34.01 DKD+ReviewKD 37.01 57.53 39.85 40.65 61.51 44.44 34.35 54.89 36.61 Table 10. Results on MS-COCO based on Faster-RCNN [27]-FPN [19]: AP evaluated on val2017.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">R-101 &amp; R-18</cell><cell></cell><cell>R-101 &amp; R-50</cell><cell>R-50 &amp; MV2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AP</cell><cell>AP50 AP75</cell><cell>AP</cell><cell>AP50 AP75</cell><cell>AP</cell><cell>AP50 AP75</cell></row><row><cell></cell><cell cols="2">teacher</cell><cell cols="4">42.04 Teacher-student pairs are ResNet-</cell></row><row><cell cols="7">101 (R-101) &amp; ResNet-18 (R-18), ResNet-101 &amp; ResNet-50 (R-50) and ResNet-50 &amp; MobileNet-V2 (MV2) respectively. All results are</cell></row><row><cell cols="5">the average over 3 trials. More details are attached in supplement.</cell><cell></cell></row><row><cell>teacher</cell><cell cols="4">W-28-2 W-40-2 W-16-4 W-28-4 75.45 75.61 77.51 78.60</cell><cell></cell></row><row><cell>KD</cell><cell>75.37</cell><cell>74.92</cell><cell>75.79</cell><cell>75.04</cell><cell></cell></row><row><cell>DKD</cell><cell>75.92</cell><cell>76.24</cell><cell>76.00</cell><cell>76.45</cell><cell></cell></row><row><cell cols="5">Table 11. Results on CIFAR-100. We set WRN-16-2 as the student</cell><cell></cell></row><row><cell cols="3">and WRN series networks as teachers.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>teacher</cell><cell cols="4">VGG13 WRN-16-4 ResNet50 74.64 77.51 79.34</cell><cell></cell></row><row><cell>KD</cell><cell>74.93</cell><cell>75.79</cell><cell></cell><cell>75.36</cell><cell></cell></row><row><cell>DKD</cell><cell>75.45</cell><cell>76.00</cell><cell></cell><cell>76.60</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feature transferability. We perform experiments to eval-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>uate the transferability of deep features to verify that our</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DKD transfers more generalizable knowledge. Follow-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table A.2. The value of ? for different teachers in</figDesc><table><row><cell></cell><cell></cell><cell>.2):</cell></row><row><cell>teacher</cell><cell>zt ? zmax</cell><cell>?</cell></row><row><cell>ResNet56</cell><cell>5.36</cell><cell>2.0</cell></row><row><cell>ResNet110</cell><cell>6.73</cell><cell>2.0</cell></row><row><cell>WRN-40-2</cell><cell>7.24</cell><cell>6.0</cell></row><row><cell>VGG13</cell><cell>8.25</cell><cell>6.0</cell></row><row><cell>ResNet50</cell><cell>8.53</cell><cell>8.0</cell></row><row><cell>ResNet32?4</cell><cell>8.40</cell><cell>8.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Training accuracies on CIFAR-100 could be 100% after convergence.<ref type="bibr" target="#b3">4</ref> All experiments from these perspectives are performed with NCKD, since we suppose that TCKD should not be singly employed according to the results inTable 1. The probable reasons and analyzes are attached in the supplement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/bhanML/Co-teaching/blob/master/data/cifar.py</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix <ref type="bibr">A.1</ref>. Details about the reformulation in Sec 3.1</p><p>Details of the mathematical derivation in Sec 3.1 of the manuscript are as follows (notations are the same in Sec 3.1 of the manuscript):</p><p>According to Eqn.(1) and Eqn.(2) of the manuscript, we havep i = p i /p \t . Thus, we can rewrite Eqn. <ref type="bibr" target="#b7">(8)</ref> to:</p><p>Since p T \t and p S \t are irrelevant to the class index i, we have:</p><p>Then,</p><p>According to the definition of KL-Divergence, Eqn. <ref type="bibr" target="#b10">(11)</ref> can be rewritten as (which is the same as Eqn.(5) of the manuscript):</p><p>A.2. Implementation: Experiments in Sec 4 CIFAR-100: Our implementation for CIFAR-100 follows the practice in <ref type="bibr" target="#b32">[33]</ref>. Teachers and students are trained for 240 epochs with SGD. As the batch size is 64, the learning rates are 0.01 for ShuffleNet <ref type="bibr" target="#b20">[21]</ref> and MobileNet-V2 <ref type="bibr" target="#b29">[30]</ref>, 0.05 for the other series (e.g. VGG <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b8">[9]</ref> and WRN <ref type="bibr" target="#b41">[42]</ref>). The learning rate is divided by 10 at 150, 180 and 210 epochs. The weight decay and the momentum are set to 5e-4 and 0.9. The weight for the cross-entropy loss is set to 1.0. The temperature is set as 4 and ? is set as 1.0 for all experiments. The proper value of ? could be different for different teachers, and the details and discussions are in the next section. And we utilize a 20-epoch linear warmup for all experiments since the value of ? could be high leading to a large initial loss.</p><p>ImageNet: Our implementation for ImageNet follows the standard practice. We train the models for 100 epochs. As the batch size is 512, the learning rate is initialized to 0.2 and divided by 10 for every 30 epochs. Weight decay is 1e-4 and the weight for the cross-entropy loss is set to 1.0. We set temperature as 1 and ? as 0.5 for all experiments. Strictly following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>, for distilling networks of the same architecture, the teacher is ResNet-34 model, the student is ResNet-18, and ? is set to 0.5. For different series, the teacher is ResNet-50 model, the student is MobileNet-V1, and ? is set to 2.0.</p><p>MS-COCO: Our implementation for MS-COCO follows the settings in <ref type="bibr" target="#b0">[1]</ref>. We use the two-stage method Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> with FPN <ref type="bibr" target="#b18">[19]</ref> as the feature extractors. ResNet <ref type="bibr" target="#b8">[9]</ref> models and MobileNet-V2 <ref type="bibr" target="#b29">[30]</ref> are selected as teachers and students. All students are trained with the 1x scheduler (schedulers and task-specific loss weights follow De-tectron2 <ref type="bibr" target="#b38">[39]</ref>). We employ the DKD loss on the R-CNN head, and set ? as 1.0, ? as 0.25, and temperature as 1 for all experiments.</p><p>Results of compared methods are reported in their original papers or reproduced by previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Guidance for tuning ?</head><p>We suppose that the importance of NCKD in knowledge transfer could be related to the confidence of the teacher. Intuitively, the more confident the teacher is, the more valuable the NCKD could be, and the larger ? should be applied. However, NCKD could increase the gradient contributed by logits of non-target classes. Thus, an improper large ? could harm the correctness of the student's prediction. If the logit value of the target class is much higher than all non-target classes, the teacher could be regarded as more confident and a large beta could be more reasonable. Thus, we suppose that the value of ? could be related to the logit value gap between the target and all non-target classes. Specifically, the gap between the logit of the target class (i.e., z t , where z represents the output logit and t represents the target class) and the max logit among non-target classes could be reliable guidance for tuning ?, which can be denoted as z t ? z max , where z max = max({z i |i ? = t})).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distilling knowledge via knowledge review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explaining knowledge distillation by quantifying the knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06872</idno>
		<title level="m">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online knowledge distillation for efficient pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigeng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shufflenet V2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards understanding knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobilenetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>T-PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07634</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. T-PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 11</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Snapshot distillation: Teacher-student optimization in one generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The gap (zt ? zmax) is also reported. training time (per batch) is the sum of (1) the data processing time (e.g., including the time to sample the contrast examples in [33]), (2) the network forward time and the gradient backward time and (3) the memory updating time (e.g.,including the time to update the contrast memory in [33]). We also report the number of extra parameters for each method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Table</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accuracy(%) on CIFAR-100 [16] with different ? and different teachers</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Besides the learnable parameters (e.g., the connectors in [10] and the ABF modules in [1]), we also calculate the extra dictionary memory(e.g., the contrast memory in [33])</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">We use the WRN-16-2 distilled from a WRN-40-2 teacher as the feature extractor (only using the feature generated by the final global average pooling module), then train linear fully-connected (FC) layers as classifier modules for STL-10 and Tiny-ImageNet datasets (the feature extractor is fixed during training). We train the FC via an SGD optimizer with 0.9 momentum and 0.0 weight decay. The number of total epochs is set as 40</title>
		<imprint/>
	</monogr>
	<note>Feature transferability. We perform linear probing experiments to verify the feature transferability of our DKD in Sec 4.2 of the manuscript. and the learning rate is set to 0.1 for a 128 batch size and divided by 10 for every 10 epochs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
