<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphMDN: Leveraging graph structure and deep learning to solve inverse problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Oikarinen</surname></persName>
							<email>tuomas@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Hannah</surname></persName>
							<email>dhannah@vectra.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">Vectra AI</orgName>
								<address>
									<postCode>95128</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohrob</forename><surname>Kazerounian</surname></persName>
							<email>sohrob@vectra.ai</email>
							<affiliation key="aff2">
								<orgName type="institution">Vectra AI</orgName>
								<address>
									<postCode>95128</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphMDN: Leveraging graph structure and deep learning to solve inverse problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent introduction of Graph Neural Networks (GNNs) and their growing popularity in the past few years has enabled the application of deep learning algorithms to non-Euclidean, graph-structured data. GNNs have achieved stateof-the-art results across an impressive array of graph-based machine learning problems. Nevertheless, despite their rapid pace of development, much of the work on GNNs has focused on graph classification and embedding techniques, largely ignoring regression tasks over graph data. In this paper, we develop a Graph Mixture Density Network (GraphMDN), which combines graph neural networks with mixture density network (MDN) outputs. By combining these techniques, GraphMDNs have the advantage of naturally being able to incorporate graph structured information into a neural architecture, as well as the ability to model multi-modal regression targets. As such, GraphMDNs are designed to excel on regression tasks wherein the data are graph structured, and target statistics are better represented by mixtures of densities rather than singular values (so-called "inverse problems"). To demonstrate this, we extend an existing GNN architecture known as Semantic GCN (SemGCN) to a GraphMDN structure, and show results from the Human3.6M pose estimation task. The extended model consistently outperforms both GCN and MDN architectures on their own, with a comparable number of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the improvements brought about by deep learning in problem domains spanning speech and language to computer vision and control, neural network architectures were largely incapable of operating on non-Euclidean, graph-structured data. This shortcoming is particularly relevant in light of the importance and ever-increasing availability of data that is intrinsically graph-structured; examples include social networks, protein structures, molecules, knowledge graphs, and computer networks. It is only with the recent introduction of Graph Neural Networks (GNNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> that deep learning techniques have been extended to directly operate over graph-structured data. By drawing inspiration from the mathematical tools that allowed traditional deep learning algorithms to excel at learning from data such as images and speech, GNNs have begun to make use of operators like graph convolutions, enabling them to operate over non-Euclidean graph structured inputs in a manner similar to standard convolutions over images. Indeed, as pointed out by Bronstein et al. <ref type="bibr" target="#b2">[3]</ref>, traditional Convolutional Neural Networks can be thought of as operating over graph structured data, where images are simply graphs sampled over a two-dimensional grid.</p><p>By combining principles from deep learning with the ability to operate over a much broader range of data, GNNs have been able to achieve impressive results on a wide range of tasks historically considered out-of-reach for traditional deep learning models (e.g., relation prediction in knowledge graphs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>, drug design and interaction prediction <ref type="bibr" target="#b33">[34]</ref>, visual and language-based recommender systems <ref type="bibr" target="#b29">[30]</ref>, binary-code analysis <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref>, etc.) Much of the focus of GNN research has been on architectures for node classification, link prediction, and clustering/embedding graph structures. Regression tasks have received less attention. In the context of graphs, the goal of a regression task is to predict some continuous-valued output at the graph, node, or edge level (or some combination thereof). While GNNs are an obvious choice for any regression task where the inputs are graph structured, they face the same difficulties as standard deep-learning architectures when predicting continuous valued outputs. If the target statistics of an output given some input are not unimodal (i.e. the mapping to be learned by the model is one-to-many, a so-called "inverse" problem), a standard output layer with a mean-squared error loss function will tend to learn the conditional average of the outputs observed for a given input <ref type="bibr" target="#b1">[2]</ref>. As a simple example, consider a neural network tasked with predicting the amount of traffic uploaded to a server from a machine on a network.</p><p>If the amount of traffic to be predicted given some input X is bimodal (e.g,. half the time uploading around 10 GB, and the other half uploading 100 GB of data), a standard neural network learning the regression target will tend towards simply predicting the mean of these values. While the conditional average may be useful in some cases, a model predicting that the machine uploads 55 GB does not fully characterize the upload behavior of this example machine.</p><p>Motivated by the use of GNNs in cybersecurity, where it is often critical that models are able to predict multi-valued quantities at the edge (e.g. bytes transferred between two hosts) and node (e.g. number of login attempts on a host in any given day) level, we introduce Graph Mixture Density Networks (GraphMDNs). GraphMDNs combine the ability of GNNs to operate over graph-structured data with the representational power of mixture density networks in modeling the statistical distributions of target data. In the present work, we demonstrate the general utility of this architecture by applying it to a common dataset for 3D human pose estimation from monocular images or 2D joint positions, and show that GraphMDNs exceed the capabilities of either a GNN or an MDN on their own with minimal changes to the architecture. Human pose estimation serves as an ideal platform with which to benchmark GraphMDN due to availability of large high quality datasets, and the fact that graph convolutional networks (GCN) and mixture density networks have separately been applied to this problem in prior works. The next section provides an overview of graph neural networks, mixture density networks and previous work in human pose estimation. Section 3 describes the architecture of GraphMDNs. Experiments on the Human3.6M dataset are discussed in section 4, with results and comparison to other models presented in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph neural networks</head><p>Graph Neural Networks (GNNs) (introduced in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b23">[24]</ref>) have extended the capabilities of neural network and deep learning models to the graph domain, and have been used for node, edge, and graph level prediction and embedding (see <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref> for recent, comprehensive reviews of the state of GNNs). Prior to GNNs, applying standard neural network architectures to graph-structured data required an unwieldy degree of hand-engineered features in order to make the data suitable for training, obviating many of the benefits that motivate the use of deep learning in the first place. By extending to the graph domain operations such as convolution, which has proved indispensable in deep learning for computer vision, GNNs enable deep learning models to represent, learn and make predictions from topological structures that they would have otherwise been incapable of processing.</p><p>Roughly speaking, the extension of neural networks to graphs can be thought of as falling into either spectral or non-spectral methods. In the former, filters (e.g., spectral graph convolutions) are defined to operate in the Fourier space of the graph <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref>, whereas in the latter, the convolution is defined to operate on the graph directly, operating over a spatial neighborhood and with a set of shared parameters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. While each approach has advantages and dis-advantages (e.g., spectral methods often require computation of the graph Laplacian, which can be computationally expensive) -the framework we introduce here combining mixture density networks and graph neural networks, can operate on either. The particular GraphMDN we demonstrate here builds on the Semantic Graph Convolutional Network (SemGCN; <ref type="bibr" target="#b30">[31]</ref>) model for pose estimation and falls into the non-spectral approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixture density networks</head><p>Mixture density networks (MDNs) were introduced by Bishop <ref type="bibr" target="#b1">[2]</ref> in order to overcome limitations in the representational capacity of traditional neural networks. In particular, Bishop demonstrated that in either regression or classification, neural networks learn to approximate the conditional average of the target data given an input. While this will often suffice in classification tasks, and can even be considered optimal (since the goal of the network is to explicitly model the posterior probability for class membership of an input), it can lead to unsatisfactory results in the case of regression tasks. This is due to the fact that the conditional average of a particular regression target can range from insufficient to misleading (as in the computer network example given in Section 1).</p><p>To overcome this problem, Bishop modified the traditional neural network architecture so that, instead of single scalar values, the network would output the parameters of a mixture of distributions. This can represent the more complex targets observed for inverse problems and other datasets where a single input results in multiple distinct outcomes. If the mixture is comprised solely of Gaussian distributions (this need not be the case in general), the conversion of a standard neural network to a MDN can be achieved by replacing the neuron(s) in the output layer with a set of 3N neurons, where N is the number of Gaussian distributions in the output mixture. Of the 3N neurons, 2N model the mean and variance, respectively, of each of the N distributions, along with N neurons that output the mixing coefficients of those distributions. Because MDNs leverage the function-approximation capabilities of neural networks to learn parameterizations of mixture models, they go beyond standard mixture models insofar as their input domains are not restricted to continuous-valued inputs, or even static data. MDNs can trivially incorporate categorical features and timeseries data as input features, while still parameterizing a mixture of distributions on the output side. Futhermore, standard mixture models define a single mixture of distributions over the whole input domain. On the other hand, MDNs learn a family of mixtures over the input domain, because for any given input, the network outputs a new parameterization and hence, a new mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Human pose estimation</head><p>2D to 3D Human Pose estimation has been well studied in the recent years. The goal of the field is to estimate a human 3D pose (location of a set of joints) based on an input image. While many approaches do this end-to-end, Martinez et al. <ref type="bibr" target="#b18">[19]</ref> proposed splitting this task into detecting 2D joint locations from an image, and then estimating 3D pose based on the 2D joint detections. This approach has proved effective and the focus of our work is on the second step of this pipeline, predicting the 3D joint locations based on 2D joint detections of some model. Martinez et al. <ref type="bibr" target="#b18">[19]</ref> use a fully connected network for this second step, which performs well, but does not take advantage of the inherent graph structure in the human skeleton. Recent approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b6">7]</ref> have successfully applied graph convolution networks (GCN) to improve over fully connected approaches. As the SemGCN <ref type="bibr" target="#b31">[32]</ref> architecture performs well on this pose estimation task and is supported by a high-quality code release, we build on it in this paper. SemGCN is described in more detail in the next section.</p><p>Recently, algorithms have been presented that outperform SemGCN and related methods by effectively utilizing additional information available in human pose estimation datasets. For example Iskakov et al. <ref type="bibr" target="#b14">[15]</ref> propose a geometry-inspired method that can reach very high accuracy by combining information from multiple camera views of the same action. In addition, several works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref> have proposed using a time-series of input frames to reduce the inherent ambiguity of the problem by analyzing how the pose evolves over time. Assuming the information is available, both of these methods could be integrated with GraphMDN for additional improvement (a point we discuss in 5), but our focus in this report is on methods that analyze a single camera and a single frame, a common scenario for real-world use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">SemGCN</head><p>Graph convolutional networks naturally operate on graph structured data. Suppose G = {V, E} is a graph given by the set of K nodes V and edges E. Standard graph convolutional networks share a learned kernel matrix across all edges in the graph; as a result, the ostensibly-variable relationships between nodes in the graph are not exploited as well as they could be. Semantic Graph Convolutional Networks address this issue by adding to this convolution operation a learnable weighting matrix that can account for differences in the relationships represented by the edges.</p><p>Like the original SemGCN work, we also make use of a non-local layer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref> to capture global semantic relations between nodes. See the original SemGCN paper <ref type="bibr" target="#b31">[32]</ref> for details of the non-local layer as well as the GCN structure used.</p><p>A schematic of our network architecture is displayed in <ref type="figure" target="#fig_0">Figure 1</ref>. Initially, a semantic graph convolution layer interleaved with a non-local layer is used to convert a set of 2D joint data (and accompanying adjacency matrix) to a latent representation. The latent embeddings are fed to a block structure which is repeated four times and consists of two semantic convolutional layers, each of which is followed by a batch normalization layer and a ReLU activation layer. Fundamentally, our neural network architecture combines Semantic Graph Convolutional Networks (SemGCN) <ref type="bibr" target="#b31">[32]</ref> with Mixture Density Networks <ref type="bibr" target="#b1">[2]</ref> and a method of going from node/joint-level outputs to graph/pose-level distributions.</p><p>Motivated by the fact that multiple 3D poses may map onto the same 2D pose, our output architecture expands upon the SemGCN by employing a mixture density network (MDN). Rather than mapping a pose (represented as a graph input) x ? R 2K to a single output y ? R 3K , we instead learn a probability density function in P : R 2K ? R 3K ? R. This is learned in the form of a mixture density network, where the SemGCN backbone with learnable parameters w learns to output parameters ? of a Gaussian mixture as a function of 2D inputs x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node level probability distributions</head><p>Since the outputs of SemGCN are node level features, the natural approach would be to learn node level distributions {p 1 (y 1 |x), ..., p K (y K |x)} where p i is the probability of a 3D joint position given a 2D input and</p><formula xml:id="formula_0">P(y|x) = K i=1 p i (y i |x).<label>(1)</label></formula><p>Each p i could then be constructed via node-level Gaussian mixtures parameterized by ? i (x, w) = ? i , ? i , ? i for node i. ? i , ? i , ? i are the means, variances, and mixing coefficients of the mixture model defined as follows:</p><formula xml:id="formula_1">? i = ? i 1 , ..., ? i M |? i j ? R 3 , ? i = ? i 1 , ..., ? i M |? i j ? R , ? i = ? ? ? ? i 1 , ..., ? i M |0 ? ? i j ? 1, j ? i j = 1 ? ? ?<label>(2)</label></formula><p>Where superscript i denotes the ith node and subscript j represents the jth kernel, M denotes the number of Gaussian kernels and w are the learnable parameters of the model. In SemGCN <ref type="bibr" target="#b31">[32]</ref> the output of each node is a 3-dimensional prediction of the joint location. In our work this is replaced by 5M -dimensional output of the mixture parameters ? i . Each of the mixture components has a different type of activation function: a tanh layer for the means (? i ) since targets in this task are in [?1, 1], a softmax for the mixture coefficient (? i ) to ensure that it sums to 1, and an ELU +1 function for the variances (? i ) to ensure they remain positive. Using these parameters we can calculate the node level probability density for the ith joint being in location y i :</p><formula xml:id="formula_2">p i (y i |x) = M j=1 ? i j f (y i | ? i j , ? i j )<label>(3)</label></formula><p>Here, f (y|? i j , ? i j ) is the probability density function of a multivariate Gaussian distribution having mean ? i j and a diagonal covariance matrix with diagonal elements ? i j , shown explicitly in Equation 4. The diagonal covariance matrix forces the x, y, and z dimensions to be independent, but with a shared variance. In our experiments, allowing the model to freely learn a covariance matrix reduced performance. We hypothesize that the reduced performance stems from a loss of regularization provided by the restricted covariance matrix.</p><formula xml:id="formula_3">f (y i | ? i j , ? i j ) = 1 (2?) 3/2 (? i j ) 3 exp ? y i ? ? i j 2 2(? i j ) 2 .<label>(4)</label></formula><p>Training of the neural network is achieved by maximizing the log posterior probability of the observed samples, a standard practice for MDNs. The loss function is shown below</p><formula xml:id="formula_4">L = ? ln P(y|x, w) = ? K i=1 ln M j=1 ? i j f (y i | ? i j , ? i j )<label>(5)</label></formula><p>This is then minimized with respect to trainable parameters w. We note that it is essential to use the log-sum-exp trick when evaluating the inner sum to avoid numerical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pose level distributions</head><p>While the node level probability distributions are a natural representation for the task, they are restricted to learning independent distributions for each node. This is problematic when predicting realistic full body poses, because some combinations of joint positions are much more likely than others. This can be overcome by using graph/pose-level mixture coefficients ?. This transforms the problem from modeling K 3-dimensional distributions to modeling one 3K-dimensional distribution. While we still model the different dimensions as having 0-covariance, now each of the M kernels represents a plausible 3d pose, and their means can be used for creating M possible predictions.</p><p>Pose-level outputs can be achieved in several ways. The most straightforward calculation entails simply averaging the logits of each node corresponding to mixture coefficients ? i before applying softmax, which we found to be effective and is the approach taken when generating the results presented here. We also explored utilizing an additional dense layer to produce pose-level outputs, but found that it made training less stable and prone to overfitting. Additionally we found it beneficial to restrict the pose level distributions to have uniform variance by similarly averaging the logits of each node corresponding to ?. The outputs for ? are created by concatenating the outputs of different nodes.</p><p>The conceptual shift to pose level distributions also changes the way probabilities are calculated. The new model outputs are:</p><formula xml:id="formula_5">? = ? 1 , ..., ? M |? j ? R 3K , ? = {? 1 , ..., ? M |? j ? R} , ? = ? ? ? ? 1 , ..., ? M |0 ? ? j ? 1, j ? j = 1 ? ? ?<label>(6)</label></formula><p>And we calculate the probabilities as follows:</p><formula xml:id="formula_6">P(y|x) = K j=1 ? j (y|x)g(y | ? j , ? j ).<label>(7)</label></formula><p>Where:</p><formula xml:id="formula_7">g(y | ? j , ? j ) = 1 (2?) 3K/2 (? j ) K exp ? y ? ? j 2 2(? j ) 2 .<label>(8)</label></formula><p>This is again optimized by minimizing:</p><formula xml:id="formula_8">L = ? ln P(y|x, w) = ? ln K j=1 ? j (y|x)g(y | ? j , ? j )<label>(9)</label></formula><p>Since we found pose level distributions to improve performance according to metrics used in previous works, for simplicity we only report results using pose level distributions in the rest of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe the details of our data set, then we describe the implementation details of our evaluation and training scheme. Finally we discuss how we select a hypothesis from the set of possible poses created by the MDN when evaluating our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate our performance on the Human3.6M dataset <ref type="bibr" target="#b13">[14]</ref>, which contains 3.6 million images attained by a motion capture system with four synchronized cameras in an indoor environment. The images themselves consist of seven professional actors performing daily-life activities such as walking, eating, sitting, smoking, and engaging in a discussion. Both 2D and 3D ground-truth joint locations are available, with the ultimate goal of learning to predict the 3D positions for each joint, given either the raw image, or the 2D ground truth locations of the joints in that image.</p><p>When predicting the 3D joint positions from a single 2D input (i.e., using inputs that are only a single camera view and a single moment in time), the task of determining the 3D joint locations is underdetermined. For example, movement in the depth-axis of the camera and occlusion of joints in a particular camera view create scenarios where many 3D joint positions and poses can be projected onto the same 2D input view. As such, state-of-the-art techniques now often sidestep the difficulties resulting from the "inverse problem" nature of the task by making use of the synchronized multi-view images, or the full temporal sequence of images for each action/subject.</p><p>In order to demonstrate how a GNN and MDN combination extends representational capacity beyond either the GNN or MDN model alone, we show results in the case of single-view/single-frame prediction given a single 2D input. We first compare results to the SemGCN model utilizing groundtruth 2D inputs for training and testing (hereafter denoted GT), and second, compare to a multi-modal MDN model using fine-tuned stacked hourglass 2D detections given by <ref type="bibr" target="#b18">[19]</ref> as an input (hereafter denoted SH). We then qualitatively show the utility in a model that can entertain and represent multiple hypotheses simultaneously, and discuss further benefits that can be derived from such a representation by incorporating multi-view or temporal information in order to more optimally select between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and evaluation protocols</head><p>Following previous work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, we train on subjects 1, 5, 6, 7, and 8 and test on subjects 9 and 11. We use all the images from all four camera views in training and testing, albeit not in a multi-view setup; the different camera views are treated as independent examples.</p><p>To evaluate our network, we consider two commonly used protocols for evaluating the performance of 3D pose estimation, henceforth referred to as Protocol #1 and Protocol #2. Protocol #1 is the mean per-joint position error (MPJPE) in millimeters between the predicted joint positions and the ground-truth joint positions. Protocol #2 computes the same error, but after applying a subsequent rigid transformation that further aligns the predictions with the ground truth, and is alternately referred to as P-MPJPE in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hypothesis selection</head><p>The MDN outputs are distributions as opposed to most approaches on Human3.6M, which only predict a single scalar. To compare with previous approaches we need to reduce the distributions to a single prediction. In this paper we use the following methods for creating a prediction:</p><p>Highest: Simply choose the mean of the kernel n with the highest mixture coefficient.? = ? n | n = argmax j ? j . This essentially represents the guess the network thinks is the most likely to be correct.</p><p>Mean: Predict the weighted average of the distribution calculated as? = K j=1 ? j ? j . This is useful because of the way error is evaluated. If we know the true distribution of target variables, in order to minimize average MPJPE-error it is beneficial to predict the mean of the distribution, even if the mean itself is not a possible pose.</p><p>Oracle: Since each kernel represents a distinct possible pose, having an Oracle to predict which one of these possible poses to choose can improve our results significantly. The Oracle selects out of the K kernel means the 3D pose that is closest to the target 3D pose. Previous works treating pose estimation as an inverse problem take this approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, and it can be considered as an upper-bound estimate for how well the GraphMDN is performing on this task. While this is not a realistic method for real-world applications (we would never need a prediction mechanism in the first place if we knew the ground truth target), we nevertheless make use of it in order to compare against alternative multi-modal methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training details</head><p>In this work we aimed to keep our training hyperparameters and setup as close to SemGCN as possible. However, in order to speed up the training process itself, we increased batch-size from 64 to 256 and replaced the decaying learning rate with the Super-Convergence LR schedule described in <ref type="bibr" target="#b25">[26]</ref> with a peak learning rate of 6 ? 10 ?3 . This allowed us to train all networks in just 2 epochs instead of 30+ used by <ref type="bibr" target="#b31">[32]</ref>. Together these changes allow us to reduce training times from 10 hours  to less than 15 minutes using the same hardware. We also reproduce the SemGCN results using Super-Convergence and our setup to isolate the effects of the new training setup from the effects of the MDN output. All our models used 5 MDN kernels unless otherwise stated. We train a single model for all actions. Our model is implemented in PyTorch, and we employ ADAM <ref type="bibr" target="#b16">[17]</ref> for optimization and initialize our network weights using Xavier initialization <ref type="bibr" target="#b9">[10]</ref>. All models used dropout with a rate of 0.1 unless otherwise mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to SemGCN</head><p>In <ref type="table">Table 1</ref> we compare our method against SemGCN <ref type="bibr" target="#b31">[32]</ref> on ground truth (GT) 2D inputs, and in <ref type="table" target="#tab_1">Table 2</ref> we do the same comparisons using Stacked Hourglass (SH) inputs. All methods were trained in 2 epochs using Super-Convergence. Our standard models used a set of hyperparameters matching the SemGCN architecture, with 4 block layers having 128-dimensional hidden layers. Additionally we experimented with a wider network using 3 blocks with a hidden dimension of 512; these results are reported are indicated as (Wide). Regardless of the input type (SH or GT), we can see that our GraphMDN noticeably improves over SemGCN, such that simply replacing SemGCN's output layer with a GraphMDN output reduces average P1-error by up to 5%. Furthermore, GraphMDN learns a richer representation of the data capable of furnishing predictions about the set of poses that reduce to a particular 2D input. It worth noting that our Oracle results are substantially better than other published methods using single frame GT inputs, indicating that refinement of hypothesis-selection procedures could yield significant improvement in real-world applications and represents a promising direction for future research.  <ref type="table">Table 3</ref>: Comparison of state of the art multimodal methods evaluated under the Oracle protocol. All models use the stacked hourglass 2D inputs. * was estimated from figure. The number in brackets is the number of pose predictions considered; i.e. the number of Gaussian kernels (our paper and <ref type="bibr" target="#b17">[18]</ref>) or samples generated <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison against multimodal methods</head><p>To evaluate the quality of our learned distributions, we compare our models against previous multimodal approaches of human pose estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> using an Oracle to choose the best prediction. The results are shown in <ref type="table">Table 3</ref>. We can see our method produces state of the art results with 5 guesses after only 2 epochs of training. Additionally our MDN scales much better to larger numbers of kernels than <ref type="bibr" target="#b17">[18]</ref>, with a significant performance gain when going from 5 to 8 kernels, while <ref type="bibr" target="#b17">[18]</ref> does not gain performance beyond 5 kernels. In addition, our method scales well even up to 200 kernels, and is able to outperform <ref type="bibr" target="#b24">[25]</ref> on a best of 200 predictions under Protocol 1, although the method in <ref type="bibr" target="#b24">[25]</ref> still outperforms GraphMDN under Protocol 2. When GraphMDN is trained with 5 kernels, it effectively generates 5 poses as predictions of the underlying 3D pose. Compared to the accuracy of 5 samples (poses) from the latent space generated in <ref type="bibr" target="#b24">[25]</ref>, GraphMDN clearly outperforms the variational autoencoder approach in <ref type="bibr" target="#b24">[25]</ref>. When training the model with 200 kernels, we found Super-Convergence to reduce Oracle performance; we hypothesize that reduced performance is due to the rapid training procedure reducing kernel diversity. Because of this, our 200 kernel model was trained with 30 epochs using an exponentially decaying learning rate starting at 10 ?3 and a dropout rate of 0.5. While some unimodal approaches have attained better results on this task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, none of the outperforming methods are comparable as they often utilize either multi-view or time-series data, or 2D joint detectors better than Stacked Hourglass. An obvious source of improvement would be to utilize a more modern 2D joint detector such as CPN <ref type="bibr" target="#b20">[21]</ref>, which would clearly improve our results, but this would also improve the results of other multimodal methods and as such is not meaningful when comparing against them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis of GraphMDNs</head><p>Although the Oracle predictions are, on their own, not a plausible mechanism for making predictions in the real-world, they nevertheless indicate that representing multiple potential alternatives can in principle result in great improvements in performance. Indeed, the Oracle result from the GraphMDN model far exceeds the performance of the SemGCN model on its own, and outperforms state-of-theart Oracle predictions from alternative multi-modal modelling techniques. An obvious question, then, is what exactly have the various kernels in our predictions learned to represent, and subsequently, is there a "fair" mechanism that would allow selection between the various hypotheses (without requiring foreknowledge of the answer)?</p><p>In order to understand how the mixture model functions in our predictions, <ref type="figure">Figure 2</ref> shows visualizations of the predicted kernels (super-imposed on one another, with shading proportional to their weighting coefficients) selected from frames in which the best performing kernel performs significantly better than the weighted mean of the predicted kernels. When we visualize the 2D inputs used to make the 3D joint predictions, it is immediately clear that there is ambiguity in the depth axis of the camera, as to where exactly some of the joints might be located. In fact, when viewing the 3D predictions from the camera angle used to generate the 2D inputs (azimuth=60; the middle column <ref type="figure">Figure 2</ref>: Qualitative results on the Human36M data set using ground-truth 2D inputs. The left column, labeled "Input", displays the input (2D) skeleton. The next three columns compare the output of our GraphMDN (labeled "MDN-Estimated Poses") with the ground truth target value (labeled "Ground truth") at three different camera azimuths (labeled on <ref type="figure">figure)</ref>. In each plot of our MDN-Estimated Poses, each skeleton is shaded according to the mixing coefficient of the kernel that generated it -kernels with low mixing coefficients will be nearly transparent, while kernels with high mixing coefficients will be opaque. From top row to bottom row, the "Greeting", "Purchasing", and "Sitting" actions are displayed.</p><p>for each of the rows of <ref type="figure">Figure 2</ref> the multiple hypotheses are sufficiently indistinguishable that they often simply appear to be a single prediction. Nevertheless, when rotating that view (the columns corresponding to Azimuth values of 0 and 90), it becomes clear that the the multiple-hypotheses all correspond to distinct joint positions along the depth axis of the camera.</p><p>As demonstrated by the feasibility of all hypotheses output by the GraphMDN (given the 2D inputs), the mixture model has learned to meaningfully represent the statistics of the output distribution. Moreover, the fact that the model is capable of providing multiple hypotheses allows for additional information to be applied when deciding which of the possible kernels represents the best predicted 3D joint locations. In this case, additional information can include anything from human-kinematic structural constraints to multi-view information as well as temporal information (for videos). Although unconstrained optimization over this information can be a potentially costly process, selecting the best of N hypotheses can often greatly simplify the solution search space. While the present GraphMDN work does not make use of these constraints in selecting between alternative hypotheses, the visual confirmation that the distribution of 3D-joint hypotheses represent alternatives that cannot be distinguished by a human observer from the 2D image alone, provides further evidence that the combined GNN + MDN approach results in useful representations that would be difficult to achieve with only one of the other techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced GraphMDN, a novel neural network architecture that combines notions from semantic graph convolutional networks and mixture density networks. Our primary aim in this paper is to introduce GraphMDN and demonstrate its general utility on a common and important dataset, where we attain noticeable performance improvements without any significant pose-estimationspecific adaptations. Indeed, pose estimation is but one potential application of the GraphMDN. Any graph-structured inverse problem (such as predicting distributions along edges or nodes) should benefit from our novel network architecture allowing a mixture density network to operate over graph-structured input data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of the GraphMDN architecture. The fundamental GraphMDN building block is a residual block (two semantic graph convolutional layers each followed by batch norm and ReLU activation) followed by a non-local layer. The fundamental building block is repeated four times and fed into a mixture density network, which generates the multiple pose hypotheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Direct. Discuss Eating Greet Phone Photo Posing Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT. Avg. SemGCN (reported)[32] 37.8 49.4 37.6 40.9 45.1 41.4 40.1 48.3 50.1 42.2 53.5 44.3 40.5 47.3 39.0 43.8 SemGCN 34.7 41.9 35.3 38.7 41.0 53.5 41.1 37.3 44.6 53.9 40.3 41.9 41.5 33.3 36.0 41.0 SemGCN (Wide) 34.4 41.6 32.9 37.5 39.2 47.5 41.0 34.1 43.6 52.3 37.6 40.7 38.6 30.5 32.9 39.0 Ours (Mean) 32.2 39.5 33.5 36.7 38.2 48.5 39.0 36.7 44.0 52.9 37.2 40.3 39.4 30.0 32.1 38.7 Ours (Highest, Wide) 34.5 40.5 33.0 35.7 37.0 44.8 39.1 33.0 41.2 50.2 36.6 38.6 38.2 28.4 31.8 37.5 Ours (Mean, Wide) 33.9 39.9 33.0 35.4 36.8 44.4 38.9 33.0 41.0 50.0 36.4 38.3 37.8 28.2 31.5 37.2</figDesc><table><row><cell>Protocol #1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (Oracle, Wide)</cell><cell>28.9 34.5</cell><cell cols="2">28.2 30.2 31.5 38.5 32.3 28.6 35.7 43.3</cell><cell>31.9 32.1 33.3 25.2 27.8 31.8</cell></row><row><cell>Protocol #2</cell><cell cols="4">Direct. Discuss Eating Greet Phone Photo Posing Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT. Avg.</cell></row><row><cell>SemGCN</cell><cell>26.2 32.7</cell><cell cols="2">28.4 31.4 31.3 40.8 31.0 28.6 35.9 43.5</cell><cell>32.0 32.2 33.2 26.8 29.2 32.2</cell></row><row><cell>SemGCN (Wide)</cell><cell>26.4 32.4</cell><cell cols="2">28.0 30.2 30.9 36.9 31.5 26.9 36.1 41.6</cell><cell>30.8 31.6 31.3 24.3 27.1 31.1</cell></row><row><cell>Ours (Mean)</cell><cell>24.6 30.5</cell><cell cols="2">26.3 29.7 28.8 37.0 30.1 26.8 34.0 40.8</cell><cell>29.5 31.5 31.0 24.2 25.9 30.0</cell></row><row><cell cols="2">Ours (Highest, Wide) 25.8 31,4</cell><cell cols="2">27.5 28.7 29.1 35.2 29.8 26.0 34.2 40.5</cell><cell>29.8 30.1 30.4 22.3 26.0 29.8</cell></row><row><cell>Ours (Mean, Wide)</cell><cell>25.4 30.9</cell><cell cols="2">27.5 28.6 29.1 35.0 29.7 25.9 34.1 40.5</cell><cell>29.8 29.9 30.2 22.5 25.9 29.7</cell></row><row><cell>Ours (Oracle, Wide)</cell><cell>22.6 27.8</cell><cell cols="2">23.9 24.9 26.0 31.4 25.6 22.8 30.3 35.9</cell><cell>26.8 26.3 26.9 20.2 22.2 26.3</cell></row><row><cell cols="5">Table 1: (P) MPJPE in millimeter on Human3.6M under protocol #1 and #2 using the ground-truth</cell></row><row><cell cols="5">2D joint positions as inputs. We can see our GCMDN consistently outperforms SemGCN using</cell></row><row><cell cols="5">comparable hypothesis selection mechanisms (Mean, Highest) and clearly beats it using Oracle which</cell></row><row><cell cols="3">highlights the usefulness of our multimodal approach.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg.</cell></row><row><cell cols="3">SemGCN (reported)[32] 48.2 60.8 51.8 64.0 64.6 53.6 51.1 67.4 88.7</cell><cell>57.7</cell><cell>73.2 65.6 48.9 64.8 51.9 60.8</cell></row><row><cell>SemGCN</cell><cell cols="2">53.7 57.9 58.0 59.1 66.0 78.1 55.7 58.1 71.7</cell><cell>96.1</cell><cell>62.8 60.6 65.4 53.3 57.3 63.6</cell></row><row><cell>SemGCN (Wide)</cell><cell cols="2">49.6 55.1 56.5 55.9 62.4 74.3 51.4 54.6 69.8</cell><cell>92.6</cell><cell>59.6 56.9 60.5 48.3 52.1 60.0</cell></row><row><cell>Ours (Mean)</cell><cell cols="2">51.9 56.1 55.3 58.0 63.5 75.1 53.3 56.5 69.4</cell><cell>92.7</cell><cell>60.1 58.0 65.5 49.8 53.6 61.3</cell></row><row><cell>Ours (Mean, Wide)</cell><cell cols="2">49.9 54.9 55.2 56.0 62.1 73.2 51.6 53.2 69.0</cell><cell>88.2</cell><cell>58.9 55.8 61.0 48.6 50.1 59.2</cell></row><row><cell>Protocol #2</cell><cell cols="4">Direct. Discuss Eating Greet Phone Photo Posing Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT. Avg.</cell></row><row><cell>SemGCN</cell><cell cols="2">40.7 44.7 45.6 46.5 50.6 56.4 40.4 41.8 57.3</cell><cell>71.1</cell><cell>49.8 45.6 50.1 40.8 44.4 48.4</cell></row><row><cell>SemGCN (Wide)</cell><cell cols="2">38.9 42.9 44.5 44.8 48.8 54.5 39.0 39.9 56.0</cell><cell>68.4</cell><cell>47.9 43.1 47.0 36.8 41.8 46.3</cell></row><row><cell>Ours (Mean)</cell><cell cols="2">39.7 43.4 44.0 46.2 48.8 54.5 39.4 41.1 55.0</cell><cell>69.0</cell><cell>48.0 43.7 49.6 38.4 42.4 46.9</cell></row><row><cell>Ours (Mean, Wide)</cell><cell cols="2">38.5 42.6 44.1 44.9 48.1 53.3 39.0 39.5 54.9</cell><cell>66.2</cell><cell>47.0 42.2 46.8 36.8 39.8 45.6</cell></row></table><note>Protocol #1 Direct. Discuss Eating Greet Phone Photo Posing Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>(P) MPJPE in millimeter on Human3.6M under protocol #1 and #2 using the fine-tuned stacked hourglass 2D inputs. Our method outperforms SemGCN using SH inputs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Protocol #1 Direct. Discuss Eating Greet Phone Photo Posing Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT. Avg. 47.2 49.4 55.1 62.0 44.8 46.9 59.7 76.5 53.2 48.9 53.6 44.5 44.4 51.9 48.1 55.2 59.3 43.9 45.8 58.6 75.2 52.5 48.2 53.2 42.2 43.6 50.9 Sharma et al.[25](200) 37.8 43.2 43.0 44.3 51.1 57.1 39.7 43.0 56.3 64.0 48.1 45.4 50.4 37.9 39.9 46.8 Ours (Wide, 200) 40.0 43.2 41.0 43.4 50.0 53.6 40.1 41.4 52.6 67.3 48.1 44.2 49.0 39.5 40.2 46.2 Protocol #2 Direct. Discuss Eating Greet Phone Photo Posing Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT. Avg.</figDesc><table><row><cell>Li et al.[18](5)</cell><cell cols="2">43.8 48.6</cell><cell cols="7">49.1 49.8 57.6 61.5 45.9 48.3 62</cell><cell>73.4</cell><cell cols="3">54.8 50.6 56</cell><cell cols="3">43.4 45.5 52.7</cell></row><row><cell cols="2">Sharma et al.[25](5)* -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.4</cell></row><row><cell>Ours (Wide, 5)</cell><cell cols="2">44.2 48.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Li et al.[18](8)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.6</cell></row><row><cell cols="11">Ours (8) 46.3 Li et al.[18](5) 43.4 47.4 35.5 39.8 41.3 42.3 46.0 48.9 36.9 37.3 51.0 60.6</cell><cell cols="6">44.9 40.2 44.1 33.1 36.9 42.6</cell></row><row><cell>Ours (Wide, 5)</cell><cell cols="2">33.8 38.7</cell><cell cols="8">38.3 39.4 43.6 47.8 34.2 35.2 48.5 58.6</cell><cell cols="6">42.7 37.8 41.4 33.5 34.4 40.5</cell></row><row><cell>Ours (8)</cell><cell cols="2">33.8 38.3</cell><cell cols="8">37.8 38.8 43.8 46.3 33.9 34.6 47.9 58.6</cell><cell cols="6">42.4 37.6 41.1 32.0 34.2 40.1</cell></row><row><cell cols="3">Sharma et al.[25](200) 27.6 27.5</cell><cell cols="8">34.9 32.3 33.3 42.7 28.7 28.0 36.1 42.7</cell><cell cols="6">36.0 30.7 37.6 24.3 27.1 32.7</cell></row><row><cell>Ours (Wide, 200)</cell><cell cols="2">30.8 34.7</cell><cell cols="8">33.6 34.2 39.6 42.2 31.0 31.9 42.9 53.5</cell><cell cols="6">38.1 34.1 38.0 29.6 31.1 36.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised features extraction for binary similarity using graph embedding neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Baldoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">Antonio</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Massarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Querzoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09683</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2026" to="2037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learnable triangulation of human pose. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01195</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">Teja</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashast</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural network-based graph embedding for cross-platform binary code similarity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="363" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
