<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Jing</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
							<email>bingkunbao@njupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Synthesizing high-quality realistic images from text descriptions is a challenging task. Existing text-to-image Generative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws. First, the stacked architecture introduces the entanglements between generators of different image scales. Second, existing studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks. Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which enhances the text-image semantic consistency without introducing extra networks, (iii) a novel deep text-image fusion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets. Code is available at https: //github.com/tobran/DF-GAN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The last few years have witnessed the great success of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref> for a variety of applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>. Among them, text-to-image synthesis is one of the most important applications of GANs. It aims to generate realistic and text-consistent images from the given natural language descriptions. Due to its practical value, text-to-image synthesis has become an active research area recently <ref type="bibr">[3, 9, 13, 19-21, 32, 33, 35, 51, 53, 60]</ref>.</p><p>Two major challenges for text-to-image synthesis are the authenticity of the generated image, and the semantic consistency between the given text and the generated image. Due to the instability of the GAN model, most recent models adopt the stacked architecture <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> as the backbone to generate high-resolution images. They employ cross-modal attention to fuse text and image features <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref> and then introduce DAMSM network <ref type="bibr" target="#b49">[50]</ref>, cycle consistency <ref type="bibr" target="#b32">[33]</ref>, or Siamese network <ref type="bibr" target="#b50">[51]</ref> to ensure the textimage semantic consistency by extra networks.</p><p>Although impressive results have been presented by previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref>, there still remain three problems. First, the stacked architecture <ref type="bibr" target="#b55">[56]</ref> introduces entanglements between different generators, and this makes the final refined images look like a simple combination of fuzzy shape and some details. As shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the final refined image has a fuzzy shape synthesized by G 0 , coarse attributes (e.g., eye and beak) synthesized by G 1 , and fine-grained details (e.g., eye reflection) added by G 2 . The final synthesized image looks like a simple combination of visual features from different image scales. Second, existing studies usually fix the extra networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> during the adversarial training, making these networks easily fooled by the generator to synthesize adversarial features <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b51">52]</ref>, thereby weakening their supervision power on semantic consistency. Third, cross-modal attention <ref type="bibr" target="#b49">[50]</ref> can not make full use of text information. They can only be applied two times on 64?64 and 128?128 image features due to its high computational cost. It limits the effectiveness of the text-image fusion process and makes the model hard to extend to higher-resolution image synthesis.</p><p>To address the above issues, we propose a novel textto-image generation method named Deep Fusion Generative Adversarial Network (DF-GAN). For the first issue, we replace the stacked backbone with a one-stage backbone. It is composed of hinge loss <ref type="bibr" target="#b53">[54]</ref> and residual networks <ref type="bibr" target="#b10">[11]</ref> which stabilizes the GAN training process to synthesize high-resolution images directly. Since there is only one generator in the one-stage backbone, it avoids the entanglements between different generators.</p><p>For the second issue, we design a Target-Aware Discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and One-Way Output to enhance the text-image semantic consistency. MA-GP is a regularization strategy on the discriminator. It pursues the gradient of discriminator on target data (real and text-matching image) to be zero. Thereby, the MA-GP constructs a smooth loss surface at real and matching data points which further promotes the generator to synthesize text-matching images. Moreover, considering that the previous Two-Way Output slows down the convergence process of the generator under MA-GP, we replace it with a more effective One-Way Output.</p><p>For the third issue, we propose a Deep text-image Fusion Block (DFBlock) to fuse the text information into image features more effectively. The DFBlock consists of several Affine Transformations <ref type="bibr" target="#b30">[31]</ref>. The Affine Transformation is a lightweight module that manipulates the visual feature maps through channel-wise scaling and shifting operation. Stacking multiple DFBlocks at all image scales deepens the text-image fusion process and makes a full fusion between text and visual features.</p><p>Overall, our contributions can be summarized as follows:</p><p>? We propose a novel one-stage text-to-image backbone that can synthesize high-resolution images directly without entanglements between different generators.</p><p>? We propose a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and One-Way Output. It significantly enhances the text-image semantic consistency without introducing extra networks.</p><p>? We propose a novel Deep text-image Fusion Block (DFBlock), which fully fuses text and visual features more effectively and deeply.</p><p>? Extensive qualitative and quantitative experiments on two challenging datasets demonstrate that the proposed DF-GAN outperforms existing state-of-the-art text-to-image models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref> are an attractive framework that can be used to mimic complex real-world distributions by solving a min-max optimization problem between a generator and discriminator <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54]</ref>. For instance, Reed et al. first applied the conditional GAN to generate plausible images from text descriptions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. StackGAN <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> generates high-resolution images by stacking multiple generators and discriminators and provides the text information to the generator by concatenating text vectors as well as the input noises. Next, AttnGAN <ref type="bibr" target="#b49">[50]</ref> introduces the cross-modal attention mechanism to help the generator synthesize images with more details. MirrorGAN <ref type="bibr" target="#b32">[33]</ref> regenerates text descriptions from generated images for text-image semantic consistency <ref type="bibr" target="#b58">[59]</ref>. SD-GAN <ref type="bibr" target="#b50">[51]</ref> employs the Siamese structure <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> to distill the semantic commons from texts for image generation consistency. DM-GAN <ref type="bibr" target="#b59">[60]</ref> introduces the Memory Network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref> to refine fuzzy image contents when the initial images are not well generated in stacked architecture. Recently, some large transformer-based text-to-image methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> show excellent performance on complex image synthesis. They tokenize the images and take the image tokens and word tokens to make auto-regressive training by a unidirectional Transformer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Our DF-GAN is much different from previous methods. First, it generates high-resolution images directly by a onestage backbone. Second, it adopts a Target-Aware Discriminator to enhance text-image semantic consistency without introducing extra networks. Third, it fuses text and image features more deeply and effectively through a sequence of DFBlocks. Compared with previous models, our DF-GAN is much simpler but more effective in synthesizing realistic and text-matching images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed DF-GAN</head><p>In this paper, we propose a simple model for text-toimage synthesis named Deep Fusion GAN (DF-GAN).To synthesize more realistic and text-matching images, we propose: (i) a novel one-stage text-to-image backbone that can synthesize high-resolution images directly without visual feature entanglements. (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and One-Way Output, which enhances the textimage semantic consistency without introducing extra networks. (iii) a novel Deep text-image Fusion Block (DF-Block), which more fully fuses text and visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Overview</head><p>The proposed DF-GAN is composed of a generator, a discriminator, and a pre-trained text encoder as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The generator has two inputs, a sentence vector encoded by text encoder and a noise vector sampled from the Gaussian distribution to ensure the diversity of the generated images. The noise vector is first fed into a fully connected layer and reshaped.We then apply a series of UP-Blocks to upsample the image features. The UPBlock is composed of an upsample layer, a residual block, and DF-Blocks to fuse the text and image features during the image generation process. Finally, a convolution layer converts image features into images.</p><p>The discriminator converts images into image features through a series of DownBlocks. Then the sentence vector will be replicated and concatenated with image features. An adversarial loss will be predicted to evaluate the visual realism and semantic consistency of inputs. By distinguishing generated images from real samples, the discriminator promotes the generator to synthesize images with higher quality and text-image semantic consistency.</p><p>The text encoder is a bi-directional Long Short-Term Memory (LSTM) <ref type="bibr" target="#b40">[41]</ref> that extracts semantic vectors from the text description. We directly use the pre-trained model provided by AttnGAN <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">One-Stage Text-to-Image Backbone</head><p>Since the instability of the GAN model, previous text-toimage GANs usually employ stacked architecture <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> to generate high-resolution images from low-resolution ones. However, the stacked architecture introduces entanglements between different generators, and it makes the final refined images look like a simple combination of fuzzy shape and some details (see <ref type="figure" target="#fig_0">Figure 1</ref>(a)).</p><p>Inspired by recent studies on unconditional image generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b53">54]</ref>, we propose a one-stage text-to-image backbone that can synthesize high-resolution images directly by a single pair of generator and discriminator. We employ the hinge loss <ref type="bibr" target="#b22">[23]</ref> to stabilize the adversarial training process.</p><p>Since there is only one generator in the one-stage backbone, it avoids the entanglements between different generators. As the single generator in our one-stage framework needs to synthesize high-resolution images from noise vectors directly, it must contain more layers than previous generators in stacked architecture. To train these layers effectively, we introduce residual networks <ref type="bibr" target="#b10">[11]</ref> to stabilize the training of deeper networks. The formulation of our one-stage method with hinge loss <ref type="bibr" target="#b22">[23]</ref> is as follows:</p><formula xml:id="formula_0">L D = ? E x?Pr [min(0, ?1 + D(x, e))] ? (1/2)E G(z)?Pg [min(0, ?1 ? D(G(z), e))] ? (1/2)E x?Pmis [min(0, ?1 ? D(x, e))] L G = ? E G(z)?Pg [D(G(z), e)]<label>(1)</label></formula><p>where z is the noise vector sampled from Gaussian distribution; e is the sentence vector; P g , P r , P mis denote the synthetic data distribution, real data distribution, and mismatching data distribution, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Target-Aware Discriminator</head><p>In this section, we detailed the proposed Target-Aware Discriminator, which is composed of Matching-Aware Gradient Penalty (MA-GP) and One-Way Output. The Target-Aware Discriminator promotes the generator to synthesize more realistic and text-image semantic-consistent images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Matching-Aware Gradient Penalty</head><p>The Matching-Aware zero-centered Gradient Penalty (MA-GP) is our newly designed strategy to enhance text-image semantic consistency. In this subsection, we first show the unconditional gradient penalty <ref type="bibr" target="#b27">[28]</ref> from a novel and clear perspective, then extend it to our MA-GP for the text-toimage generation task.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), in unconditional image generation, the target data (real images) correspond to a low discriminator loss. Correspondingly, the synthetic images correspond to a high discriminator loss. The hinge loss limits the range of discriminator loss between -1 and 1. The gradient penalty on real data will reduce the gradient of the real data point and its vicinity. The surface of the loss function around the real data point is then smoothed which is helpful for the synthetic data point to converge to the real data point.</p><p>Based on the above analysis, we find that the gradient penalty on target data constructs a better loss landscape to help the generator converge. By leveraging the view into the text-to-image generation. As shown in <ref type="figure" target="#fig_2">Figure 3(b)</ref>, in text-to-image generation, the discriminator ob-serves four kinds of inputs: synthetic images with matching text (fake, match), synthetic images with mismatched text (fake, mismatch), real images with matching text (real, match), real images with mismatched text (real, mismatch). For text-visual semantic consistency, we tend to apply gradient penalty on the text-matching real data, the target of text-to-image synthesis. Therefore, in MA-GP, the gradient penalty should be applied on real images with matching text. The whole formulation of our model with MA-GP is as follows:</p><formula xml:id="formula_1">L D = ? E x?Pr [min(0, ?1 + D(x, e))] ? (1/2)E G(z)?Pg [min(0, ?1 ? D(G(z), e))] ? (1/2)E x?Pmis [min(0, ?1 ? D(x, e))] + kE x?Pr [( ? x D(x, e) + ? e D(x, e) ) p ] L G = ? E G(z)?Pg [D(G(z), e)]<label>(2)</label></formula><p>where k and p are two hyper-parameters to balance the effectiveness of gradient penalty.</p><p>By using the MA-GP loss as a regularization on the discriminator, our model can better converge to the textmatching real data, therefore synthesizing more textmatching images. Besides, since the discriminator is jointly trained in our network, it prevents the generator from synthesizing adversarial features of the fixed extra network. Moreover, since MA-GP does not incorporate any extra networks for text-image consistency and the gradients are already computed by back propagation process, the only computation introduced by our proposed MA-GP is the gradient summation, which is more computational friendly than extra networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">One-Way Output</head><p>In the previous text-to-image GANs <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, image features extracted by discriminator are usually used in two ways <ref type="figure" target="#fig_3">(Figure 4(a)</ref>): one determines whether the image is real or fake, the other concatenates the image feature and sentence vector to evaluate text-image semantic consistency. Correspondingly, the unconditional loss and the conditional loss are computed in these models.</p><p>However, it is shown that the Two-Way Output weakens the effectiveness of MA-GP and slows down the convergence of the generator. Concretely, as depicted in <ref type="figure" target="#fig_2">Figure  3</ref>(b), the conditional loss gives a gradient ? pointing to the real and matching inputs after back propagation, while the unconditional loss gives a gradient ? only pointing to the real images. However, the direction of the final gradient which just simply sums up ? and ? does not point to the real and matching data points as we expected. Since the target of the generator is to synthesize real and text-matching images, the final gradient with deviation cannot well achieve text-image semantic consistency and slows down the convergence process of the generator. Therefore, we propose the One-Way Output for text-toimage synthesis. As shown in <ref type="figure" target="#fig_3">Figure 4(b)</ref>, our discriminator concatenates the image feature and sentence vector, then outputs only one adversarial loss through two convolution layers. Through the One-Way Output, we are able to make the single gradient ? pointed to the target data points (real and match) directly, which optimize and accelerate the convergence of the generator.</p><p>By combining the MA-GP and the One-Way Output, our Target-Aware Discriminator can guide the generator to synthesize more real and text-matching images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Efficient Text-Image Fusion</head><p>To fuse text and image features efficiently, we propose a novel Deep text-image Fusion Block (DFBlock). Compared with previous text-image fusion modules, our DF-Block deepens the text-image fusion process to make a full text-image fusion.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the generator of our DF-GAN consists of 7 UPBlocks. A UPBlock contains two Text-Image Fusion blocks. To fully utilize the text information in fusion, we propose the Deep text-image Fusion Block (DF-Block) which stacks multiple Affine Transformations and ReLU layers in Fusion Block. For Affine transformation, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(c), we adopt two MLPs (Multilayer Perceptron) to predict the language-conditioned channel-wise scaling parameters ? and shifting parameters ? from sentence vector e, respectively:</p><formula xml:id="formula_2">? = M LP 1 (e), ? = M LP 2 (e).<label>(3)</label></formula><p>For a given input feature map X?R B?C?H?W , we first conduct the channel-wise scaling operation on X with the scaling parameter ?, then apply the channel-wise shifting operation with the shifting parameter ?. Such a process can be expressed as follows:</p><formula xml:id="formula_3">AF F (x i |e) = ? i ? x i + ? i ,<label>(4)</label></formula><p>where AF F denotes the Affine Transformation; x i is the i th channel of visual feature maps; e is the sentence vector; ? i and ? i are scaling parameter and shifting parameter for the i th channel of visual feature maps. The Affine layer expands the conditional representation space of the generator. However, the Affine transformation is a linear transformation for each channel. It limits the effectiveness of text-image fusion process. Thereby, we add a ReLU layer between two Affine layers which brings the nonlinearity into the fusion process. It enlarges the conditional representation space compared with only one Affine layer. A larger representation space is helpful for the generator to map different images to different representations according to text descriptions.</p><p>Our DFBlock is partly inspired by Conditional Batch Normalization (CBN) <ref type="bibr" target="#b4">[5]</ref> and Adaptive Instance Normalization (AdaIN) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> which contain the Affine transformation. However, both CBN and AdaIN employ normalization layers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44]</ref> which transform the feature maps into the normal distribution. It generates an opposite effect to the Affine Transformation which is expected to increase the distance between different samples. It is then unhelpful for the conditional generation process. To this end, we remove the normalization process. Furthermore, our DFBlock deepens the text-image fusion process. We stack multiple Affine layers and add a ReLU layer between. It promotes the diversity of visual features and enlarges the representation spaces to With the deepening of the fusion process, the DFBlock brings two main benefits for text-to-image generation: First, it makes the generator more fully exploit the text information when fusing text and image features. Second, deepening the fusion process enlargers the representation space of the fusion module, which is beneficial to generate semantic consistent images from different text descriptions.</p><p>Furthermore, compared with previous text-to-image GANs <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>, the proposed DFBlock makes our model no longer consider the limitation from image scales when fusing the text and image features. This is because existing text-to-image GANs generally employ the crossmodal attention mechanism which suffers a rapid growth of computation cost along with the increase of image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the datasets, training details, and evaluation metrics used in our experiments, then evaluate DF-GAN and its variants quantitatively and qualitatively. Datasets. We follow previous work <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref> and evaluate the proposed model on two challenging datasets, i.e., CUB bird <ref type="bibr" target="#b46">[47]</ref> and COCO <ref type="bibr" target="#b24">[25]</ref>. The CUB dataset contains 11,788 images belonging to 200 bird species. Each bird image has ten language descriptions. The COCO dataset contains 80k images for training and 40k images for testing. Each image in this dataset has five language descriptions. Training Details. We optimize our network using Adam <ref type="bibr" target="#b17">[18]</ref> with ? 1 =0.0 and ? 2 =0.9. The learning rate is set to 0.0001 for the generator and 0.0004 for the discriminator according to Two Timescale Update Rule (TTUR) <ref type="bibr" target="#b11">[12]</ref>. Evaluation Details. Following previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b59">60]</ref>, we choose the Inception Score (IS) <ref type="bibr" target="#b39">[40]</ref> and Fr?chet Inception Distance (FID) <ref type="bibr" target="#b11">[12]</ref> to evaluate the performance of our net-work. Specifically, IS computes the Kullback-Leibler (KL) divergence between a conditional distribution and marginal distribution. Higher IS means higher quality of the generated images, and each image clearly belongs to a specific class. FID <ref type="bibr" target="#b11">[12]</ref> computes the Fr?chet distance between the distribution of the synthetic images and real-world images in the feature space of a pre-trained Inception v3 network. Contrary to IS, more realistic images have a lower FID. To compute both IS and FID, each model generates 30,000 images (256?256 resolution) from text descriptions randomly selected from the test dataset.</p><p>As stated in the recent works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b57">58]</ref>, the IS cannot evaluate the image quality well on the COCO dataset, which also exists in our proposed method. Moreover, we find that some GAN-based models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b59">60]</ref> achieve significant higher IS than Transformer-based large text-to-image models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref> on the COCO dataset, but the visual quality of synthesized images is obviously lower than Transformerbased models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref>. Thus, we do not compare IS on the COCO dataset. In contrast, FID is more robust and aligns human qualitative evaluation on the COCO dataset.</p><p>Moreover, we evaluate the number of parameters (NoP) to compare the model size with current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation</head><p>We compare the proposed method with several state-ofthe-art methods, including StackGAN <ref type="bibr" target="#b55">[56]</ref>, StackGAN++ <ref type="bibr" target="#b56">[57]</ref>, AttnGAN <ref type="bibr" target="#b49">[50]</ref>, MirrorGAN <ref type="bibr" target="#b32">[33]</ref>, SD-GAN <ref type="bibr" target="#b50">[51]</ref>, and DM-GAN <ref type="bibr" target="#b59">[60]</ref>, which have achieved the remarkable success of text-to-image synthesis by using stacked structures. We also compared with more recent models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref>. It should be pointed that recent models always use extra knowledge or supervisions. For example, CPGAN <ref type="bibr" target="#b21">[22]</ref> uses the extra pretrained YOLO-V3 <ref type="bibr" target="#b35">[36]</ref>, XMC-GAN <ref type="bibr" target="#b54">[55]</ref> uses the extra pretrained VGG-19 <ref type="bibr" target="#b41">[42]</ref> and Bert <ref type="bibr" target="#b5">[6]</ref>, DAE-GAN <ref type="bibr" target="#b38">[39]</ref> uses extra NLTK POS tagging and manually designs rules for different datasets, and TIME <ref type="bibr" target="#b25">[26]</ref> uses extra 2-D positional encoding. <ref type="figure">Figure 6</ref>. Examples of images synthesized by AttnGAN <ref type="bibr" target="#b49">[50]</ref>, DM-GAN <ref type="bibr" target="#b59">[60]</ref>, and our proposed DF-GAN conditioned on text descriptions from the test set of COCO and CUB datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluation</head><p>We also compare the visualization results synthesized by AttnGAN <ref type="bibr" target="#b49">[50]</ref>, DM-GAN <ref type="bibr" target="#b59">[60]</ref>, and the proposed DF-GAN.</p><p>It can be seen that images synthesized by AttnGAN <ref type="bibr" target="#b49">[50]</ref> and DM-GAN <ref type="bibr" target="#b59">[60]</ref> in <ref type="figure">Figure 6</ref> look like a simple combination of fuzzy shape and some visual details (1 st , 3 rd , 5 th , 7 th , and 8 th columns). As shown in the 5 th , 7 th , and 8 th columns, the birds synthesized by AttnGAN <ref type="bibr" target="#b49">[50]</ref> and DM-GAN <ref type="bibr" target="#b59">[60]</ref> contain wrong shapes. Moreover, the images synthesized by our DF-GAN have better object shapes and realistic fine-grained details (e.g., 1 st , 3 rd , 7 th , and 8 th columns). Besides, the posture of the bird in our DF-GAN result is also more natural (e.g., 7 th and 8 th columns).</p><p>Comparing the text-image semantic consistency with other models, we find that our DF-GAN can also capture more fine-grained details in text descriptions. For example, as the results shown in 1 st , 2 th , 6 th columns in <ref type="figure">Figure 6</ref>, other models cannot synthesize the "holding ski poles", "train track", and "a black stripe by its eyes" described in the text well, but the proposed DF-GAN can synthesize them more correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we conduct ablation studies on the testing set of the CUB dataset to verify the effectiveness of each component in the proposed DF-GAN. The com- which is an extra network widely employed in current models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref>. We first evaluate the effectiveness of OS-B, MA-GP, and OW-O. We conducted a user study to evaluate the text-image semantic consistency (SC), and we asked ten users to score the 100 randomly synthesized images with text descriptions. The scores range from 1 (worst) to 5 (best). The results on the CUB dataset are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Baseline. Our baseline employs stacked framework and Two-Way Output with the same Adversarial loss as Stack-GAN <ref type="bibr" target="#b55">[56]</ref>. In baseline, the sentence vector is naively  <ref type="bibr" target="#b15">[16]</ref> and AFFBlock. The AFFBlock employs one Affine Transformation layer to fuse text and image features. MA-GP GAN is the model that employs One-Stage text-to-image Backbone, Matching-Aware Gradient Penalty, and One-Way Output. From the results in <ref type="table" target="#tab_3">Table 3</ref>, we find that, compared with other fusion methods, concatenation cannot efficiently fuse text and image fea- tures. The comparison among CBN, AdaIN, and AFFBlock proves that Normalization is not essential in Fusion Block, and removing normalization even slightly improves the results. The comparison between DFBlock and AFFBlock demonstrates the effectiveness of deepening the text-image fusion process. In sum, the comparison results prove the effectiveness of our proposed DFBlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitations</head><p>Although DF-GAN shows superiority in text-to-image synthesis, some limitations must be taken into consideration in future studies. First, our model only introduces the sentence-level text information, which limits the ability of fine-grained visual feature synthesis. Second, introducing pre-trained large language models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref> to provide additional knowledge may further improve the performance. We will try to address these limitations in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we propose a novel DF-GAN for the textto-image generation tasks. We present a one-stage text-toimage Backbone that can synthesize high-resolution images directly without entanglements between different generators. We also propose a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and One-Way Output. It can further enhance the textimage semantic consistency without introducing extra networks. Besides, we introduce a novel Deep text-image Fusion Block (DFBlock) which fully fuses text and image features more effectively and deeply. Extensive experiment results demonstrate that our proposed DF-GAN significantly outperforms current state-of-the-art models on the CUB dataset and more challenging COCO dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Existing text-to-image models stack multiple generators to generate high-resolution images. (b) Our proposed DF-GAN generates high-quality images directly and fuses the text and image features deeply by our deep text-image fusion blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed DF-GAN for text-to-image synthesis. DF-GAN generates high-resolution images directly by one pair of generator and discriminator and fuses the text information and visual feature maps through multiple Deep text-image Fusion Blocks (DFBlock) in UPBlocks. Armed with Matching-Aware Gradient Penalty (MA-GP) and One-Way Output, our model can synthesize more realistic and text-matching images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) A comparison of loss landscape before and after applying gradient penalty. The gradient penalty smooths the discriminator loss surface which is helpful for generator convergence. (b) A diagram of MA-GP. The data point (real, match) should be applied MA-GP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between Two-Way Output and our One-Way Output. (a) The Two-Way Output predicts conditional loss and unconditional loss and sums them up as the final adversarial loss. (b) Our One-Way Output predicts the whole adversarial loss directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) A typical UPBlock in the generator network. The UPBlock upsamples the image features and fuses text and image features by two Fusion Blocks. (b) The DFBlock consists of two Affine layers, two ReLU activation layers, and a Convolution layer. (c) The illustration of the Affine Transformation. (d) Comparison between (d.1) the generator with cross-modal attention [50, 60] and (d.2) our generator with DFBlock. represent different visual features according to different text descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results of IS, FID and NoP compared with the stateof-the-art methods on the test set of CUB and COCO. GAN improves the IS metric from 4.36 to 5.10 and decreases the FID metric from 23.98 to 14.81 on the CUB dataset. And our DF-GAN decreases FID from 35.49 to 19.32 on the COCO dataset. Compared with MirrorGAN [33] and SD-GAN [51] which employ cycle consistency and Siamese network to ensure text-image semantic consistency, our DF-GAN improves IS from 4.56 and 4.67 to 5.10. respectively on the CUB dataset. Compared with DM-GAN [60] which introduces Memory Network to refine fuzzy image contents, our model also improves IS from 4.75 to 5.10 and decreases FID from 16.09 to 14.81 on CUB, and also decreases FID from 32.64 to 19.32 on the COCO. Moreover, compared with recent models which introduce extra knowledge, our DF-GAN still achieves a competitive performance. The quantitative comparisons prove that our model is much simpler but more effective.</figDesc><table><row><cell>Model</cell><cell cols="4">CUB IS ? FID ? FID ? NoP ? COCO</cell></row><row><cell>StackGAN [56]</cell><cell>3.70</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">StackGAN++ [57] 3.84</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>AttnGAN [50]</cell><cell cols="2">4.36 23.98</cell><cell>35.49</cell><cell>230M</cell></row><row><cell>MirrorGAN [33]</cell><cell cols="2">4.56 18.34</cell><cell>34.71</cell><cell>-</cell></row><row><cell>SD-GAN [51]</cell><cell>4.67</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DM-GAN [60]</cell><cell cols="2">4.75 16.09</cell><cell>32.64</cell><cell>46M</cell></row><row><cell>CPGAN [22]</cell><cell>-</cell><cell>-</cell><cell>55.80</cell><cell>318M</cell></row><row><cell>XMC-GAN [55]</cell><cell>-</cell><cell>-</cell><cell>9.30</cell><cell>166M</cell></row><row><cell>DAE-GAN [39]</cell><cell cols="2">4.42 15.19</cell><cell>28.12</cell><cell>98M</cell></row><row><cell>TIME [26]</cell><cell cols="2">4.91 14.30</cell><cell>31.14</cell><cell>120M</cell></row><row><cell>DF-GAN (Ours)</cell><cell cols="2">5.10 14.81</cell><cell>19.32</cell><cell>19M</cell></row><row><cell cols="5">As shown in Table 1, compared with other leading mod-</cell></row><row><cell cols="5">els, our DF-GAN has a significant smaller Number of</cell></row><row><cell cols="5">Parameters (NoP) but still achieves a competitive perfor-</cell></row><row><cell cols="5">mance. Compared with AttnGAN [50] which employs</cell></row><row><cell cols="5">cross-modal attention to fuse text and image features, our</cell></row><row><cell>DF-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The performance of different components of our model on the test set of CUB.</figDesc><table><row><cell>Architecture</cell><cell cols="2">IS ? FID ? SC ?</cell></row><row><cell>Baseline</cell><cell>3.96 51.34</cell><cell>-</cell></row><row><cell>OS-B</cell><cell>4.11 43.45</cell><cell>1.46</cell></row><row><cell>OS-B w/ DAMSM</cell><cell>4.28 36.72</cell><cell>1.79</cell></row><row><cell>OS-B w/ MA-GP</cell><cell>4.46 32.52</cell><cell>3.55</cell></row><row><cell cols="2">OS-B w/ MA-GP w/ OW-O 4.57 23.16</cell><cell>4.61</cell></row><row><cell cols="3">ponents include One-Stage text-to-image Backbone (OS-</cell></row><row><cell cols="3">B), Matching-Aware Gradient Penalty (MA-GP), One-Way</cell></row><row><cell cols="3">Output (OW-O), Deep text-image Fusion Block (DFBlock).</cell></row><row><cell cols="3">We also compare our Target-Aware Discriminator with</cell></row><row><cell cols="3">Deep Attentional Multimodal Similarity Model (DAMSM)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>concatenated to the input noise and intermediate feature maps. Effect of One-Stage Backbone. Our proposed OS-B improves IS from 3.96 to 4.11 and decreases FID from 43.45 to 32.52. The results demonstrate that our one-stage backbone is more effective than stacked architecture. Effect of MA-GP. Armed with MA-GP, the model further improves IS to 4.46, SC to 3.55, and decreases FID to 32.52 significantly. It demonstrates that the proposed MA-GP can promote the generator to synthesize more realistic and textimage semantic consistent images. Effect of One-Way Output. The proposed OW-O also improves IS from 4.46 to 4.57, SC from 3.55 to 4.61, and decreases FID from 32.52 to 23.16. It also demonstrates that the One-Way Output is more effective than a Two-Way Output in the text-to-image generation task. Effect of Target-Aware Discriminator. Compared with DAMSM, our proposed Target-Aware Discriminator composed of MA-GP and OW-O improves IS from 4.28 to 4.57, SC from 1.79 to 4.61, and decreases FID from 36.72 to 23.16. The results demonstrate that our Target-Aware Discriminator is superior to extra networks. Effect of DFBlock. We compare our DFBlock with CBN [1, 5, 29], AdaIN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The performance of MA-GP GAN with different modules on the test set of CUB. GP GAN w/ DFBLK (DF-GAN) 5.10 14.81    </figDesc><table><row><cell>Architecture</cell><cell>IS?</cell><cell>FID ?</cell></row><row><cell>MA-GP GAN w/ Concat</cell><cell cols="2">4.57 23.16</cell></row><row><cell>MA-GP GAN w/ CBN</cell><cell cols="2">4.81 18.56</cell></row><row><cell>MA-GP GAN w/ AdaIN</cell><cell cols="2">4.85 17.52</cell></row><row><cell>MA-GP GAN w/ AFFBLK</cell><cell cols="2">4.87 17.43</cell></row><row><cell>MA-</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rifegan: Rich feature generation for text-toimage synthesis from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanling</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10911" to="10920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fashion meets computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shintami</forename><surname>Chusnul Hidayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?mie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<title level="m">Mastering text-to-image generation via transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Segattngan: Text to image generation with segmentation attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchuan</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiancheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12444</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic neural turing machine with continuous and discrete addressing schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="857" to="884" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2065" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring global and local linguistic representation for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cpgan: Contentparsing generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiadong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00823</idno>
		<title level="m">A chinese multimodal pretrainer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Time: text and image mutual-translation adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13192</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for image and video synthesis: Algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="839" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">cgans with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Benchmark for compositional text-toimage synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learn, imagine and create: Text-to-image generation from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="887" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dae-gan: Dynamic aspectaware gan for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shulan Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Xinggan for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning for image super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3365" to="3387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2327" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The defense of adversarial example with conditional generative adversarial networks. Security and Communication Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwen</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ckd: Cross-task knowledge distillation for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cross-modal contrastive learning for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stack-gan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dtgan: Dual attention generative adversarial networks for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
