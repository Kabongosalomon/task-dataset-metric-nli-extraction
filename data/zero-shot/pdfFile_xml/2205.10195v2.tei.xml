<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowan</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to properly model the inter-frame relation within the video sequence is an important but unsolved challenge for video restoration (VR). In this work, we propose an unsupervised flowaligned sequence-to-sequence model (S2SVR) to address this problem. On the one hand, the sequence-to-sequence model, which has proven capable of sequence modeling in the field of natural language processing, is explored for the first time in VR. Optimized serialization modeling shows potential in capturing long-range dependencies among frames. On the other hand, we equip the sequence-to-sequence model with an unsupervised optical flow estimator to maximize its potential. The flow estimator is trained with our proposed unsupervised distillation loss, which can alleviate the data discrepancy and inaccurate degraded optical flow issues of previous flowbased methods. With reliable optical flow, we can establish accurate correspondence among multiple frames, narrowing the domain difference between 1D language and 2D misaligned frames and improving the potential of the sequence-tosequence model. S2SVR shows superior performance in multiple VR tasks, including video deblurring, video super-resolution, and compressed video quality enhancement. https://github. com/linjing7/VR-Baseline</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video restoration (VR) aims to reconstruct high-quality (HQ) video from its degraded low-quality (LQ) counterpart, including video deblurring <ref type="bibr" target="#b51">(Xiang et al., 2020)</ref>  resolution (SR) <ref type="bibr" target="#b34">(Sajjadi et al., 2018;</ref><ref type="bibr">Chan et al., 2021)</ref>, and compressed video enhancement <ref type="bibr" target="#b13">(Guan et al., 2019)</ref>.</p><p>Task-driven networks often have complex structures that are elaborately designed for a specific task. These methods may be inapplicable when transferred to a new scenario or a different video restoration task <ref type="bibr" target="#b8">Deng et al., 2020;</ref><ref type="bibr" target="#b3">Cao et al., 2021)</ref>. Therefore, it is of great significance to explore a unified and versatile framework that can be used for multiple video restoration tasks.</p><p>Early works simply extend single image restoration <ref type="bibr" target="#b7">(Dai et al., 2015;</ref><ref type="bibr" target="#b35">Shahar et al., 2011;</ref><ref type="bibr" target="#b23">Liao et al., 2015;</ref><ref type="bibr">Cai et al., 2021)</ref> to video restoration. These image-based methods ignore inter-frame correlation, leading to limited performance. Some CNN-based methods <ref type="bibr" target="#b33">Pan et al., 2020;</ref><ref type="bibr" target="#b8">Deng et al., 2020)</ref> utilize information from the frames within a short temporal window. The ignorance of distant frames significantly limits the performance of these methods. Some researchers use the recurrent neural network (RNN) <ref type="bibr" target="#b17">(Isobe et al., 2020;</ref><ref type="bibr" target="#b55">Yang et al., 2019;</ref><ref type="bibr" target="#b60">Zhong et al., 2020;</ref><ref type="bibr">Chan et al., 2021)</ref> to propagate the hidden state in the time domain to expand the temporal receptive field. However, as analyzed in <ref type="bibr" target="#b19">(Jozefowicz et al., 2015)</ref>, RNN suffers from both exploding and vanishing gradients. As a result, RNN is difficult to learn the long-term dependencies and can not be stacked into very deep models, limiting the representation capacity of restoration network. The transformerbased model <ref type="bibr" target="#b3">(Cao et al., 2021;</ref><ref type="bibr" target="#b24">Lin et al., 2022a)</ref> can process a video sequence in parallel with self-attention mechanism. Nonetheless, the model complexity is quadratic to the number of tokens. For video restoration with an immense number of tokens, modeling long-range dependencies means huge computational costs and memory occupation. Thus, the problem of modeling long-term inter-frame relations with an affordable cost remains formidable.</p><p>Based on the sequence nature of videos, our insight into this problem is to treat it as a sequence modeling task and try to solve it with the sequence-to-sequence (seq2seq) model. Seq2seq model has proven capable of sequence modeling <ref type="bibr" target="#b40">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Chopra et al., 2016;</ref><ref type="bibr" target="#b32">Ott et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2018)</ref> in the field of natural language processing (NLP), showing great potential in modeling the inter-frame relation within the video sequence. Seq2seq model is devised to serially encode the input sequence into latent vectors and then dynamically decode a target sequence out of that representations. However, the migration of the seq2seq model is inevitably hindered by the domain discrepancy between NLP and VR. The video signal is composed of multiple misaligned 2D frames, while the seq2seq model can only handle continuous 1D input (e.g., , language sequence, time series) canonically. So we need to establish accurate correspondences among multiple frames by performing a spatial alignment with optical flow estimator.</p><p>Previous flow-based <ref type="bibr" target="#b50">(Wedel et al., 2009;</ref><ref type="bibr" target="#b39">Sun et al., 2018;</ref><ref type="bibr" target="#b43">Teed &amp; Deng, 2020)</ref> methods perform spatial alignment with a pretrained optical flow network. <ref type="bibr">(Chan et al., 2021)</ref> prove that feature alignment, i.e., estimating optical flow from the LQ videos and using it to warp the hidden state, can yield a better restoration result than image alignment. However, these flow-based methods may be suboptimal and suffer from the following issues: Firstly, the data discrepancy between synthetic flow dataset and real-world video affects the performance of the pretrained optical flow module in VR. Secondly, the optical flow estimated from the LQ input video (LQ flow) may be unreliable since the video degradation may seriously distort video contents and break pixel-wise correspondences between frames <ref type="bibr">(Zheng et al., 2021)</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the LQ flows lose some motion details, and the frames aligned by the LQ flows (LQ-aligned frames) contain blurry edges. In contrast, the HQ flow is more detailed, and the HQ-aligned frames contain sharper semantics. Besides, for feature alignment, the motion information estimated from the LQ video may be inconsistent with that of the hidden state, which is expected to be spatially aligned with the HQ video. So some artifacts will be brought when the LQ flow is used for feature alignment.</p><p>We attempt to address the data discrepancy and inaccurate LQ flow issues with unsupervised distillation optical flow loss. To be specific, we train an optical flow estimator on the VR dataset with unsupervised loss. The data discrepancy naturally disappear since the training and testing dataset both come from the real-world VR dataset. Furthermore, a novel data distillation loss is designed to generate more accurate LQ flows, in which the optical flows estimated from the HQ video serve as the pseudo-labels of the LQ flows. This loss encourages the LQ flows to imitate the HQ flows, which are more accurate and spatial consistent with the motion information of the hidden state. Therefore, the unsupervised flow-aligned sequence-tosequence model is proposed for video restoration tasks (S2SVR). We migrate and improve the seq2seq model from NLP to VR task, and maximize the potential of the seq2seq model with an unsupervised optical flow estimator. In a nutshell, our contributions can be summarized as follows:</p><p>? This is the first VR work to explore the sequence-tosequence model, which comes from NLP and is intrinsically suitable for video sequence modeling. ? The proposed unsupervised distillation optical flow loss alleviates the data discrepancy and inaccurate LQ flow issues of previous flow-based methods, narrowing the domain difference between NLP and VR. ? Extensive experiments show that our method achieves state-of-the-art performance in three typical video restoration tasks, including video deblurring, video super-resolution, and compressed video enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Restoration</head><p>Early work <ref type="bibr" target="#b41">(Takeda et al., 2009;</ref><ref type="bibr" target="#b35">Shahar et al., 2011;</ref><ref type="bibr" target="#b7">Dai et al., 2015)</ref> adopt an image restoration model for video restoration and do not take advantage of information in the neighbouring frames. The ignorance of the inter-frame correlation severely limits the restoration result. Some CNNbased methods <ref type="bibr" target="#b44">Tian et al., 2020)</ref> employ deformable convolution to perform feature-level alignment. The RNN-based methods design the recurrent structure and attempt to model the long-term dependencies by propagating the hidden state <ref type="bibr" target="#b17">(Isobe et al., 2020;</ref><ref type="bibr" target="#b55">Yang et al., 2019;</ref><ref type="bibr" target="#b60">Zhong et al., 2020)</ref>. <ref type="bibr">(Chan et al., 2021)</ref> prove that the combination of bidirectional propagation and optical flow estimation can achieve ideal results. <ref type="bibr">(Deng et al., 2021)</ref> propose a recurrent model with separable-patch architecture and multi-scale integration scheme for fast and accurate video deblurring. However, the RNN-based methods inevidently suffer from the vanishing gradient problem and have difficulty in capturing the long-range temporal dependencies.</p><p>Recently, the emerging Transformer model has been applied in image and video restoration tasks <ref type="bibr" target="#b22">Liang et al., 2022;</ref><ref type="bibr" target="#b25">Lin et al., 2022b;</ref><ref type="bibr" target="#b3">Cao et al., 2021;</ref><ref type="bibr" target="#b2">Cai et al., 2022b)</ref>. Nonetheless, the token-based self-attention module has enormous computational and memory cost in restoring long video sequence. Thus, the problem of effectively modeling long-range temporal dependencies within the video sequence remains formidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sequence-to-Sequence Learning</head><p>Seq2seq model is first proposed by <ref type="bibr" target="#b40">(Sutskever et al., 2014)</ref> for the machine translation task in which a long short-term</p><formula xml:id="formula_0">Decoder Decoder 1 i c ? i s 1 i s ? M Encoder 1 z 1 x 2 x N x 2 z N z M Decoder 2 i s ? M i c 2 i c ? . . . . . . M . . . 1 i s ? Local Attention . . . . . . Latent Vectors S2SVR Warp t x -1 t x 1 t s ? Optical Flow Estimator t o 1 t s ? i r z ? + i r z ? Conv Conv tanh 1 i s ? i c ? ResConvGRU ResConvGRU ResConvGRU 1 l t z ? 1 l t z ? ConvGRU RB RB ? l t z Conv Conv ReLU</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResConvGRU (c) Encoder (Decoder)</head><p>Encoder Encoder <ref type="figure">Figure 2</ref>. The architecture of the proposed unsupervised flow-aligned seq2seq model (S2SVR). The modules with different background colors on the right show the internal details of (a) local attention, (b) motion compensation and (c) Encoder (Decoder). memory (LSTM) encodes the input sequence into a latent representation and then another LSTM decodes the target sequence out of that representation. The model is intrinsically suitable for long-range coding tasks. Various variants of the seq2seq model have been applied to many sequence modeling tasks, such as speech recognition <ref type="bibr" target="#b45">(Venugopalan et al., 2015)</ref>, time series analysis <ref type="bibr" target="#b20">(Kuznetsov &amp; Mariet, 2019;</ref><ref type="bibr">He et al., 2021)</ref>, and text summarization <ref type="bibr" target="#b36">(Shi et al., 2021)</ref>. Due to the fundamental difference between video and language, the potential of this serialized encoding-decoding structure in assisting continuous-frame VR is unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Local Attention (b) M :Motion Compensation</head><formula xml:id="formula_1">1 1 l t z + ? 2 1 l t z + ? 1 L t z ? 1 i y ? i y 2 i y ? 1 z . . . . . . 2 z 1 N z ? 2 i y ? 1 i y ? 3 i y ? 1 l t z + 2 l t z + t z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Optical Flow Estimation</head><p>With the development of deep learning, some optical flow estimation networks <ref type="bibr" target="#b39">(Sun et al., 2018;</ref><ref type="bibr" target="#b43">Teed &amp; Deng, 2020)</ref> trained on synthetic datasets have achieved better results than non-learning methods <ref type="bibr" target="#b27">(M?min &amp; P?rez, 1998;</ref><ref type="bibr" target="#b50">Wedel et al., 2009</ref>). The domain difference between synthetic optical flow and real-world optical flow datasets leads to limited model performance.  suggest using an unsupervised optical flow estimator to circumvent the need for labels.  improve the performance of unsupervised optical methods by proposing a new warping module to facilitate large motion learning and model occlusion explicitly. <ref type="bibr" target="#b37">(Shi et al., 2017)</ref> train a task-oriented flow module jointly with the video enhancement module in the supervision of L 1 loss. But the jointly-trained flow module becomes unsuitable when cooperating with other video processing modules. Besides, they have not solved the problem that it's difficult to estimate accurate motion information from the severely degraded input frames. Based on the LQ-HQ paired characteristics of VR tasks, we propose a data distillation loss to improve the quality of the LQ flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present our S2SVR model. We first introduce the overall framework of the seq2seq model. Then, we explain the unsupervised distillation optical flow method, which narrow the domain discrepancy between NLP and VR and improve the potential of the seq2seq model in VR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sequence-to-Sequence Learning</head><p>To promise that the scalable seq2seq architectures and their efficient implementations can be preserved, S2SVR follows the seq2seq framework from NLP as closely as possible. As shown in <ref type="figure">Fig. 2</ref>, S2SVR is composed of four components: encoder, decoder, local attention, and optical flow estimator.</p><p>For notation, we use capital letters to represent sequences,(e.g., X,Y ), lower case to denote individual frames in a sequence, (e.g., x 1 ,x 2 ). Let X = {x 1 , x 2 , . . . , x N } represent the input low-quality video sequence and Y = {y 1 , y 2 , . . . , y N } be the corresponding high-quality video sequence, where N is the length of the sequence. The goal of our S2SVR is to estimate the conditional probability of the target sequence respective to the input sequence P (Y |X).</p><p>Encoder. Firstly, the encoder read sequentially each x i ? X and transforms the source sequence into a list of latent vectors Z = {z 1 , z 2 , . . . , z N }:</p><formula xml:id="formula_2">z i = F e (z i?1 , x i ),<label>(1)</label></formula><p>where z i denotes the latent vector at time step i, and F e denotes the function of the encoder, which in our implementation is a residual stacked ConvGRU (ResConvGRU). The ResConvGRU will be introduced in the next subsection.</p><p>Decoder. Next, the decoder sequentially produces the out-put video based on the encoded vectors. Specifically, using the chain rule, the conditional probability P (Y |X) can be decomposed as:</p><formula xml:id="formula_3">P (Y |X) = P (y 1 , y 2 , . . . , y N |z 1 , z 2 , . . . , z N ) = N t=1 P (y t |y 1 , . . . , y t?1 ; z 1 , . . . , z N ).<label>(2)</label></formula><p>We serially generate the subsequent output based on the source sequence encoding and the decoded sequence so far:</p><formula xml:id="formula_4">y t = F d (y 1 , y 2 , . . . , y t?1 ; z 1 , z 2 , . . . , z N ).<label>(3)</label></formula><p>F d represents the decoder, which is composed of a ResCon-vGRU and a feed-forward network. ResConvGRU generates a hidden state s i , and then s i passes through the feedforward network to produce the output frame:</p><formula xml:id="formula_5">s i = F r (s i?1 , y i?1 , c i ), y i = F f (s i ),<label>(4)</label></formula><p>where F r is the ResConvGRU and F f denotes the feedforward network. s i , y i refer to the hidden state of ResCon-vGRU and output frame at i th time step, respectively. And c i is a context vector generated by the local attention module based on the latent vectors Z = {z 1 , z 2 , . . . , z N }.</p><p>Local Attention. As shown in <ref type="figure">Fig. 2(a)</ref>, the attention module generates a context vector c i for each time step, allowing the decoder to extract information from different parts of the input sequence. Specifically, we represent the context vector c i as a weighted sum of a subset of the latent vectors:</p><formula xml:id="formula_6">c i = i+r j=i?r ? ij z j ,<label>(5)</label></formula><p>where r is the the subset radius and the weight ? ij is:</p><formula xml:id="formula_7">? ij = exp(e ij ) i+r k=i?r exp(e ik ) .<label>(6)</label></formula><p>e ij = F a (s i?1 , z j ) is an attention model scoring the correspondence between the i th input and the j th output based on s i?1 and z j . Similar to <ref type="bibr" target="#b35">(Shahar et al., 2011)</ref>, a two-layer feed-forward network is adopted as the attention model:</p><formula xml:id="formula_8">e ij = V a ? tanh(W a [s i?1 , z j ]),<label>(7)</label></formula><p>where V a and W a denote the first and second convolution layers of the feed-forward network, respectively. And [?, ?] refers to concatenation along the channel dimension.</p><p>Motion Compensation. To improve the performance of the seq2seq model in VR, we need to establish accurate spatial correspondences among multiple frames. Similar to previous methods <ref type="bibr" target="#b17">(Isobe et al., 2020;</ref><ref type="bibr" target="#b55">Yang et al., 2019</ref>;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student Net</head><p>Warp ? sm ( 12 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Teacher Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warp</head><p>Step 1</p><p>Step 2  <ref type="figure">Figure 3</ref>. Illustration of the unsupervised optical flow method. <ref type="bibr" target="#b60">Zhong et al., 2020;</ref><ref type="bibr">Chan et al., 2021)</ref>, we adopt an optical flow estimator for motion compensation. Specifically, as shown in <ref type="figure">Fig. 2(b)</ref>, we employ a flow estimator to predict the motion between two consecutive frames. Then we warp the hidden state of ResConvGRU at last time step s t?1 , making it spatially aligned with the input at the current step:</p><formula xml:id="formula_9">o t = F o (x t , x t?1 ), s t?1 = F w (s t?1 , o t ),<label>(8)</label></formula><p>where F o and F w respectively refer to the optical flow estimator and spatial warping module. o t is the optical flow field between the adjacent input frames x t and x t?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Stacked ConvGRU</head><p>We use a deep-stacked ConvGRU for both the encoder and the decoder. Considering the video characteristics, as shown in <ref type="figure">Fig. 2(c)</ref>, we make two modifications to the original ConvGRU. Firstly, to improve the image processing ability, several residual blocks are concatenated after the ConvGRU. Besides, motivated by the idea of modeling the difference between an intermediate layer's output and the target, we introduce residual among the layers in a stack. We define the ConvGRU and residual blocks as F g (?) and F b (?):</p><formula xml:id="formula_10">z l t = z l?1 t + F b (F g (z l t?1 , z l?1 t )),<label>(9)</label></formula><p>where z l t denote the hidden state of l th ConvGRU at time step t. In this way, the vanishing gradient problem can be addressed, allowing us to model the long-term temporal dependencies. More details are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unsupervised Optical Flow Estimator</head><p>As analyzed in Sec. 1, previous flow-based motion compensation methods suffer from the data discrepancy between synthesized and real-world datasets, as well as inaccurate LQ flows. To solve these problems, we propose an unsupervised scheme equipped with a novel distillation loss to train the flow estimator on the VR dataset as shown in <ref type="figure">Fig. 3</ref>.</p><p>Let X denotes a LQ input video, and Y is the corresponding HQ video. Our goal is to train a flow network F o that can estimate accurate motion information from the LQ videos (HQ videos are unavailable during inference) by predicting the optical flow F x 12 for two consecutive LQ frames {x 1 , x 2 }:</p><formula xml:id="formula_11">F x 12 = F o (x 1 , x 2 ).<label>(10)</label></formula><p>The unsupervised scheme is summarized in Algorithm 1 to better understand the proposed unsupervised optical flow estimation method. Firstly, we train a teacher flow estimation network parameterized by ? t on the HQ videos with photometric loss and smooth loss. After convergence, we use the pretraied teacher estimator to generate pseudo-labels and train a student flow network parameterized by ? o on the LQ video. In the following, we explain the proposed unsupervised optical flow training scheme step by step.</p><p>Step 1. We train an optical flow estimator F t with photometric loss and smooth loss on the HQ video Y . This optical flow estimator F t will be frozen and serves as a teacher network in the next step. The photometric loss <ref type="bibr" target="#b57">(Yu et al., 2016)</ref> is based on the assumption that the same object in two consecutive frames must have similar intensities:</p><formula xml:id="formula_12">L ph (F y 12 ) = p ?(y 1 (p), y 2 (p + F y 12 (p))) ? O y (p), (11)</formula><p>where p is the coordinate and O y is the occlusion mask to discard the loss on the occurred region generated by the bidirectional checking , ?(?) is the 1 loss, and F y 12 is the optical flow field for two consecutive frames in the HQ videos Y :</p><formula xml:id="formula_13">F y 12 = F t (y 1 , y 2 ).<label>(12)</label></formula><p>Further, we adopt a one-order smooth loss <ref type="bibr" target="#b12">(Godard et al., 2017)</ref> to encourage collinearity of neighboring flows:</p><formula xml:id="formula_14">L sm (F y 12 ) = d?x,y p |? d F y 12 (p)|e ?|? d y1(p)|<label>(13)</label></formula><p>And then we formulate the loss used in the first step as:</p><formula xml:id="formula_15">L = ? ph ? L ph (F y 12 ) + ? sm ? L sm (F y 12 ).<label>(14)</label></formula><p>We respectively set the weights ? ph and ? sm to 0.15 and 50.</p><p>Step 2. Now we have trained a teacher optical flow estimator F t which can predict the accurate optical flow F y 12 for two consecutive HQ frames {y 1 , y 2 } ? Y :</p><formula xml:id="formula_16">F y 12 = F t (y 1 , y 2 ).<label>(15)</label></formula><p>Based on the assumption that the HQ flow is more accurate for motion compensation, we use F y 12 as the pseudo-labels of the LQ flows F x 12 and and propose the distillation loss:</p><formula xml:id="formula_17">L dis (F x 12 , F y 12 ) = p |F y 12 (p) ? F u (F x 12 )(p)|,<label>(16)</label></formula><p>Algorithm 1 Unsupervised Distillation Optical Flow Loss Inputs: teacher and student net parameterized by ? t , ? o , cost function parameters: loss weights {? ph , ? sm , ? dis }, optimization parameters: number of iterations T Output: pretrained student network // Step1: train the teacher network for j = 0 to T do compute photometric loss L ph (using Eq. <ref type="formula" target="#formula_2">(11)</ref>) compute smooth loss L sm (using Eq. <ref type="formula" target="#formula_2">(13)</ref>) L tot = ? ph ? L ph + ? sm ? L sm , ?L(? t ) = ?Ltot ??t , ? t = ? t ? ??L(? t ) end for // Step2: train the student network for j = 0 to T do compute photometric loss L ph (using Eq. <ref type="formula" target="#formula_2">(11)</ref>) compute smooth loss L sm (using Eq. <ref type="formula" target="#formula_2">(13)</ref>) compute data distillation loss L dis (using Eq. <ref type="formula" target="#formula_2">(16)</ref>)</p><formula xml:id="formula_18">L tot = ? ph L ph + ? sm L sm + ? dis L dis , ?L(? o ) = ?Ltot ??o , ? o = ? o ? ??L(? o ) end for Return student network parameters ? o</formula><p>where F u is a upsample operation to ensure that F x 12 has the same size as F y 12 in video super-resolution task. Along with the photometric loss and smoothness regularization, we train the student flow estimator F o on the LQ dataset:</p><formula xml:id="formula_19">L = ? ph L ph (F x 12 ) + ? sm L sm (F x 12 ) + ? dis L dis (F x 12 , F y 12 ).</formula><p>(17) We set the weights to {? ph = 0.15, ? sm = 50, ? dis = 0.1}. The student network will be later used as our optical flow estimator for motion compensation as in Eq. (8). In implementation, we adopt a lightweight flow model pwclite <ref type="bibr" target="#b26">(Liu et al., 2020)</ref> as our optical flow network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Datasets. For video SR, the benchmark datasets consist of REDS4 <ref type="bibr" target="#b29">(Nah et al., 2019a)</ref> and Vimeo-90K-T <ref type="bibr" target="#b52">(Xue et al., 2019)</ref>. For video deblurring, we use the GOPRO dataset <ref type="bibr" target="#b28">(Nah et al., 2017)</ref>, where 22 videos are used for training and 11 videos for testing. For compressed video enhancement, our models are trained with the MFQEv2 dataset <ref type="bibr" target="#b13">(Guan et al., 2019)</ref> including 108 lossless videos. We adopt the dataset from ITU-T <ref type="bibr" target="#b31">(Ohm et al., 2012)</ref> containing 18 videos for evaluation. We compress videos by HEVC reference software HM16.5 under Low Delay P (LDP) configuration <ref type="bibr" target="#b13">(Guan et al., 2019;</ref><ref type="bibr" target="#b8">Deng et al., 2020)</ref>. Evaluation metrics include PSNR and SSIM <ref type="bibr" target="#b49">(Wang et al., 2004)</ref>.</p><p>Settings. Models are trained with nature videos and their degraded counterparts. During unsupervised optical flow training, the learning rate is set to 1 ? 10 ?4 . And during restoration training, the initial learning rate of the flow esti-  mator and the other modules are set to 5?10 ?5 and 2?10 ?4 , respectively. We use PyTorch to implement our models and train them on 8 Tesla V100 GPUs. More details are provided in the supplementary material due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video Super-Resolution</head><p>Quantitative Comparison. We compare our method with previous methods: TOFlow <ref type="bibr" target="#b52">(Xue et al., 2019)</ref>, DUF <ref type="bibr" target="#b18">(Jo et al., 2018)</ref>, RBPN <ref type="bibr" target="#b14">(Haris et al., 2019)</ref>, EDVR-M , EDVR , PFNL <ref type="bibr" target="#b56">(Yi et al., 2019)</ref>, MuCAN , BasicVSR <ref type="bibr">(Chan et al., 2021)</ref>, IconVSR <ref type="bibr">(Chan et al., 2021)</ref>, and VSR-Transformer <ref type="bibr" target="#b3">(Cao et al., 2021)</ref>. As shown in Tab.  <ref type="table">Table 2</ref>. Video deblurring performance comparison and model parameter analysis on the GOPRO dataset <ref type="bibr" target="#b28">(Nah et al., 2017).</ref> our performance is slightly lower than VSR-Transformer, but S2SVR only requires 41% parameters compared with the latter. It shows that we only need half the parameters to obtain comparable performance to transformer-based models. Note that Vimeo-90K-T contains sequences with seven frames. So it also indicates that our method performs better in restoring long sequences. Serialized modeling of seq2seq models and accurate optical flow estimation facilitates the capture of long-range inter-frame dependencies.</p><p>Visual Comparison. From the comparison with other methods in <ref type="figure" target="#fig_1">Fig. 4</ref>  <ref type="bibr" target="#b10">(Dong et al., 2015</ref><ref type="bibr" target="#b58">) (Zhang et al., 2017</ref>      <ref type="bibr" target="#b13">(Guan et al., 2019)</ref>  <ref type="bibr">(Deng et al., 2020) (Ours)</ref>  <ref type="table">Table 3</ref>. Overall comparison of compressed video enhancement for ?PSNR (dB) and ?SSIM (?10 ?4 ) over test sequences at QP=37. We experiment with five different video resolutions: A (2,560?1,600), B (1,920?1,080), C (832?480), D (480?240), E (1,280?720). (Hyun , Nah et al. <ref type="bibr" target="#b30">(Nah et al., 2019b)</ref>, EDVR , STFAN <ref type="bibr" target="#b61">(Zhou et al., 2019)</ref>, TSP , and UHDVD <ref type="bibr">(Deng et al., 2021)</ref>. The Tab. 2 shows the quantitative results on the GOPRO dataset <ref type="bibr" target="#b28">(Nah et al., 2017</ref>  On the RaceHorses dataset with the 832 ?480 input size, our method can outperform the latest method by at least 0.304dB, which shows the superiority of our method in processing long video sequences with large motion. Visual Comparison. As shown in <ref type="figure">Fig. 6</ref>, some structural distortions and color deviations make the results of previous methods unconvincing. Our S2SVR network guarantees the basic structural texture and semantic content. Due to space limitations, we put more comparisons in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>Unsupervised Optical Flow Estimator. To demonstrate the effectiveness of the unsupervised training scheme, we retrain our optical flow network pwclite in a supervised manner with the optical flow dataset FlyingChairs <ref type="bibr" target="#b11">(Dosovitskiy et al., 2015)</ref>. We also adopt pre-trained RAFT <ref type="bibr" target="#b43">(Teed &amp; Deng, 2020)</ref>, the SOTA supervised optical flow network, as our optical flow estimator. As shown in Tab. 4, the pwclite trained with our unsupervised distillation loss can outperform the supervised counterpart by 0.17 dB. It indicates that the flow estimator trained in our unsupervised scheme fits the VR tasks well. Notably, it achieves a better result than the state-of-the-art supervised method RAFT by 0.08dB with a lower cost. These results show the effectiveness of our unsupervised optical flow method. Motion Compensation Visualization. We visualize the feature saliency maps with (W/I) and without (W/O) motion compensation in <ref type="figure">Fig. 7</ref>. Obviously, the video frame will lose lots of motion details and texture edges without motion compensation. It is caused by the misalignment among multiple frames, which limits the potential of the seq2seq model in VR. In contrast, with motion compensation, the feature map is much sharper and preserves more movement details, which benefits from our accurate optical flow estimation. Motion compensation narrows the domain difference between NLP and VR, facilitating information aggregation. Long Sequence Reconstruction. To validate the effectiveness of our S2SVR in capturing long-range temporal dependencies, we separate a video in REDS4 dataset into 4 segments with different lengths, including 5, 15, 30, and 50 frames, respectively. And we use S2SVR, EDVR, and EDVR-M to restore these sequences independently. Our method performs the best among the three methods in Tab. 5. And the longer the sequence is, the more superior our S2SVR shows. It suggests that our method has an excellent performance in modeling long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose an unsupervised flow-aligned seq2seq model for multiple video restoration tasks. Our work aims at solving the challenges of properly modeling the inter-frame relation within the video sequence. The sequence-to-sequence learning is explored for the first time in VR to capture long-term temporal dependencies at a low cost. What's more, we design an unsupervised optical method equipped with a novel distillation loss to improve the performance of the seq2seq model in VR. Extensive experiments show that the proposed method achieves comparable performance in video deblurring, video super-resolution, and compressed video quality enhancement tasks with moderate model size, especially in long sequence VR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Optical flow estimated from LQ and HQ videos respectively (top), and visual comparison of the aligned frames (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison of video 4? SR results on the REDS4 (Nah et al., 2019a) dataset. Please zoom in for a better comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison of video deblurring results on the GOPRO (Nah et al., 2017) dataset. Please zoom in for a better comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Visual comparison on Video BasketballPass at QP = 37. Visualization with and without motion compensation. enhancement. We compare S2SVR with AR-CNN<ref type="bibr" target="#b10">(Dong et al., 2015)</ref>,DnCNN (Zhang et al., 2017), DS-CNN, MFQE 1.0, MFQE 2.0<ref type="bibr" target="#b13">(Guan et al., 2019)</ref>, and STDF-R3L. As shown in Tab. 3, S2SVR almost outperforms all compared methods in ?PSNR and ?SSIM by a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, video super-Equal contribution 1 Shenzhen International Graduate School, Tsinghua University 2 Huawei Noah's Ark Lab 3 ETH Z?rich. Correspondence to: Xueyi Zou &lt;zouxueyi@huawei.com&gt;, Haoqian Wang &lt;wanghaoqian@tsinghua.edu.cn&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). Our proposed method performs favorably against other methods and has an absolute advantage on PSNR in video deblurring. Specifically, the S2SVR model achieves a performance gain of 0.14dB on the dataset with a lightweight structure. We also report the size of the open-source model in Tab. 2. As the largest model, EDVR's parameter is up to 23M, but its performance is unsatisfactory. Our S2SVR network contains 8.44M parameters. Compared with the TSP, our model achieves a higher PSNR performance with only one-half of its size. Visual Comparison. From the comparison results inFig. 5, it can be seen that our method can restore the original structure as much as possible from the severely degraded scene. Digital restoration of blurred scenes is difficult. It can be seen that no other method except ours can guarantee the semantics while still retaining the satisfying visual results.</figDesc><table /><note>4.4. Compressed Video Enhancement Quantitative Comparison. We evaluate the performance of compressed video enhancement by ?PSNR and ?SSIM, which measure the PSNR and SSIM improvement after the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Model analysis with supervised (Sup.) and unsupervised (Unsup.) optical flow estimation model training. Quantitative comparison on sequences of different length.</figDesc><table><row><cell>Method</cell><cell cols="2">RAFT Sup. Unsup.</cell><cell cols="2">Pwclite Sup. Unsup.</cell></row><row><cell>Params</cell><cell>4.81 M</cell><cell>-</cell><cell cols="2">2.24 M 2.24 M</cell></row><row><cell>PSNR (dB)</cell><cell>31.88</cell><cell>-</cell><cell>31.79</cell><cell>31.96</cell></row><row><cell cols="5">Length EDVR-M EDVR S2SVR (Ours)</cell></row><row><cell>5</cell><cell>27.78</cell><cell>28.05</cell><cell cols="2">28.10 (+ 0.05)</cell></row><row><cell>15</cell><cell>28.47</cell><cell>28.80</cell><cell cols="2">29.25 (+ 0.45)</cell></row><row><cell>30</cell><cell>27.76</cell><cell>28.01</cell><cell cols="2">28.53 (+ 0.52)</cell></row><row><cell>50</cell><cell>27.52</cell><cell>27.77</cell><cell cols="2">28.34 (+ 0.57)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work is partially supported by the NSFC fund (61831014), the Shenzhen Science and Technology Project under Grant (CJGJZD20200617102601004, JSGG20210802153150005).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to generate realistic noisy images via pixel-level noise-aware adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-stage spectralwise transformer for efficient spectral reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mst++</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Video superresolution transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06847</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basicvsr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dictionary-based multiple frame video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatio-temporal deformable convolution for compressed video quality enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale separable network for ultra-high-definition video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mfqe 2.0: A new approach for multi-frame quality enhancement on compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent backprojection network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Checkerboard context model for efficient learned image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video super-resolution with recurrent structure-detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Foundations of sequence-tosequence modeling for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mariet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mucan: Multi-correspondence aggregation network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vrt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12288</idno>
		<title level="m">A video restoration transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video superresolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Flow-guided sparse transformer for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01893</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Coarse-to-fine sparse transformer for hyperspectral image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04845</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense estimation and object-based segmentation of the optical flow with robust techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Comparison of the coding efficiency of video coding standards-including high efficiency video coding (hevc)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cascaded deep video deblurring using temporal sharpness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Framerecurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Space-time superresolution from a single video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural abstractive text summarization with sequence-tosequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keneshloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Data Science</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3215</idno>
		<title level="m">Sequence to sequence learning with neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Superresolution without explicit subpixel motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalerecurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tdan: Temporallydeformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structureand motion-adaptive regularization for high accuracy optic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep video deblurring using sharpness features from exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Enhancing quality for hevc compressed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-frame quality enhancement for compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Quality-gated convolutional lstm for enhancing compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting nonlocal spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration Zhang</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive spatial-temporal fusion of multiobjective networks for compressed video perceptual enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
