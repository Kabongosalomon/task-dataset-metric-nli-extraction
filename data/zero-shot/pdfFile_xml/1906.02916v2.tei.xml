<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-hop Reading Comprehension through Question Decomposition and Rescoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
							<email>vzhong@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-hop Reading Comprehension through Question Decomposition and Rescoring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as humanauthored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HOTPOTQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-hop reading comprehension (RC) is challenging because it requires the aggregation of evidence across several paragraphs to answer a question. <ref type="table">Table 1</ref> shows an example of multi-hop RC, where the question "Which team does the player named 2015 Diamond Head Classics MVP play for?" requires first finding the player who won MVP from one paragraph, and then finding the team that player plays for from another paragraph.</p><p>In this paper, we propose DECOMPRC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question. For example, for the question in Table 1, we can create the sub-questions "Which player named 2015 Diamond Head Classics MVP?" and "Which team does ANS play for?", Q Which team does the player named 2015 Diamond Head Classic's MVP play for? P1 The 2015 Diamond Head Classic was ... Buddy Hield was named the tournament's MVP. P2 Chavano Rainier Buddy Hield is a Bahamian professional basketball player for the Sacramento Kings ... Q1 Which player named 2015 Diamond Head Classic's MVP? Q2 Which team does ANS play for? <ref type="table">Table 1</ref>: An example of multi-hop question from HOT-POTQA. The first cell shows given question and two of given paragraphs (other eight paragraphs are not shown), where the red text is the groundtruth answer. Our system selects a span over the question and writes two sub-questions shown in the second cell.</p><p>where the token ANS is replaced by the answer to the first sub-question. The final answer is then the answer to the second sub-question.</p><p>Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms <ref type="bibr">(Talmor and Berant, 2018)</ref>, making it difficult to generalize to diverse natural language questions such as those on HOTPOTQA <ref type="bibr">(Yang et al., 2018)</ref>. In contrast, our method presents a new approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model. Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning.</p><p>Our experiments show that DECOMPRC outperforms other published methods on HOT-POTQA <ref type="bibr">(Yang et al., 2018)</ref>, while providing explainable evidence in the form of sub-questions. In addition, we evaluate with alternative distrator paragraphs and questions and show that our decomposition-based approach is more ro-arXiv:1906.02916v2 [cs.CL] 30 Jun 2019 bust than an end-to-end BERT baseline <ref type="bibr">(Devlin et al., 2019)</ref>. Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance.</p><p>Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document <ref type="bibr">(Richardson et al., 2013)</ref>. Recently, the availability of large-scale reading comprehensiondatasets <ref type="bibr">(Hermann et al., 2015;</ref><ref type="bibr">Rajpurkar et al., 2016;</ref><ref type="bibr">Joshi et al., 2017)</ref> has led to the development of advanced RC models <ref type="bibr">(Seo et al., 2017;</ref><ref type="bibr">Xiong et al., 2018;</ref><ref type="bibr">Yu et al., 2018;</ref><ref type="bibr">Devlin et al., 2019)</ref>. Most of the questions on these datasets can be answered in a single sentence <ref type="bibr">(Min et al., 2018)</ref>, which is a key difference from multi-hop reading comprehension.</p><p>Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries <ref type="bibr">(Welbl et al., 2017;</ref><ref type="bibr">Talmor and Berant, 2018)</ref>. In contrast, <ref type="bibr">HOTPOTQA (Yang et al., 2018)</ref>, on which we evaluate our method, contains more natural, hand-written questions that are not based on relational queries.</p><p>Prior methods on multi-hop reading comprehension focus on answering relational queries, and emphasize attention models that reason over coreference chains <ref type="bibr">(Dhingra et al., 2018;</ref><ref type="bibr">Zhong et al., 2019;</ref><ref type="bibr">Cao et al., 2019)</ref>. In contrast, our method focuses on answering natural language questions via question decomposition. By providing decomposed single-hop sub-questions, our method allows the model's decisions to be explainable.</p><p>Our work is most related to <ref type="bibr">Talmor and Berant (2018)</ref>, which answers questions over web snippets via decomposition. There are three key differences between our method and theirs. First, they decompose questions that are correspond to relational queries, whereas we focus on natural language questions. Next, they rely on an underly-ing relational query (SPARQL) to build distant supervision data for training their model, while our method requires only 400 decomposition examples. Finally, they decide on a decomposition operation exclusively based on the question. In contrast, we decompose the question in multiple ways, obtain answers, and determine the best decomposition based on all given context, which we show is crucial to improving performance.</p><p>Semantic Parsing. Semantic parsing is a larger area of work that involves producing logical forms from natural language utterances, which are then usually executed over structured knowledge graphs <ref type="bibr">(Zelle and Mooney, 1996;</ref><ref type="bibr">Zettlemoyer and Collins, 2005;</ref><ref type="bibr">Liang et al., 2011)</ref>. Our work is inspired by the idea of compositionality from semantic parsing, however, we focus on answering natural language questions over unstructured text documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In multi-hop reading comprehension, a system answers a question over a collection of paragraphs by combining evidence from multiple paragraphs. In contrast to single-hop reading comprehension, in which a system can obtain good performance using a single sentence <ref type="bibr">(Min et al., 2018)</ref>, multi-hop reading comprehension typically requires more complex reasoning over how two pieces of evidence relate to each other.</p><p>We propose DECOMPRC for multi-hop reading comprehension via question decomposition. DE-COMPRC answers questions through a three step process:</p><p>1. First, DECOMPRC decomposes the original, multi-hop question into several single-hop sub-questions according to a few reasoning types in parallel, based on span predictions. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an example in which a question is decomposed through four different reasoning types. Section 3.2 details our decomposition approach.</p><p>2. Then, for every reasoning types DECOMPRC leverages a single-hop reading comprehension model to answer each sub-question, and combines the answers according to the reasoning type. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example for which bridging produces 'City of New   York' as an answer while intersection produces 'Columbia University' as an answer. Section 3.3 details the single-hop reading comprehension procedure.</p><p>3. Finally, DECOMPRC leverages a decomposition scorer to judge which decomposition is the most suitable, and outputs the answer from that decomposition as the final answer.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, "City of New York", obtained via bridging, is decided as the final answer. Section 3.4 details our rescoring step.</p><p>We identify several reasoning types in multi-hop reading comprehension, which we use to decompose the original question and rescore the decompositions. These reasoning types are bridging, intersection and comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decomposition</head><p>The goal of question decomposition is to convert a multi-hop question into simpler, single-hop subquestions. A key challenge of decomposition is that it is difficult to obtain annotations for how to decompose questions. Moreover, generating the question word-by-word is known to be a difficult task that requires substantial training data and is not straight-forward to evaluate <ref type="bibr">(Gatt and Krahmer, 2018;</ref><ref type="bibr">Novikova et al., 2017)</ref>. Instead, we propose a method to create sub-questions using span prediction over the question.</p><p>The key idea is that, in practice, each sub-question can be formed by copying and lightly editing a key span from the original question, with different span extraction and editing required for each reasoning type. For instance, the bridging question in <ref type="table" target="#tab_1">Table 2</ref> requires finding "the player named 2015 Diamond Head Classic MVP" which is easily extracted as a span. Similarly, the intersection question in <ref type="table" target="#tab_1">Table 2</ref> specifies the type of entity to find ("which actor and comedian"), with two conditions ("Stories USA starred" and "from "The Office""), all of which can be extracted. Comparison questions compare two entities using a discrete operation over some properties of the entities, e.g., "which is smaller". When two entities are extracted as spans, the question can be converted into two sub-questions and one discrete operation over the answers of the sub-questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Span Prediction for Sub-question Generation</head><p>Our approach simplifies the sub-question generation problem into a span prediction problem that requires little supervision (400 annotations). The annotations are collected by mapping the question into several points that segment the question into spans (details in Section 4.2). We train a model Pointer c that learns to map a question into c points, which are subsequently used to compose sub-questions for each reasoning type through Algorithm 1.</p><p>Pointer c is a function that points to c indices ind 1 , . . . , ind c in an input sequence. 1 Let S = [s 1 , . . . , s n ] denote a sequence of n words in the input sequence. The model encodes S using BERT <ref type="bibr">(Devlin et al., 2019)</ref>:</p><formula xml:id="formula_0">U = BERT(S) ? R n?h ,<label>(1)</label></formula><p>where h is the output dimension of the encoder. Let W ? R h?c denote a trainable parameter matrix. We compute a pointer score matrix</p><formula xml:id="formula_1">Y = softmax(U W ) ? R n?c ,<label>(2)</label></formula><p>where P(i = ind j ) = Y ij denotes the probability that the ith word is the jth index produced by the pointer. The model extracts c indices that yield the 1 c is a hyperparameter which differs in different reasoning types.</p><p>2 Details for find op, form subq in Appendix B.</p><p>Algorithm 1 Sub-questions generation using</p><formula xml:id="formula_2">Pointer c . 2 procedure GENERATESUBQ(Q : question, Pointerc) /* Find q b 1 and q b 2 for Bridging */ ind1, ind2, ind3 ? Pointer3(Q) q b 1 ? Q ind 1 :ind 3 q b 2 ? Q :ind 1 : ANS : Q ind 3 : article in Q ind 2 ?5:ind 2 ? 'which' /* Find q i 1 and q i 2 for Intersecion */ ind1, ind2 ? Pointer2(Q) s1, s2, s3 ? Q :ind 1 , Q ind 1 :ind 2 , Q ind 2 : if s2 starts with wh-word then q i 1 ? s1 : s2, q i 2 ? s2 : s3 else q i 1 ? s1 : s2, q i 2 ? s1 : s3 /* Find q c 1 , q c 2 and q c 3 for Comparison */ ind1, ind2, ind3, ind4 ?Pointer4(Q) ent1, ent2 ? Q ind 1 :ind 2 , Q ind3:ind 4 op ? find op(Q, ent1, ent2) q c 1 , q c 2 ? form subq(Q, ent1, ent2, op) q c 3 ? op (ent1, ANS) (ent2, ANS)</formula><p>highest joint probability at inference:</p><formula xml:id="formula_3">ind 1 , . . . , ind c = argmax i 1 ?????ic c j=1 P(i j = ind j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Single-hop Reading Comprehension</head><p>Given a decomposition, we use a single-hop RC model to answer each sub-question. Specifically, the goal is to obtain the answer and the evidence, given the sub-question and N paragraphs.</p><p>Here, the answer is a span from one of paragraphs, yes or no. The evidence is one of N paragraphs on which the answer is based. Any off-the-shelf RC model can be used. In this work, we use the BERT reading comprehension model <ref type="bibr">(Devlin et al., 2019)</ref> combined with the paragraph selection approach from Clark and Gardner (2018) to handle multiple paragraphs. Given N paragraphs S 1 , . . . , S N , this approach independently computes answer i and y none i from each paragraph S i , where answer i and y none i denote the answer candidate from ith paragraph and the score indicating ith paragraph does not contain the answer. The final answer is selected from the paragraph with the lowest y none i . Although this approach takes a set of multiple paragraphs as an input, it is not capable of jointly reasoning across different paragraphs.</p><p>For each paragraph S i , let U i ? R n?h be the BERT encoding of the sub-question concatenated with a paragraph S i , obtained by Equation 1. We compute four scores, y span i y yes i , y no i and y none i , indicating if the answer is a phrase in the paragraph, yes, no, or does not exist.</p><formula xml:id="formula_4">[y span i ; y yes i ; y no i ; y none i ] = max(U i )W 1 ? R 4 ,</formula><p>where max denotes a max-pooling operation across the input sequence, and W 1 ? R h?4 denotes a parameter matrix. Additionally, the model computes span i , which is defined by its start and end points start i and end i .</p><formula xml:id="formula_5">start i , end i = argmax j?k P i,start (j)P i,end (k),</formula><p>where P i,start (j) and P i,end (k) indicate the probability that the jth word is the start and the kth word is the end of the answer span, respectively. P i,start (j) and P i,end (k) are obtained by the jth element of p start i and the kth element of p end </p><formula xml:id="formula_6">i from p start i = softmax(U i W start ) ? R n (3) p end i = softmax(U i W end ) ? R n<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decomposition Scorer</head><p>Each decomposition consists of sub-questions, their answers, and evidence corresponding to a reasoning type. DECOMPRC scores decompositions and takes the answer of the top-scoring decomposition to be the final answer. The score indicates if a decomposition leads to a correct final answer to the multi-hop question.</p><p>Let t be the reasoning type, and let answer t and evidence t be the answer and the evidence from the reasoning type t. Let x denote a sequence of n words formed by the concatenation of the question, the reasoning type t, the answer answer t , and the evidence evidence t . The decomposition scorer encodes this input x using BERT to obtain U t ? R n?h similar to Equation (1). The score p t is computed as</p><formula xml:id="formula_7">p t = sigmoid(W T 2 max(U t )) ? R,</formula><p>where W 2 ? R h is a trainable matrix. During inference, the reasoning type is decided as argmax t p t . The answer corresponding to this reasoning type is chosen as the final answer.</p><p>Pipeline Approach. An alternative to the decomposition scorer is a pipeline approach, in which the reasoning type is determined in the beginning, before decomposing the question and obtaining the answers to sub-questions. Section 4.6 compares our scoring step with this approach to show the effectiveness of the decomposition scorer. Here, we briefly describe the model used for the pipeline approach.</p><p>First, we form a sequence S of n words from the question and obtainS ? R n?h from Equation 1. Then, we compute 4-dimensional vector p t by:</p><formula xml:id="formula_8">p t = softmax(W 3 max(S)) ? R 4</formula><p>where W 3 ? R h?4 is a parameter matrix. Each element of 4-dimensional vector p t indicates the reasoning type is bridging, intersection, comparison or original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HOTPOTQA</head><p>We experiment on HOTPOTQA (Yang et al., 2018), a recently introduced multi-hop RC dataset over Wikipedia articles. There are two types of questions-bridge and comparison. Note that their categorization is based on the data collection and is different from our categorization (bridging, intersection and comparison) which is based on the required reasoning type. We evaluate our model on dev and test sets in two different settings, following prior work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementations Details</head><p>Training Pointer for Decomposition. We obtain a set of 200 annotations for bridging to train To convert SQUAD to a multi-paragraph setting, we retrieve n other Wikipedia paragraphs based on TF-IDF similarity between the question and the paragraph, using Document Retriever from DrQA <ref type="bibr">(Chen et al., 2017)</ref>. We train 3 instances with n = 0, 2, 4 for an ensemble, which we use as the single-hop model.</p><p>To deal with sections/ungrammatical questions generated through our decomposition procedure, we augment the training data with ungrammatical samples. Specifically, we add noise in the question by randomly dropping tokens with probability of 5%, and replace wh -word into 'the' with probability of 5%.</p><p>Training Decomposition Scorer We create training data by making inferences for all reasoning types on HOTPOTQA medium and hard examples. We take the reasoning type that yields the correct answer as the gold reasoning type. Appendix C provides the full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Models</head><p>We compare our system DECOMPRC with the state-of-the-art on the HOTPOTQA dataset as well as strong baselines. BERT is a large, language-model-pretrained model, achieving the state-of-the-art results across many different NLP tasks <ref type="bibr">(Devlin et al., 2019)</ref>. This model is the same as our single-hop model described in Section 3.3, but trained on the entirety of HOTPOTQA. BERT-1hop train is the same model but trained on single-hop QA data without HOTPOTQA medium and hard examples. DECOMPRC-1hop train is a variant of DECOM-PRC that does not use multi-hop QA data except 400 decomposition annotations. Since there is no access to the groundtruth answers of multihop questions, a decomposition scorer cannot be trained. Therefore, a final answer is obtained based on the confidence score from the single-hop RC model, without a rescoring procedure. <ref type="table" target="#tab_5">Table 3</ref> compares the results of DECOMPRC with other baselines on the HOTPOTQA development set. We observe that DECOMPRC outperforms all baselines in both distractor and full wiki settings, outperforming the previous published result by a large margin. An interesting observation is that DECOMPRC not trained on multi-hop QA pairs (DECOMPRC-1hop train) shows reasonable performance across all data splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We also observe that BERT trained on singlehop RC achieves a high F1 score, even though it does not draw inferences across different paragraphs. For further analysis, we split the HOT-POTQA development set into single-hop solvable (Single) and single-hop non-solvable (Multi). <ref type="bibr">4</ref> We observe that DECOMPRC outperforms BERT by a large margin in single-hop non-solvable (Multi) examples. This supports our attempt toward more explainable methods for answering multihop questions.</p><p>Finally, <ref type="table" target="#tab_6">Table 4</ref> shows the F1 score on the test set for distractor setting and full wiki setting on the leaderboard. 5 These include unpublished models that are concurrent to our work. DECOMPRC achieves the best result out of models that report both distractor and full wiki setting.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluating Robustness</head><p>In order to evaluate the robustness of different methods to changes in the data distribution, we set up two adversarial settings in which the trained model remains the same but the evaluation dataset is different.</p><p>Modifying Distractor Paragraphs. We collect a new set of distractor paragraphs to evaluate if the models are robust to the change in distractors. <ref type="bibr">6</ref> In particular, we follow the same strategy as the original approach (Yang et al., 2018) using TF-IDF similarity between the question and the paragraph, but with no overlapping distractor paragraph with the original distractor paragraphs. <ref type="table" target="#tab_8">Table 5</ref> compares the F1 score of DECOMPRC and BERT in the original distractor setting and in the modified distractor setting. As expected, the performance of both methods degrade, but DE- <ref type="bibr">6</ref> We choose 8 distractor paragraphs that do not to change the groundtruth answer. COMPRC is more robust to the change in distractors. Namely, DECOMPRC-1hop train degrades much less (only 3.41 F1) compared to other approaches because it is only trained on single-hop data and therefore does not exploit the data distribution. These results confirm our hypothesis that the end-to-end model is sensitive to the change of the data and our model is more robust.</p><p>Adversarial Comparison Questions. We create an adversarial set of comparison questions by altering the original question so that the correct answer is inverted. For example, we change "Who was born earlier, Emma Bull or Virginia Woolf?" to "Who was born later, Emma Bull or Virginia Woolf?" We automatically invert 665 questions (details in Appendix D). We report the joint F1, taken as the minimum of the prediction F1 on the original and the inverted examples. <ref type="table" target="#tab_8">Table 5</ref> shows the joint F1 score of DECOMPRC and BERT. We find that DECOMPRC is robust to inverted questions, and outperforms BERT by 36.53 F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablations</head><p>Span-based vs. Free-form sub-questions. We evaluate the quality of generated sub-questions using span-based question decomposition. We replace the question decomposition component using Pointer 3 with (i) sub-question decomposition through groundtruth spans, (ii) sub-question decomposition with free-form, hand-written subquestions (examples shown in <ref type="table" target="#tab_9">Table 6</ref>).    <ref type="table" target="#tab_7">Table 7</ref>: Left: ablations in sub-questions. F1 score on a sample of 50 bridging questions from the dev set of HOTPOTQA, Pointer c is our span-based model trained with 200 or 400 annotations. Right: ablations in decomposition decision method. F1 score on the dev set of HOTPOTQA with ablating decomposition decision method. Oracle indicates that the ground truth reasoning type is selected. and sub-questions written by human. This indicates that our span-based sub-questions are as effective as free-form sub-questions. In addition, Pointer 3 trained on 200 or 400 examples obtains close to human performance. We think that identifying spans often rely on syntactic information of the question, which BERT has likely learned from language modeling. We use the model trained on 200 examples for DECOMPRC to demonstrate sample-efficiency, and expect performance improvement with more annotations.</p><p>Ablations in decomposition decision method. For comparison, we report the F1 score of the confidence-based method which chooses the decomposition with the maximum confidence score from the single-hop RC model, and the pipeline approach which independently selects the reasoning type as described in Section 3.4. In addition, we report an oracle which takes the maximum F1</p><p>Breakdown of 15 failure cases Incorrect groundtruth 1 Partial match with the groundtruth 3 Mistake from human 3 Confusing question 1 Sub-question requires cross-paragraph reasoning 2 Decomposed sub-questions miss some information 2 Answer to the first sub-question can be multiple 3 score across different reasoning types to provide an upperbound. A pipeline method gets lower F1 score than the decomposition scorer. This suggests that using more context from decomposition (e.g., the answer and the evidence) helps avoid cascading errors from the pipeline. Moreover, a gap between DECOMPRC and oracle (6.2 F1) indicates that there is still room to improve.</p><p>Q What country is the Selun located in? P1 Selun lies between the valley of Toggenburg and Lake Walenstadt in the canton of St. Gallen. P2 The canton of St. Gallen is a canton of Switzerland.</p><p>Q Which pizza chain has locations in more cities, Round Q Which magazine had more previous names, Watercolor Artist or The General? P1 Watercolor Artist, formerly Watercolor Magic, is an American bi-monthly magazine that focuses on ... P2 The General (magazine): Over the years the magazine was variously called 'The Avalon Hill General', 'Avalon Hill's General', 'The General Magazine', or simply 'General'. Q1 Watercolor Artist had how many previous names? Q2 The General had how many previous names?  <ref type="table" target="#tab_11">Table 8</ref> reports the breakdown of fifteen error cases. 53% of such cases are due to the incorrect groundtruth, partial match with the groundtruth or mistake from humans. 47% are genuine failures in the decomposition. For example, a multi-hop question "Which animal races annually for a national title as part of a post-season NCAA Division I Football Bowl Subdivision college football game?" corresponds to the last category in <ref type="table" target="#tab_11">Table 8</ref>. The question can be decomposed into "Which post-season NCAA Division I Football Bowl Subdivision college football game?" and "Which animal races annually for a national title as part of ANS?". However in the given set of paragraphs, there are multiple games that can be the answer to the first sub-question. Although only one of them is held with the animal racing, it is impossible to get the correct answer only given the first subquestion. We think that incorporating the original question along with the sub-questions can be one 8 A full set of samples is shown in Appendix E. solution to address this problem, which is partially done by a decomposition scorer in DECOMPRC.</p><p>Limitations. We show the overall limitations of DECOMPRC in <ref type="table" target="#tab_13">Table 9</ref>. First, some questions are not compositional but require implicit multihop reasoning, hence cannot be decomposed. Second, there are questions that can be decomposed but the answer for each sub-question does not exist explicitly in the text, and must instead by inferred with commonsense reasoning. Lastly, the required reasoning is sometimes beyond our reasoning types (e.g. counting or calculation). Addressing these remaining problems is a promising area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed DECOMPRC, a system for multihop RC that decomposes a multi-hop question into simpler, single-hop sub-questions. We recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions. Moreover, DECOMPRC achieved further gains from the decomposition scoring step. DECOMPRC achieved the state-of-the-art on HOTPOTQA distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines. the span, and the head-word (top four examples in <ref type="figure">Figure 2</ref>). After three clicks are all made, the annotator can see the heuristically generated subquestions. If the question type is intersection, the annotator is asked to make two clicks for the start and the end of the second segment out of three segments (bottom three examples in <ref type="figure">Figure 2</ref>). Similarly, the annotator can see the heuristically generated sub-questions after two clicks. If the question type is one-hop or neither, the annotator does not have to make any click. If the question can be decomposed into more than one way, the annotator is asked to choose the more natural decomposition. If the question is ambiguous, the annotator is asked to pass the example, and only annotate for the clear cases. For the quality control, all annotators have enough in person, one-on-one tutorial sessions and are given 100 example annotations for the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Decompotision for Comparison</head><p>In this section, we describe the decomposition procedure for comparison, which does not require any extra annotation.</p><p>Comparison requires to compare a property of two different entities, usually requiring discrete operations. We identify 10 discrete operations which sufficently cover comparison operations, shown in <ref type="table" target="#tab_15">Table 10</ref>. Based on these pre-defined discrete operations, we decompose the question through the following three steps.</p><p>First, we extract two entities under comparison. We use Pointer 4 to obtain ind 1 , . . . , ind 4 , where ind 1 and ind 2 indicate the start and the end of the first entity, and ind 3 and ind 4 indicate those of the second entity. We create a training data which each example contains the question and four points as follows: we filter out bridge questions in HOTPOTQA to leave comparison questions, extract the entities using Spacy 10 NER tagger in the question and in two supporting facts (annotated sentences in the dataset which serve as evidence to answer the question), and match them to find two entities which appear in one supporting sentence but not in the other supporting sentence.</p><p>Then, we identity the suitable discrete operation, following Algorithm 2.</p><p>Finally, we generate sub-questions according to the discrete operation. Two sub-questions are obtained for each entity.  ANS is the answer of each query, and ENT is the entity corresponding to each query. The answer of each query is shown in the right side of ?. If the question and two entities for comparison are given, queries and a discrete operation can be obtained by heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>Implementation Details. We use PyTorch (Paszke et al., 2017) on top of Hugging Face's BERT implementation. <ref type="bibr">11</ref> We tune our model from Google's pretrained BERT-BASE (lowercased) 12 , containing 12 layers of Transformers <ref type="bibr">(Vaswani et al., 2017)</ref> and a hidden dimension of 768. We optimize the objective function using Adam (Kingma and Ba, 2015) with learning rate 5 ? 10 ?5 . We lowercase the input and set the maximum sequence length |S| to 300 for models which input is both the question and the paragraph, and 50 for the models which input is the question only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Creating Inverted Binary Comparison Questions</head><p>We identify the comparison question with 7 out of 10 discrete operations (Is greater, Is smaller, 11 https://github.com/huggingface/ pytorch-pretrained-BERT 12 https://github.com/google-research/ bert Which is greater, Which is smaller, Which is true, Is equal, Not equal) can automatically be inverted. It leads to 665 inverted questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E A Set of Samples used for Ablations</head><p>A set of samples used for ablations in Section 4.6 is shown in <ref type="table">Table 11</ref>.</p><p>Algorithm 2 Algorithm for Identifying Discrete Operation. First, given two entities for comparison, the coordination and the preconjunct or the predeterminer are identified. Then, the quantitative indicator and the head entity is identified if they exist, where a set of uantitative indicators is pre-defined. In case any quantitative indicator exists, the discrete operation is determined as one of numeric operations. If there is no quantitative indicator, the discrete operation is determined as one of logical operations or string operations.</p><p>procedure <ref type="figure" target="#fig_0">FIND OPERATION(question, entity1, entity2</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall diagram of how our system works. Given the question, DECOMPRC decomposes the question via all possible reasoning types (Section 3.2). Then, each sub-question interacts with the off-the-shelf RC model and produces the answer (Section 3.3). Lastly, the decomposition scorer decides which answer will be the final answer (Section 3.4). Here, "City of New York", obtained by bridging, is determined as a final answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Distractor setting contains the question and a collection of 10 paragraphs: 2 paragraphs are provided to crowd workers to write a multi-hop question, and 8 distractor paragraphs are collected separately via TF-IDF between the question and the paragraph. The train set contains easy, medium and hard examples, where easy examples are single-hop, and medium and hard examples are multi-hop. The dev and test sets are made up of only hard examples. Full wiki setting is an open-domain setting which contains the same questions as distractor setting but does not provide the collection of paragraphs. Following Chen et al. (2017), we retrieve 30 Wikipedia paragraphs based on TF-IDF similarity between the paragraph and the question (or subquestion).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Pointer 3 , and another set of 200 annotations for intersection to train Pointer 2 , hence 400 in total. Each bridging question pairs with three points in the question, and each intersection question pairs with two points in the question. For comparison, we create training data in which each question pairs with four points (the start and end of the first entity and those of the second entity) to train Pointer 4 , requiring no extra annotation. 3 Training Single-hop RC Model. We create single-hop QA data by combining HOTPOTQA easy examples and SQuAD (Rajpurkar et al., 2016) examples to form the training data for our single-hop RC model described in Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>BiDAF is the state-of-the-art RC model on HOT-POTQA, originally from Seo et al. (2017) and implemented by Yang et al. (2018). 3 Details in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The example multi-hop questions from each category of reasoning type on HOTPOTQA. Q indicates the original, multi-hop question, while Q1, Q2 and Q3 indicate sub-questions. DECOMPRC predicts span and through Pointer c , generates sub-questions, and answers them iteratively through single-hop RC model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows ex-</cell></row><row><cell>amples of each reasoning type. On a sample of</cell></row><row><cell>200 questions from the dev set of HOTPOTQA,</cell></row><row><cell>we find that 92% of multi-hop questions belong to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Here, W start , W end ? R h are the parameter matrices. Finally, answer i is determined as one of span i , yes or no based on which of y span i , y yes i and y no i is the highest. The model is trained using questions that only require single-hop reasoning, obtained from SQUAD(Rajpurkar et al., 2016)  and easy examples of HOTPOTQA (Yang et al., 2018) (details in Section 4.2). Once trained, it is used as an offthe-shelf RC model and is never directly trained on multi-hop questions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>DECOMPRC 70.57 72.53 62.78 84.31 58.74 43.26 40.30 55.04 52.11 35.64 1hop train 61.73 61.57 62.36 79.38 46.53 39.17 35.30 54.57 50.03 29.83 BERT 67.08 69.41 57.81 82.98 53.38 38.40 34.77 52.85 46.14 31.74 1hop train 56.27 62.77 30.40 87.21 29.64 29.97 32.15 21.29 47.14 15.18</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Distractor setting</cell><cell></cell><cell></cell><cell>Full wiki setting</cell></row><row><cell></cell><cell>All</cell><cell cols="3">Bridge Comp Single Multi</cell><cell>All</cell><cell cols="2">Bridge Comp Single Multi</cell></row><row><cell>BiDAF</cell><cell cols="2">58.28 59.09 55.05</cell><cell>-</cell><cell>-</cell><cell cols="2">34.36 30.42 50.70</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>F1 scores on the dev set of HOTPOTQA in both distractor (left) and full wiki settings (right). We compare DECOMPRC (our model), BERT, and BiDAF, and variants of the models that are only trained on single-hop QA data (1hop train). Bridge and Comp indicate original splits in HOTPOTQA; Single and Multi refer to dev set splits that can be solved (or not) by all of three BERT models trained on single-hop QA data.</figDesc><table><row><cell>Model</cell><cell cols="2">Dist F1 Open F1</cell></row><row><cell>DECOMPRC</cell><cell>69.63</cell><cell>40.65</cell></row><row><cell>Cognitive Graph</cell><cell>-</cell><cell>48.87</cell></row><row><cell>BERT Plus</cell><cell>69.76</cell><cell>-</cell></row><row><cell>MultiQA</cell><cell>-</cell><cell>40.23</cell></row><row><cell>DFGN+BERT</cell><cell>68.49</cell><cell>-</cell></row><row><cell>QFE</cell><cell>68.06</cell><cell>38.06</cell></row><row><cell>GRN</cell><cell>66.71</cell><cell>36.48</cell></row><row><cell>BiDAF</cell><cell>59.02</cell><cell>32.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>F1 score on the test set of HOTPOTQA distrac- tor and full wiki setting. All numbers from the official leaderboard. All models except BiDAF are concurrent work (not published). DECOMPRC achieves the best result out of models reported to both distractor and full wiki setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>(left) compares the question answer-</cell></row><row><cell>ing performance of DECOMPRC when replaced</cell></row><row><cell>with alternative sub-questions on a sample of</cell></row><row><cell>50 bridging questions. 7 There is little differ-</cell></row><row><cell>ence in model performance between span-based</cell></row><row><cell>7 A full set of samples is shown in Appendix E.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Left: modifying distractor paragraphs. F1 score on the original dev set and the new dev set made up with a different set of distractor paragraphs. DECOMPRC is our model and DECOMPRC-1hop train is DECOM-PRC trained on only single-hop QA data and 400 decomposition annotations. BERT and BERT-1hop train are the baseline models, trained on HOTPOTQA and single-hop data, respectively. Right: adversarial comparison questions. F1 score on a subset of binary comparison questions. Orig F1, Inv F1 and Joint F1 indicate F1 score on the original example, the inverted example and the joint of two (example-wise minimum of two), respectively.</figDesc><table><row><cell>Question</cell><cell>Robert Smith founded the multinational company headquartered in what city?</cell></row><row><cell>Span-based</cell><cell>Q1: Robert Smith founded which multinational company? Q2: ANS headquartered in what city?</cell></row><row><cell>Free-form</cell><cell>Q1: Which multinational company was founded by Robert Smith? Q2: Which city contains a headquarter of ANS?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>An example of the original question, span-based human-annotated sub-questions and free-form humanauthored sub-questions.</figDesc><table><row><cell>Sub-questions</cell><cell>F1</cell><cell>Decomposition decision method</cell><cell>F1</cell></row><row><cell cols="2">Span (Pointerc trained on 200) 65.44</cell><cell>Confidence-based</cell><cell>61.73</cell></row><row><cell cols="2">Span (Pointerc trained on 400) 69.44</cell><cell>Pipeline</cell><cell>63.59</cell></row><row><cell>Span (human)</cell><cell>70.41</cell><cell>Decomposition scorer (DECOMPRC)</cell><cell>70.57</cell></row><row><cell>Free-form (human)</cell><cell>70.76</cell><cell>Oracle</cell><cell>76.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 (</head><label>7</label><figDesc>right) compares different ablations to evaluate the effect of the decomposition scorer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>The error analyses of human experiment, where the upperbound F1 score of span-based subquestions with no decomposition scorer is measured.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table</head><label></label><figDesc>Pizza or Marion's Piazza? P1 Round Table Pizza is a large chain of pizza parlors in the western United States. P2 Marion's Piazza ... the company currently operates 9 restaurants throughout the greater Dayton area. Q1 Round Table Pizza has locations in how many cities? Q2 Marion 's Piazza has locations in how many cities?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>The failure cases of DECOMPRC, where Q, P1 and P2 indicate the given question and paragraphs, and Q1 and Q2 indicate sub-questions from DECOMPRC. (Top) The required multi-hop reasoning is implicit, and the question cannot be decomposed. (Middle) DECOMPRC decomposes the question well but fails to answer the first sub-question because there is no explicit answer. (Bottom) DECOMPRC is incapable of counting.</figDesc><table><row><cell>Upperbound of Span-based Sub-questions</cell></row><row><cell>without a decomposition scorer. To measure</cell></row><row><cell>an upperbound of span-based sub-questions with-</cell></row><row><cell>out a decomposition scorer, where a human-level</cell></row><row><cell>RC model is assumed, we conduct a human</cell></row><row><cell>experiment on a sample of 50 bridging ques-</cell></row><row><cell>tions. 8 In this experiment, humans are given each</cell></row><row><cell>sub-question from decomposition annotations and</cell></row><row><cell>are asked to answer it without an access to the</cell></row><row><cell>original, multi-hop question. They are asked to</cell></row><row><cell>answer each sub-question with no cross-paragraph</cell></row><row><cell>reasoning, and mark it as a failure case if it is</cell></row><row><cell>impossible. The resulting F1 score, calculated by</cell></row><row><cell>replacing RC model to humans, is 72.67 F1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Is greater (ANS) (ANS) ? yes or no Is smaller (ANS) (ANS) ? yes or no Which is greater (ENT, ANS) (ENT, ANS) ? ENT Which is smaller (ENT, ANS) (ENT, ANS) ? ENT Did the Battle of Stones River occur before the Battle of Saipan? Q1: The Battle of Stones River occur when? ? 1862 Q2: The Battle of Saipan River occur when? ? 1944 Q3: Is smaller (the Battle of Stones River, 1862) (the Battle of Saipan, 1944) ? yes</figDesc><table><row><cell>Operation &amp; Example</cell></row><row><cell>Type: Numeric</cell></row><row><cell>Type: Logical</cell></row><row><cell>And (ANS) (ANS) ? yes or no</cell></row><row><cell>Or (ANS) (ANS) ? yes or no</cell></row><row><cell>Which is true (ENT, ANS) (ENT, ANS) ? ENT</cell></row><row><cell>In between Atsushi Ogata and Ralpha Smart who graduated from Harvard College?</cell></row><row><cell>Q1: Atsushi Ogata graduated from Harvard College? ? yes</cell></row><row><cell>Q2: Ralpha Smart graduated from Harvard College? ? no</cell></row><row><cell>Q3: Which is true (Atsushi Ogata, yes) (Ralpha Smart, no) ? Atsushi Ogata</cell></row><row><cell>Type: String</cell></row><row><cell>Is equal (ANS) (ANS) ? yes or no</cell></row><row><cell>Not equal (ANS) (ANS) ? yes or no</cell></row><row><cell>Intersection (ANS) (ANS) ? string</cell></row><row><cell>Are Cardinal Health and Kansas City Southern located in the same state?</cell></row><row><cell>Q1: Cardinal Health located in which state? ? Ohio</cell></row><row><cell>Q2: Cardinal Health located in which state? ? Missouri</cell></row><row><cell>Q3: Is equal (Ohio) (Missouri) ? no</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>A set of discrete operations proposed for comparison questions, along with the example on each type.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>) coordination, preconjunct ? f (question, entity1, entity2) Determine if the question is either question or both question from coordination and preconjunct head entity ? f head (question, entity1, entity2) if more, most, later, last, latest, longer, larger, younger, newer, taller, higher in question then if head entity exists then discrete operation ? Which is greater else discrete operation ? Is greater else if less, earlier, earliest, first, shorter, smaller, older, closer in question then if head entity exists then discrete operation ? Which is smaller else discrete operation ? Is smaller else if head entity exists then discrete operation ? Which is true else if question is not yes/no question and asks for the property in common then discrete operation ? Intersection else if question is yes/no question then Determine if question asks for logical comparison or string comparison if question asks for logical comparison then if either question then discrete operation ? Or else if both question then discrete operation ? And else if question asks for string comparison then if asks for same? then discrete operation ? Is equal else if asks for difference? then discrete operation ? Not equal return discrete operation</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We consider an example to be solvable if all of three models of the BERT-1hop train ensemble obtains nonnegative F1. This leads to 3426 single-hop solvable and 3979 single-hop non-solvable examples out of 7405 development examples, respectively. 5 Retrieved on March 4th 2019 from https://https: //hotpotqa.github.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Note that we exclude comparison questions for annotations, since comparison questions are already labeled on HOTPOTQA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://spacy.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), NSF (IIS 1616112, IIS 1252835, IIS 1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, and Amazon.</p><p>We thank the anonymous reviewers and UW NLP members for their thoughtful comments and discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Span Annotation In this section, we describe span annotation collection procedure for bridging and intersection questions.</p><p>The goal is to collect three points (bridging) or two points (intersection) given a multi-hop question. We design an interface to annotate span over the question by clicking the word in the question. First, given a question, the annotator is asked to identify which reasoning type out of bridging, intersection, one-hop and neither is the most proper. 9 Since bridging type is the most common, bridging is checked by default. If the question type is bridging, the annotator is asked to make three clicks for the start of the span, the end of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Question IDs from a set of samples used for ablations</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>in Section 4.6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
