<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focus on Local: Detecting Lane Marker from Bottom Up via Key Point</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Qu</surname></persName>
							<email>quzhan@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Jin</surname></persName>
							<email>jinhuan3@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<email>zhouyang116@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<email>yang.zhen@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang Noah&amp;apos;s Ark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focus on Local: Detecting Lane Marker from Bottom Up via Key Point</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mainstream lane marker detection methods are implemented by predicting the overall structure and deriving parametric curves through post-processing. Complex lane line shapes require high-dimensional output of CNNs to model global structures, which further increases the demand for model capacity and training data. In contrast, the locality of a lane marker has finite geometric variations and spatial coverage. We propose a novel lane marker detection solution, FOLOLane, that focuses on modeling local patterns and achieving prediction of global structures in a bottom-up manner. Specifically, the CNN models lowcomplexity local patterns with two separate heads, the first one predicts the existence of key points, and the second refines the location of key points in the local range and correlates key points of the same lane line. The locality of the task is consistent with the limited FOV of the feature in CNN, which in turn leads to more stable training and better generalization. In addition, an efficiency-oriented decoding algorithm was proposed as well as a greedy one, which achieving 36% runtime gains at the cost of negligible performance degradation. Both of the two decoders integrated local information into the global geometry of lane markers. In the absence of a complex network architecture design, the proposed method greatly outperforms all existing methods on public datasets while achieving the best state-of-the-art results and real-time processing simultaneously.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In autonomous driving system (ADS), lane detection plays an important role. On the one hand, the location of host and other traffic participants in the lane forms the basis of autonomous driving decisions. On the other hand, the geometry of a lane marker can be viewed as an important landmark of the environment and aligned with a highresolution or vector map for high-precision positioning. At the same time, lane detection has been widely used in Ad-* Z. Qu is the corresponding author.</p><p>vanced Driver Assistance Systems (ADAS) and is the basis for some common features such as Lane Keep Assist (LKA) and Adaptive Cruise Control (ACC).</p><p>Recent advances in lane detection can be attributed to the development of convolutional neural networks (CNN). Most existing methods adopt well-studied frameworks such as semantic segmentation and object detection to parse lane markers and transform the network output into parametric curves through post-processing. However, the mostly used frameworks can not be seamlessly generalized to curvedshaped lane lines because lane detection task requires precise representation of local positions and global shapes simultaneously, showing their own limitations.</p><p>The semantic segmentation-based approach predicts binary masks of lane marker regions, inserts clustering models into training and inference, groups masked pixels into individual instances, and finally uses curve fitting to parametric results. However, the clustering procedure complicates the training and inference pipeline. In addition, pixellevel inputs to curve fitting are often redundant and noisy, all of which bring negative impact to the accuracy of the final results. <ref type="figure" target="#fig_1">Fig.1(a)</ref> shows several cases where the prediction errors may increase. Object detection approaches are originally designed for compact target and produce bounding box as output, which is insensitive to pixel-level error when faced with large-scale object. As for lane markers, they typically span half or more of the image, and pixel-level localization errors significantly impair detection performance, which can be attributed to the limited field of view (FOV) of features learned through CNN being insufficient to model content that is too far apart. <ref type="figure" target="#fig_1">Fig.1(b)</ref> illustrates the effect of FOV in complex scenario. Moreover, most of these solutions model global geometry directly, and the network must produce high-dimensional outputs to describe the curves. Theoretically, however, uncompact outputs increase the demand for data and model capacity, ultimately masking the generalization ability of the resulting model.</p><p>Although the global structure of lane markers has some complexity, we note that local lane markers are extremely simple and that global lane markers can be approximated by a combination of local line segments. Moreover, spa-(a) Segmentation based method and intermediate results.  tial locality is more suitable for modeling with CNN. Following this intuition, a novel lane marker detection method, FOLOLane, is proposed that focuses on modeling local geometry and integrating them into the global results in a bottom-up manner. Specifically, the geometry of the lane marker is predicted by estimating adjacent keypoints on the it. In the bottom stage, a fully convolutional network is used to capture keypoints in the local scope through two separate heads. The first one gives the probability that keypoints appear in pixel space, and the second one gives the offset between keypoints and the most spatially correlated local lane marker, which is used to refine the positions of keypoints generated by the first head and construct associations between keypoints on the same lane markers. Based on the local information, two decoding algorithms with different preferences are proposed to predict global geometry of lane markers. The bottom-up pipeline of the proposed method is shown in <ref type="figure" target="#fig_2">Fig.2</ref>.</p><p>Compared with existing works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>, the proposed approach concentrates the capabilities of CNN on a local scale, which is suitable for CNN's limited FOV, and significantly reduces the complexity of the task and the dimension of the output. As a result, the compact output leads to stable and efficient training without additional effort in network architecture design and data collection. Considering the continuity of lane markers, the proposed decoder is able to associate keypoints of the same instance and optimize the geometry of network predictions without affecting performance and efficiency. Furthermore, during network training and instance decoding, we model and predict keypoints using features with the highest spatial correlation guided by coarse-to-fine strategies. The proposed bottom-up solution achieves the best state-of-the-art level, Acc: 96.92% on TuSimple and F1 score: 78.8% on CULane, and excellent generalization in the two public datasets. Together with the compatibility with network architectures, our approach shows a promising application future.</p><p>We emphasize that our method is the first to formulate lane detection into multi-key-points estimation and association problem, which is inspired by the bottom-up human pose estimation framework <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>. The proposed local scope based method avoids the inaccurate prediction where far from the anchor, which occurs in detection-based methods. And the sparsity of key points prevents the noisy and redundant output occurred in segmentation-based methods, which decrease the precision and increase the delay of curve fitting. With extensive experiments, our solution proves the potential of applying pose estimation approaches on lane detection, which opens up a new direction to solve this important application problem. Our solution does not depend on CNN architecture, is readily compatible to newly developed architecture and shows scalable potential on accuracy and efficiency.</p><p>Our contributions can be summarized as follows:</p><p>? Lane detection is firstly decompose into subtasks of modelling local geometry, which is achieved by estimating keypoints on local curve. Simplified targets and focus on spatially limited scope helps the network to provide precise estimation of local curve.</p><p>? Two decoding algorithms with different preferences are designed to integrate local information into global prediction, which enable the system to achieve high accuracy in ultra real time.</p><p>? Experimental results showed that our approach outperforms all existing methods by a substantial margin. Besides, our model shows the best generalization ability in comparison, which further proves the potential for productization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Lane Marker Detection. Lane marker detection based on deep learning can be categorized into two groups: detection based and segmentation based. The former one: <ref type="bibr" target="#b1">[2]</ref> proposed an anchor-based lane marker detection model for forward-looking cameras. Lane markers were uniformly sampled along the vertical axis in the image, and dense regression was performed by predicting the offset between each sample point and an anchor line, then Non-Maximum Suppression(NMS) was applied to suppress the overlapping detection and select the best lane marker with the highest score. <ref type="bibr" target="#b18">[19]</ref> proposed the use of neural architecture search(NAS) to find a better backbone and a point blending based post processing to further improve the performance of lane marker detection task. <ref type="bibr" target="#b5">[6]</ref> proposed to train a CNN to predict the existence, position and feature embedding of lane markers in an image. A lane marker instance was clustered based on the trained feature embedding. <ref type="bibr" target="#b15">[16]</ref> formulated lane marker detection as a pixel-wise classification problem for each row of an image. A specific feature map was predicted to indicate the position of a lane marker on each row.</p><p>Segmentation based: <ref type="bibr" target="#b7">[8]</ref> proposed a multitask framework, which predicted pixel-wise multi-label and clustered the pixels belonging to same lane instance in bird eye view image using DBSCAN. It also added an auxiliary task: vanish point estimation, to increase the stability of lane marker detection. <ref type="bibr" target="#b9">[10]</ref> proposed an end-to-end joint semantic segmentation and feature embedding network architecture. Pixels on the same lane marker were assigned an identical instance id. <ref type="bibr" target="#b11">[12]</ref> also designed an instance segmentation network for lane marker detection problem. Different from <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> predicted a probability map for each lane marker separately and used cubic splines to fit it. In stead of using pixel-wise classification, <ref type="bibr" target="#b19">[20]</ref> introduced a row-wise classification architecture. For each row, it predicted the most possible grid of a lane marker in an image and recovered a lane marker instance through post processing. <ref type="bibr" target="#b8">[9]</ref> proposed a CycleGAN based method to enhance lane detection performance in low light conditions. <ref type="bibr" target="#b3">[4]</ref> claimed a more accurate method by using EL-GAN for lane marker detection, which used a generator to segment the lane markers and a discriminator to refine the segmentation result. <ref type="bibr" target="#b4">[5]</ref> proposed a self-attention distillation method for lane marker segmentation task by forcing shallow layers to learn rich context feature from deep layers.</p><p>Bottom-Up Human Key Point Detection. <ref type="bibr" target="#b14">[15]</ref> proposed a bottom-up method for crowded scenes, which detected keypoints and built a densely connected graph, the weight of each edge represented the correlation of two keypoints. By optimizing the graph, keypoints belonging to one person were clustered. <ref type="bibr" target="#b0">[1]</ref> predicted a heat map for each keypoint and part affinity fields (PAFs) which were used to associate body parts with individuals in the image. Similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>, <ref type="bibr" target="#b10">[11]</ref> introduced feature embedding to facilitate keypoints clustering of one person while predicting the heat map of keypoints. <ref type="bibr" target="#b12">[13]</ref> further split the problem into two stages: (1) predicting heat map and short-range offset for keypoints detection, <ref type="bibr" target="#b1">(2)</ref> clustering key points using midrange offset for one person.</p><p>We find that lane marker detection can be abstracted as discrete keypoints detection and association problem, which is very similar to bottom-up human key point detection task. <ref type="bibr" target="#b13">[14]</ref> proposed a method based on this idea. A network was trained to extract all possible lane marker pixels and output the pixels in the neighboring row, which belongs to the same lane as the current lane marker pixel. As the problems discussed above, the inherent segmentation-based method inhibited the precise representation of a lane marker. In addition, the pixel-wise joint distribution prediction was redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As shown in <ref type="figure" target="#fig_2">Fig.2</ref>, we proposed a bottom-up lane detection method by estimating the existence and the offsets of the local lane point through the network, followed by a novel global geometry decoder to generate the final curve instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network for local geometry</head><p>In the proposed approach, each predicted marker curve is represented as an ordered keypoints set, where the key points are of fixed/predefined vertical interval ?y across neighboring rows. First of all, the task of curve prediction is decomposed into local subtasks via a fully convolutional network with two heads. The heatmap outputted by the first head expresses the possibility that keypoint appears, which resolves the existence of local curves. The second head predicts offsets to key points of the most closed local curve, which describes the precise geometry of the local curve.</p><p>Key point estimation. Motivated by a curve constituted of points, we adopt a keypoint-estimation-based framework. The network firstly outputs a heatmap with the same resolution as input, which models the probability that pixel is a keypoint of the curve. In the training phase, the points set as annotation of the j ? th curve are interpolated to be continuous in pixel space as l j . Each pixel of the curve l j is considered as a key point and yields ground-truth value for neighbors via unnormalized Gaussian kernel. The standard deviation ? h depends on the scale of input, and if the ground-truth value of some pixel is assigned by multiple keypoints, the maximum will be kept.</p><p>To deal with the class imbalance problem coming with the sparsity of key points, we employ penalty-reduced focal loss for this head as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, where only pixels with ground truth equal to 1 are considered positive and all others are negative. The penalty from negative pixels arises with the distance to positive, which helps to reduce the influence of ambiguity. We denote the output of i?th pixel at heatmap as s i and the ground-truth value assigned by Gaussian Kernel as g i . Define penalty coefficients? i and? i as:</p><formula xml:id="formula_0">g i = 0 if g i = 1 g i otherwise ,? i = s i if g i = 1 1 ? s i otherwise ,<label>(1)</label></formula><p>and the loss function for heatmap head is constructed as:</p><formula xml:id="formula_1">Loss h = ? 1 N N i (1 ?? i ) ? (1 ?? i ) ? log(? i ),<label>(2)</label></formula><p>where ? and ? are tunable hyperparameters, controlling the penalty reduction for ambiguous and simple samples respectively. N is the number of key points in the current image.</p><p>Compared with segmentation-based methods, the loss function Eq.2 guides the network to learn positive and negative samples of keypoint with reduced supervision from the total pixels, prompting pixels best suited for expressing geometry to the response. An example of the heatmap can be found in <ref type="figure" target="#fig_2">Fig.2</ref> as the first output of the network, the center of lane marker responses highest, and the neighborhood became colder gradually, which helps prevent the noise and redundancy from propagating to subsequent procedures as well.</p><p>Local geometry construction. For precise geometry, the second head of the network regresses a vector [?x ? (p), ?x ? (p), ?x ? (p)] T , describing the local geometry of the closest curve to pixel p. The elements indicate the horizontal offsets to 3 neighboring key points with fixed vertical interval ?y, which have been colorized for visualization in <ref type="figure" target="#fig_2">Fig.2</ref>. Given the vector, we can simply recover the local curve related to pixel p:</p><formula xml:id="formula_2">l(p) = ? ?p ? (p) p ? (p) p ? (p) ? ? = p + ? ? ?x ? (p) ??y ?x ? (p) 0 ?x ? (p) ?y ? ? ,<label>(3)</label></formula><p>wherep?(p),p?(p) andp?(p) denote the actual location with fixed vertical interval ?y to pixel p, respectively. In the training phase, all pixels within a fixed distance from key points of the curve l, N ?g(l) , are taken to compute loss for ?x ? , ?x ? .</p><formula xml:id="formula_3">Loss ? (l) = 1 |N ?g (l)| ? p?N? g (l) ||p ? (p) ? ?(l, f y (p) ? ?y)|| 1 , Loss ? (l) = 1 |N ?g (l)| ? p?N? g (l) ||p ? (p) ? ?(l, f y (p) + ?y)|| 1 ,<label>(4)</label></formula><p>where f y (?) denotes the function retrieving vertical coordinate of the pixel, ?(l, y) is function retrieving horizontal coordinate of curve l on specific row y.</p><p>For ?x ? , a coarse-to-fine strategy is employed:</p><formula xml:id="formula_4">Loss ? (l) = 1 2|N ?g (l)| ? p?N? g (l) ( ||p ? ((p ? (p))) ? ?(l, f y (p) ? ?y)|| 1 + ||p ? ((p ? (p))) ? ?(l, f y (p) + ?y)|| 1 ),<label>(5)</label></formula><p>where the training pixels come from the decoded prediction of ?x ? and ?x ? in Eq.4, which is used to compensate for the error in predicting ?x ? and ?x ? and keeps in line with the coarse-to-fine behavior in the decoding stage. L1 loss is employed for all the regression terms. Network architecture. To justify the effectiveness of focusing on local geometry, we adopt light-weight architecture ERFNet <ref type="bibr" target="#b16">[17]</ref> and BiSeNet <ref type="bibr" target="#b20">[21]</ref>, which were originally designed for semantic segmentation on mobile devices. During the feature extraction, the encoder abstracts image into downsampled feature map, then the decoder broadcasts the high-level semantics to the same resolution as input. All 4 logits are yielded by the last block of the decoder for saving memory. Most experiments in this paper are performed basing on ERFNet. Since the method is designed for working in real traffic scenarios, which is required to handle the case of a merged or split marker and any number of instances, there is no extra branch specialized for predefined lane markers as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref>. The final cost function is formulated as</p><formula xml:id="formula_5">Loss = Loss h + ?(Loss ? + Loss ? + Loss ? ),<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoder for global geometry</head><p>In the above section, CNN produces pixel-wise heatmap and offset for keypoints in local scope. These local information are subsequently integrated into prediction of global curve. Specifically, the heatmap is used to determine emergence and termination of curve. The offsets is used to associate keypoints on same curve instance and refine geometry further. To this end, we propose two novel and simple algorithms for decoding the output of CNN under different demand scenarios, which responds to preferences for accuracy and efficiency respectively.</p><p>Greedy decoder works through iteratively extending the neighbors of keypoint in a greedy search-like manner. For each input image,  Step1 Find the row containing greatest number of local maximum response on heatmap. This row and the points are taken as starting line and current keypoints.</p><p>Step2 Refine the position of current keypoints. For point p, refinement can be formulated asp = p + [?x ? (p), 0] T .</p><p>Step3 Explore the vertical neighbors of current points, the coordinates of which can be computed as p ? =p + [?x ? (p), ??y] T and p ? =p + [?x ? (p), ?y] T .</p><p>Step4 Examine the heatmap value of p ? and p ? . If the value reaches threshold ? h , the corresponding neighboring points is used to update current keypoint, and</p><p>Step2?4 are repeated. Otherwise the search is terminated, all the points searched from one single point are taken as one global curve.</p><p>To sum up, the decoding algorithm gradually extends the global curve by exploring neighbors of keypoint, and refine the geometry of curve in a coarse-to-fine manner. This algorithm can produce precise geometry of curve, but its low efficiency limits the useability in practical application. The process have been shown in color in <ref type="figure" target="#fig_4">Fig.4</ref>. Efficient decoder is proposed in order to solve the inefficiency problem of greedy decoders, which utilizes the parallelism of computing devices. For each image,</p><p>Step1 Extract rows at equal interval ?y on heatmap. On each row, take the points with local maximum response as current keypoints.</p><p>Step2 For each keypoint p, compute three related points as</p><formula xml:id="formula_6">p ? = p + [?x ? (p), 0] T , p ? = p + [?x ? (p), ??y] T and p ? = p + [?x ? (p), ?y] T .</formula><p>Step3 Construct association among current keypoints located in neighboring rows. For a point p in i-th row, two points in (i ? ?y)-th row and (i + ?y)-th row will be associated with it, which are closest to the position of p ? and p ? respectively.</p><p>Step4 Starting with the row with maximum number of current keypoints. According to the association relationship created in Step3, for each current keypoint, all the keypoints associated with it in above/below rows are iteratively taken out as a single group. Each keypoint group is considered as a global curve, and p ? of points are used to refine geometry of curve further.</p><p>The efficient decoding algorithm leverages the parallel computing power of device, to create association among keypoints and refine their position, from step1 to step3. Step4 involves only index operations, thus the time overhead is very low. The process have been shown in color in <ref type="figure" target="#fig_3">Fig.3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, firstly we describe the implementation details and evaluation datasets. Followed by the results of comparison with the state-of-the-art, including quantitative and qualitative results. Finally, the discussion of ablation study and generalization are detailed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) FP FN SCNN <ref type="bibr" target="#b11">[12]</ref> 96.53 0.0617 0.0180 LaneNet(+H-Net) <ref type="bibr" target="#b9">[10]</ref> 96.40 0.0780 0.0244 EL-GAN <ref type="bibr" target="#b3">[4]</ref> 96.39 0.0412 0.0336 PointLaneNet <ref type="bibr" target="#b1">[2]</ref> 96.34 0.0467 0.0518 FastDraw <ref type="bibr" target="#b13">[14]</ref> 95.2 0.0760 0.0450 ENet-SAD <ref type="bibr" target="#b4">[5]</ref> 96.64 0.0602 0.0205 ERFNet-E2E <ref type="bibr" target="#b19">[20]</ref> 96.02 0.0321 0.0428 PINet(4H) <ref type="bibr" target="#b5">[6]</ref> 96.75 0.0310 0.0250 FOLOLane(ours) 96.92 0.0447 0.0228 <ref type="table">Table 3</ref>. Performance of different methods on TuSimple testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We first resized the width of an image to 976 and kept the aspect ratio on both datasets. The ?y was set as 10 pixels for a trade-off between precision and efficiency. The weight ? for loss function in Eq. <ref type="bibr" target="#b5">[6]</ref> was set as 0.02. For optimization, we used Adam optimizer and poly learning rate schedule with an initial learning rate of 0.001. Each mini-batch contained 16 images per GPU and we trained the model using 8 V-100 GPUs for 40 epochs on CULane and 200 epochs on TuSimple, respectively. To reduce overfitting, we used a 0.3 probability of dropout and weight decay with 0.0001. Furthermore, we also applied data augmentation, including random scaling, cropping, horizontal flipping, random rotation, and color jittering, which have been proved to be effective. In the testing phase, we set the threshold of lane existence confidence as 0.5.</p><p>As illustrated in <ref type="table" target="#tab_0">Table 1</ref>, the basic information of TuSim-ple and CULane datasets are detailed. And for evaluation criteria, we follow the official metric used in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>In this section, we show the results on two lane detection datasets. In all experiments, ERFNet <ref type="bibr" target="#b16">[17]</ref> is used as our baseline network if not specially mentioned.</p><p>Quantitative results. To verify the effectiveness of our proposed method, we compared it with state-of-the-art algorithms based on either segmentation or object detection, including SCNN <ref type="bibr" target="#b11">[12]</ref>, LaneNet(+H-Net) <ref type="bibr" target="#b9">[10]</ref>, EL-GAN <ref type="bibr" target="#b3">[4]</ref>, PointLaneNet <ref type="bibr" target="#b1">[2]</ref>, FastDraw <ref type="bibr" target="#b13">[14]</ref>, ENet-SAD <ref type="bibr" target="#b4">[5]</ref>, ERFNet-E2E <ref type="bibr" target="#b19">[20]</ref>, SIM-CycleGAN+ERFNet <ref type="bibr" target="#b8">[9]</ref>, UFNet <ref type="bibr" target="#b15">[16]</ref> and PINet <ref type="bibr" target="#b5">[6]</ref>.</p><p>As illustrated in <ref type="table">Table 2</ref>, the proposed method achieves a new SOTA result on the CULane testing set with a 78.8 F1 measure. Compared with the best model as far as we know, PINet(4H), our method outperforms almost all of the scenarios, whose F1 measure improves 4.4%. Because of local occlusions and fogged traffic lines, PINet shows degraded performance in some categories, such as Crowded, Arrow and Curve. Although our method and PINet are both based on key points estimation, in the aforementioned categories, our method outperforms PINet with 5.5%, 5.3%, and 3.8% F1 measure improvements respectively, which indicates our local geometry modeling model and bottom-up pipeline have better lane marker representation capabilities. Besides, an interesting point is that SIM-CycleGAN+ERFNet, which aims at dealing with low light conditions using CycleGAN, is not comparable to our lane marker detection model in the night and dazzle light scenarios, which implies that our approach is of better generalization ability even than GAN augmented data.</p><p>The results of different methods on the TuSimple testing set are shown in <ref type="table">Table 3</ref>. Due to the limited scale (train/test:3.3k/2.8k) and homogeneous scenario (highway), most methods achieved near-saturated accuracy (more than 96%). Despite this, our method still outperforms the 2nd by 0.17%, close to the difference between 2nd and 4th.</p><p>Qualitative results.</p><p>We also show qualitative results of the proposed method and SCNN, SIM-CycleGAN+ERFNet, UFNet, PINet on the CULane testing set. As shown in <ref type="figure" target="#fig_5">Fig.5</ref>, our method focusing on local geometry and bottom-up strategy helps to distinguish the occlusion of crowded roads and the missing lane marker clues. Through keypoint estimation, the proposed method could yield a smoother and more accurate curve than the others do. Even though in night and dazzle light scenarios, the predicted results are still satisfactory. In conclusion, the proposed method leads to visible improvements in lane marker detection among recently developed segmentationbased and regression-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To investigate the effects of the locally based designs, an ablation study is carried out on the CULane dataset. The experiments are all conducted with the same settings as described in Sec. 4.1 if not specially mentioned.</p><p>Key point estimation. Different from segmentationbased solutions, our key point estimation method focuses on the center of the lane marker, which achieves a impressive result. the F1 measure from 74.2 to 76.6, which indicates that the suppression of ambiguous and noisy pixels helps achieve accurate geometry and fewer false positives, improving the performance of a system in turn.</p><p>Coarse-to-fine geometry refinement. During both network training and instance decoding, we adopt a coarse-tofine geometry refinement for a more accurate position of key points. In the training phase, the training pixels come from the decoded prediction of ?x ? and ?x ? . In the inference phase, The predicted ?x ? is employed to refine the position of initial key points and newly explored neighboring key points. The results of different configurations are shown in <ref type="table">Table 4</ref>. Only using coarse-to-fine in inference improves the F1-measure 0.9%. When coarse-to-fine is extended to training, the performance outperforms that of uni- form sampling in N ?g (l) significantly by 1.3%. The result shows that the direct prediction leads to suboptimal position estimation and our coarse-to-fine strategy could guide the spatially most related representation to capture the geometry of the curve and achieve a more accurate prediction.</p><p>Efficiency-oriented implementation. As mentioned in Sec. 3.2, efficient decoding is aimed at real-time processing. The main difference from a greedy decoder is that the iteration of decoding neighboring key points is replaced by parallel processing. The parallel decoding significantly improves the efficiency, which achieves 16 ms (64%) runtime gains than greedy decoder at the cost of 0.8% performance degradation. The reason can be attributed to the lack of local optimal estimation in each iteration of greedy decoding.</p><p>To maximize efficiency for application, we further replace the basic network from ERFNet to BiSeNet, which is a real-time semantic segmentation network originally designed for mobile devices. Since the output of BiSeNet is 8 times downsampled from the input size, real-time performance is achieved by reaching more than 100 fps and 77.5 F1 measure simultaneously, which is still the best stateof-the-art results excluding the accuracy-oriented version of our approach. On the other side, the experiment also proves the compatibility of the proposed system, which can be readily adapted for more powerful and efficient network architectures up to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalization</head><p>To further verify the generalization of our proposed method, we employ the checkpoint trained from the CU-Lane training set to inference on the TuSimple testing set. To our knowledge, this is the first attempt to investigate the generalization between these two widely used datasets. <ref type="table">Table 5</ref> shows that the proposed method achieves obvious superiority with an accuracy of 84.36%, which surpasses other methods by a significant margin of nearly 20%. The SCNN and PINet(4H) approaches suffer most from the generalization ability, which decreases 90% and 60% respectively. The generalized visualization results on the TuSimple testing set are shown in <ref type="figure" target="#fig_6">Fig.6</ref>. This result indicates that the simplified task and the compact output of the network reduce the demand for model capacity and training data, the resulting stableness and efficiency in training finally lead to advantageous generalization to other domains, which shows promising potential for application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) FP FN SCNN <ref type="bibr" target="#b11">[12]</ref> 0.29 0.0068 1.0 SIM-CycleGAN+ERFNet <ref type="bibr" target="#b8">[9]</ref> 62.58 0.9886 0.9909 UFNet <ref type="bibr" target="#b15">[16]</ref> 65.53 0.5680 0.6546 PINet(4H) <ref type="bibr" target="#b5">[6]</ref> 36.31 0.4886 0.8988 FOLOLane(ours) 84.36 0.3964 0.3841 <ref type="table">Table 5</ref>. Evaluation of generalization ability of different methods from CULane training set to TuSimple testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we propose a local-based bottom-up solution for lane detection. Experimental results show the keypoint estimation and the coarse-to-fine refinement strategy circumvent the influence from ambiguous and noisy pixels, effectively improves the accuracy of curve geometry. More importantly, the principle of focusing on local geometry and the bottom-up pipeline have been proved to be particularly resultful, which significantly simplifies the task by reducing the dimension of the output of CNN and is believed to be the principal cause of the excellent performance and generalization capacity.</p><p>The proposed method also shows superiority in adaptation to the rapid evolution of neural networks for performance and efficiency. We have plan to incorporate more powerful architectures into FOLOLane framework, e.g. the ones with self-attention mechanism, to improve the performance further. We also want to use FOLOLane on MindSpore 1 , which is a new deep learning computing framework. These problems are left for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Object detection based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Pipeline of existing methods and illustration of common error prediction. (a). the enlarged window in the middle of the pipeline shows the incorrect clustering of the segmentation mask, and the other two orange windows in the curve fitting output show the position deviation of the prediction curve due to redundant and noisy pixels, note the solid line is ground truth, dotted line is prediction. (b). shows the influence of FOV in detection based methods, the brightness of the right image reflects the practical FOV of anchor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The inference process of FOLOLane. The network produces 4 logits expressing the geometry of the local curve. The decoder module constituted with low-level operators integrates the local information into curve instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of greedy decoding process. All the keypoints found in process have been shown in color. The colored arrows indicate the refinement of position of keypoints, or the prediction of neighboring points. The invalid points is displayed in gray. Finally, the decoded curve instance is represented as set of keypoints in same color. It's best to zoom in the figure and view it in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of efficient decoding process. Different from greedy decoder, which searches keypoints in an iterative manner, the efficient decoder found all the keypoint candidates at the beginning. For these candidates, the position refinement, neighbor prediction and association construction are perform in one step through parallel computing. The white curve indicates association relationship among keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualized results of SCNN, SIM-CycleGAN+ERFNet, UFNet, PINet and FOLOLane on CULane testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visualization results of generalizing SCNN, SIM-CycleGAN+ERFNet, UFNet, PINet and FOLOLane on TuSimple testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Basic information of two lane marker detection datasets.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="4"># Frame Train Validation</cell><cell>Test</cell><cell>Resolution</cell><cell>Road type</cell><cell># Lane</cell></row><row><cell cols="2">TuSimple</cell><cell></cell><cell>6408</cell><cell>3268</cell><cell>358</cell><cell cols="2">2782 1280?720</cell><cell>highway</cell><cell>&lt;=5</cell></row><row><cell cols="2">CULane</cell><cell></cell><cell cols="2">133235 88880</cell><cell>9675</cell><cell cols="4">34680 1640?590 urban, rural and highway</cell><cell>&lt;=4</cell></row><row><cell cols="10">Category Proportion SCNN[12] ENet-SAD[5] ERFNet-E2E[20] SIM-CycleGAN UFNet[16] PINet(4H)[6] FOLOLane</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ERFNet[9]</cell><cell></cell><cell>(ours)</cell></row><row><cell>Normal</cell><cell cols="2">27.7%</cell><cell>90.6</cell><cell>90.1</cell><cell></cell><cell>91.0</cell><cell>91.8</cell><cell>90.7</cell><cell>90.3</cell><cell>92.7</cell></row><row><cell>Crowded</cell><cell cols="2">23.4%</cell><cell>69.7</cell><cell>68.8</cell><cell></cell><cell>73.1</cell><cell>71.8</cell><cell>70.2</cell><cell>72.3</cell><cell>77.8</cell></row><row><cell>Night</cell><cell cols="2">20.3%</cell><cell>66.1</cell><cell>66.0</cell><cell></cell><cell>67.9</cell><cell>69.4</cell><cell>66.7</cell><cell>67.7</cell><cell>74.5</cell></row><row><cell>No line</cell><cell cols="2">11.7%</cell><cell>43.4</cell><cell>41.6</cell><cell></cell><cell>46.6</cell><cell>46.1</cell><cell>44.4</cell><cell>49.8</cell><cell>52.1</cell></row><row><cell>Shadow</cell><cell>2.7%</cell><cell></cell><cell>66.9</cell><cell>65.9</cell><cell></cell><cell>74.1</cell><cell>76.2</cell><cell>69.3</cell><cell>68.4</cell><cell>79.3</cell></row><row><cell>Arrow</cell><cell>2.6%</cell><cell></cell><cell>84.1</cell><cell>84.0</cell><cell></cell><cell>85.8</cell><cell>87.8</cell><cell>85.7</cell><cell>83.7</cell><cell>89.0</cell></row><row><cell cols="2">Dazzle light 1.4%</cell><cell></cell><cell>58.5</cell><cell>60.2</cell><cell></cell><cell>64.5</cell><cell>66.4</cell><cell>59.5</cell><cell>66.3</cell><cell>75.2</cell></row><row><cell>Curve</cell><cell>1.2%</cell><cell></cell><cell>64.4</cell><cell>65.7</cell><cell></cell><cell>71.9</cell><cell>67.1</cell><cell>69.5</cell><cell>65.6</cell><cell>69.4</cell></row><row><cell>Crossroad</cell><cell>9.0%</cell><cell></cell><cell>1990</cell><cell>1998</cell><cell></cell><cell>2022</cell><cell>2346</cell><cell>2037</cell><cell>1427</cell><cell>1569</cell></row><row><cell>Total</cell><cell>-</cell><cell></cell><cell>71.6</cell><cell>70.8</cell><cell></cell><cell>74.0</cell><cell>73.9</cell><cell>74.4</cell><cell>72.3</cell><cell>78.8</cell></row></table><note>Table 2. Performance of different methods on CULane testing set, with IoU threshold=0.5. For crossroad, only FP are shown.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 Table 4 .</head><label>44</label><figDesc>shows that the proposed method improves Heatmap Coarse-to-fine Decoder Architecture F1 Rt. Se. Ke. @ test @ train Gre. Eff. Ablation studies on CULane testing set. Se.: Semantic segmentation. Ke.: Keypoint estimation based heatmap. @test: use ?x? to refine the geometry of the curve in testing. @train: use coarse-to-fine strategy to sample training data for ?x? in training. Gre.: Greedy decoding. Eff.: Efficient decoding. Rt.: runtime.</figDesc><table><row><cell>ERF BiSe</cell></row><row><cell>74.2 -</cell></row><row><cell>76.6 -</cell></row><row><cell>77.5 -</cell></row><row><cell>78.8 25ms</cell></row><row><cell>78.3 16ms</cell></row><row><cell>77.5 9ms</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.mindspore.cn/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointlanenet: Efficient end-to-end cnns for accurate real-time lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shaul Oron, and Bat El Shlomo. Semi-local 3d lane detection and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netalee</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bluvstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05257</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">El-gan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N?ra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghwuy</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06604</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, and In So Kweon. Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1947" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lane detection in low-light conditions using an efficient data enhancement: Light conditions style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01177</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06080</idno>
		<title level="m">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11757</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tusimple</forename><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmarkAccessed" />
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lanesensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
