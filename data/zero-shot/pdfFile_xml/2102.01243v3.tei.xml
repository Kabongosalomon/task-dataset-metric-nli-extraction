<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yuan</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">James</forename><surname>Glass</surname></persName>
						</author>
						<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio tagging</term>
					<term>Audio event classification</term>
					<term>trans- fer learning</term>
					<term>imbalanced learning</term>
					<term>noisy label</term>
					<term>ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio tagging is an active research area and has a wide range of applications. Since the release of AudioSet, great progress has been made in advancing model performance, which mostly comes from the development of novel model architectures and attention modules. However, we find that appropriate training techniques are equally important for building audio tagging models with AudioSet, but have not received the attention they deserve. To fill the gap, in this work, we present PSLA, a collection of model agnostic training techniques that can noticeably boost the model accuracy including ImageNet pretraining, balanced sampling, data augmentation, label enhancement, model aggregation. While many of these techniques have been previously explored, we conduct a thorough investigation on their design choices and combine them together. By training an EfficientNet with pretraining, balanced sampling, data augmentation, and model aggregation, we obtain a single model (with 13.6M parameters) and an ensemble model that achieve mean average precision (mAP) scores of 0.444 and 0.474 on AudioSet, respectively, outperforming the previous best system of 0.439 with 81M parameters. In addition, our model also achieves a new state-of-the-art mAP of 0.567 on FSD50K. We also investigate the impact of label enhancement on the model performance. Code at https://github.com/YuanGongND/psla.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Audio tagging aims to identify sound events that occur in a given audio recording, and enables a variety of Artificial Intelligence-based systems to disambiguate sounds and understand the acoustic environment. Audio tagging has a wide range of health and safety applications in the home, office, industry, transportation, and has become an active research topic in the field of acoustic signal processing.</p><p>In recent years, audio tagging and classification research has moved from small and/or constrained datasets such as ESC-50 <ref type="bibr" target="#b0">[1]</ref> and CHiME-Home <ref type="bibr" target="#b1">[2]</ref> to much larger datasets with a greater variety and range of real-world audio events and substantially more training data. A significant milestone in this field occurred with the release of the AudioSet corpus <ref type="bibr" target="#b2">[3]</ref> containing over 2 million 10-second audio clips extracted from video and tagged at the utterance level with a set of 527 event labels. AudioSet is currently the largest and most comprehensive publicly available dataset for audio tagging. Not surprisingly, it has subsequently become the primary source of training and evaluation material for audio tagging research. The availability of AudioSet has encouraged much audio tagging research that has steadily seen the standard <ref type="figure">Fig. 1</ref>. The proposed Pretraining, Sampling, Labeling, and Aggregation (PSLA) training pipeline. AudioSet is extremely class imbalanced and has prevalent annotation errors, we propose a data augmentation/balanced sampling strategy and a label enhancement strategy to alleviate these two problems. We also pretrain the convolutional neural networks with ImageNet and find it leads to a noticeable performance improvement. By further aggregating multiple models with weight averaging and ensemble techniques, we get a model that performs much better than that trained with a conventional pipeline and achieves a new state-of-the-art mAP of 0.474. evaluation metric of mean average precision (mAP) increase from, for example, 0.314 with shallow fully-connected networks <ref type="bibr" target="#b2">[3]</ref>, to 0.392 with a residual network with attention <ref type="bibr" target="#b3">[4]</ref> to, most recently, 0.439 with spectrogram and waveform-based convolutional neural networks (CNNs) <ref type="bibr" target="#b4">[5]</ref>. In order to cope with the weakly labeled data, multiple instance learning and attention mechanisms have also been the subject of much investigation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>In our audio tagging experiments using Audioset we have observed that, in addition to the particular model architecture being evaluated, significant performance improvements can be achieved via training techniques including cross-modal pretraining, data augmentation, label enhancement, and ensemble modeling. Our empirical evaluations show that these model agnostic techniques lead to significant accuracy improvements, and combining them together can further boost the model accuracy. Specifically, we train an ensemble of EfficientNet <ref type="bibr" target="#b9">[10]</ref> models with the proposed set of training techniques and achieve a new state-of-the-art mAP of 0.474 on AudioSet, our single model with 13.6M parameters also achieves an mAP of 0.444, outperforming the previous the best system that contained 81M parameters. In addition, our model also achieves a new state-of-the-art mAP of 0.567 on the FSD50K benchmark <ref type="bibr" target="#b10">[11]</ref>.</p><p>As shown in <ref type="figure">Figure 1</ref>, the training techniques we investi-arXiv:2102.01243v3 [cs.SD] 17 Nov 2021 gated fall into four main categories. First, we find cross-modal pretraining with ImageNet <ref type="bibr" target="#b11">[12]</ref> improves the performance of audio tagging CNNs even though AudioSet already contains a substantial amount of in-domain data. Second, we address the Audioset label imbalance by adopting balanced sampling and data augmentation. Third, we observed that there are pervasive annotation errors in AudioSet and studied the impact of such annotation errors on the model performance. We further developed a method to improve training label quality. Fourth, we use weight averaging and ensemble methods to improve the overall performance. Many of these techniques have been proposed previously in isolation. For example, ImageNet pretraining has been used in <ref type="bibr" target="#b12">[13]</ref> for small datasets, balanced sampling and data augmentation have been used in <ref type="bibr" target="#b4">[5]</ref>, label enhancement has been proposed in <ref type="bibr" target="#b13">[14]</ref>, and ensemble modeling has been used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. To the best of our knowledge however, none of the prior efforts have used more than two of these simultaneously, and the particular implementation is often only briefly mentioned in the literature. In this paper, we thoroughly investigate each of these techniques, a more thorough understanding of the benefits of different training techniques should facilitate a more meaningful comparison between various works because performance differences due to the particular training procedure could overshadow the model architecture or other novel techniques being investigated. The training pipeline we propose is model-agnostic and can serve as a recipe for AudioSet tagging experiments to facilitate fair comparisons with new techniques. The contributions of this work are summarized as follows: 1) We present a collection of training strategies and design choices for audio tagging. We quantify the improvement of each component via extensive experimentation. 2) By training an ensemble of standard EfficientNet models with the proposed training procedure, we achieve a new state-of-the-art mAP of 0.474 on AudioSet, outperforming the best previous system of 0.439. 3) We release the code, model, and enhanced label set.</p><p>The training pipeline can serve as a recipe of AudioSet training to facilitate future audio tagging research. The paper is organized as follows. We first describe the baseline model architecture in Section II, then we gradually improve the baseline model performance on AudioSet by adding new training techniques in Sections III, IV, V, and VI. In each section, we first review the corresponding technique and then present our implementation and results. We present an ablation study, experiments on FSD50K and other model architectures, and a discussion of the results in Section VII. We conclude the paper in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPERIMENT SETTING AND BASELINE MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>In this work, we mainly focus on AudioSet <ref type="bibr" target="#b2">[3]</ref>, a collection of over 2 million 10-second audio clips excised from YouTube videos and labeled with the sounds that the clip contains from a set of 527 labels. AudioSet is a weakly labeled and multilabel dataset, i.e., labels are given to a clip with no indication of where in the clip the associated sound occurred, and every  <ref type="bibr" target="#b2">[3]</ref>. Specifically, we downloaded 20,785 (94%), 1,953,082 (95%), and 19,185 (94%) recordings for the balanced train, full train, and evaluation set, respectively, which is consistent with previous literature (e.g., <ref type="bibr" target="#b4">[5]</ref>). Therefore, we do make fair comparisons with previous stateof-the-art models by evaluating on the same subset of the evaluation dataset. We also evaluate the proposed PSLA training framework on FSD50K <ref type="bibr" target="#b10">[11]</ref>, a recently collected data set of sound event audio clips with 200 classes drawn from the AudioSet ontology to see how the PSLA framework generalizes. FSD50K contains 37,134 audio clips for training, 4,170 audio clips for validation, and 10,231 audio clips for evaluation. The audio clips are of variable length from 0.3 to 30s with an average of 7.6s (7.1s for the training and validation set, 9.8s for the evaluation set). For both AudioSet and FSD50K, we sample the audio at 16kHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Evaluation Details</head><p>For all AudioSet experiments in this paper, we train the neural network model with a batch size of 100, the Adam optimizer <ref type="bibr" target="#b16">[17]</ref>, and use binary cross-entropy (BCE) loss. We use a fixed initial learning rate of 1e-3 and 1e-4 and cut it in half every 5 epochs after the 35 th and 10 th epoch for all balanced set and full set experiments, respectively. The reason why a smaller learning rate is used for the full AudioSet is that the full set is about 100 times larger than the balanced set, using a smaller learning rate can avoid the model falling into a local minima before it sees all samples. We use a linear learning rate warm-up strategy for the first 1,000 iterations. As in previous efforts, we train the model with 60 and 30 epochs for all balanced set and full set experiments, respectively, and report the mean result on the evaluation set of the last 5 epochs.</p><p>We use the mean average precision (mAP) of all the classes as our main evaluation metric since it is the most commonly used audio tagging evaluation metric on AudioSet. Mean average precision is an approximation of the area under a We apply a frequency mean pooling to produce a 33 ? 1408 representation that is fed into a 4-headed attention pooling module. In each head, the CNN output is transformed into a 33 ? 527 dimensional tensor via a set of 1?1 convolution layers with a parallel attention branch and classification branch. We multiply the output of each branch element-wise and apply a temporal mean pooling (implemented by summation). Finally, we sum the weighted output of each attention head after it has been scaled by a learnable weight and produce the final prediction for all classes.</p><p>class's precision-recall curve, which is more informative of performance when dealing with imbalanced datasets such as AudioSet and FSD50k compared with the average area under the curve of the receiver operating characteristic curve <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In the discussion section, we also report the average area under the curve (AUC) of the receiver operating characteristic curve and sensitivity index (d-prime) in order to compare our model with previous work that only reports AUC and d-prime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Model</head><p>In this work, we use a similar model structure as in <ref type="bibr" target="#b3">[4]</ref>, illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Each 10-second audio waveform is first converted to a sequence of 128 dimensional log Mel filterbank (fbank) features computed with a 25ms Hamming window every 10ms. We conduct zero padding to make all audio clips have 1056 frames. This results in a 1056 ? 128 feature vector that is input to a CNN model. In <ref type="bibr" target="#b3">[4]</ref> the CNN was based on the ResNet50 model <ref type="bibr" target="#b19">[20]</ref>. In our work, the CNN is based on the EfficientNet-B2 model <ref type="bibr" target="#b9">[10]</ref> since it requires a smaller number of parameters and is faster for training and inference. The EfficientNet model effectively downsamples the time and frequency dimensions by a factor of 32. The penultimate output of the model is a 33 ? 4 ? 1408 tensor. We apply mean pooling over the 4 frequency dimensions to produce a 33 ? 1408 representation that is fed into a multi-head attention module. The attention module consists of an attention branch and a classification branch. Each branch transforms the CNN mean pooled output into a 33 ? 527 dimensional tensor via a set of 1 ? 1 convolutional filters. After a sigmoid non-linearity and a normalization on the attention branch, we combine the two branches via a element-wise product. A temporal mean pooling (implemented by summation) is then performed to produce a final 527 dimensional output for each class label. Unlike <ref type="bibr" target="#b3">[4]</ref>, we use a 4-headed attention module instead of a single-head one in this work. We sum the weighted output of each attention head after it has been scaled by a learnable weight to produce the final output. EfficientNet <ref type="bibr" target="#b9">[10]</ref> is a recent proposed convolutional neural network architecture that has shown an advantage on both accuracy and efficiency over previous architectures. Such advantage mainly comes from two design: First, EfficientNet is based on the mobile inverted bottleneck convolution (MB-Conv) block <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, an efficient residual convolution block. Second, EfficientNet scales the network on all dimensions (i.e., width, depth, and input resolution), which is demonstrated to be a better strategy than scaling only one dimension. In this work, we use EfficientNet-B2 that consists of 9 stages, 339 layers. The original EfficientNet-B2 model for image classification has 9.11M parameters, after adding the attention module and adjusting the classification layer, our audio tagging model has 13.64M parameters in total. As shown in <ref type="table" target="#tab_0">Table II</ref>, the EfficientNet model achieves slightly worse performance than the ResNet-50 model, but has 12 million fewer parameters. In the rest of the paper, we keep using the EfficientNet model and show that a significant improvement can be achieved without modifying its model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NETWORK PRETRAINING</head><p>Transfer learning and network pretraining have been widely used in computer vision, natural language processing, speech and audio processing in recent years <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The typical process is to first train a model with either a large out-of-domain or unlabeled dataset using an auxiliary task and then fine-tune the model with in-domain data for the main task. The idea being that the knowledge learned from the pretraining task can be transferred to the main task. For the audio tagging task, both supervised pretraining (e.g., in <ref type="bibr" target="#b4">[5]</ref>) and self-supervised pretraining (e.g., in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>) using audio data have been studied in recent years. Performance improvement is typically achieved when the indomain dataset is small (e.g., ESC-50 <ref type="bibr" target="#b0">[1]</ref>, UrbanSound <ref type="bibr" target="#b30">[31]</ref>, and balanced AudioSet). However, it has not been reported that a pretrained model can outperform a state-of-the-art audio tagging model trained from scratch using the full AudioSet, possibly because the full AudioSet contains 2 million audio recordings and there is no larger annotated dataset available. While theoretically self-supervised pretraining can leverage an unlimited amount of unlabelled audio data, in practice it takes effort to find and process large scale data with sufficient variety and coverage of the 527 sound classes. In contrast to the above-mentioned efforts, we find noticeable performance improvement can be achieved by pretraining the CNN with the ImageNet dataset <ref type="bibr" target="#b11">[12]</ref> used for visual object classification, even when the training data for the end task of audio tagging is the full AudioSet. In our experiment, we initialize the EfficientNet (the second to the penultimate layer) with 1) ImageNet-pretrained weights (released by the authors of <ref type="bibr" target="#b9">[10]</ref>), and 2) random weights (He Uniform initialization <ref type="bibr" target="#b31">[32]</ref>). We then train both models in exactly the same way as described in Section II-B.</p><p>As shown in <ref type="table" target="#tab_0">Table III</ref>, ImageNet pretraining leads to a 51.9% and 5.8% relative improvement for the balanced set and full set experiment, respectively. To see the relationship between the performance improvement and the end-task training data volume, we further evaluate the performance when the audio tagging training data volume is 100k, 200k, 300k, and 500k (all comprised of the entire balanced set and samples randomly taken from the full set). As shown in <ref type="figure">Figure 3</ref>, the performance improvement decreases with the training data volume, but is always noticeable. In addition, we find the performance improvement led by ImageNet pretraining is much larger than that led by more training iterations, e.g., when trained with the balanced AudioSet, the model trained with 120 epochs achieves an mAP of 0.1694, which is only slightly better than the model trained with 60 epochs and is significantly worse than the model trained with ImageNet pretraining that achieves an mAP of 0.2385.</p><p>In some sense, it is surprising that pretraining a model with data from a different modality can be effective. However, transfer learning from computer vision tasks to audio tasks is not new and has been previously studied in <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, we believe this is the first time it has been demonstrated to be effective when the dataset of the audio task is at this scale, indicating the auxiliary image classification task helps the model learn some complementary knowledge. We hypothesize that the improvements may be due to the model learning to recognize low-level features such as edges during pretraining. Such knowledge could potentially be relevant for finding acoustic "edges" in the spectrogram.</p><p>In practice, many commonly used CNN architectures (e.g., Inception <ref type="bibr" target="#b35">[36]</ref>, ResNet <ref type="bibr" target="#b19">[20]</ref>, EfficientNet <ref type="bibr" target="#b9">[10]</ref>) have off-theshelf ImageNet-pretrained models for both TensorFlow and PyTorch. It is also straightforward to adapt these off-the-shelf models to audio tasks. The only things that need to be modified are the first convolution layer and the last classification layer. Since the input of vision tasks is a 3-channel image while the input to the audio task is a single-channel spectrogram, we adjust the input channel of the first convolutional layer from 3 to 1 and initialize it with random weights. Since the classification task is essentially different, we abandon the last classification layer of the pretrained model and feed the output of the penultimate layer to our succeeding layers. We implement this using the efficientnet_pytorch 1 package.</p><p>In summary, the advantages of using ImageNet pretraining are as follows. First, no additional in-domain labeled or unlabeled datasets are needed. This is important because currently there is no audio tagging dataset of comparable size to AudioSet. Second, ImageNet pretraining can lead to consistent performance improvement even when the in-domain training data size is huge. Third, ImageNet pretraining is practically easy to implement. The limitation is that it is only applicable to models that take 2D image-like input (e.g., spectrogram). Nevertheless, a majority of deep learning models for audio tasks do fall in this category. In the following sections, we use Imagenet pretraining by default for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BALANCED SAMPLING AND DATA AUGMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Balanced Sampling</head><p>As might be expected, the frequency of occurrence of different sound events ranges widely. It is not surprising then that a large scale audio tagging dataset is class imbalanced. As shown in <ref type="figure">Figure 4</ref>, the most frequent AudioSet class is "Music" which has 949,029 samples, while the most infrequent class "Toothbrush" only has 61 samples, leading to a ratio of 15,557. Such imbalances can have a large impact on performance, particularly for low-frequency classes <ref type="bibr" target="#b36">[37]</ref>.</p><p>With such large data imbalance, simple upsampling or downsampling are difficult to implement because upsampling will make the dataset unacceptably large while downsampling will waste a large portion of the data. Moreover, AudioSet is a multi-label dataset, making it even harder to implement up/downsampling methods. In this work, we propose a random balanced sampling method to alleviate the class imbalance problem. Note that balanced sampling on AudioSet has been  <ref type="figure">4</ref>. Sample count of each class in the full AudioSet (vertical axis is in log scale). Note that the sample count of the "Speech" class is substantially larger than the sum of sample counts of the "Male Speech", "Female Speech", and "Child Speech" class. Similarly, the sample count of the "Music" class is substantially larger than the sum of sample counts of the "Happy music" and "Sad music" class. This indicates a potential prevalent miss annotation issue in AudioSet.</p><p>used in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>, but is only briefly mentioned and the details can only be found in the source code. The proposed random balanced sampling approach is shown in Algorithm 1, lines 1-8. We first count the sample number c k of each class k over the entire dataset. We then assign a sampling weight for each sample, specifically, the weight w (i) of the i th sample is 527 k=1 1 {k?y (i) } 1/c k . This assigns a higher weight for samples containing rare audio events and also takes all audio events that appear in the sample into consideration. During training, we still feed N samples (N is the dataset size) to the model for each epoch, but instead of traversing the dataset, we draw a sample from the multinomial distribution parameterized by the above-mentioned sample weights with replacement. That makes rare sound event samples more likely to be seen by the model. The advantages of the proposed random sampling are 1) it is a compromise of upsampling and downsampling. It wastes fewer samples than downsampling while keeping the number of N samples fed to the model every epoch; 2) it is applicable to multi-label datasets; and 3) the model sees a different set of data every epoch, so the model checkpoints after every epoch have a greater diversity, which is helpful for ensembles <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, as we will discuss in Section VI.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 5</ref>, while the proposed balanced sampling algorithm greatly alleviates the data imbalance issue, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Balanced Sampling and Data Augmentation</head><p>Require:</p><formula xml:id="formula_0">Multi-label Dataset D = {x (i) , y (i) }, i ? {1, ..., N } Procedure 1: Generate Sampling Weight Input: Label Set {y (i) } Output: Sample Weight Set W = {w (i) }, i ? {1, ..., N } 1: traverse {y (i) }, count sample number c k of each class k 2: initialize w (i) = 0, i ? {1, ..., N } 3: for each sample i do 4:</formula><p>for each class k ? y (i) do 5: for n ? {1, ..., N } do <ref type="bibr">8:</ref> draw i ? multinomial(W) <ref type="bibr">9:</ref> if unif (0, 1) &lt; mixup rate M then 10:</p><formula xml:id="formula_1">w (i) = w (i) + 1/c k return W = {w (i) }</formula><formula xml:id="formula_2">draw j ? unif {1, N } 11: draw ? ? Beta(?, ?) 12: x = ?x (i) + (1 ? ?)x (j) 13: y = ?y (i) + (1 ? ?)y (j)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>else 15: <ref type="bibr">19:</ref> use (x, y) to train the neural network the sampled frequency of each class is still imbalanced after the balanced sampling algorithm is applied. This is because AudioSet is a multi-label dataset and minority classes are usually paired with majority classes, thus oversampling the minority class also directly oversamples the majority class. We compare the performance of the model trained with plain dataset traversal (with data reshuffled at every epoch) and with the proposed random sampling. As shown in <ref type="table" target="#tab_0">Table IV</ref>, we find random balanced sampling actually lowers the performance. This result is not surprising because: 1) while better than downsampling, there is still a substantial amount of data wasted every epoch. As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, 40.9% data is not seen by the model after 30 training epochs; 2) while the lowfrequency class samples and high-frequency class samples are roughly equally seen by the model, the low-frequency class samples are actually repeated samples. Both issues increase the risk of model overfitting. Therefore, we explored the use of data augmentation to overcome this problem.</p><formula xml:id="formula_3">x = x (i) , y = y (i) 16: draw f ? unif (0, F ), f 0 ? unif (0, 128 ? f ) 17: draw t ? unif (0, T ), t 0 ? unif (0, 1056 ? t) 18: x = M asking(f 0 , t 0 , f, t)(x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time and Frequency Masking</head><p>We first consider simple time and frequency masking for data augmentation, which has been found to be effective for audio tagging <ref type="bibr" target="#b4">[5]</ref> and speech recognition <ref type="bibr" target="#b39">[40]</ref>. Frequency masking is applied so that f consecutive frequency chan-</p><formula xml:id="formula_4">nels [f 0 , f 0 + f ) are masked, where f ? unif (0, F ), f 0 ? unif (0, 128 ? f )</formula><p>, and F is the maximum possible length of the frequency mask. Similarly, time masking is applied so that t consecutive frequency channels</p><formula xml:id="formula_5">[t 0 , t 0 + t) are masked, where t ? unif (0, T ), t 0 ? unif (0, 1056 ? t)</formula><p>, and T is the maximum possible length of the frequency mask. Note that 128 and 1056 are the input dimensions of our model. We use the implementation of torchaudio.transforms.FrequencyMasking and TimeMasking, F = 48 and T = 192. The masking parameters f 0 , t 0 , f, t are sampled on-the-fly for each audio sample during training to minimize the chance of repeated audio samples being fed to the model. As shown in <ref type="table" target="#tab_0">Table IV</ref>, time and frequency masking improves audio tagging performance considerably, with relative improvements of 18.2% and 14.6% achieved for the balanced set and full set experiment, respectively. Note that the overall amount of training samples per epoch remains the same. We hypothesize that the effectiveness of masking is due to the reduction of repeated samples in the training data, especially for low-frequency samples.  <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and audio tagging <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Specifically, mixup training constructs augmented training examples as follows:</p><formula xml:id="formula_6">x = ?x (i) + (1 ? ?)x (j) y = ?y (i) + (1 ? ?)y (j) TABLE VI PERFORMANCE AS A FUNCTION OF ? (TRAINING ON BALANCED SET WITH MIX-UP RATE= 0.5). ? ?? 0.1 1 10 mAP 0.2818 0.3004 0.3087 0.3108</formula><p>where x (i) and x (j) are two different training audio samples, y (i) and y (j) are the corresponding labels, ? ? [0, 1] and x is the mixed-up new audio sample, and y is the resulting label. We conduct mix-up on the waveform level.</p><p>Past explanations for why mix-up training improves performance include: 1) it increases the variation of the training data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b42">[43]</ref>; 2) it leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>; and 3) it reduces the model's memorization of corrupt labels <ref type="bibr" target="#b41">[42]</ref>.</p><p>In addition to these observations, we find mix-up training has an additional advantage for imbalanced datasets. As we discussed in Section IV-A, balanced sampling, while making the low-frequency class samples more prevalent, has the unfortunate side effect of wasting a large number of (40.9%) class samples. By adopting the mixup strategy, the model can see twice the number of samples within the same training epoch. This advantage can be increased if one of the two mixed-up samples is drawn from a uniform distribution, while the other is drawn using the balanced sampling multinomial distribution introduced in the previous section. Intuitively, mixing up a rare sound event (e.g., toothbrush) with a frequent one (e.g., music) is more reasonable than mixing up two rare sound events. Some previous synthetic audio event detection datasets use a similar method to construct samples <ref type="bibr" target="#b43">[44]</ref>. As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, the mix-up strategy can reduce the unseen samples to almost zero in just a few epochs.</p><p>We further make two modifications based on previous efforts. In prior work ? is drawn from a uniform distribution unif (0, 1) <ref type="bibr" target="#b42">[43]</ref> or Beta distribution Beta(?, ?) with ? &lt; 1 <ref type="bibr" target="#b41">[42]</ref>, where</p><formula xml:id="formula_7">Beta(?, ?) : prob(x; ?, ?) = x ??1 (1 ? x) ??1 B(?, ?)</formula><p>where B is the beta function</p><formula xml:id="formula_8">B(?, ?) = 1 0 t ??1 (1 ? t) ??1 dt</formula><p>Thus ? has a relatively high likelihood to be close to either 0 or 1. From the perspective of sound mixing and reducing the number of unseen samples, a ? close to 0.5 could be more reasonable since it leads to more "evenly" mixed up samples and the model can see both samples. Second, since samples in the evaluation set are not mixed up, mixing up all samples during training might lead to a gap between training and evaluation. Thus we set a mix-up rate to control the number of samples to mix up during training, a mixup rate of 0.5 means that 50% training samples are mixup samples and the rest 50% training samples are non-synthetic samples. Therefore, the model can see non-synthetic samples during training. As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, a mix-up rate of 0.5 results in 95% samples being seen by the model in 5 epochs. For non mix-up samples, the data loader only needs to load one audio sample instead of two. A low mix-up rate can also reduce the data loading and pre-processing cost during training, which is non-negligible because it is almost impossible to fit the full AudioSet into memory.</p><p>We evaluate the impact of mix-up rate and ?, as shown in Tables V and VI. A larger ? and a medium mix-up rate indeed lead to better classification performance. Combining them achieves 0.3108 mAP, which is better than a plain setting of ?=mixup rate=1 that achieves 0.3079 mAP. We use ? = 10 and mix-up rate= 0.5 in all subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Summary</head><p>We combine the balanced sampling and masking and mixup data augmentation strategies together, as described in Algorithm 1. We summarize the contribution of each component in <ref type="table" target="#tab_0">Table IV</ref>. It is worth mentioning that while balanced sampling alone lowers the performance, it is helpful when combined with data augmentation strategies. By adopting balanced sampling and data augmentation, an 11.6% relative improvement and an mAP of 0.4397 are achieved for the full set experiment. We only do data augmentation for balanced set experiments as the data is already roughly balanced and obtain a 30.3% relative improvement and an mAP of 0.3108, demonstrating the effectiveness of data augmentation for small datasets. Finally, it is worth mentioning that by merely adopting ImageNet pretraining, balanced sampling, and data augmentation with a standard EfficientNet architecture, the model already outperforms the previous best system. In the following sections, we use balanced sampling (for the full AudioSet) and data augmentation as defaults for all experiments. <ref type="figure">Fig. 7</ref>. Sorted class-wise average precision (AP) and its standard deviation of the model trained on full set. Note that the "Speech" class has a much higher AP than the "Male Speech", "Female Speech", and "Child Speech" class. Similarly, the "Music" class has a much higher AP than the "Happy Music" and "Sad Music" class. "Singing" and "Song" have similar definition but very different AP. Classes with low AP also have a larger AP variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LABEL ENHANCEMENT</head><p>In this section, we explore the noisy label aspect of Au-dioSet: how it impacts audio tagging performance, and how to alleviate it. This line of research is motivated by observing the model's class-wise performance. In <ref type="figure">Figure 7</ref>, we show the class-wise average precision (AP) of the model trained with the full set. From the figure it is immediately apparent that the AP of each class differs greatly, indicating that the model has a range of ability to recognize various sounds. This is not an issue specific to our model or training pipeline, but has been widely reported in prior work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The order of class-specific performance reported by independent research also appears to be similar. For example, the "Male speech", "Bicycle", "Harmonic", "Rattle", and "Scrape" classes are among the 10 worst performing classes in <ref type="bibr" target="#b44">[45]</ref>, and they are also are among the 10 worst performing classes for our model when trained with the balanced set. We further confirm that models with different architectures have similar class-specific performance order with experiments in Section VII-B. This consistency suggests that the issue might be due to an intrinsic problem with the data or the task. Since the class-wise AP is not strongly correlated with either class sample count in the training set or the class annotation quality estimate released by the AudioSet authors (as shown in <ref type="table" target="#tab_0">Table VII</ref>), it has been hypothesized that the class-wise performance variation is due in part to the difficulty in reliably tagging the different sound classes themselves <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>While we agree that the poor performance of some classes could be due to particular audio events being difficult to identify, it is not true for all poor-performing classes. For example, the "Male Speech", "Female Speech", and "Child Speech" classes have APs of 0.07, 0.09, 0.45, respectively while the AP of the "Speech" class is 0.80. This discrepancy cannot be explained by the class difficulty hypothesis because recognizing speaker gender from speech is a relatively easy task <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, and the performances of the speech classes should not be so disparate. By examining the class sample counts, we find another issue that the sample count of the "Speech" class is substantially larger than the sum of sample counts of the "Male Speech", "Female Speech", and "Child Speech" classes. Specifically, in the balanced set, there are 5,309 audio clips with the label "Speech" but only 55, 55, 128 audio clips are with label "Male Speech", "Female Speech", and "Child Speech", respectively. The same thing happens in the full set (shown in <ref type="figure">Figure 4</ref>): the "Speech Class" has 947,009 samples while the sum of the other three classes is 34,878. In other words, only 4.5% and 3.7% of speech samples are labeled as either male, female, or child speech in the balanced and full AudioSet, respectively. This indicates that a large portion of samples are not correctly labeled. Based on these two observations, we hypothesize that the low performance of the male, female, and child speech classes is not due a small number of samples, or inherent classification difficulty, but that they have only a small fraction of correctly labeled data, which ultimately confuses the model. We refer to this phenomenon as a Type I error.</p><p>We also find that there are substantial samples labeled with sub-classes, but not with the corresponding parent class defined by the AudioSet ontology. For example, there are 40 and 3,201 audio clips labeled as either "Male Speech", "Female Speech", or "Child Speech", but not labeled as "Speech" in the balanced and full AudioSet, respectively. We refer to this phenomenon as Type II error.</p><p>We formalize the two types of error as follows: 1) Type I error: an audio clip is labeled with a parent class, but not also labeled as a child class when it does in fact contain the audio event of the child class. 2) Type II error: an audio clip is labeled with a child class, but not labeled with corresponding parent classes. It is worth mentioning that neither type of error are included in the quality estimate released by the AudioSet authors because the quality estimate checked 10 random audio clips of each class and verified that they actually contained the corresponding sound event. In other words, the quality estimate counts the false positive annotation errors, but not false negatives. As a consequence, the quality estimate of the "Male Speech", "Female Speech", and "Child Speech" is 90%, 100%, and 100%, respectively, while they have obvious false negative annotation errors.</p><p>Unfortunately, false negatives are prevalent in AudioSet. Another example are the music classes (see <ref type="figure">Figure 4</ref> and 7 for sample counts and class-wise AP of music classes). The reason  if M(x (i) )(k n ) &gt; t kn and k n ? y (i) then 8:</p><formula xml:id="formula_9">t k = N i=1 1 {k?y (i) } M(x (i) )(k)/ N i=1 1 {k?y (i) } return T = {t k }</formula><formula xml:id="formula_10">y (i) = y (i) ? {k n } return {y (i) }</formula><p>for these types of errors is due to the AudioSet annotation pipeline. In the pipeline, the human annotator verifies the candidate labels nominated by a series of automatic methods (e.g., by using metadata). Also, the list of candidate labels is limited to ten labels per clip. Since the automatic methods for nomination are not perfect, some existing sound events fail to be nominated, or are nominated but ranked below the top ten, thus leading to missing labels <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>As seen in the speech class example, annotation error can impact performance, but has not received much attention. To the best of our knowledge, only a few efforts have covered the missing label issue. In <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b49">[50]</ref>, a synthetic error is studied, however, the real-world noisy labels are believed to be much harder to deal with than the synthetic labels. In <ref type="bibr" target="#b13">[14]</ref>, the authors propose a loss masking based teacherstudent model. In this section, we propose an ontology-based label enhancement method to alleviate the noisy label problem. Our approach differs from previous work in three aspects: First, we work on real-world noisy labels rather than synthetic corrupted labels; Second, we explicitly modify the labels of the training data rather than using loss masking during training. Thus the enhanced label set can be used in the exact same way as the original set (no need to modify the model and training pipeline). We plan to release the enhanced label set to facilitate future research. Third, we leverage the AudioSet ontology to constrain label modification, which reduces the chance of incorrect modifications. For example, for an audio clip labeled as "Speech", we only consider adding child or parent labels in the specific "Speech" branch of the ontology.</p><p>As shown in Algorithm 2, the proposed approach consists of the following steps. First, we train a teacher model using the full AudioSet with the original label set. Second, we set a label modification threshold for each audio tagging, specifically, we set the threshold of a class as the teacher model's mean prediction score of all audio clips originally labeled as that class (lines 1-2). The threshold can also be set as other values such as the 5th, 10th, or 25th percentile of the teacher model's prediction score. The lower the threshold, the more labels are added. We then identify all samples that need to be relabeled. For each sample, we compile all child (Type I) and/or parent (Type II) labels of all original labels as the candidate set according to the AudioSet ontology (line 6). For each label in the candidate set, if the teacher model's prediction score of the class is greater than the corresponding label modification threshold, we add it to the labels of the sample (line 7-8).</p><p>Finally, we retrain the model from scratch with the enhanced label set. We apply the proposed label enhancement method (with the teacher model's mean prediction score as the label modification threshold) on the balanced training set and show the results in <ref type="table" target="#tab_0">Table VIII</ref>. Note the model without label enhancement has an mAP of 0.3108?0.0013 (the model from the previous section). The key findings are as follows: First, a noticeable number of labels are added, and over half of the classes are impacted, which further indicates that the missing label issue is prevalent in AudioSet. Second, enhancing the label improves the performance of both impacted and non-impacted classes, but the impacted classes have a larger relative improvement. Third, the mean class-wise relative AP improvement is larger than the relative mean AP (mAP) improvement, indicating that more of the classes that improved originally had belowaverage performance. This supports our hypothesis that the missing label problem lowers the performance of a sound class. Fourth, we evaluate the performance of fixing Type I errors, Type II errors, and fixing both. The improvement achieved by fixing Type I errors is larger than fixing Type II errors. Fixing both cannot further improve the performance. Fifth, since the performance improvement is relatively minor, we run all experiments three times with different random seeds and report both the mean and standard deviation. As shown in the table, the results verify the statistical significance of the improvement. Finally, we also applied the label enhancement method on the full AudioSet, however, we did not observe a performance improvement. Fixing Type I, Type II, and both errors leads to mAPs of 0.4400, 0.4387, and 0.4386, respectively, while the model without label enhancement achieves an mAP of 0.4397?0.0007. We believe the main reason for the relatively small improvement achieved by label enhancement is that the same label noise exists consistently in both the training set and evaluation set. Therefore, merely applying label enhancement on the training set leads to a mismatch between the training and evaluation sets. The performance results do not therefore fully reflect the actual improvement. In addition, it is possible that the label modification threshold is not appropriate for the full AudioSet.</p><p>In order to verify these hypotheses, we evaluate our model on existing datasets with more accurate annotation including ESC-50 <ref type="bibr" target="#b0">[1]</ref> and FSD50K <ref type="bibr" target="#b10">[11]</ref>, and also test various label modification thresholds. ESC-50 contains 2,000 audio samples of 50 sound classes, among which 40 classes are overlapped with the AudioSet. Therefore, we evaluate our model trained with AudioSet on the 1,600 samples that are labeled as these 40 overlapped classes. FSD50K is a recently collected data set of sound event audio clips with 200 classes drawn from the AudioSet ontology. The FSD50K evaluation set is more carefully annotated compared with the training and validation set and can be used as fair references. Since the length of AudioSet model input is 10s while a small portion of FSD50K audio clips are longer than 10s, we cut all FSD50K audio clips to 10s for testing. In addition, we also apply the proposed label enhancement algorithm on the AudioSet evaluation set and generate enhanced evaluation sets. We include the enhanced AudioSet evaluation sets as additional evaluation sets.</p><p>We evaluate various label modification thresholds including the mean, 25th percentile (25P), 10th percentile (10P), and 5th percentile (5P) of the teacher model's prediction score of all audio clips originally labeled as that class. The lower the threshold, the more labels are modified, e.g., using the 5th percentile of the prediction score as the threshold changes the largest number of labels. We then train models with the four enhanced label sets and compare their results on seven evaluation sets (ESC-50, FSD50K, original AudioSet evaluation set, and four enhanced AudioSet evaluation set with different label modification thresholds). <ref type="table" target="#tab_0">Table IX</ref>, we find that models trained with enhanced AudioSet label sets consistently outperforms the model trained with the original AudioSet label set on all evaluation sets except the original AudioSet evaluation set, demonstrating that the proposed label enhancement algorithm is able to improves the model performance, the reason why we cannot observe the improvement on the AudioSet evaluation set is that the evaluation set itself contains annotation errors. While there is no threshold that is optimal for all evaluation sets, for both balanced and full AudioSet experiments, we find the mean and 25th percentile of the teacher model's prediction score are the most appropriate label modification thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>We believe it is an important and non-negligible topic for future AudioSet and general audio tagging research because noisy labels are inevitable for a large-scale dataset and errors will impact model performance. In the following section, we use models trained with the enhanced label set as default for all balanced set experiments.  <ref type="table" target="#tab_0">(5P) OF THE PREDICTION SCORE AS THE LABEL MODIFICATION THRESHOLDS AND GENERATE 4 ENHANCED AUDIOSET TRAINING LABEL  SETS AND EVALUATION LABEL SETS. WE THEN TRAIN THE MODEL WITH THE ENHANCED TRAINING SETS AND EVALUATE IT ON VARIOUS EVALUATION  SETS. THE RESULTS SHOW THAT THE MODEL TRAINED WITH ENHANCED LABEL SETS CONSISTENTLY OUTPERFORMS THE MODEL TRAINED WITH  ORIGINAL LABEL SETS ON ALL EVALUATION SETS EXCEPT THE ORIGINAL AUDIOSET EVALUATION</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. WEIGHT AVERAGING AND ENSEMBLE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Weight Averaging</head><p>In this section, we explore improving model performance by aggregating multiple models. The first strategy we explore is weight averaging <ref type="bibr" target="#b50">[51]</ref>. Weight averaging performs an equal average of the weights traversed by the optimizer, which makes the solution fall in the center, rather than the boundary, of a wide flat low-loss region and thus lead to better generalization than conventional training. Empirically, weight averaging has been shown to improve the performance of various models such as VGG <ref type="bibr" target="#b51">[52]</ref>, ResNets <ref type="bibr" target="#b19">[20]</ref>, and DenseNets <ref type="bibr" target="#b52">[53]</ref> on a variety of tasks <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b53">[54]</ref>. While weight averaging is usually applied with a high constant or cyclical learning rate, we find it is helpful even when used together with a weight decay strategy.</p><p>In this work, we simply average all weights of the model checkpoints at multiple epochs. For both balanced set and full set experiments, we start averaging model checkpoints of every epoch after the learning rate is decreased to 1/4 of the initial learning rate (i.e., the 41 st and the 16 th epochs, respectively) until the end of the training. As shown in <ref type="table">Table X</ref>, weight averaging leads to a 0.9% improvement for both balanced set and full set experiment. We further find the improvement is not sensitive to exactly when weight averaging begins. As shown in <ref type="figure">Figure 8</ref>, starting averaging at any epoch after the 10 th epochs (until the last epoch) can outperform any single checkpoint model for the full set experiment.</p><p>In summary, weight averaging is easy to implement, adds no additional cost to training and inference, but can consistently improve model performance. By applying weight averaging <ref type="figure">Fig. 8</ref>. Relationship of the performance of averaging models with the epoch starts to average. For both weight and prediction averaging, we average all checkpoints from the starting epoch to the last epoch, i.e., the earlier to start averaging, the more checkpoints are averaged. Note that the improvement of model averaging is not sensitive to exactly when weight averaging begins. For weight averaging, the optimal starting epoch is around the 15 th epoch while starting averaging at any epoch after the 10 th epochs can outperform any single checkpoint. For prediction averaging, starting averaging from the first epoch leads to the highest mAP, indicating averaging all checkpoints is optimal, while starting averaging at any epoch can outperform any single checkpoint. However, averaging the predictions of the last few checkpoints barely outperforms single checkpoints, indicating the importance of diversity.</p><p>to our models, we get our best single model with an mAP of 0.3192 and 0.4435 for balanced and full AudioSet experiment, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ensemble</head><p>Finally, we explore a series of ensemble strategies. The goal of ensemble methods is to combine the predictions of several models to improve generalizability and robustness over any single model. Previously, ensemble of audio tagging models has been studied in in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b3">[4]</ref>, but typically only one strategy is covered in each of these previous efforts. In this work, we use the simple voting algorithm, but compare multiple ways of building the model committee. The reason why we do not use iterative ensemble methods (e.g., Boosting) is because AudioSet training is expensive making iterative training computationally unreasonable for this work. 1) Checkpoint Averaging: The first strategy investigated is checkpoint averaging, whereby the output of checkpoint models at multiple epochs are averaged together. The implementation is similar to weight averaging, but is conducted in the model space rather than the weight space. Since we conduct random sampling with replacement during full set training, the combination with checkpoint averaging is the same as bootstrap aggregating (i.e., Bagging) <ref type="bibr" target="#b38">[39]</ref>. In our experiment, we average the output of all checkpoint models (i.e., 60 and 30 checkpoint models for the balanced set and full set, respectively). As shown in the upper part of <ref type="table" target="#tab_0">Table XI</ref>, this approach works well. Specifically, the ensembled model noticeably outperforms the best checkpoint model in the committee. In addition, as shown in <ref type="figure">Figure 8</ref>, starting averaging from the first epoch leads to the highest mAP, indicating averaging all checkpoints is optimal. Averaging from any epoch can outperform the best single checkpoint model, which can be a simple alternative. However, this approach greatly increases the computational overhead of inference, which makes it less practical in deployment.</p><p>2) Averaging Models Trained with Different Random Seeds: Previous work suggests that ensembles generalize better when they constitute members that form a diverse and accurate set <ref type="bibr" target="#b57">[58]</ref>. As shown in <ref type="figure">Figure 8</ref>, starting averaging the checkpoint predictions from the last few epochs can only slightly outperform the best single checkpoint model, even though these checkpoint models are quite accurate, indicating the importance of diversity. Therefore, we run the experiment three times with the exact same setting, but with a different random seed. We then average the output of the last checkpoint model of each run. As shown in the middle part of Table XI, this approach leads to an even larger improvement than checkpoint averaging with only three models in the committee. Therefore, averaging models trained with different random seeds, while increasing the training cost (due to the repeat runs), is more practical for deployment and offers better performance.</p><p>3) Averaging Models Trained with Different Settings: Finally, we explore averaging more models with greater diversity. Specifically, we ensemble models trained with all different settings tested in this paper, including whether pretraining is used (pretrain), different mix-up rates (mixup rate), different mix-up ? (mix-up-?), different augmentation settings (augment), and different label enhancement strategies (label). As shown in the lower part of <ref type="table" target="#tab_0">Table XI</ref>, no matter how the model committee is built, ensemble always improves the performance and outperforms the best model in the committee. In the literature, diversity is usually introduced with an intuitive motivation. For example, in <ref type="bibr" target="#b14">[15]</ref>, the authors ensemble models use different scale inputs because they believe the optimal input scale varies with the target audio events, and ensembles allows the model to extract relevant information from inputs with various scales. But according to our experimental results, the source of the diversity seems to be less important, i.e., the diversity caused by any factor is helpful for an ensemble.</p><p>In addition, we find the performance of the ensemble model is positively correlated with the accuracy of the models in the committee as well as the number of the models. For both the balanced set and full set experiments, our best model is achieved when all available models form an ensemble. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. SUPPLEMENTARY EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study</head><p>From Section III to Section VI, we incrementally improve model performance from the baseline by incorporating a new technique with other techniques that have been found to be effective. In order to clearly identify the contribution of each technique and verify that all are necessary for the best model, we conduct an ablation study on balanced and full AudioSet. Specifically, we set the PSLA model with checkpoints ensemble as the baseline (the best model for a single training run), and then remove techniques from PSLA one by one, and check the performance. As shown in <ref type="table" target="#tab_0">Table XII</ref>, removing any technique from PSLA leads to a performance drop, demonstrating that all proposed techniques are useful. It is worth mentioning that removing balanced sampling leads to a significant performance drop for AudioSet, the performance of the model is worse than the model only with pretraining (0.3939 mAP, in <ref type="table" target="#tab_0">Table IV</ref>), indicating that other techniques (e.g., masking, mixup, and ensemble) should be used together with balanced sampling for AudioSet. Besides balanced sampling, removing pretraining leads to the largest performance drop, followed by ensemble, time and frequency masking, and mixup training for the full AudioSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment with Various Audio Tagging Models</head><p>In the previous sections, we focus on the EfficientNet-B2 with a 4-headed attention model described in Section II-C. In order to identify if the proposed PSLA framework is modelagnostic and explore the model size-performance trade-offs, in this section, we evaluate the PSLA framework using 6 different models. All models take the same input and are trained with the same setting as mentioned in Section II-B.</p><p>1) MobileNet V2 <ref type="bibr" target="#b20">[21]</ref>. The MobileNet model does not have an attention module. We use a fully connected layer as the classification layer. 2) EfficientNet-B0 with single-headed attention model. The model architecture is the same as the model described in Section II-C except that it is based on a smaller EfficientNet-B0 and only has one attention module. 3) EfficientNet-B2 with mean pooling model. The model architecture is the same as the model described in Section II-C except that it uses mean pooling rather than attention pooling. 4) EfficientNet-B2 with single-headed attention model. The model architecture is the same as the model described in Section II-C except that it only has one attention module. 5) EfficientNet-B2 with 4-headed attention model. This is the model we use in from Section III to Section VI and is described in Section II-C. 6) ResNet50 with single-headed attention module. This is the model proposed in <ref type="bibr" target="#b3">[4]</ref>. To save compute, for all PSLA models, we use the checkpoint averaging ensemble that only requires a single training process, we also report the single model with weight averaging for all full AudioSet experiments. As shown in <ref type="table" target="#tab_0">Table XIII</ref>, when trained with PSLA techniques, all models can achieve a noticeable performance improvement. This justifies that the proposed PSLA framework is model-agnostic.</p><p>Comparing the EfficientNet-B2 models with 4-headed attention, single-headed attention, and mean pooling, we find while the single 4-headed attention model performs best (0.4435 mAP), the single-headed attention model and the mean pooling model only perform slightly worse. The EfficientNet-B0 model with single-headed attention that has 5.36M parameters also achieves a comparable performance with the best existing model that has 81M parameters <ref type="bibr" target="#b4">[5]</ref>. The choice of the model depends on the application, e.g., attention-based models can be used for frame-level tagging; models with mean pooling can be used for streaming applications; smaller models are preferable for resource-constrained devices.</p><p>We also compute the Pearson correlation of class-wise APs between these models and find that the correlation of classwise APs are high (over 0.95), this confirms that the poor performance of some class is not due to model architecture, but due to the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment on FSD50K</head><p>In the previous sections, we focus on AudioSet. To check the generalizability of the proposed PSLA techniques, we also conduct a set of experiments on FSD50K <ref type="bibr" target="#b10">[11]</ref>. Specifically, we train the EfficientNet-B2 model with a 4-headed attention module with an initial learning rate of 5e-4 and a batch size of 24 for 40 epochs. The learning rate is cut in half every 5 epochs after the 10 th epoch. Since the maximum input audio length of FSD50K is 30s, we pad all input audio clips to 30s. For the single model, we train it with the FSD50K training set, validate it on the FSD50K validation set, and evaluate it on the FSD50K evaluation set. We use the same weight averaging and checkpoint averaging ensemble setting as the AudioSet experiments. We also conduct an ablation study on FSD50K.</p><p>As shown in <ref type="table" target="#tab_0">Table XIV</ref>, our single model, weight averaging model, and ensemble model achieve an mAP of 0.5535, 0.5571, and 0.5671 on the FSD50K evaluation set, respectively, all outperform the best existing model <ref type="bibr" target="#b58">[59]</ref>. Removing any technique from PSLA leads to a performance drop, demonstrating that all proposed techniques can be generalized to the FSD50K dataset. We show the learning curve of our best single EfficientNet B2 with 4-headed attention model (without weight averaging) in <ref type="figure" target="#fig_4">Figure 9</ref>. For both the balanced set and full set experiment, we repeat the training process three times with different random seeds and show the standard deviation in the plot. As we can see, the training converges, and the performance of the model barely varies with the random seed, i.e., the three runs achieve almost the same result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning Curve of PSLA models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we describe several techniques that improve the performance of a CNN-based neural model for audio tagging. First, we show an ImageNet-pretrained CNN can noticeably improve performance. While it is straightforward to implement for CNN-based models it has seldom been used in audio tagging research. Second, due to an imbalance in sound class samples in Audioset, we describe several data balancing and augmentation strategies that alleviate the data imbalance issue and help improve performance. We argue that balanced sampling and data augmentation should be a standard component for AudioSet modeling. Third, by observing variation in class-specific performance, we identified a missing label issue with Audioset and proposed a label enhancement method that shows improvement on the balanced training set. The enhanced label set can be used in the same way as the original label set in future research. We were not able to observe a performance improvement by enhancing the full set labels, possibly due to similar missing labels in the evaluation set. Due to its impact on performance, we believe addressing the noisy label issue is an important research topic for audio tagging. Finally, we describe weight averaging and ensemble strategies that are both simple and effective for audio tagging.</p><p>By combining all these training techniques, we are able to improve the performance of a normal EfficientNet model by 130.6% and 28.2% without modifying the model architecture for the balanced and full AudioSet experiment, respectively. This magnitude of improvement is larger than was achieved by many previous model architecture or attention module development efforts, indicating that appropriate training techniques are equally important. As a consequence, by training an EfficientNet with these techniques, we obtain a single model (with 13.6M parameters) and an ensemble model that achieve mean average precision (mAP) scores of 0.444 and 0.474 on AudioSet, respectively, outperforming the previous best system of 0.439 with 81M parameters <ref type="bibr" target="#b4">[5]</ref>. Our best model trained with only the balanced AudioSet (? 1% of the full set) outperforms our baseline and many previous models trained with the full set. We show the AUC and d-prime of our models and compare them with previous efforts in <ref type="table" target="#tab_5">Table XV</ref>. The proposed model outperform previous models for all evaluation metrics. The work in this paper can serve as a recipe for AudioSet training. Most of the proposed methods are model agnostic and can be combined together with various model architectures and attention modules. As we showed in the paper, the same model can perform much better when it is trained with appropriate techniques. We hope this work can facilitate future audio tagging research by documenting a set of strong and useful training techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. ACKNOWLEDGMENT</head><p>This work was supported in part by Signify.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Sorted sampled frequency of each class after 30 training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Procedure 2 :</head><label>2</label><figDesc>Sampling and Augmentation in Training Input: {x (i) , y (i) }, W, F , T , M 6: for every epoch do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>The proportion of unseen samples with the training epochs. Mixup rate is the probability that the sample input to the model is a mixed-up sample. In our implementation, one of the two mixed-up samples is drawn from a uniform distribution, while the other is drawn using the balanced sampling multinomial distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Procedure 2 :</head><label>2</label><figDesc>Enhance the Label Set Input: M, D, O, T Output: Enhanced Label Set {y (i) }, i ? {1, ..., N } 3: Initialize {y (i) } = {y (i) } 4: for i ? {1, ..., N } do 5:for k ? y (i) do6:    for k n ? O(k) do parent or child class of k 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>The learning curve of our experiments. Each experiment is run three times, and the stand deviation is shown in the shade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>AUDIOSET [3] STATISTICS.</figDesc><table><row><cell></cell><cell cols="3">Balanced Train Full Train Evaluation</cell></row><row><cell>AudioSet</cell><cell>22,176</cell><cell>2,065,161</cell><cell>20,383</cell></row><row><cell>Downloaded</cell><cell>20,785</cell><cell>1,953,082</cell><cell>19,185</cell></row><row><cell>Downloaded Ratio</cell><cell>93.7%</cell><cell>94.6%</cell><cell>94.1%</cell></row><row><cell cols="4">clip can, and often does, have multiple labels associated</cell></row><row><cell cols="4">with it. As shown in Table I, the dataset is split into three</cell></row><row><cell cols="4">subsets: balanced train, unbalanced train, and evaluation. In</cell></row><row><cell cols="4">this paper, we combine the balanced and unbalanced training</cell></row><row><cell cols="4">set as the full training set. The balanced train dataset is a</cell></row><row><cell cols="4">set of 22,176 recordings, where each class has at least 49</cell></row><row><cell cols="4">samples, while the full train set contains the entire 2 million</cell></row><row><cell cols="4">recordings. The evaluation set consists of 20,383 recordings</cell></row><row><cell cols="4">and contains at least 59 examples for each class. To obtain</cell></row></table><note>the raw audio, we extracted the dataset from YouTube. Due to the constant change in video availability (e.g., videos being removed, taken down) there is a natural shrinkage (about 5%) from the original dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II MEAN</head><label>II</label><figDesc>AVERAGE PRECISION (MAP) COMPARISON OF THE RESNET MODEL [4] AND THE EFFICIENTNET MODEL USED IN THIS PAPER.</figDesc><table><row><cell></cell><cell cols="3"># Parameters Balanced Set Full Set</cell></row><row><cell>ResNet-50</cell><cell>25.66M</cell><cell>0.1635</cell><cell>0.3790</cell></row><row><cell>EfficientNet-B2</cell><cell>13.64M</cell><cell>0.1570</cell><cell>0.3723</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>IMPACT ON MAP DUE TO PRETRAINING WITH IMAGENET DATA.</figDesc><table><row><cell></cell><cell cols="2">Balanced Set Full Set</cell></row><row><cell>No pretraining</cell><cell>0.1570</cell><cell>0.3723</cell></row><row><cell>With pretraining</cell><cell>0.2385</cell><cell>0.3939</cell></row><row><cell cols="3">Fig. 3. Comparison of the performance of ImageNet-pretrained model and</cell></row><row><cell cols="3">random-initialized model with different training data volume.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>IMPACT ON MAP DUE TO VARIOUS BALANCED SAMPLING AND DATA AUGMENTATION STRATEGIES.</figDesc><table><row><cell></cell><cell cols="2">Balanced Set Full Set</cell></row><row><cell>Baseline</cell><cell>0.2385</cell><cell>0.3939</cell></row><row><cell>+ Balanced Sampling</cell><cell>-</cell><cell>0.3721</cell></row><row><cell>+ Time-Frequency Masking</cell><cell>0.2818</cell><cell>0.4265</cell></row><row><cell>+ Mix-up Training</cell><cell>0.3108</cell><cell>0.4397</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>AS A FUNCTION OF MIX-UP RATE (TRAINING ON BALANCED SET WITH ? = 10). An additional form of data augmentation we explored is called mix-up training where weighted combinations of audio samples are combined to make new samples. Mix-up training creates convex combinations of pairs of examples and their corresponding labels. Studies have shown it can improve the performance of image classification, voice command recognition</figDesc><table><row><cell>Mixup Rate</cell><cell>0</cell><cell>0.2</cell><cell>0.5</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell>mAP</cell><cell cols="5">0.2818 0.3060 0.3108 0.3119 0.2928</cell></row><row><cell cols="2">C. Mix-up Training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII CORRELATION</head><label>VII</label><figDesc>COEFFICIENTS BETWEEN CLASS-WISE AP AND CLASS SAMPLE COUNT/ANNOTATION QUALITY ESTIMATE RELEASED BY AUDIOSET AUTHORS.</figDesc><table><row><cell></cell><cell cols="2">Balanced Set Full Set</cell></row><row><cell>AP and Sample Count</cell><cell>0.1692</cell><cell>0.0946</cell></row><row><cell>AP and Annotation Quality Estimate</cell><cell>0.2464</cell><cell>0.2629</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Dataset D = {x (i) , y (i) }, i ? {1, ..., N } Threshold Set T = {t k }, k ? {1, ..., 527} 1: for k ? {1, ..., 527} do</figDesc><table><row><cell>Algorithm 2 Label Enhancement</cell></row><row><cell>Require:</cell></row><row><cell>Teacher Model M</cell></row><row><cell>Label Ontology O</cell></row><row><cell>Procedure 1: Generate Label Modification Threshold</cell></row><row><cell>Input: M, D</cell></row><row><cell>Output: 2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII RESULT</head><label>VIII</label><figDesc>OF LABEL ENHANCEMENT ON THE BALANCED SET (NOTE THE MAP WITHOUT LABEL ENHANCEMENT IS 0.3108?0.0013).</figDesc><table><row><cell></cell><cell>Type I</cell><cell>Type II</cell><cell>Type I and II</cell></row><row><cell># Impact Classes</cell><cell>212</cell><cell>93</cell><cell>274</cell></row><row><cell>Label Added (%)</cell><cell>3.7%</cell><cell>3.9%</cell><cell>7.2%</cell></row><row><cell>Impacted Class Improvement</cell><cell>4.5%</cell><cell>3.8%</cell><cell>4.5%</cell></row><row><cell>Non-impacted Class Improvement</cell><cell>1.9%</cell><cell>2.1%</cell><cell>1.3%</cell></row><row><cell>Mean Class-wise Relative Improv.</cell><cell>3.0%</cell><cell>2.4%</cell><cell>2.9%</cell></row><row><cell>mAP Improvement</cell><cell>1.9%</cell><cell>1.5%</cell><cell>1.7%</cell></row><row><cell>mAP</cell><cell>0.3166 ?0.0016</cell><cell>0.3156 ?0.0007</cell><cell>0.3162 ?0.0005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX AUDIOSET</head><label>IX</label><figDesc>LABEL ENHANCEMENT (LE) EXPERIMENT RESULTS. WE USE THE MEAN, 25TH PERCENTILE (25P), 10TH PERCENTILE (10P), AND 5TH PERCENTILE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI RESULTS</head><label>XI</label><figDesc>OF MODEL ENSEMBLE. FOR EACH EXPERIMENT, WE SHOW THE NUMBER OF THE MODELS IN THE COMMITTEE (# MODELS), THE AVERAGE MAP OF MODELS IN THE COMMITTEE (AVG MAP), THE MAP OF THE BEST MODEL IN THE COMMITTEE (BEST MAP), AND THE MAP OF THE ENSEMBLE MODEL (ENSEMBLE MAP). NOTE THAT FOR ALL EXPERIMENTS, THE ENSEMBLE MAP IS HIGHER THAN THE BEST MAP.</figDesc><table><row><cell></cell><cell cols="4"># Models Avg mAP Best mAP Ensemble mAP</cell></row><row><cell cols="2">Checkpoints of a Single Run</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Balanced</cell><cell>60</cell><cell>0.2369</cell><cell>0.3169</cell><cell>0.3280</cell></row><row><cell>Full</cell><cell>30</cell><cell>0.4236</cell><cell>0.4406</cell><cell>0.4518</cell></row><row><cell cols="3">Multiple Runs with Same Setting</cell><cell></cell><cell></cell></row><row><cell>Balanced</cell><cell>3</cell><cell>0.3162</cell><cell>0.3167</cell><cell>0.3446</cell></row><row><cell>Full</cell><cell>3</cell><cell>0.4397</cell><cell>0.4405</cell><cell>0.4641</cell></row><row><cell cols="3">Models Trained with Different Settings</cell><cell></cell><cell></cell></row><row><cell>Bal-pretrain</cell><cell>2</cell><cell>0.1978</cell><cell>0.2385</cell><cell>0.2410</cell></row><row><cell>Bal-mixup rate</cell><cell>5</cell><cell>0.3009</cell><cell>0.3123</cell><cell>0.3476</cell></row><row><cell>Bal-mixup-?</cell><cell>3</cell><cell>0.3071</cell><cell>0.3123</cell><cell>0.3418</cell></row><row><cell>Bal-augment</cell><cell>3</cell><cell>0.2775</cell><cell>0.3123</cell><cell>0.3281</cell></row><row><cell>Bal-label</cell><cell>4</cell><cell>0.3146</cell><cell>0.3169</cell><cell>0.3503</cell></row><row><cell>Bal-top5</cell><cell>5</cell><cell>0.3168</cell><cell>0.3180</cell><cell>0.3527</cell></row><row><cell>Bal-all</cell><cell>20</cell><cell>0.2987</cell><cell>0.3180</cell><cell>0.3620</cell></row><row><cell>Full-pretrain</cell><cell>2</cell><cell>0.3831</cell><cell>0.3939</cell><cell>0.4006</cell></row><row><cell>Full-augment</cell><cell>4</cell><cell>0.4080</cell><cell>0.4396</cell><cell>0.4578</cell></row><row><cell>Full-label</cell><cell>4</cell><cell>0.4397</cell><cell>0.4400</cell><cell>0.4653</cell></row><row><cell>Full-top5</cell><cell>5</cell><cell>0.4396</cell><cell>0.4405</cell><cell>0.4690</cell></row><row><cell>Full-all</cell><cell>10</cell><cell>0.4201</cell><cell>0.4405</cell><cell>0.4744</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XII ABLATION</head><label>XII</label><figDesc>STUDY RESULTS ON AUDIOSET.</figDesc><table><row><cell></cell><cell cols="2">Balanced AudioSet Full AudioSet</cell></row><row><cell>PSLA Model</cell><cell>0.3280</cell><cell>0.4518</cell></row><row><cell>PSLA Model -Pretrain</cell><cell>0.2379</cell><cell>0.4302</cell></row><row><cell>PSLA Model -Balanced Sampling</cell><cell>-</cell><cell>0.3688</cell></row><row><cell>PSLA Model -Masking</cell><cell>0.3154</cell><cell>0.4430</cell></row><row><cell>PSLA Model -Mixup</cell><cell>0.3181</cell><cell>0.4493</cell></row><row><cell>PSLA Model -Label Enhancement</cell><cell>0.3229</cell><cell>-</cell></row><row><cell>PSLA Model -Ensemble</cell><cell>0.3162</cell><cell>0.4397</cell></row><row><cell>PSLA Model -Ensemble + WA</cell><cell>0.3192</cell><cell>0.4435</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIII COMPARISON</head><label>XIII</label><figDesc>OF THE PERFORMANCE ON MAP OF VARIOUS MODELS TRAINED WITH PSLA AND WITHOUT PSLA ON THE BALANCED AND FULL AUDIOSET.</figDesc><table><row><cell></cell><cell># Params</cell><cell cols="3">Balanced AudioSet</cell><cell></cell><cell>Full AudioSet</cell><cell></cell></row><row><cell></cell><cell></cell><cell>No PSLA</cell><cell>PSLA</cell><cell cols="2">Imp.(%) No PSLA</cell><cell>PSLA</cell><cell>Imp.(%)</cell></row><row><cell>MobileNet V2</cell><cell>2.90M</cell><cell>0.1612</cell><cell>0.2650</cell><cell>64.4</cell><cell>0.3032</cell><cell>0.4058 (Single: 0.3940)</cell><cell>33.8</cell></row><row><cell>EfficientNet-B0, Single-headed Attention</cell><cell>5.36M</cell><cell>0.1529</cell><cell>0.3350</cell><cell>119.1</cell><cell>0.3789</cell><cell>0.4493 (Single: 0.4391)</cell><cell>18.6</cell></row><row><cell>EfficientNet-B2, Mean Pooling</cell><cell>8.44M</cell><cell>0.1903</cell><cell>0.3317</cell><cell>74.3</cell><cell>0.3325</cell><cell>0.4455 (Single: 0.4382)</cell><cell>34.0</cell></row><row><cell>EfficientNet-B2, Single-headed Attention</cell><cell>9.19M</cell><cell>0.1478</cell><cell>0.3406</cell><cell>130.4</cell><cell>0.3818</cell><cell>0.4556 (Single: 0.4414)</cell><cell>19.3</cell></row><row><cell>EfficientNet-B2, 4-headed Attention</cell><cell>13.64M</cell><cell>0.1570</cell><cell>0.3280</cell><cell>108.9</cell><cell>0.3723</cell><cell>0.4518 (Single: 0.4435)</cell><cell>21.4</cell></row><row><cell>ResNet-50, Single-headed Attention</cell><cell>25.66M</cell><cell>0.1635</cell><cell>0.3180</cell><cell>94.5</cell><cell>0.3790</cell><cell>0.4477 (Single: 0.4042)</cell><cell>18.1</cell></row><row><cell>TABLE XIV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">EXPERIMENT RESULT ON FSD50K DATASET.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FSD50K Eval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSD50K Baseline [11]</cell><cell>0.434</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio Transformers [59]</cell><cell>0.537</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model</cell><cell>0.5671</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Pretrain</cell><cell>0.4524</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Balanced Sampling</cell><cell>0.5626</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Masking</cell><cell>0.5617</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Mixup</cell><cell>0.5164</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Label Enhancement</cell><cell>0.5583</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Ensemble</cell><cell>0.5535</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSLA Model -Ensemble + WA</cell><cell>0.5571</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XV COMPARISON</head><label>XV</label><figDesc>WITH PREVIOUS METHODS (UPPER: BALANCED AUDIOSET EXPERIMENTS, LOWER: FULL AUDIOSET EXPERIMENTS).</figDesc><table><row><cell></cell><cell>#Params</cell><cell>mAP</cell><cell>AUC</cell><cell>d</cell></row><row><cell>Wu-minimal [60], 2018</cell><cell>2.6M</cell><cell>-</cell><cell>0.916</cell><cell>1.950</cell></row><row><cell>Kumar [61], 2018</cell><cell>-</cell><cell>0.213</cell><cell>0.927</cell><cell>2.056</cell></row><row><cell>Wu-best [60], 2018</cell><cell>56M</cell><cell>-</cell><cell>0.927</cell><cell>2.056</cell></row><row><cell>Kong [8], 2019</cell><cell>-</cell><cell>0.274</cell><cell>0.949</cell><cell>2.316</cell></row><row><cell>PANNs [5], 2020</cell><cell>81M</cell><cell>0.278</cell><cell>0.905</cell><cell>1.853</cell></row><row><cell>Our Baseline</cell><cell>13.6M</cell><cell>0.1570</cell><cell>0.9108</cell><cell>1.903</cell></row><row><cell>Proposed Single Model</cell><cell>13.6M</cell><cell>0.3192 ?0.0015</cell><cell>0.9534 ?0.0005</cell><cell>2.374 ?0.007</cell></row><row><cell>Proposed 68M Model</cell><cell>13.6M?5</cell><cell>0.3527</cell><cell>0.9602</cell><cell>2.479</cell></row><row><cell>Proposed Full Model</cell><cell>13.6M?20</cell><cell>0.3620</cell><cell>0.9638</cell><cell>2.541</cell></row><row><cell>AudioSet Baseline [3]</cell><cell>-</cell><cell>0.314</cell><cell>0.959</cell><cell>2.452</cell></row><row><cell>Kong [6], 2018</cell><cell>-</cell><cell>0.327</cell><cell>0.965</cell><cell>2.558</cell></row><row><cell>Yu [7], 2018</cell><cell>-</cell><cell>0.360</cell><cell>0.970</cell><cell>2.660</cell></row><row><cell>TALNet [62], 2019</cell><cell>-</cell><cell>0.362</cell><cell>0.965</cell><cell>2.554</cell></row><row><cell>Kong [8], 2019</cell><cell>-</cell><cell>0.369</cell><cell>0.969</cell><cell>2.639</cell></row><row><cell>DeepRes [4], 2019</cell><cell>26M</cell><cell>0.392</cell><cell>0.971</cell><cell>2.682</cell></row><row><cell>PANNs [5], 2020</cell><cell>81M</cell><cell>0.439</cell><cell>0.973</cell><cell>2.725</cell></row><row><cell>Our Baseline</cell><cell>13.6M</cell><cell>0.3723</cell><cell>0.9706</cell><cell>2.672</cell></row><row><cell>Proposed Single Model</cell><cell>13.6M</cell><cell>0.4435 ?0.0008</cell><cell>0.9753 ?0.0003</cell><cell>2.778 ?0.007</cell></row><row><cell>Proposed 68M Model</cell><cell>13.6M?5</cell><cell>0.4690</cell><cell>0.9789</cell><cell>2.872</cell></row><row><cell>Proposed Full Model</cell><cell>13.6M?10</cell><cell>0.4744</cell><cell>0.9810</cell><cell>2.936</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lukemelas/EfficientNet-PyTorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chime-home: A dataset for sound source recognition in a domestic environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sigtia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krstulovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A deep residual network for large-scale acoustic scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2568" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio set classification with attention model: A probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="316" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-level attention model for weakly supervised audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Barsim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly labelled audioset tagging with attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1791" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Class-aware self-attention for audio event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fsd50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00475</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking cnn models for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11154</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Addressing missing labels in large-scale sound event recognition using a teacher-student framework with loss masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1235" to="1239" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ensemble of convolutional neural networks for weakly-supervised sound event detection using multiple scale input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ensemble of convolutional neural networks for the DCASE 2020 acoustic scene classification challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lopez-Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Del Hoyo Ontiveros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The use of the area under the roc curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="146" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Contrastive learning of general-purpose audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10915</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards learning a universal non-semantic representation of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haviv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="140" to="144" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pre-training audio representations with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="600" to="604" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of semantic audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Contrastive predictive coding of audio with an adversary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="826" to="830" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep image features in music information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gwardys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Grzywczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Electronics and Telecommunications</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Esresnet: Environmental sound classification based on visual domain models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4933" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Urban sound tagging using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5486" to="5494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dcase 2017 challenge setup: Tasks, datasets and baseline system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A closer look at weak label learning for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09288</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Large-scale acoustic scene analysis with deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Ford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gender recognition from speech. part i: Coarse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Childers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASA</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1828" to="1840" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gender recognition from speech. part ii: Fine analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Childers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASA</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1841" to="1856" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5150" to="5154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The impact of missing labels and overlapping sound events on multi-label multi-instance learning for sound event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karsmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="159" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hodgepodge: Sound event detection based on ensemble of semi-supervised learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>First</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos</title>
		<imprint>
			<publisher>ACIIW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sound event detection in domestic environments using ensemble of convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Trade-off between diversity and accuracy in ensemble generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-objective machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="429" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Audio transformers: Transformer architectures for large scale audio understanding. adieu convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00335</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reducing model complexity for dnn based largescale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Knowledge transfer from weakly labeled audio using convolutional neural network for sound events and scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khadkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?gen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="326" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
