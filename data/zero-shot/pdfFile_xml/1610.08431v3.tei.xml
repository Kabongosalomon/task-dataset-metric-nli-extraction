<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Broad Context Language Modeling as Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02-16">16 Feb 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Chu</surname></persName>
							<email>zeweichu@uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
							<email>haiwang@ttic.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<email>kgimpel@ttic.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
							<email>mcallester@ttic.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Broad Context Language Modeling as Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02-16">16 Feb 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension <ref type="bibr" target="#b3">(Hermann et al., 2015)</ref>. We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We now describe the models that we employ for the LAMBADA task (Section 2.1) as well as our dataset construction procedure (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Readers</head><p>Hermann et al. (2015)  developed the CNN/Daily Mail comprehension tasks and introduced ques-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The LAMBADA dataset <ref type="bibr" target="#b8">(Paperno et al., 2016)</ref> was designed by identifying word prediction tasks that require broad context. Each instance is drawn from the BookCorpus <ref type="bibr" target="#b12">(Zhu et al., 2015)</ref> and consists of a passage of several sentences where the task is to predict the last word of the last sentence. The instances are manually filtered to find cases that are guessable by humans when given the larger context but not when only given the last sentence. The expense of this manual filtering has limited the dataset to only about 10,000 instances which are viewed as development and test data. The training data is taken to be books in the corpus other than those from which the evaluation passages were extracted. <ref type="bibr" target="#b8">Paperno et al. (2016)</ref> provide baseline results with popular language models and neural network architectures; all achieve zero percent accuracy. The best accuracy is 7.3% obtained by randomly choosing a capitalized word from the passage.</p><p>Our approach is based on the observation that in 83% of instances the answer appears in the context. We exploit this in two ways. First, we automatically construct a large training set of 1.8 million instances by simply selecting passages where the answer occurs in the context. Second, we treat the problem as a reading comprehension task similar to the CNN/Daily Mail datasets introduced by <ref type="bibr" target="#b3">Hermann et al. (2015)</ref>, the Children's Book Test (CBT) of <ref type="bibr" target="#b4">Hill et al. (2016)</ref>, and the Who-did-What dataset of . We show that standard models for reading comprehension, trained on our automatically generated training set, improve the state of the art on the LAMBADA test set from 7.3% to 49.0%. This is in spite of the fact that these models fail on the 17% of instances in which the answer is not in the context.</p><p>We also perform a manual analysis of the LAM-BADA task, provide an estimate of human performance, and categorize the instances in terms of the phenomena they test. We find that the comprehension models perform best on instances that require selecting a name from the context based on dialogue or discourse cues, but struggle when required to do coreference resolution or when external knowledge could help in choosing the answer. tion answering models based on neural networks. Many others have been developed since. We refer to these models as "neural readers". While a detailed survey is beyond our scope, we briefly describe the neural readers used in our experiments: the Stanford <ref type="bibr" target="#b0">(Chen et al., 2016)</ref>, Attention Sum <ref type="bibr" target="#b6">(Kadlec et al., 2016)</ref>, and Gated-Attention <ref type="bibr" target="#b2">(Dhingra et al., 2016)</ref> Readers. These neural readers use attention based on the question and passage to choose an answer from among the words in the passage. We use d for the context word sequence, q for the question (with a blank to be filled), A for the candidate answer list, and V for the vocabulary. We describe neural readers in terms of three components:</p><p>1. Embedding and Encoding: Each word in d and q is mapped into a v-dimensional vector via the embedding function e(w) ? R v , for all w ? d ? q. 1 The same embedding function is used for both d and q. The embeddings are learned from random initialization; no pretrained word embeddings are used. The embedded context is processed by a bidirectional recurrent neural network (RNN) which computes hidden vectors h i for each position i:</p><formula xml:id="formula_0">h ? = fRNN (? ? d , e(d)) h ? = bRNN (? ? d , e(d)) h = h ? , h ?</formula><p>where ? ? d and ? ? d are RNN parameters, and each of fRNN and bRNN return a sequence of hidden vectors, one for each position in the input e(d). The question is encoded into a single vector g which is the concatenation of the final vectors of two RNNs:</p><formula xml:id="formula_1">g ? = fRNN (? ? q , e(q)) g ? = bRNN (? ? q , e(q)) g = g ? |q| , g ? 0</formula><p>The RNNs use either gated recurrent units <ref type="bibr" target="#b1">(Cho et al., 2014)</ref> or long short-term memory <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attention:</head><p>The readers then compute attention weights on positions of h using g. In general, we define ? i = softmax(att (h i , g)), where i ranges over positions in h. The att function is an inner product in the Attention Sum Reader and a bilinear product in the Stanford Reader. The computed attentions are then passed through a softmax function to form a probability distribution. The Gated-Attention Reader uses a richer attention architecture <ref type="bibr" target="#b2">(Dhingra et al., 2016)</ref>; space does not permit a detailed description.</p><p>3. Output and Prediction: To output a prediction a * , the Stanford Reader computes the attentionweighted sum of the context vectors and then an inner product with each candidate answer:</p><formula xml:id="formula_2">c = |d| i=1 ? i h i a * = argmax a?A o(a) ? c</formula><p>where o(a) is the "output" embedding function.</p><p>As the Stanford Reader was developed for the anonymized CNN/Daily Mail tasks, only a few entries in the output embedding function needed to be well-trained in their experiments. However, for LAMBADA, correct answers can range over the entirety of V, making the output embedding function difficult to train. Therefore we also experiment with a modified version of the Stanford Reader that uses the same embedding function e for both input and output words:</p><formula xml:id="formula_3">a * = argmax a?A e(a) ? W c<label>(1)</label></formula><p>where W is an additional parameter matrix used to match dimensions and model any additional needed transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For the Attention Sum and Gated-Attention</head><p>Readers the answer is computed by:</p><formula xml:id="formula_4">?a ? A, P (a|d, q) = i?I(a,d) ? i a * = argmax a?A P (a|d, q)</formula><p>where I(a, d) is the set of positions where a appears in context d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Data Construction</head><p>Each LAMBADA instance is divided into a context (4.6 sentences on average) and a target sentence, and the last word of the target sentence is the target word to be predicted. The LAM-BADA dataset consists of development (DEV) and test (TEST) sets; <ref type="bibr" target="#b8">Paperno et al. (2016)</ref> also provide a control dataset (CONTROL), an unfiltered sample of instances from the BookCorpus. We construct a new training dataset from the BookCorpus. We restrict it to instances that contain the target word in the context. This decision is natural given our use of neural readers that assume the answer is contained in the passage. We also ensure that the context has at least 50 words and contains 4 or 5 sentences and we require the target sentences to have more than 10 words. Some neural readers require a candidate target word list to choose from. We list all words in the context as candidate answers, except for punctuation. 2 Our new dataset contains 1,827,123 instances in total. We divide it into two parts, a training set (TRAIN) of 1,618,782 instances and a validation set (VAL) of 208,341 instances. These datasets can be found at the authors' websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We use the Stanford Reader <ref type="bibr" target="#b0">(Chen et al., 2016)</ref>, our modified Stanford Reader (Eq. 1), the Attention Sum (AS) Reader <ref type="bibr" target="#b6">(Kadlec et al., 2016)</ref>, and the Gated-Attention (GA) Reader <ref type="bibr" target="#b2">(Dhingra et al., 2016)</ref>. We also add the simple features from  to the AS and GA Readers. The features are concatenated to the word embeddings in the context. They include: whether the word appears in the target sentence, the frequency of the word in the context, the position of the word's first occurrence in the context as a percentage of the context length, and whether the text surrounding the word matches the text surrounding the blank in the target sentence. For the last feature, we only consider matching the left word since the blank is always the last word in the target sentence.</p><p>All models are trained end to end without any warm start and without using pretrained embeddings. We train each reader on TRAIN for a max of 10 epochs, stopping when accuracy on DEV decreases two epochs in a row. We take the model from the epoch with max DEV accuracy and evaluate it on TEST and CONTROL. VAL is not used.</p><p>We evaluate several other baseline systems inspired by those of <ref type="bibr" target="#b8">Paperno et al. (2016)</ref>, but we focus on versions that restrict the choice of answers to non-stopwords in the context. <ref type="bibr">3</ref> We found this 2 This list of punctuation symbols is at https: //raw.githubusercontent.com/ZeweiChu/ lambada-dataset/master/stopwords/ shortlist-stopwords.txt <ref type="bibr">3</ref> We use the stopword list from <ref type="bibr" target="#b9">Richardson et al. (2013)</ref>.  <ref type="table">Table 1</ref>: Accuracies on TEST and CONTROL datasets, computed over all instances ("all") and separately on those in which the answer is in the context ("context"). The first section is from <ref type="bibr" target="#b8">Paperno et al. (2016)</ref>. * Estimated from 100 randomly-sampled DEV instances. ? Estimated from 100 randomly-sampled CONTROL instances. strategy to consistently improve performance even though it limits the maximum achievable accuracy. We consider two n-gram language model baselines. We use the SRILM toolkit <ref type="bibr" target="#b10">(Stolcke, 2002)</ref> to estimate a 4-gram model with modified Kneser-Ney smoothing on the combination of TRAIN and VAL. One uses a cache size of 100 and the other does not use a cache. We use each model to score each non-stopword from the context. We also evaluate an LSTM language model. We train it on TRAIN, where the loss is cross entropy summed over all positions in each instance. The output vocabulary is the vocabulary of TRAIN, approximately 130k word types. At test time, we again limit the search to non-stopwords in the context.</p><p>We also test simple baselines that choose particular non-stopwords from the context, including a random one, the first in the context, the last in the context, and the most frequent in the context. <ref type="table">Table 1</ref> shows our results. We report accuracies on the entirety of TEST and CONTROL ("all"), as well as separately on the part of CONTROL where the target word is in the context ("context"). The first part of the table shows results from <ref type="bibr" target="#b8">Paperno et al. (2016)</ref>. We then show our baselines that choose a word from the context. Choosing the most frequent yields a surprisingly high accuracy of 11.7%, which is better than all results from Paperno et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our language models perform comparably, with the n-gram + cache model doing best. By forcing language models to select a word from the context, the accuracy on TEST is much higher than the analogous models from Paperno et al., though accuracy suffers on CONTROL.</p><p>We then show results with the neural readers, showing that they give much higher accuracies on TEST than all other methods. The GA Reader with the simple additional features  yields the highest accuracy, reaching 49.0%. We also measured the "top k" accuracy of this model, where we give the model credit if the correct answer is among the top k ranked answers. On TEST, we reach 65.4% top-2 accuracy and 72.8% top-3.</p><p>The AS and GA Readers work much better than the Stanford Reader. One cause appears to be that the Stanford Reader learns distinct embeddings for input and answer words, as discussed above. Our modified Stanford Reader, which uses only a single set of word embeddings, improves by 10.4% absolute. Since the AS and GA Readers merely score words in the context, they do not learn separate answer word embeddings and therefore do not suffer from this effect.</p><p>We suspect the remaining accuracy difference between the Stanford and the other readers is due to the difference in the output function. The Stanford Reader was developed for the CNN and Daily Mail datasets, in which correct answers are anonymized entity identifiers which are reused across instances. Since the identifier embeddings are observed so frequently in the training data, they are frequently updated. In our setting, however, answers are words from a large vocabulary, so many of the word embeddings of correct answers may be undertrained. This could potentially be addressed by augmenting the word embeddings with identifiers to obtain some of the modeling benefits of anonymization .</p><p>All context restricted models yield poor accuracies on the entirety of CONTROL. This is due to the fact that only 14.1% of CONTROL instances  have the target word in the context, so this sets the upper bound that these models can achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Manual Analysis</head><p>One annotator, a native English speaker, sampled 100 instances randomly from DEV, hid the final word, and attempted to guess it from the context and target sentence. The annotator was correct in 86 cases. For the subset that contained the answer in the context, the annotator was correct in 79 of 87 cases. Even though two annotators were able to correctly answer all LAMBADA instances during dataset construction <ref type="bibr" target="#b8">(Paperno et al., 2016)</ref>, our results give an estimate of how often a third would agree. The annotator did the same on 100 instances randomly sampled from CONTROL, guessing correctly in 36 cases. These results are reported in <ref type="table">Table 1</ref>. The annotator was correct on 6 of the 12 CONTROL instances in which the answer was contained in the context. We analyzed the 100 LAMBADA DEV instances, tagging each with labels indicating the minimal kinds of understanding needed to answer it correctly. 4 Each instance can have multiple labels. We briefly describe each label below:</p><p>? single name cue: the answer is clearly a name according to contextual cues and only a single name is mentioned in the context.</p><p>? simple speaker tracking: instance can be answered merely by tracking who is speaking without understanding what they are saying.</p><p>? basic reference: answer is a reference to something mentioned in the context; simple understanding/context matching suffices.</p><p>? discourse inference rule: answer can be found by applying a single discourse inference rule, such as the rule: "X left Y and went in search of Z" ? Y = Z.</p><p>? semantic trigger: amorphous semantic information is needed to choose the answer, typically related to event sequences or dialogue turns, e.g., a customer says "Where is the X?" and a supplier responds "We got plenty of X".</p><p>? coreference: instance requires non-trivial coreference resolution to solve correctly, typically the resolution of anaphoric pronouns.</p><p>? external knowledge: some particular external knowledge is needed to choose the answer. <ref type="table" target="#tab_2">Table 2</ref> shows the breakdown of these labels across instances, as well as the accuracy on each label of the GA Reader with features.</p><p>The GA Reader performs well on instances involving shallower, more surface-level cues. In 9 cases, the answer is clearly a name based on contextual cues in the target sentence and there is only one name in the context; the reader answers all but one correctly. When only simple speaker tracking is needed (19 cases), the reader gets 84% correct.</p><p>The hardest instances are those that involve deeper understanding, like semantic links, coreference resolution, and external knowledge. While external knowledge is difficult to define, we chose this label when we were able to explicitly write down the knowledge that one would use when answering the instances, e.g., one instance requires knowing that "when something explodes, noise emanates from it". These instances make up nearly a quarter of those we analyzed, making LAMBADA a good task for work in leveraging external knowledge for language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head><p>On CONTROL, while our readers outperform our other baselines, they are outperformed by the language modeling baselines from Paperno et al. This suggests that though we have improved the state of the art on LAMBADA by more than 40% absolute, we have not solved the general language modeling problem; there is no single model that performs well on both TEST and CONTROL. Our 36% estimate of human performance on CONTROL shows the difficulty of the general problem, and reveals a gap of 14% between the best language model and human accuracy.</p><p>A natural question to ask is whether applying neural readers is a good direction for this task, since they fail on the 17% of instances which do not have the target word in the context. Furthermore, this subset of LAMBADA may in fact display the most interesting and challenging phenomena. Some neural readers, like the Stanford Reader, can be easily used to predict target words that do not appear in the context, and the other readers can be modified to do so. Doing this will require a different selection of training data than that used above. However, we do wish to note that, in addition to the relative rarity of these instances in LAMBADA, we found them to be challenging for our annotator (who was correct on only 7 of the 13 in this subset).</p><p>We note that TRAIN has similar characteristics to the part of CONTROL that contains the answer in the context (the final column of <ref type="table">Table 1</ref>). We find that the ranking of systems according to this column is similar to that in the TEST column. This suggests that our simple method of dataset creation could be used to create additional training or evaluation sets for challenging language modeling problems like LAMBADA, perhaps by combining it with baseline suppression .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We constructed a new training set for LAMBADA and used it to train neural readers to improve the state of the art from 7.3% to 49%. We also provided results with several other strong baselines and included a manual evaluation in an attempt to better understand the phenomena tested by the task. Our hope is that other researchers will seek models and training regimes that simultaneously perform well on both LAMBADA and CONTROL, with the goal of solving the general problem of language modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Labels derived from manual analysis of</cell></row><row><cell>100 LAMBADA DEV instances. An instance can</cell></row><row><cell>be tagged with multiple labels, hence the sum of</cell></row><row><cell>instances across labels exceeds 100.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We overload the e function to operate on sequences and denote the embedding of d and q as matrices e(d) and e(q).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The annotations are available from the authors' websites.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Denis Paperno for answering our questions about the LAMBADA dataset and we thank NVIDIA Corporation for donating GPUs used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Who did What: A large-scale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germn</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<title level="m">Emergent logical structure in vector representations of neural readers</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
