<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">2nd Place Solution for Waymo Open Dataset Challenge -Real-time 2D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Xing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">2nd Place Solution for Waymo Open Dataset Challenge -Real-time 2D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In an autonomous driving system, it is essential to recognize vehicles, pedestrians and cyclists from images. Besides the high accuracy of the prediction, the requirement of real-time running brings new challenges for convolutional network models. In this report, we introduce a real-time method to detect the 2D objects from images. We aggregate several popular one-stage object detectors and train the models of variety input strategies independently, to yield better performance for accurate multi-scale detection of each category, especially for small objects. For model acceleration, we leverage TensorRT to optimize the inference time of our detection pipeline. As shown in the leaderboard, our proposed detection framework ranks the 2nd place with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the Waymo Open Dataset Challenges, while our framework achieves the latency of 45.8ms/frame on an Nvidia Tesla V100 GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is practical to recognize different objects on the road for an autonomous driving system, especially object detection tasks are the basic solutions for images captured by cameras on the public roads. The Waymo Open Dataset challenges provide a platform for different scenes. The Waymo Open Dataset <ref type="bibr" target="#b6">[7]</ref> collects the labeled images captured from high-quality cameras and LiDAR sensors which are utilized on the vehicle for multi-view modeling in real autonomous driving. In real-time 2D detection task, there are three categories (vehicle, pedestrian and cyclist) annotated with 2D bounding boxes. Compared with the 2D detection challenge in 2020, the real-time 2D detection challenge requests that the submitted model must run faster than 70ms/frame on a Nvidia Tesla V100 GPU in this year. We conduct a state-of-the-art real-time 2D object detection framework in this challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Solution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Base Detectors</head><p>Deep learning based object detectors are roughly divided into two mainstreams, one-stage detector and two-stage detector. In order to achieve robust detection results, we aggregate the popular one-stage object detectors with different settings. Considering of the runtime, we choose the popular one-stage detector YOLOR <ref type="bibr" target="#b8">[9]</ref>, which is the one of the upgraded version from YOLOv1 <ref type="bibr" target="#b3">[4]</ref>, and achieves the stateof-the-art of real-time object detection on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">YOLO</head><p>The core idea of YOLOv1 is to use the whole graph as the input of the network and directly return to the location and category of the bounding box in the output layer. Compared with other object detection algorithms, the detection speed of YOLO is very fast. In YOLOv2 <ref type="bibr" target="#b4">[5]</ref>, Some new features have been added such as batch normalization, high resolution classifier, convolutional with anchor boxes, dimension clusters and Darknet-19 network. YOLOv3 <ref type="bibr" target="#b5">[6]</ref> uses Darknet-53 as the backbone and uses the idea of pyramid feature map for reference, small feature map is used to detect large objects, while large feature map is used to detect small objects. The main innovations of YOLOv4 <ref type="bibr" target="#b6">[7]</ref> are mosaic data enhancement, self-adversarial training and cross mini-batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">YOLOR</head><p>YOLOR applies the implicit and explicit knowledge to the model training at the same time so that it can learn a general representation and complete various tasks through this general representation. The implicit knowledge can be modeled by vector z, neural network W z, or matrix factorization Z T c. Through kernel space alignment, prediction refinement, and multi-task learning, implicit knowledge is integrated into the explicit knowledge which can greatly improve the performance and generalization ability of the model. Compared with other detection methods on COCO dataset, the mAP of YOLOR is 3.8% higher than PP-YOLOv2 <ref type="bibr" target="#b1">[2]</ref> at the same inference speed and the inference speed has been increased 88% at the same accuracy compared with Scaled-YOLOv4-P7 <ref type="bibr" target="#b7">[8]</ref>. We choose YOLOR as our detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scale Enhancement for Small Objects</head><p>According to the statistical distribution of the waymoopen-dataset. Nearly one-third of objects belong to small objects, whose areas are less than 32?32 pixels. Due to the fixed camera position in waymo data-collection car, the scene of the target is relatively fixed. The top-left image in <ref type="figure">Figure 2</ref> is a typical example. The small vehicle objects always locate at the mid part of the image. And we make a statistic on this attribute. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, most of the small box centers locate at the center area of the y-axis. So, we crop the center area and resize it into 1.5? larger. This strategy can help with detecting these small objects. As shown in <ref type="figure">Figure 2</ref>, given an image, we first use the trained model to predict the bounding-box in original shape (1920?1280 or 1920?886), which we called Result A. Next, we crop the target area (0 ? W , 0.3 ? H, 1.0 ? W , 0.8 ? H) and resize the cropped image into 1.5?scale (2880?960 or 2880?704). Then we input the enlarged image to the same model to get an enlarged result and rescale it into the original size for the results ensemble, which is called Result B. Because we use the rescaled image to make the finest predictions on small objects. So, we remove all targets whose areas are larger than 96?96 in Result B. Then the Result A and the bounding-boxes whose areas are less than 96?96 in Result B are collected together for non-maximum-suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Anchor Selection</head><p>We use K-Means clustering to generate the hyperparameters for Real-time 2D Detection on the Waymo Open Dataset. However, the model fails to outperform baseline using our hyperparameter setting, as shown in <ref type="table" target="#tab_0">Table  1</ref>. It may possibly result from the fact that the width and height of small anchor set changes a lot after clustering with medium and large objects, e.g. the aspect ratio of the smallest anchor is 1.07, but it is 0.703 in original set. To validate our conjecture, we only do clustering for small objects (W ? H &lt; 64 ? 64) and achieve better results for vehicle and pedestrian but worse results for cyclist, where vehicle and pedestrian involve more small objects than cyclist. Since the AP of vehicle and pedestrian is much lower than YOLO-W6 in <ref type="table" target="#tab_3">Table 4</ref>, and only the cyclist result of YOLO-P6 is adopted in final model ensembling, we adopt baseline result of YOLO-P6 to get better results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Model Acceleration</head><p>Real-time 2D Detection means our model not only needs to achieve accurate detection but also runs at a fast inference time. Although the YOLOR-P6 <ref type="bibr" target="#b8">[9]</ref> can meet this demand, we still need to accelerate the inference time for the multimodel ensemble. To this end, we adopt TensorRT for model inference accelerating. TensorRT is a high-performance deep learning inference optimizer and can greatly reduce the inference time while reserving the accuracy. <ref type="table" target="#tab_1">Table 2</ref> is our experiment result on the mini-val dataset, all results are evaluated at the resolution of (1920, 886). Besides, it supports FP16 precision inference on Nvidia Tesla V100 GPU (16GB), which can further accelerate the inference process and reduce GPU memory cost. The inference time of FP32 precision on Tesla V100 GPU is 37ms while the inference time of FP16 precision achieves 19ms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Training Data Filtering</head><p>When we analyze the current data annotations, we find that there are some difficult annotations. Some objects are occluded, but they are still annotated since the former or the latter frame could indicate its class. As shown in <ref type="figure">Figure 4</ref>, the green box is occluded in T frame, but it is still labeled as cyclist according to the T ? 1 frame. These annotations are valuable for tracking but bring challenges for the 2D detection method which does not use temporal information. Because these difficult objects may cause problems for the training. Therefore, we learn from the self-learning method which use multi-different models check with each other to automatically clean the dataset during the model training process to solve this problem and improve the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Evaluation</head><p>Dataset. The Real-time 2D Detection for Waymo Open Dataset <ref type="bibr" target="#b2">[3]</ref> contains 798, 202 and 150 video sequences in the training, validation, and testing sets, respectively. Each sequence has 5 views, where each camera captures 171-200 frames with the image resolution of 1920 ? 1280 pixels or 1920 ? 886 pixels. Our models are pre-trained on the COCO dataset then fine-tuned on the Real-time 2D Detection for Waymo Open Dataset. We follow <ref type="bibr" target="#b0">[1]</ref> and sample the dataset to construct a small dataset that can represent the entire dataset for quick experiment. In the final submission, we use the full dataset to train the model to get better performance. Evaluation Metrics. According to the Real-time 2D Detection for Waymo Open Dataset track, we report detection results on the Level 2 Average Precision (AP) that averages over vehicle, pedestrian, and cyclist classes. The positive IoU thresholds are set to 0.7, 0.5, and 0.5 for evaluating vehicles, cyclists, and pedestrians, respectively. In this track, another important evaluation indicator is speed, which is limited to 70ms/frame on Nvidia Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>YOLOR-P6 and YOLOR-W6 are two different variants of YOLOR network, both of them use the same downsampling method, and YOLOR-W6 is a wider version than YOLOR-P6. The base channel number of YOLOR-P6 is {128, 256, 384, 512, 640}, while that of YOLOR-W6 is YOLOR-P6. Cause YOLOR-P6 Network is the fastest network variant in YOLOR series <ref type="bibr" target="#b8">[9]</ref>. We choose YOLOR-P6 as our baseline for meeting the demand of latency and train a 3-class model directly. We fine-tune the pre-trained model in Waymo Open Dataset. To speed up the training process, we use the same data sampling strategy like <ref type="bibr" target="#b0">[1]</ref>. All models are trained for 50 epochs with a warm-up strategy from 4e-4 to 1e-2. Then the learning rate drops from 1e-2 to 1e-3 in a cosine function. We also use multi-scale training, and the input size multiplies a random coefficient range from (0.5, 1.2). During testing, the model makes predictions on the input at (1920, 1280), which is the original shape of the front camera image. For a side camera image whose resolution is (1920, 886), a constant value is used to pad the image. We use random data augmentation to enhance the data, which includes random flip left-right, random Hue, random Saturation, and image translation. Finally, a classaware NMS is applied to filter out the overlapped boxes. Different class has different NMS IoU threshold, the setting used in our experiment is { vehicle : 0.75, pedestrian : 0.55, cyclist : 0.55}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YOLOR-W6</head><p>. Compared with YOLOR-P6, YOLOR-W6 has the same structure but has more parameters than YOLOR-P6. We use the same training setting mentioned above in training the YOLOR-W6 network.</p><p>Multi-Model Ensemble. For each network structure, we first use the scale enhancement strategy mentioned at 2.2 to ensemble the predictions of different scales. Then, according to <ref type="table" target="#tab_3">Table 4</ref>, YOLOR-W6 performs well in the vehicle and pedestrian class while YOLOR-P6 performs well in the cyclist class. We choose the vehicle and pedestrian boxes from YOLOR-W6 and gather the cyclist boxes from YOLOR-P6. In this strategy, we can collect advantages from different models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>As shown in <ref type="table" target="#tab_4">Table 5</ref>, based on the YOLOR detector, we obtained the baseline model and carried out five improvement experiments to steadily improve the results. In the baseline model, we find the small objects are missed and some error results are detected due to difficult annotations. Next, we propose auto data cleaning, which improves the quality of the dataset a lot and increase the performance by 1.30%; Then, we add multi-scale training and get an improvement by 1.26%; So far, our model worked pretty well for large objects. For small objects, we use scale enhancement strategy and get 1.88% increasing; In addition, we introduce independent threshold-NMS to get better performance. In the last, we ensemble different model which performs better on different class to improve the final performance. At this point, our algorithm's total performance in mAP has increased from 61.54% to 66.67%, an increase of 5.13%. Details are shown in <ref type="table" target="#tab_4">Table 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this report, we present a state-of-the-art real-time 2D object detection framework for self-driving scenarios. Specifically, in order to achieve robust detection results, we aggregate the popular one-stage object detectors with the scale enhancement strategy. In addition, we train the models of various input strategies independently, to yield better performance for accurate multi-scale detection of each category. Our overall detection framework achieves the 2nd place in the realtime 2D detection track of the Waymo Open Dataset Challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Examples of images in Waymo Open Dataset, there are totally 5 cameras with different resolution. The pipeline of our solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Statistics heatmap of small objects' center in the training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>T- 1 TFigure 4 .</head><label>14</label><figDesc>Difficult annotations are colored in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Anchor Selection Experiment. Veh., Ped. and Cyc. refer to Vehicle, Pedestrian and Cyclist, correspondingly.</figDesc><table><row><cell>Method</cell><cell cols="2">Veh. Ped. Cyc. Mean</cell></row><row><cell>Baseline</cell><cell>61.5 73.5 57.3</cell><cell>64.1</cell></row><row><cell>All Objects Cluster</cell><cell>61.5 74.1 56.2</cell><cell>63.9</cell></row><row><cell cols="2">Small Objects Cluster 61.7 74.6 55.1</cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>TensorRT Experiments on Nvidia Tesla P40 GPU.</figDesc><table><row><cell>Method</cell><cell cols="2">Veh. Ped. Cyc. Latency (s)</cell></row><row><cell>Baseline</cell><cell>61.5 73.5 57.3</cell><cell>0.085</cell></row><row><cell cols="2">With TensorRT 61.5 74.1 56.2</cell><cell>0.051</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Leaderboard of the Waymo Open Dataset Challenge -Real-time 2D Detection track<ref type="bibr" target="#b2">[3]</ref>. The top-5 methods are listed here. The top-2 results are highlighted in red and blue colors, respectively.</figDesc><table><row><cell>Method</cell><cell cols="3">AP/L1 (%) AP/L2 (%) Latency (s)</cell></row><row><cell>LeapMotor Det</cell><cell>0.7571</cell><cell>0.7041</cell><cell>0.0616</cell></row><row><cell>YOLOR TensorRT (Ours)</cell><cell>0.7500</cell><cell>0.6972</cell><cell>0.0458</cell></row><row><cell>YOLOR P6 TRT</cell><cell>0.7484</cell><cell>0.6956</cell><cell>0.0374</cell></row><row><cell>dereyly self ensemble</cell><cell>0.7173</cell><cell>0.6565</cell><cell>0.0687</cell></row><row><cell>YOLO v5</cell><cell>0.7025</cell><cell>0.6414</cell><cell>0.0381</cell></row><row><cell>{128, 256, 512, 768, 1024}.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Experiment on mini-val dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Veh. Ped. Cyc. Mean</cell></row><row><cell>YOLOR-P6</cell><cell>63.5 74.9 60.4</cell><cell>66.3</cell></row><row><cell cols="2">YOLOR-W6 64.3 75.3 58.9</cell><cell>66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the mini-val dataset.</figDesc><table><row><cell>Method</cell><cell>AP/L2 (%)</cell></row><row><cell>YOLOR Baseline</cell><cell>61.54</cell></row><row><cell>+ Auto Data Cleaning</cell><cell>62.84</cell></row><row><cell>+ Multi-Scale Training</cell><cell>64.10</cell></row><row><cell>+ Scale Enhancement</cell><cell>65.98</cell></row><row><cell>+ Independent threshold-NMS</cell><cell>66.30</cell></row><row><cell>+ Model Ensemble</cell><cell>66.67</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2nd place solution for waymo open dataset challenge -2d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangzhuang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaying</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10419</idno>
		<title level="m">Pp-yolov2: A practical object detector</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="https://waymo.com/open/challenges/2021/real-time-2d-prediction/.3,4" />
		<title level="m">Waymo Open Dataset Challenge real-time 2D Detection Leaderboard</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2446" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08036</idno>
		<title level="m">Scaled-yolov4: Scaling cross stage partial network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">You only learn one representation: Unified network for multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04206</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
