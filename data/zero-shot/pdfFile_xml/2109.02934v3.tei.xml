<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Ram?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
						</author>
						<title level="a" type="main">Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning robust models that generalize well under changes in the data distribution is critical for realworld applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains -while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization -named Fishr -that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https: //github.com/alexrame/fishr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of deep neural networks in supervised learning <ref type="bibr" target="#b38">(Krizhevsky et al., 2012)</ref> relies on the crucial assumption that the train and test data distributions are identical. In particular, the tendency of networks to rely on simple features <ref type="bibr">(Valle-Perez et al., 2019;</ref><ref type="bibr">Geirhos et al., 2020)</ref> is generally a desirable behavior reflecting Occam's razor. However, in case of distribution shift, this simplicity bias deteriorates performance when more complex features are needed <ref type="bibr" target="#b83">(Tenenbaum, 2018;</ref><ref type="bibr">Shah et al., 2020)</ref>. For example, in the  Specifically, Fishr matches the domain-level gradient variances of the distributions across the two training domains:</p><formula xml:id="formula_0">A ({g i A } n A i=1 in orange) and B ({g i B } n B i=1</formula><p>in blue). We will show how this regularization during the learning of ? improves the out-of-distribution generalization properties by aligning the domain-level loss landscapes at convergence. recent fight against Covid-19, most of the deep learning methods developed to detect coronavirus from chest scans were shown useless for clinical use <ref type="bibr">(DeGrave et al., 2021;</ref><ref type="bibr">Roberts et al., 2021)</ref>: indeed, networks exploited simple bias in the training datasets such as patients' age or body position rather than 'truly' analyzing medical pathologies.</p><p>To better generalize under distribution shifts, most works <ref type="bibr" target="#b5">(Blanchard et al., 2011;</ref><ref type="bibr" target="#b58">Muandet et al., 2013)</ref> assume that the training data is divided into different training domains in which there is a constant underlying causal mechanism <ref type="bibr" target="#b65">(Peters et al., 2016)</ref>. To remove the domain-dependent explanations, different invariance criteria across those training domains have been proposed. <ref type="bibr">Ganin et al. (2016)</ref>; <ref type="bibr" target="#b82">Sun et al. (2016)</ref>; <ref type="bibr" target="#b82">Sun &amp; Saenko (2016)</ref> enforce similar feature distributions, others <ref type="bibr" target="#b2">(Arjovsky et al., 2019;</ref><ref type="bibr">Krueger et al., 2021)</ref> force the classifier to be simultaneously optimal across all domains. Yet, despite the popularity of this research topic, none of these methods perform significantly better than the classical Empirical Risk Minimization (ERM) when applied with controlled model selection and restricted hyperparameter search <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021;</ref><ref type="bibr">Ye et al., 2021)</ref>. arXiv:2109.02934v3 <ref type="bibr">[cs.</ref>LG] 1 Jun 2022</p><p>Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization These failures motivate the need for new ideas.</p><p>To foster the emergence of a shared mechanism with consistent generalization properties, our intuition is that learning should progress consistently and similarly across domains. Besides, the learning procedure of deep neural networks is dictated by the distribution of the gradients with respect to the network weights <ref type="bibr">(Yin et al., 2018;</ref><ref type="bibr">Sankararaman et al., 2020)</ref> -usually backpropagated in the network during gradient descent. Additionally, individual gradients are expressive representations of the input <ref type="bibr">(Fort et al., 2019;</ref><ref type="bibr" target="#b9">Charpiat et al., 2019)</ref>. Thus, we seek distributional invariance across domains in the gradient space: domain-level gradients should be similar, not only in average direction, but most importantly in statistics such as variance and disagreements.</p><p>In this paper, we propose the Fishr regularization for outof-distribution generalization in classification for computer vision -summarized in <ref type="figure" target="#fig_1">Fig. 1</ref>. We match the domainlevel gradient variances, i.e., the second moment of the gradient distributions. In contrast, previous gradient-based works such as Fish <ref type="bibr">(Shi et al., 2021)</ref> only match the domainlevel gradients means, i.e., the first moment.</p><p>Our strategy is also motivated by the close relations between the gradient variance, the Fisher Information <ref type="bibr">(Fisher, 1922)</ref> and the Hessian. This explains the name of our work, Fishr, using gradients as in Fish and related to the Fisher Matrix. Notably, we will study how Fishr forces the model to have similar domain-level Hessians and promotes consistent explanations -by generalizing the inconsistency formalism introduced in <ref type="bibr">Parascandolo et al. (2021)</ref>.</p><p>To reduce the computational cost, we justify an approximation that tackles the gradients only in the classifier, easily implemented with BackPACK <ref type="bibr">(Dangel et al., 2020)</ref>.</p><p>We summarize our contributions as follows:</p><p>? We introduce Fishr, a scalable regularization that brings closer the domain-level gradient variances.</p><p>? We theoretically justify that Fishr matches domainlevel risks and Hessians, and consequently, reduces inconsistencies across domains.</p><p>Empirically, we first validate that Fishr tackles distribution shifts on the synthetic Colored MNIST <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref>. Then, we show that Fishr performs best on the Do-mainBed benchmark <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021</ref>) when compared with state-of-the-art counterparts. Critically, Fishr is the only method to perform systematically better than ERM on all real datasets -PACS, VLCS, OfficeHome, TerraIncognita and DomainNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Context and Related Work</head><p>We first describe our task and provide the notations used along our paper. Then we remind some important related works to understand how our Fishr stands in a rich literature.</p><p>Problem definition and notations. We study out-ofdistribution (OOD) generalization for classification. Our model is a deep neural network (DNN) f ? (parametrized by ?) made of a deep features extractor ? ? on which we plug a dense linear classifier w ? : f ? = w ? ? ? ? and ? = (?, ?).</p><p>In training, we have access to different domains E: for each domain e ? E, the dataset D e = x i e , y i e ne i=1 contains n e i.i.d. (input, labels) samples drawn from a domaindependent probability distribution. Combined together, the datasets {D e } e?E are of size n = e?E n e . Our goal is to learn weights ? so that f ? predicts well on a new test domain, unseen in training. As described in <ref type="bibr">Koh et al. (2020)</ref> and <ref type="bibr">Ye et al. (2021)</ref>, most common distribution shifts are diversity shifts -where the training and test distributions comprise data from related but distinct domains, for instance pictures and drawings of the same objects -or correlation shifts -where the distribution of the covariates at test time differs from the one during training. To generalize well despite these distribution shifts, f ? should ideally capture an invariant mechanism across training domains. Following standard notations, M 2 F denotes the Frobenius norm of matrix M ; v 2 2 denotes the euclidean norm of vector v; 1 is a column vector with all elements equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The standard</head><p>Empirical Risk Minimization (ERM) <ref type="bibr">(Vapnik, 1999)</ref>  f ? x i e , y i e and is the negative log-likelihood loss. Many approaches try to exploit some external source of knowledge <ref type="bibr">(Xie et al., 2021)</ref>, in particular the domain information. As a side note, these partitions may be inferred if not provided <ref type="bibr">(Creager et al., 2021)</ref>. Some works explore data augmentations to mix samples from different domains <ref type="bibr">(Wang et al., 2020;</ref><ref type="bibr">Wu et al., 2020)</ref>, some re-weight the training samples to favor underrepresented groups <ref type="bibr">(Sagawa et al., 2020a;</ref><ref type="bibr">Zhang et al., 2021)</ref> and others include domain-dependent weights <ref type="bibr">(Ding &amp; Fu, 2017;</ref><ref type="bibr" target="#b54">Mancini et al., 2018)</ref>. Yet, most recent works promote invariance via a regularization criterion and only differ by the choice of the statistics to be matched across training domains. They can be categorized into three groups: these methods enforce agreement either (1) in features (2) in predictors or <ref type="bibr" target="#b64">(3)</ref> in gradients.</p><p>First, some approaches aim at extracting domain-invariant features and were extensively studied for unsupervised domain adaptation. The features are usually aligned with adversarial methods <ref type="bibr">(Ganin et al., 2016;</ref><ref type="bibr" target="#b19">Gong et al., 2016;</ref><ref type="bibr" target="#b47">Li et al., 2018b;</ref> or with kernel methods <ref type="bibr" target="#b58">(Muandet et al., 2013;</ref><ref type="bibr" target="#b51">Long et al., 2014)</ref>. Yet, the simple covariance matching in CORAL <ref type="bibr" target="#b82">(Sun et al., 2016;</ref><ref type="bibr" target="#b82">Sun &amp; Saenko, 2016)</ref> performs best on various tasks for OOD generalization <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021)</ref>. With Z ij e the jth dimension of the features extracted by ? ? for the i-</p><formula xml:id="formula_1">th example x i e of domain e ? E = {A, B}, CORAL minimizes Cov(Z A ) ? Cov(Z B ) 2 F where Cov(Z e ) = 1 ne?1 (Z e Z e ? 1 ne 1 Z e 1 Z e</formula><p>) is the feature covariance matrix. CORAL is more powerful than mere feature</p><formula xml:id="formula_2">matching 1 n A 1 Z A ? 1 n B 1 Z B 2 2</formula><p>as in Deep Domain Confusion (DDC) <ref type="bibr">(Tzeng et al., 2014</ref>). Yet, <ref type="bibr" target="#b30">Johansson et al. (2019)</ref> and <ref type="bibr">Zhao et al. (2019)</ref> show that these approaches are insufficient to guarantee good generalization.</p><p>Motivated by arguments from causality <ref type="bibr" target="#b63">(Pearl, 2009)</ref> and the idea that statistical dependencies are epiphenomena of an underlying structure, Invariant Risk Minimization (IRM) <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref> explains that the predictor should be invariant <ref type="bibr" target="#b65">(Peters et al., 2016;</ref><ref type="bibr" target="#b69">Rojas-Carulla et al., 2018)</ref>, i.e., simultaneously optimal across all domains. Yet, recent works point out pitfalls of IRM <ref type="bibr">(Guo et al., 2021;</ref><ref type="bibr">Kamath et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2019)</ref>, that does not provably work with non-linear data <ref type="bibr">(Rosenfeld et al., 2021)</ref> and could not improve over ERM when hyperparameter selection is restricted <ref type="bibr">(Koh et al., 2020;</ref><ref type="bibr">Gulrajani &amp; Lopez-Paz, 2021</ref>). Among many suggested improvements <ref type="bibr">(Chang et al., 2020;</ref><ref type="bibr">Idnani &amp; Kao, 2020;</ref><ref type="bibr">Teney et al., 2020;</ref><ref type="bibr">Ahmed et al., 2021)</ref>, Risk Extrapolation (V-REx) <ref type="bibr">(Krueger et al., 2021)</ref> argues that training risks from different domains should be similar and thus penalizes |R A ? R B | 2 when E = {A, B}.</p><p>A third and most recent line of work promotes agreements between gradients with respect to network weights. Gradient agreements help batches from different tasks to cooperate, and have been previously employed for multitasks <ref type="bibr">(Du et al., 2018;</ref><ref type="bibr">Yu et al., 2020</ref><ref type="bibr">), continual (Lopez-Paz &amp; Ranzato, 2017</ref><ref type="bibr">), meta (Finn et al., 2017</ref><ref type="bibr">Zhang et al., 2020)</ref> and reinforcement <ref type="bibr" target="#b1">(Zhang et al., 2019)</ref> learning. In OOD generalization, <ref type="bibr">Koyama &amp; Yamaguchi (2020)</ref>; <ref type="bibr">Parascandolo et al. (2021)</ref>; <ref type="bibr">Shi et al. (2021)</ref> try to find minimas in the loss landscape that are shared across domains. Specifically, these works tackle the domain-level expected gradients:</p><formula xml:id="formula_3">g e = E (xe,ye)?De ? ? (f ? (x e ), y e ) .<label>(1)</label></formula><p>When E = {A, B}, IGA <ref type="bibr">(Koyama &amp; Yamaguchi, 2020)</ref> minimizes ||g A ? g B || 2 2 ; Fish <ref type="bibr">(Shi et al., 2021)</ref> increases g A ? g B ; AND-mask <ref type="bibr">(Parascandolo et al., 2021)</ref> and others <ref type="bibr">(Mansilla et al., 2021;</ref><ref type="bibr">Shahtalebi et al., 2021)</ref> update weights only when g A and g B point to the same direction.</p><p>Along with the increased computation cost, the main limitation of previous gradient-based methods is the per-domain batch averaging of gradients: this removes more granular statistics, in particular the information from pairwise interactions between gradients from samples in a same domain. In opposition, our new regularization for OOD generalization keeps extra information from individual gradients and matches across domains the domain-level gradient variances. In a nutshell, Fishr is similar to the covariance-based CORAL <ref type="bibr" target="#b82">(Sun et al., 2016;</ref><ref type="bibr" target="#b82">Sun &amp; Saenko, 2016)</ref> but in the gradient space rather than in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fishr</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gradient variance matching</head><p>The individual gradient g i e = ? ? f ? (x i e ), y i e is the firstorder derivative for the i-th data example x i e , y i e from domain e ? E with respect to the weights ?. Previous methods have matched the gradient means g e = 1 ne ne i=1 g i e for each domain e ? E. These gradient means capture the average learning direction but can not capture gradient disagreements <ref type="bibr">(Sankararaman et al., 2020;</ref><ref type="bibr">Yin et al., 2018)</ref>. With G e = [g i e ] ne i=1 of size n e ? |?|, we compute the domain-level gradient variance vectors of size |?|:</p><formula xml:id="formula_4">v e = Var(G e ) = 1 n e ? 1 ne i=1 g i e ? g e 2 ,<label>(2)</label></formula><p>where the square indicates an element-wise product. To reduce the distribution shifts in the network f ? across domains, we bring the domain-level gradient variances {v e } e?E closer. Hence, our Fishr regularization is:</p><formula xml:id="formula_5">L Fishr (?) = 1 |E| e?E v e ? v 2 2 ,<label>(3)</label></formula><p>the square of the Euclidean distance between the gradient variance from the different domains e ? E and the mean gradient variance v = 1 |E| e?E v e . Balanced with a hyperparameter coefficient ? &gt; 0, this Fishr penalty complements the original ERM objective, i.e., the empirical training risks:</p><formula xml:id="formula_6">L(?) = 1 |E| e?E R e (?) + ?L Fishr (?).<label>(4)</label></formula><p>Remark 3.1. Gradients g i e can be computed on all network weights ?. Yet, to reduce the memory and training costs, they will often be computed only on a subset of ?, e.g., only on classification weights ?. This approximation is discussed in Section 4.2.2 and Appendix D.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Theoretical analysis</head><p>We theoretically motivate our Fishr regularization by leveraging the domain inconsistency score introduced in ANDmask <ref type="bibr">(Parascandolo et al., 2021)</ref>. We first derive a generalized upper bound for this score. Then, we show that Fishr minimizes this upper bound by matching simultaneously domain-level risks and Hessians. </p><formula xml:id="formula_7">? * at convergence. N 0.2 A,? * contains weights ? for which R A (?) is low (? 0.2) but R B (?) is high (? 0.9).</formula><p>This inconsistency is due to conflicting domain-level loss landscapes, specifically gaps between domain-level risks and curvatures at ? * . This is visible in the disagreements across the variances of gradients <ref type="bibr">Parascandolo et al. (2021)</ref> argues that "patchwork solutions sewing together different strategies" for different domains may not generalize well: good weights should be optimal on all domains and "hard to vary" (Deutsch, 2011). They formalize this insight with an inconsistency score:</p><formula xml:id="formula_8">{g i A } n A i=1 and {g i B } n B i=1 .</formula><formula xml:id="formula_9">1 2 ? H B ?).<label>(7)</label></formula><p>The Hessian being positive definite is a standard hypothesis, notably used in <ref type="bibr">Parascandolo et al. (2021)</ref>, that is empirically reasonable <ref type="bibr" target="#b73">(Sagun et al., 2018)</ref>: "in only very few steps . . . large negative eigenvalues disappear" <ref type="bibr" target="#b18">(Ghorbani et al., 2019)</ref>.</p><p>The first term in the RHS of Proposition 1 is the difference between domain-level risks, whose square is the criterion minimized in V-REx <ref type="bibr">(Krueger et al., 2021)</ref>. We will prove and show that Fishr forces this term to be small in Section 3.2.2. In contrast, <ref type="bibr">Parascandolo et al. (2021)</ref> made the strong assumption:</p><formula xml:id="formula_10">R A (? * ) = R B (? * ) = 0.</formula><p>While <ref type="bibr">Parascandolo et al. (2021)</ref> ignored this first term, we follow their diagonal approximation of the Hessians to analyze the second term. In that case, H e = diag (? e 1 , ? ? ? , ? e h ) with ?i ? {1, . . . , h} , ? e i &gt; 0. Then:</p><formula xml:id="formula_11">max 1 2 ? H A ?? 1 2 ? H B ? = max ? 2 2 ? i? 2 i ? B i /? A i = ? max i ? B i /? A i .<label>(8)</label></formula><p>This is large when exists i such that ? A i is small but ? B i is large: indeed, a small weight perturbation in the direction of the associated eigenvector would change the loss slightly in the domain A but drastically in domain B. Thus, this second term decreases when H A and H B have similar eigenvalues. This result holds when Hessians are co-diagonalizable. In conclusion, this explains why forcing H A = H B reduces inconsistencies in the loss landscape and thus improves generalization. AND-mask matches Hessians by zeroing out gradients with inconsistent directions across domains; however, this masking strategy introduces dead zones <ref type="bibr">(Shahtalebi et al., 2021)</ref> in weights where the model could get stuck, ignores gradient magnitudes and empirically performs poorly with real datasets from DomainBed. As shown in Section 3.2.3, Fishr proposes a new method to align domain-level Hessians leveraging the close relations between the gradient variance, the Fisher Information and the Hessian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">FISHR MATCHES THE DOMAIN-LEVEL RISKS</head><p>Gradients take into account the label Y , which appears as an argument for the loss . Hence, gradient-based approaches are 'label-aware' by design. In contrast, feature-based methods were shown to fail in case of label shifts, because they do not consider Y <ref type="bibr" target="#b30">(Johansson et al., 2019;</ref><ref type="bibr">Zhao et al., 2019)</ref>.</p><p>The fact that the label and the loss appear in the formula of the gradients has another important consequence: matching gradient distributions also matches training risks, as motivated in V-REx <ref type="bibr">(Krueger et al., 2021)</ref>. We confirm this insight in <ref type="table" target="#tab_2">Table 2</ref>: matching gradient variances with Fishr</p><formula xml:id="formula_12">induces |R A ? R B | 2 ? 0 when E = {A, B}.</formula><p>Intuitively, gradient amplitudes are directly weighted by the loss values: multiplying the loss by a constant will also multiply the gradients by the same constant. Thus roughly, if the domain-level empirical training risks are different, then the domain-level gradient norms should also differ.</p><p>Theoretically, we prove in Appendix A.2 that Fishr regularization component with reference to the classification bias is exactly the difference between domain-level mean squared errors. We recover the objective from V-REx <ref type="bibr">(Krueger et al., 2021)</ref>, with a different loss (squared error instead of negative log likelihood). More generally, we show in this Appendix that Fishr in the classifier w ? acts as a feature-adaptive version of V-REx: the components in Fishr adaptively force the risks to be similar across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">FISHR MATCHES THE DOMAIN-LEVEL HESSIANS</head><p>The Hessian matrix H = n i=1 ? 2 ? f ? (x i ), y i is of key importance in deep learning. Yet, H cannot be computed efficiently in general. Recent methods <ref type="bibr" target="#b26">(Izmailov et al., 2018;</ref><ref type="bibr">Parascandolo et al., 2021;</ref><ref type="bibr">Foret et al., 2021)</ref> tackled the Hessian indirectly by modifying the learning procedure. In contrast, we use the fact that the diagonal of H is approximated by the gradient variance Var(G); this is confirmed in <ref type="table" target="#tab_1">Table 1</ref>. This result is derived below from 3 individual and standard approximation steps. The Hessian and the 'true' Fisher Information Matrix (FIM). <ref type="bibr">Fisher, 1922;</ref><ref type="bibr" target="#b10">C.R., 1945)</ref> approximates the Hessian H with theoretically probably bounded errors under mild assumptions <ref type="bibr" target="#b76">(Schraudolph, 2002)</ref>.</p><formula xml:id="formula_13">The 'true' FIM F = n i=1 E? ?P ? (?|x i ) ? ? log p ? (?|x i )? ? log p ? (?|x i )<label>(</label></formula><p>The 'true' FIM and the 'empirical' FIM. Yet, F remains costly as it demands one backpropagation per class. That's why most empirical works <ref type="bibr">(e.g., in compression (Frantar et al., 2021;</ref><ref type="bibr">Liu et al., 2021)</ref> and optimization <ref type="bibr">(Dangel et al., 2021)</ref> <ref type="bibr" target="#b56">(Martens, 2014)</ref> where p ? (?|x) is the density predicted by f ? on input x. While F uses the model distribution P ? (?|X),F uses the data distribution P (Y |X). Despite this key difference,F and F were shown to share the same structure and to be similar up to a scalar factor <ref type="bibr">(Thomas et al., 2020)</ref>. They also have analogous properties: Tr(F ) ? Tr(F ). This was discussed in <ref type="bibr">Li et al. (2020)</ref> and further highlighted even at early stages of training (before overfitting) in the <ref type="figure" target="#fig_1">Fig. 1</ref> and the Appendix S3 of <ref type="bibr">Singh &amp; Alistarh (2020)</ref>.</p><formula xml:id="formula_14">) approximate the 'true' FIM F with the 'empirical' FIMF = G e G e = n i=1 ? ? log p ? (y i |x i )? ? log p ? (y i |x i )</formula><p>The 'empirical' FIM and the gradient covariance. Critically,F is nothing else than the unnormalized uncentered covariance matrix when is the negative loglikelihood. Thus, the gradient covariance matrix C = 1 n?1 G G ? 1 n 1 G 1 G of size |?| ? |?| and F are equivalent (up to the multiplicative constant n) at any first-order stationary point: C ? ?F . Overall, this suggests that C and H are closely related <ref type="bibr" target="#b28">(Jastrzebski et al., 2018)</ref>;.  <ref type="table">)</ref>). The gradient variance, computable efficiently with a unique backpropagation, serves as a proxy for the Hessian. Details and more experiments in Section 4.1 (notably <ref type="figure" target="#fig_3">Fig. 3</ref>) and in Appendix C.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ERM Fishr</head><formula xml:id="formula_15">Var(G 90% ) ? Var(G 80% ) 2 F 1.6 4.1 ? 10 ?5 |R 90% ? R 80% | 2 1.0 ? 10 ?2 3.8 ? 10 ?6 Diag (H 90% ? H 80% ) 2 F 2.9 ? 10 ?1 2.7 ? 10 ?4</formula><p>Consequences for Fishr. Critically, Fishr considers the gradient variance Var(G), i.e., the diagonal components of C. In our multi-domain framework, we define the domain-level matrices with the subscript e. Remark 3.2. Limitation of our approximation. We acknowledge that approximating the 'true' FIM F by the 'empirical' FIMF is not fully justified theoretically <ref type="bibr" target="#b56">(Martens, 2014;</ref><ref type="bibr" target="#b40">Kunstner et al., 2019)</ref>. Indeed, this approximation is valid only under strong assumptions, in particular ? 2 convergence of predictions P ? (?|X) towards labels P (Y |X)as detailed in Proposition 1 from <ref type="bibr">Thomas et al. (2020)</ref>. In this paper, we trade off theoretical guarantees for efficiency. Remark 3.3. Diagonal approximation. The empirical similarities between C and H motivate using gradient variance rather than gradient covariance, which scales down the number of targeted components from |?| 2 to |?|. Indeed, diagonally approximating the Hessian is common: e.g., for OOD generalization <ref type="bibr">(Parascandolo et al., 2021)</ref>, optimization <ref type="bibr" target="#b44">(LeCun et al., 2012;</ref><ref type="bibr" target="#b33">Kingma &amp; Ba, 2014)</ref>, continual learning <ref type="bibr" target="#b34">(Kirkpatrick et al., 2017)</ref> and pruning <ref type="bibr" target="#b42">(LeCun et al., 1990;</ref><ref type="bibr">Theis et al., 2018)</ref>. This is based on the empirical evidence <ref type="bibr" target="#b3">(Becker &amp; Le Cun, 1988)</ref> that Hessians are diagonally dominant at the end of training. Our diagonal approximation is also motivated by the critical importance of Tr(C) <ref type="bibr">(Jastrzebski et al., 2021;</ref><ref type="bibr">Faghri et al., 2020)</ref> to analyze the generalization properties of DNNs. We confirm empirically in Appendix C.2.3 that considering the off-diagonal parts of C performs no better that just matching the diagonals.</p><p>Conclusion. Fishr efficiently matches (1) domain-level empirical risks and (2) domain-level Hessians across the training domains, using gradient variances as a proxy. This will align domain-level loss landscapes, reduce domain inconsistencies and increase domain generalization. In particular, the domain-level Hessian matching illustrates that Fishr is more than just a generalization of gradient-mean approaches such as Fish <ref type="bibr">(Shi et al., 2021)</ref>.</p><p>Finally, we refer the readers to Appendix A.3 where we leverage the Neural Tangent Kernel (NTK) <ref type="bibr" target="#b27">(Jacot et al., 2018)</ref> theory to further motivate the gradient variance matching during the optimization process -and not only at convergence. In brief, as F and the NTK matrices share the same non-zero eigenvalues, similar {C e } e?E during training reduce the simplicity bias by preventing the learning of different domain-dependent shortcuts at different training speeds: this favors a shared mechanism that predicts the same thing for the same reasons across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We prove Fishr effectiveness on Colored MNIST <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref> and then on the DomainBed benchmark <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021)</ref>. To facilitate reproducibility, the code is available at https://github.com/ alexrame/fishr. Moreover, we show in Appendix B that Fishr is effective in the linear setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Proof of concept on Colored MNIST</head><p>The task in <ref type="bibr">Colored MNIST (Arjovsky et al., 2019)</ref> is to predict whether the digit is below or above 5. Moreover, the labels are flipped with 25% probability (except in Appendix C.2.2). Critically, the digits' colors spuriously correlate with the labels: the correlation strength varies across the two training domains E = {90%, 80%}. To test whether the model has learned to ignore the color, this correlation is reversed at test time. In brief, a biased model that only considers the color would have 10% test accuracy whereas an oracle model that perfectly predicts the shape would have 75%. As previously done in V-REx <ref type="bibr">(Krueger et al., 2021)</ref>, we strictly follow the IRM implementation and just replace the IRM penalty by our Fishr penalty. This means that we use the exact same MLP and hyperparameters, notably the same two-stage scheduling selected in IRM for the regularization strength ?, that is low until epoch 190 and then jumps to a large value, which was optimized via a gridsearch for IRM. More experimental details are provided in Appendix C.1. <ref type="table" target="#tab_4">Table 3</ref> reports the accuracy averaged over 10 runs with standard deviation. Fishr ? (i.e., applying Fishr on all weights ?) obtains the best trade-off between train and test accuracies; notably in test, it reaches 71.2%, or 70.2% when digits are grayscale. Moreover, computing the gradients only in the classifier w ? performs almost as well (69.5% in test for Fishr ? ) while reducing drastically the computational cost. Finally, Fishr ? only in the features extractor ? works best in test, though it has lower train accuracy. This last experiment shows that we can reduce domain shifts without  explicitly forcing the predictors to be simultaneously optimal. These results highlight the effectiveness of gradient variance matching -even with standard hyperparameters -at different layers of the network.</p><p>The main advantage of this synthetic dataset is the possibility of empirically validating some theoretical insights. For example, the training dynamics in <ref type="figure" target="#fig_3">Fig. 3</ref> show that the domain-level empirical risks get closer once the Fishr ? gradient variance matching loss is activated after step 190 (|R 90% ? R 80% | ? 0), even though predicting accurately on the domain 90% is easier than on the domain 80%. This confirms insights from Section 3.2.2. Similarly, we observe that Fishr matches Hessians across the two training domains. This is confirmed by further experiments in Appendix C.2, and validates insights from Section 3.2.3. Overall, Fishr regularization reduces train accuracy, but sharply increases test accuracy. Yet, the main drawback of Colored MNIST is its insufficiency to ensure generalization for real-world datasets.</p><p>Overall, it should be considered as a proof-of-concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">DomainBed benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">DATASETS AND PROCEDURE</head><p>We conduct extensive experiments on the DomainBed benchmark <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021)</ref>. In addition to the synthetic Colored MNIST <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref> and Rotated MNIST <ref type="bibr" target="#b17">(Ghifary et al., 2015)</ref>, the multi-domain image classification datasets are the real VLCS (Fang et al., 2013), PACS <ref type="bibr" target="#b45">(Li et al., 2017)</ref>, OfficeHome (Venkateswara et al., 2017), TerraIncognita <ref type="bibr" target="#b4">(Beery et al., 2018)</ref> and Do-mainNet <ref type="bibr" target="#b64">(Peng et al., 2019)</ref>. To limit access to test domain, the framework enforces that all methods are trained with only 20 different configurations of hyperparameters and Algorithm 1 Training procedure for Fishr on DomainBed.</p><p>Input:</p><formula xml:id="formula_16">DNN f ? , observations D e = x i e , y i e ne i=1</formula><p>for domains e ? E, regularization weight ?, warmup iteration i warmup , exponential moving average ? and batch size b s Initialize: moving averages: ?e ? E, v mean e ? 0 for iter from 1 to #iters do {# Step 1: standard ERM procedure} for e ? E do Randomly select batch:</p><formula xml:id="formula_17">{(x i e , y i e )} i?B of size b s Compute predictions: ?i ? B,? i e ? f ? (x i e ) Compute empirical risks: R e (?) ? i?B ? i e , y i e end for L(?) = 1 |E| e?E R e (?) {# Step 2: gradient variances in classifier} for e ? E do Compute individual gradients in w ? with Back- PACK: ?i ? B, g i e ? ? ? ? i e , y i e Compute domain gradient variances v e (Eq. 2) Update v mean e = v e ? ?v mean e + (1 ? ?)v iter e end for if iter ? i warmup then L(?) += ?L Fishr (?) (Eq. 3) end if {# Step 3: gradient descent in the whole network} Backpropagate gradients ? ? L(?) in the network f ? with standard PyTorch end for</formula><p>for the same number of steps. Results are averaged over three trials. This experimental setup is further described in Appendix D.1. By imposing the datasets, the training procedure and controlling the hyperparameter search, DomainBed is arguably the fairer open-source benchmark to rigorously compare the different strategies for OOD generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">IMPLEMENTATION DETAILS</head><p>We systematically apply Fishr only in the classifier w ? in DomainBed. Indeed, keeping individual gradients in memory for ? from a ResNet-50 was impossible for computational reasons. Fishr ? and Fishr ? performed similarly in previous Section 4.1. This is partly because the gradients in ? still depend on ? ? . Additionally, as highlighted in Appendix D.3.2, this relaxation may improve results for real-world datasets. Indeed, while Colored MNIST is a correlation shift challenge, the other datasets mostly demonstrate diversity shifts where "each domain represents a certain spectrum of diversity in data" <ref type="bibr">(Ye et al., 2021)</ref>. Then, as the pixels distribution are quite different across domains, low-level layers may need to adapt to these domain-dependent peculiarities. Moreover, if we used all weights ? = (?, ?) to compute gradient variances, the invariance in w ? may be overshadowed by ? ? due to |?| |?|. Finally, it's worth noting that this last-layer approximation is consistent with the IRM condition <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref> and is common for unsupervised domain adaptation <ref type="bibr">(Ganin et al., 2016)</ref>.</p><p>Fishr relies on three hyperparameters. First, the ? coefficient controls the regularization strength: with ? = 0 we recover ERM while a high ? may cause underfitting. We show that Fishr is robust to the choice of the sampling distribution for hyperparameter ? in Appendix D.3.3. Second the warmup iteration defines the step at which we activate the regularization. This warmup strategy is taken from previous works such as IRM <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref>, V-REx <ref type="bibr">(Krueger et al., 2021)</ref> or Spectral Decoupling <ref type="bibr">(Pezeshki et al., 2021)</ref>. Before that step, the DNN is trained with ERM to learn predictive features. After that step, the Fishr regularization encourages the DNN to have invariant gradient variances. Lastly, the domain-level gradient variances are more accurate when estimated over more data points. Rather than increasing the batch size, we follow Le Roux et al. <ref type="formula" target="#formula_3">(2011)</ref> and leverage an exponential moving average for computing stable gradient variances. Therefore our third hyperparameter is the coefficient ? controlling the update speed: at step t, we matchv t e = ?v t?1 e + (1 ? ?)v t e rather than of v t e from Eq. 2. The closer ? is to 1, the smoother the variance is along training.v t?1 e from previous step t ? 1 is 'detached' from the computational graph. Similar strategies have already been used for OOD generalization <ref type="bibr">(Nam et al., 2020;</ref><ref type="bibr">Blanchard et al., 2021)</ref>. The memory overhead is (|E| ? |?|). We study by ablation the importance of this warmup strategy and this ? in Appendices D.3.1 and D.3.2.</p><p>Fishr is simple to implement (see the Algorithm 1) using the BackPACK <ref type="bibr">(Dangel et al., 2020)</ref> package. While PyTorch (Paszke et al., 2019) can compute efficiently batch gradients, BackPACK optimizes the computation of individual gradients, sample per sample, at almost no time overhead. Thus, Fishr is also at low computational costs. For example, on PACS (7 classes and |?| = 14, 343) with a ResNet-50 and batch size 32, Fishr induces an overhead in memory of +0.2% and in training time of +2.7% (with a Tesla V100) compared to ERM; on the larger-scale DomainNet (345 classes and |?| = 706, 905), the overhead is +7.0% in memory and +6.5% in training time. As a side note, keeping the full covariance of size |?| 2 ? 5 ? 10 8 on DomainNet would not have been possible. In contrast, Fish <ref type="bibr">(Shi et al., 2021)</ref> leverages a meta-learning algorithm that is impractical as |E| times longer to train than ERM. ERM was carefully tuned in DomainBed and thus remains a strong baseline. Moreover, all previous methods are far from the best score on at least one dataset. Invariant predictors (IRM, V-REx) and gradient masking (AND-mask) approaches perform poorly on real datasets. Additionally, CORAL not only performs worse than ERM on TerraIncognita, but most importantly fails to detect correlation shifts on Colored MNIST: this is because feature-based approaches do not take into account the label, as previously stated in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">RESULTS</head><p>Contrarily, Fishr is the only method to efficiently tackle correlation and diversity shifts, as defined in <ref type="bibr">(Ye et al., 2021)</ref>. Indeed, not only Fishr outperforms ERM on Colored MNIST (68.8% vs. 57.8%), but Fishr also systematically performs better than ERM on all real datasets: the differences are over standard errors on VLCS (78.2% vs. 77.6%), Office-Home (68.2% vs. 66.4%) and on the larger-scale Domain-Net (41.8% vs. 41.3%). Appendix D.3.2 shows that Fishr performs even better when combined with gradient-mean matching. In summary, Fishr consistently beats ERM (despite the restricted hyperparameter search): this is the main point to validate the effectiveness of our method.</p><p>Additionally, Fishr performs best after averaging: Firshr reaches 70.8% vs. 69.2% for the second best CORAL. When ignoring the Colored MNIST task, averaging over the 6 other datasets leads to a similar ranking: 1.Fishr(avg=71.1), 2.CORAL(71.0), 3.Mixup(70.8) and 4.ERM(70.5). This arguably partial metric is confirmed by the more robust ranking information; Fishr's median ranking of second reflects that Fishr is consistently among the best methods. Overall, Fishr is the state-of-the-art approach, not only in average accuracy, but most importantly in average ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we addressed the task of out-of-distribution generalization for classification in computer vision. We derive a new and simple regularization -Fishr -that matches the gradient variances across domains as a proxy for matching domain-level risks and Hessians. We prove that this reduces inconsistencies across domains. Zhao, H., Des Combes, R. T., Zhang, K., and Gordon, G. On learning invariant representations for domain adaptation.</p><p>In <ref type="bibr">ICML, 2019. (pp. 3, 5)</ref>.</p><p>These Appendices complement the main paper.</p><p>1. We first detail some theoretical points. Appendix A.1 demonstrates our Proposition 1. Appendix A.2 shows that Fishr acts as a feature-adaptive V-REx. Appendix A.3 motivates Fishr with intuitions from the Neural Tangent Kernel theory.</p><p>2. Appendix B proves the effectiveness of our approach for a linear toy dataset.</p><p>3. Appendix C enriches the Colored MNIST experiment in the IRM setup. In detail, we first describe the experimental setup in Appendix C.1. We then validate in Appendix C.2 some insights provided in the main paper; in particular, Appendix C.2.3 motivates the diagonal approximation of the gradient covariance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Theoretical Analysis</head><p>A.1. Demonstration of Proposition 1 from Section 3.2.1 Assumption A.1. We make the quadratic bowl assumption around the local minima ? * on all domains : ?e ? E,</p><formula xml:id="formula_18">R e (?) = R e (? * ) + 1 2 (? ? ? * ) H e (? ? ? * ),<label>(9)</label></formula><p>where H e is positive definite of eigenvalues ? e 1 ? ? ? ? ? ? e h &gt; 0. Remark A.2. Assumption A.1 is milder on N e,? * for low . Indeed, when ? 0, then max ??N e,? * ? ? ? * 2 2 ? 0 and the quadratic approximation coincides with the second-order Taylor expansion around ? * . Moreover, this approximation is common in optimization <ref type="bibr" target="#b75">(Schaul et al., 2013;</ref><ref type="bibr" target="#b28">Jastrzebski et al., 2018)</ref>.</p><p>Proposition 2. (Reformulation of Proposition 1, illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>). Let &gt; 0, weights ? * . ?(A, B) ? E 2 , with N A,? * the largest path-connected region of weights space where the risk R A remains in an interval around R A (? * ), we note:</p><formula xml:id="formula_19">I (A, B) = max ??N A,? * |R B (?) ? R A (? * )| , R(A, B) = R B (? * ) ? R A (? * ), H (A, B) = max 1 2 (??? * ) H A (??? * )? 1 2 (? ? ? * ) H B (? ? ? * ).<label>(10)</label></formula><p>If ?(A, B) ? E 2 such as R(A, B) &lt; 0, we have:</p><formula xml:id="formula_20">? ?R(A, B) ? ? A h ? B 1 ,<label>(11)</label></formula><p>then under previous Assumption A.1,</p><formula xml:id="formula_21">max (A,B)?E 2 I (A, B) = max (A,B)?E 2 (R(A, B) + H (A, B))<label>(12)</label></formula><p>Proof We first prove that, under quadratic Assumption A.1, ?A ? E, N A,? * = {?| |R A (?) ? R A (? * )| ? }. Indeed, the former is always included in the latter by definition. Reciprocally, be given ? in the latter, {?? * + (1 ? ?)?|? ? [0, 1]} linearly connects ? * to ? in parameter space with the risk R A remaining in an interval around R</p><formula xml:id="formula_22">A (? * ) because ?? ? [0, 1] we have |R A (?? * + (1 ? ?)?) ? R A (? * )| = (1 ? ?) 2 |R A (?) ? R A (? * )| ? (1 ? ?) 2 ? .</formula><p>Therefore ?(A, B) ? E 2 :</p><formula xml:id="formula_23">I (A, B) = max |R A (?)?R A (? * )|? |R B (?) ? R A (? * )| = max 1 2 (??? * ) H A (??? * )? R(A, B) + 1 2 (? ? ? * ) H B (? ? ? * )<label>(13)</label></formula><p>As the Hessians are positive, H (A, B) &gt; 0. We now need to split the analysis based on the sign of R(A, B). (R <ref type="figure">(A, B) + H (A, B)</ref>) .</p><p>Case R(A, B) &lt; 0 Leveraging ? B 1 the largest eigenvalue from H B and ? A h the lowest eigenvalue from H A , we upper bound:</p><formula xml:id="formula_25">H (A, B) ? max ? A h 2 ??? * 2 2 ? ? B 1 2 ? ? ? * 2 2 = ? ? B 1 ? A h .<label>(15)</label></formula><p>Then Eq. 11 gives H (A, B) &lt; ?R <ref type="figure">(A, B)</ref>. Thus the number inside the absolute value from the RHS of Eq. 13 is negative. This leads to:  <ref type="figure">H (A, B)</ref>) can not be achieved for (A, B) with R(A, B) &lt; 0. We obtain:</p><formula xml:id="formula_26">I (A, B) = ?R(A, B) ? H (A, B) &lt; ?R(A, B) = R(B, A) &lt; I (B,</formula><formula xml:id="formula_27">max (A,B)?E 2 (R(A, B) + H (A, B)) = max (A,B)?E 2 |R(A,B)?0 (R(A, B) + H (A, B))<label>(17)</label></formula><p>Conclusion Combining Eq. 14, Eq. 16 and Eq. 17, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Fishr as a feature-adaptive version of V-REx</head><p>We delve into the theoretical analysis of the Fishr regularization in the classifier w ? , that leverages p features extracted from ?. We note z i e ? R p the features for the i-th example from the domain e,? i e ? [0, 1] the predictions after sigmoid and y i e ? {0, 1} the one-hot encoded target. The linear layer W is parametrized by weights {w k } p k=1 and bias b. The gradient of the loss for this sample with respect to the bias b is ? b (y i e ,? i e ) = (? i e ? y i e ). Thus, the uncentered gradient variance in b for domain e is: v b e = 1 ne ne i=1 (? i e ? y i e ) 2 , which is exactly the mean squared error (MSE) between predictions and targets in domain e. Thus, matching gradient variances in b will match risks across domains. This is the objective from V-REx <ref type="bibr">(Krueger et al., 2021)</ref>, where the squared error has replaced the negative log likelihood.</p><p>We can also look at the gradients with respect to the weight w k : ? w k (y i e ,? i e ) = (? i e ? y i e )z i e [k]. Thus, the uncentered gradient variance in w k for domain e is:</p><formula xml:id="formula_28">v w k e = 1 ne ne i=1 (? i e ? y i e )z i e [k]</formula><p>2 . This is the squared error, weighted for each sample (z i e , y i e ) by the square of the k-th feature z i e [k]: matching gradient variances directly matches these weighted squared errors, with k different weighting schemes, that depend on the features distribution. This describes Fishr as a feature-adaptive version of V-REx <ref type="bibr">(Krueger et al., 2021</ref>). An intuitive example is when features are binary (z i e ? {0, 1}); in that case, Fishr matches domain-level risks on groups of samples having a shared feature.</p><p>More exactly in Fishr, we match centered gradient variances, equivalent to the uncentered variance gradient matching at convergence under the assumption g e ? 0. Experiments in <ref type="table" target="#tab_10">Table 5</ref> and in Appendix C.2.4 confirm that centering or not the variances perform similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Neural Tangent Kernel perspective</head><p>In this Section we motivate the matching of gradient covariances with new arguments from the Neural Tangent Kernel (NTK) <ref type="bibr" target="#b27">(Jacot et al., 2018)</ref> theory. As a reminder, the NTK K ? R n?n is the gramian matrix with entries K[i, j] = ? ? f ? (x i ) ? ? ? f ? (x j ) that measure the gradients similarity at two different input points x i and x j . This kernel dictates the training dynamics of the DNN and remains fixed in the infinite width limit. Most importantly, as stated in Yang &amp; Salman (2019), "the simplicity bias of a wide neural network can be read off quickly from the spectrum of K: if the largest eigenvalue [? max ] of K accounts for most of Tr(K), then a typical random network looks like a function from the top eigenspace of K": this holds for ReLu networks. In summary, gradient descent mostly happens in a tiny subspace (Gur-Ari et al., 2018) whose directions are defined by the main eigenvectors from K. Moreover, the learning speed is dictated by ? max , which can be used to estimate a condition for a learning rate ? to converge: ? &lt; 2/? max <ref type="figure" target="#fig_1">(Karakida et al., 2019)</ref>.</p><p>In a multi-domain framework, having similar spectral decompositions across {K e } e?E during the optimization process would improve OOD generalization for two reasons:</p><p>1. Having similar top eigenvectors across {K e } e?E would delete detrimental domain-dependent shortcuts and favor the learning of a common mechanism. Indeed, truly informative features should remain consistent across domains.</p><p>2. Having similar top eigenvalues across {K e } e?E would improve the optimization schema for simultaneous training at the same speed. Indeed, it would facilitate the finding of a learning rate for simultaneous convergence on all domains. It's worth noting that if we quickly overfit on a first domain using spurious explanations, invariances will then be hard to learn due to the gradient starvation phenomena <ref type="bibr">(Pezeshki et al., 2021)</ref>.</p><p>Directly matching K e would require assuming that each domain coincides and contains the same samples; for example, with different pose angles <ref type="bibr" target="#b17">(Ghifary et al., 2015)</ref>. To avoid such a strong assumption, we leverage the fact that the 'true' Fisher Information Matrix F and the NTK K share the same non-zero eigenvalues since F is dual to K (see Appendix C.1 in <ref type="bibr" target="#b53">Maddox et al. (2019)</ref>, notably for classification tasks). Moreover, their eigenvectors are strongly related (see Appendix C in <ref type="bibr" target="#b36">Kopitkov &amp; Indelman (2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on a Linear Example</head><p>We experimentally prove that Fishr is effective in the linear setting. To do so, we consider the binary classification dataset introduced in the Section 3.2 from Fish <ref type="bibr">(Shi et al., 2021)</ref>. Each example is composed of 4 static features (f 1 , f 2 , f 3 , f 4 ). While f 1 is invariant across the two train domains and the test domain, the three other features are spurious: their correlations with the label vary in each domain. The model is a linear logistic regression, with trainable weights W and bias b. As f 2 and f 3 have higher correlations with the label than f 1 in training, ERM relies mostly on f 2 and f 3 . This is indicated in the first line of <ref type="table" target="#tab_10">Table 5</ref>    <ref type="bibr">, 2010)</ref>, it has 2 main differences. First, 0-4 and 5-9 digits are each collapsed into a single class, with a 25% chance of label flipping. Second, digits are either colored red or green, with a strong correlation between label and color in training. However, this correlation is reversed at test time. Specifically, in training, the model has access to two domains E = {90%, 80%}: in the first domain, green digits have a 90% chance of being in 5-9; in the second, this chance goes down to 80%. In test, green digits have a 10% chance of being in 5-9. Due to this modification in correlation, a model should ideally ignore the color information and only rely on the digits' shape: this would obtain a 75% test accuracy.</p><p>In the experimental setup from IRM, the network is a 3 layers MLP with ReLu activation, optimized with Adam <ref type="bibr" target="#b33">(Kingma &amp; Ba, 2014)</ref>. IRM selected the following hyperparameters by random search over 50 trials: hidden dimension of 390, l 2 regularizer weight of 0.00110794568, learning rate of 0.0004898536566546834, penalty anneal iters (or warmup iter) of 190, penalty weight (?) of 91257.18613115903, 501 epochs and batch size 25,000 (half of the dataset size). We strictly keep the same hyperparameters values in our proof of concept in Section 4.1. The code is almost unchanged from https://github.com/facebookresearch/InvariantRiskMinimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Empirical validation of some key insights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1. HESSIAN MATCHING</head><p>Based on empirical works <ref type="bibr">(Li et al., 2020;</ref><ref type="bibr">Singh &amp; Alistarh, 2020;</ref><ref type="bibr">Thomas et al., 2020)</ref>, we argue in Section 3.2.3 that gradient covariance C can be used as a proxy to regularize the Hessian H -even though the proper approximation bounds are out of scope of this paper. This was empirically validated at convergence in <ref type="table" target="#tab_2">Table 2</ref> and during training in <ref type="figure" target="#fig_3">Fig. 3</ref>. We leveraged the DiagHessian method from BackPACK to compute Hessian diagonals, in all network weights ?. Notably, Hessians are impractical in a training objective as computing "Hessian is an order of magnitude more computationally intensive" (see <ref type="figure">Fig. 9</ref> in <ref type="bibr">Dangel et al. (2020)</ref>). This Appendix further analyzes the Hessian trajectory during training.    This is also visible in <ref type="figure" target="#fig_10">Fig. 7</ref>, which is equivalent to <ref type="figure" target="#fig_3">Fig. 3</ref>, but for ERM (without the Fishr regularization). The distance between domain-level gradient variances (red) keeps increasing across domains E = {90%, 80%}: so does the distance across Hessians (purple). The distance across risks (pink) decreases, but slower than with Fishr regularization. Overall, the network still predicts the digit's color while only slightly using the digit's shape. That's why the test accuracy (blue) remains low. To further validate that Fishr can tackle distribution shifts, we investigate Colored MNIST but without the 25% label flipping. In <ref type="table" target="#tab_11">Table  6</ref>, the label is then fully predictable from the digit shape. Using hyperparameters defined previously in Appendix C.1, we recover that IRM (82.2%) fails when the invariant feature is fully predictive <ref type="bibr" target="#b1">(Ahuja et al., 2019)</ref>: indeed, it performs worse than ERM (91.8%). In contrast, V-REx and Fishr ? perform better (95.3%): in conclusion, Fishr works even without label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2. COLORED MNIST WITHOUT LABEL FLIPPING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3. GRADIENT VARIANCE OR COVARIANCE ?</head><p>We have justified ignoring the off-diagonal parts of the covariance to reduce the memory overhead. For the sake of completeness, the second line in <ref type="table" target="#tab_12">Table 7</ref> shows results with the full covariance matrix. This experiment is possible only when considering gradient in the classifier w ? for memory reasons. Overall, results are similar (or slightly worse) as when using only the diagonal: the slight difference may be explained by the approaches' different suitability to the hyperparameters (that were optimized for IRM). In conclusion, this preliminary experiment suggests that targeting the diagonal components is the most critical. We hope future works will further investigate this diagonal approximation or provide new methods to reduce the computational costs, such as K-FAC approximations <ref type="bibr" target="#b23">(Heskes, 2000;</ref><ref type="bibr" target="#b57">Martens &amp; Grosse, 2015)</ref>. In Section 3.2.3, we argue that the gradient centered covariance C and the empirical Fisher Information Matrix (or uncentered covariance)F are highly related and equivalent when the DNN is at convergence and the gradient means are zero. So, we could have tackled the diagonals of the domain-level {F e } e?E across domains, i.e., without centering the variances.</p><p>Empirically, comparing the first and third lines in <ref type="table" target="#tab_12">Table 7</ref> shows that centering or not the variance are almost equivalent. This holds true when applying Fishr on all weights ? (as lines fourth and six are also very similar). This was empirically confirmed in DomainBed: for example, Fishr with either centered or uncentered variances reach 67.8. Still, it's worth noting that explicitly matching simultaneously the gradient centered variances along with the gradient means performs best in Appendix D.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DomainBed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Description of the DomainBed benchmark</head><p>We now further detail our experiments on the DomainBed benchmark. Scores from most baselines are taken from the DomainBed <ref type="bibr">(Gulrajani &amp; Lopez-Paz, 2021)</ref> paper. Scores for AND-mask and SAND-mask are taken from the SAND-mask paper <ref type="bibr">(Shahtalebi et al., 2021)</ref>. Scores for IGA <ref type="bibr">(Koyama &amp; Yamaguchi, 2020)</ref> are not yet available: yet, for the sake of completeness, we analyze IGA in Appendix D.3.2. Missing scores will be included when available.</p><p>The same procedure was applied for all methods: for each domain, a random hyperparameter search of 20 trials over a joint distribution, described in <ref type="table" target="#tab_13">Table 8</ref>, is performed. We discuss the choice of these distributions in Appendix D.3.3. The learning rate, the batch size (except for ARM), the weight decay and the dropout distributions are shared across all methodsall trained with Adam <ref type="bibr" target="#b33">(Kingma &amp; Ba, 2014)</ref>. Specific hyperparameter distributions for concurrent methods can be found in the original work of <ref type="bibr">Gulrajani &amp; Lopez-Paz (2021)</ref>. The data from each domain is split into 80% (used as training and testing) and 20% (used as validation for hyperparameter selection) splits. This random process is repeated with 3 different seeds: the reported numbers are the means and the standard errors over these 3 seeds. We clarify a subtle point (omitted in the Algorithm 1) concerning the hyperparameter ? that controls:v t e = ?v t?1 e +(1??)v t e at step t. We remind thatv t?1 e from previous step t ? 1 is 'detached' from the computational graph. Thus when L from Eq. 4 is differentiated during SGD, the gradients going through v t e are multiplied by (1 ? ?). To compensate this and decorrelate the impact of ? and of ? (that controls the regularization strength), we match 1 1??v t e . Finally, with this (1 ? ?) correction, the gradients' strength backpropagated in the network is independent of ?.</p><p>Here we list all concurrent approaches.</p><p>? ERM: Empirical Risk Minimization <ref type="bibr">(Vapnik, 1999)</ref> ? IRM: Invariant Risk Minimization <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref> ? GroupDRO: Group Distributionally Robust Optimization <ref type="bibr">(Sagawa et al., 2020a)</ref> ? Mixup: Interdomain Mixup (Yan et al., 2020)</p><p>? MLDG: Meta Learning Domain Generalization <ref type="bibr" target="#b46">(Li et al., 2018a)</ref> ? CORAL: Deep CORAL <ref type="bibr" target="#b82">(Sun &amp; Saenko, 2016)</ref> ? MMD: Maximum Mean Discrepancy <ref type="bibr" target="#b47">(Li et al., 2018b)</ref> ? DANN: Domain Adversarial Neural Network <ref type="bibr">(Ganin et al., 2016)</ref> ? CDANN: Conditional Domain Adversarial Neural Network <ref type="bibr" target="#b49">(Li et al., 2018c)</ref> ? MTL: Marginal Transfer Learning <ref type="bibr">(Blanchard et al., 2021)</ref> Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization ? SagNet: Style Agnostic Networks <ref type="bibr">(Nam et al., 2021)</ref> ? ARM: Adaptive Risk Minimization <ref type="bibr">(Zhang et al., 2020)</ref> ? V-REx: Variance Risk Extrapolation <ref type="bibr">(Krueger et al., 2021)</ref> ? RSC: Representation Self-Challenging <ref type="bibr">(Huang et al., 2020)</ref> ? AND-mask: Learning Explanations that are Hard to Vary <ref type="bibr">(Parascandolo et al., 2021)</ref> ? SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization <ref type="bibr">(Shahtalebi et al., 2021)</ref> ? IGA: Out-of-distribution generalization with maximal invariant predictor <ref type="bibr">(Koyama &amp; Yamaguchi, 2020)</ref> ? Fish: Gradient Matching for Domain Generalization <ref type="bibr">(Shi et al., 2021)</ref> We omitted the recent weight averaging approaches <ref type="bibr">(Cha et al., 2021;</ref><ref type="bibr">Rame et al., 2022)</ref> whose contribution is complementary to others, that uses a custom hyperparameter search and does not report scores with the 'Test-domain' model selection.</p><p>DomainBed includes seven multi-domain computer vision classification datasets:  <ref type="bibr" target="#b64">(3,</ref><ref type="bibr">224,</ref><ref type="bibr">224)</ref> and 345 classes.</p><p>The convolutional neural network architecture used for the MNIST experiments is the one introduced in DomainBed: note that this is not the same MLP (described in Appendix C.1) as in our proof of concept in Section 4.1. All real datasets leverage a 'ResNet-50' pretrained on ImageNet, with a dropout layer before the newly added dense layer and fine-tuned with frozen batch normalization layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. 'Training-domain' model selection</head><p>In the main paper, we focus on the 'Test-domain' model selection, where the validation set follows the same distribution as the test domain. This is important to adapt the degree of model invariance according to the test domain. For Fishr, if the domain-dependant correlations are useful in test, the selected ? would be small and Fishr would behave like ERM; in contrast, if the domain-dependant correlations are detrimental in test, the selected ? would be large, and Fishr would improve over ERM by enforcing invariance. In <ref type="table" target="#tab_15">Table 9</ref>, we use the 'Training-domain' model selection: the validation set is formed by randomly collecting 20% of each training domain. Fishr performs better than ERM on all real datasets (over standard errors for OfficeHome and DomainNet), except for PACS where the two reach 85.5%. In average, Fishr (67.1%) finishes third and is above most methods such as V-REx (65.6%). Fishr median ranking is fifth, with a mean ranking of 5.6. These additional results were not included in the main paper due to space constraints and also because this 'Training-domain' model selection has three clear limitations.</p><p>First, learning causal mechanisms can be useless in this 'Training-domain' setup. Indeed, when the correlations are more predictive in training than the causal features, the variant model may be selected over the invariant one. This explains the poor results for all methods in 'Training-domain' Colored MNIST, where the color information is more predictive than the shape information in training. The best model on this task is ARM <ref type="bibr">(Zhang et al., 2020)</ref> that uses test time adaptation -thus in a sense uses information from the test-domain -and whose contribution is mostly complementary to ours.</p><p>Second, the 'Training-domain' setup suffers from underspecification: "predictors with equivalently strong held-out performance in the training domain [...] can behave very differently" in test <ref type="bibr">(D'Amour et al., 2020)</ref>. This underspecification favors low regularization thus low values of ?. To select the model with the best generalization properties, future benchmarks may consider the training calibration <ref type="bibr">(Wald et al., 2021)</ref> rather than merely selecting the model with the best training accuracy.</p><p>Third, the 'Test-domain' model selection is more realistic for real applications. Indeed, one user would easily label some samples to validate the efficiency of its algorithm. It's not realistic to believe that the users would simply deploy their new algorithm without at least checking that the performances are correct. We recall that the 'Test-domain' setup in DomainBed benchmark is quite restricting, allowing only one evaluation per choice of hyperparameters, without early-stopping.</p><p>That's why Teney et al. (2021) even states that "OOD performance cannot, by definition, be performed with a validation set from the same distribution as the training data". Both opinions being reasonable and arguable, we included 'Trainingdomain' results for the sake of completeness, where Fishr remains stronger than ERM. Yet, our state-of-the-art results on the 'Test-domain' setup from <ref type="table" target="#tab_5">Table 4</ref> alone are sufficient to prove the usefulness of our approach for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Fishr component analysis on DomainBed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.1. FOCUS ON THE EXPONENTIAL MOVING AVERAGE</head><p>Following Le <ref type="bibr" target="#b41">Roux et al. (2011)</ref>, we use an exponential moving average (ema) parameterized by ? for computing gradient variances in DomainBed: the closer ? is to 1, the longer a batch will impact the variance from later steps. We now further analyze the impact of this strategy, which is not specific to Fishr and was used previously in other works <ref type="bibr">(Nam et al., 2020;</ref><ref type="bibr">Blanchard et al., 2021;</ref><ref type="bibr">Zhang et al., 2021)</ref> for OOD generalization. Notably, this ema strategy could be applied to better estimate domain-level empirical risks in V-REx <ref type="bibr">(Krueger et al., 2021)</ref>. For a fair comparison, we introduce a new approach -V-REx with ema -that penalizes |R t A ?R t B | 2 at step t whereR t e = ?R t?1 e + (1 ? ?)R t e when E = {A, B}. Thus, we compare V-REx and Fishr, with ? = 0 () or with ? ? Uniform(0.9, 0.99) (, as described in <ref type="table" target="#tab_13">Table 8</ref>). On the   <ref type="table" target="#tab_1">Table 11</ref>, the ema is less beneficial (from 67.5% to 68.2% in 'Test-domain' for Fishr). Notably, it worsens V-REX. Overall, Fishr -with and without ema -outperforms V-REx on OfficeHome.</p><p>We speculate that ema mainly helps when the batch size is not sufficiently large to detect 'slight' correlation shifts in the training datasets: e.g., when batch size ? 2 Uniform <ref type="bibr" target="#b64">(3,</ref><ref type="bibr">9)</ref> and training datasets E = {90%, 80%} in Colored MNIST. We remind that when the batch size was 25,000 in the Colored MNIST setup from IRM, Fishr reached 69.5% (without ema) in <ref type="table" target="#tab_4">Table 3</ref> from Section 4.1. On the contrary, when the shift is more prominent as in OfficeHome, the ema may be less necessary. Most importantly, Fishr -with and without ema -improves over ERM on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.2. COMPONENT ANALYSIS BY COMPARING GRADIENT VARIANCE VERSUS GRADIENT MEAN MATCHING</head><p>As a reminder from the Section 2, IGA <ref type="bibr">(Koyama &amp; Yamaguchi, 2020)</ref> is an unpublished gradient-based approach that matches gradient means across domains, i.e., minimizes ||g A ? g B || 2 2 when E = {A, B} and where g e = 1 ne ne i=1 ? ? (f ? (x e ), y e ). Scores for IGA are not available publicly and thus were not included in Section 4.2.1. Moreover, IGA is very costly and impractical: IGA is approximately (|E| + 1) times longer to train than ERM. Yet, we ran the DomainBed implementation of IGA on one 'synthetic' and one 'real' dataset. <ref type="table" target="#tab_1">Table 12</ref> shows that the IGA has little effect on . Moreover, on OfficeHome in <ref type="table" target="#tab_1">Table 13</ref>, . In brief, the seminal "IGA [. . .] could completely fail when generalizing to unseen domains", as stated in Fish <ref type="bibr">(Shi et al., 2021)</ref>.</p><p>In the rest of this Section, we include IGA in Fishr codebase so that both methods leverage the same implementation choices: this enables fairer comparisons between gradient mean matching and gradient variance matching. These experiments provide further insights regarding Fishr main components: specifically, enforcing invariance (1) only in the  classifier's weights ? (2) after a warmup period and <ref type="formula" target="#formula_5">(3)</ref> with an exponential moving average.</p><p>First, Fishr only considers gradient variances in the classifier's weights ?. Similarly, we try to apply IGA's gradient mean matching but only in w ? rather than in f ? . This new method works significantly better (67.2% when</p><formula xml:id="formula_29">g e = 1 ne ne i=1 ? ? (f ? (x e ), y e ) vs. 56.9% when g e = 1 ne ne i=1 ? ? (f ? (x e )</formula><p>, y e ) for 'Test-domain' OfficeHome in <ref type="table" target="#tab_1">Table 13</ref>) while reducing the computational overhead. This further motivates the invariance in the classifier rather than in the low-level layers (which need to adapt to shifts in pixels for instance). We have done this analysis on IGA and not on Fishr because keeping all individual gradients for a ResNet-50 in the GPU memory was not possible on our hardware.</p><p>Second, Fishr uses a double-stage scheduling inherited from IRM <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref>: the DNN first learns predictive features with standard ERM (? = 0) until a given epoch, at which ? takes its true (high) value to then force domain invariance. This warmup strategy slightly increases 'Test-domain' results on Colored MNIST (from 58.6% to 59.8% for Fishr, from 58.3% to 59.2% for IGA) but does not seem critical: in particular, it reduces IGA 'Test-domain' scores on OfficeHome.</p><p>Third, the estimation of gradient variances was improved with an exponential moving average (see Section 4.2.1 and Appendix D.3.1). We now use this strategy with domain-level gradient means for IGA in ?:? t e = ?? t?1 e + (1 ? ?)g t e . This improves IGA (from 67.0% to 67.2% in 'Test-domain' on OfficeHome): yet, these scores remain consistently worse than Fishr's (from 67.5% to 68.2%).</p><p>In conclusion, this complements the experiments in Section 4.2.1 which showed that tackling gradient variance does better than tackling gradient mean: indeed, Fishr performed better than Fish <ref type="bibr">(Shi et al., 2021)</ref>, AND-mask <ref type="bibr">(Parascandolo et al., 2021)</ref> and SAND-mask <ref type="bibr">(Shahtalebi et al., 2021)</ref>. As a final note, Fishr + IGA -i.e., matching simultaneously gradient means (the first moment) and variances (the second moment) -performs best. Future works may further analyze the complementary of these gradient-based methods. This Section is a preliminary introduction to a meta-discussion, not about the methodology to select the best hyperparameters, but about the methodology to select the hyperparameter distributions in DomainBed. This question has not been discussed in previous works (as far as we know).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.3. HYPERPARAMETER DISTRIBUTIONS</head><p>After few initial iterations on the main idea of the paper, we had to select the distributions to sample our three hyperparameters from, as described in <ref type="table" target="#tab_13">Table 8</ref>. First, to select the ema ? distribution, we knew that the authors from Le Roux et al. <ref type="formula" target="#formula_3">(2011)</ref> have not noticed "any significant difference in validation errors" for different values higher than 0.9. Moreover ? should remain strictly lower than 1. Thus, sampling from Uniform(0.9, 0.99) seemed appropriate. Second, sampling the number of warmup iterations uniformly along training from Uniform(0, 5000) seemed the most natural and neutral choice. Lastly, the choice of the ? distribution was more complex. As a reminder, a low ? inactivates the regularization while an extremely high ? may destabilize the training.</p><p>In <ref type="table" target="#tab_1">Table 14</ref>, we investigate two distributions: ? ? 10 Uniform(1,4) (eventually chosen for Fishr) and ? ? 10 Uniform <ref type="bibr">(1,</ref><ref type="bibr">5)</ref> . First, we observe that results are mostly similar: it confirms that Fishr is consistently better than ERM (where ? = 0), and in average is the best approach with the 'Test-domain' model selection and among the best approaches with the 'Trainingdomain' model selection. Second, the existence of consistent differences in results suggests that the best hyperparameter distribution depends on the dataset at hand and that the performance gap depends on the selection method.</p><p>While out of the scope of this paper, we believe these results were important for transparency (along with publishing our code), and may motivate the need for new protocols -for example with bayesian hyperparameter search (Turner et al., 2021) -that future benchmarks may introduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Full DomainBed results</head><p>Tables below detail results for each dataset with 'Test-domain' and 'Training-domain' model selection methods. We format first and second best accuracies. Note that the per-dataset results for Fish <ref type="bibr">(Shi et al., 2021)</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Fishr principle. Fishr considers the individual (per-sample) gradients of the loss in the network weights ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FishrFigure 2 :</head><label>2</label><figDesc>Loss landscapes around inconsistent weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Colored MNIST dynamics. At epoch 190, ?   strongly steps up: then, the Fishr ? regularization matches the domain-level gradient variances (red) across domains E = {90%, 80%}, and consequently, the training empirical risks (dotted pink) and Hessians (purple). This reduces train accuracy (orange) but increases test accuracy (blue) as the network learns to predict the digit's shape. As shown inFig. 7, training dynamics are different for ERM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>4. Appendix D enriches the DomainBed experiments. After a description of the benchmark protocols in Appendix D.1, Appendix D.2 discusses the model selection strategy. Then Appendix D.3 provides additional experiments to analyze key components of Fishr. Specifically, D.3.1 analyzes the exponential moving average; D.3.2 compares gradient mean versus gradient variance matching and also motivates ignoring the gradients in the features extractor; D.3.3 discusses the methodology to select hyperparameter distributions. Finally, Appendix D.4 provides the per-dataset results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fishr:Figure 4 :</head><label>4</label><figDesc>Invariant Gradient Variances for Out-of-Distribution Generalization Inconsistency I (A, B) between domains A and B, decomposed into R(A, B) depending on domain-level risks and H (A, B) depending on domain-level curvatures at ? * . Case R(A, B) ? 0 Both R(A, B) and H (A, B) are non-negative. Removing the absolute value from the RHS of Eq. 13 gives: I (A, B) = R(A, B) + H (A, B). Taking the maximum over (A, B) ? E 2 where R(A, B) ? 0 gives:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A). Thus the max over E 2 of function (A, B) ? I (A, B) can not be achieved for (A, B) with R(A, B) &lt; 0. We obtain: max (A,B)?E 2 I (A, B) = max (A,B)?E 2 |R(A,B)?0 I (A, B) (16) Similarly, R(A, B) + H (A, B) ? 0 &lt; R(B, A) + H (B, A). Thus the max over E 2 of function (A, B) ? (R(A, B) +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>illustrates the dynamics for Fishr ? : following the scheduling previously described in Appendix C.1, ? jumping to a high value at epoch 190 activates the regularization. After this epoch, the domain-level Hessians are not only close in Frobenius distance, but also have similar norms and directions. On the contrary, when using only ERM inFig. 6, the distance between domain-level Hessians keeps increasing with the number of epochs. As a side note, flatter loss landscapes in ERM -as reflected by the Hessian norms in orange -do not correlate with improved generalization(Dinh et al., 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Hessian dynamics on Colored MNIST with Fishr: at epoch 190, ? steps up. Then domain-level Hessians are matched across domains (purple). More precisely, they take similar directions -high cosine similarity (red) -and similar norms (blue). The Hessians' norms (orange) remain quite high thus the loss landscapes are rather sharp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Hessian dynamics on Colored MNIST with ERM: ? = 0 along training. The Frobenius distance between domain-level Hessians (purple) keeps increasing: so does the distance between their norms (blue). Their cosine similarity (red) steadily decreases. The loss landscapes are flat at convergence (low Hessian norms in orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Colored MNIST dynamics with ERM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>? 0.1 98.9 ? 0.1 99.0 ? 0.0 99.0 ? 0.0 98.9 ? 0.0 96.0 ? 0.2 97.9 8 DANN 95.0 ? 0.5 98.9 ? 0.1 99.0 ? 0.0 99.0 ? 0.1 98.9 ? 0.0 96.3 ? 0.2 97.8 13 CDANN 95.7 ? 0.2 98.8 ? 0.0 98.9 ? 0.1 98.9 ? 0.1 98.9 ? 0.1 96.1 ? 0.3 97.9 8 MTL 95.6 ? 0.1 99.0 ? 0.1 99.0 ? 0.0 98.9 ? 0.1 99.0 ? 0.1 95.8 ? 0.2 97.9 8 SagNet 95.9 ? 0.3 98.9 ? 0.1 99.0 ? 0.1 99.1 ? 0.0 99.0 ? 0.1 96.3 ? 0.1 98.0 2 ARM 96.7 ? 0.2 99.1 ? 0.0 99.0 ? 0.0 99.0 ? 0.1 99.1 ? 0.1 96.5 ? 0.4 98.0.2 98.8 ? 0.1 98.9 ? 0.0 98.7 ? 0.0 98.7 ? 0.1 95.5 ? 0.4 97.6 16 SAND-mask 94.5 ? 0.4 98.6 ? 0.1 98.8 ? 0.1 98.7 ? 0.1 98.6 ? 0.0 95.5 ? 0Gradient Variances for Out-of-Distribution Generalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>framework simply minimizes the average empirical risk over all training domains, i.e.,</figDesc><table><row><cell></cell><cell>1 |E|</cell><cell>e?E R e (?) where</cell></row><row><cell>R e (?) = 1 ne</cell><cell>ne i=1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Cosine similarity between Hessian diagonals and gradient variances cos (Diag (H e ) , Var(G e )), for an ERM at convergence on Colored MNIST with the two training domains e ? {90%, 80%}.</figDesc><table><row><cell></cell><cell>e = 90%</cell><cell>e = 80%</cell></row><row><cell>On classifier weights w</cell><cell cols="2">0.9999980 0.9999905</cell></row><row><cell cols="3">On all network weights ? 0.9971040 0.9962264</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Invariance analysis at convergence on Colored MNIST across the two training domains E = {90%, 80%}.</figDesc><table><row><cell>Compared to ERM, Fishr matches the gradient variance</cell></row><row><cell>(Diag(C 90% ) ? Diag(C 80% )) in all network weights</cell></row><row><cell>?. Most importantly, this enforces invariance in domain-</cell></row><row><cell>level risks (R 90% ? R 80% ) and in domain-level Hes-</cell></row><row><cell>sians (Diag(H 90% ) ? Diag(H 80%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>empirically confirms that matching {Diag(C e )} e?E -i.e., {Var(G e )} e?E -with Fishr forces the domain-level Hes-sians {Diag(H e )} e?E to be aligned at convergence (on the diagonal for computational reasons). Tackling the second moment of the first-order derivatives enables to regularize the second-order derivatives. Moreover, Appendix C.2.4 shows that matching the diagonals of {C e } e?E or {F e } e?E -i.e., centering or not the variances -perform similarly.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Colored MNIST results. All methods use hyperparameters optimized for IRM.</figDesc><table><row><cell cols="2">Method Train acc.</cell><cell>Test acc.</cell><cell>Gray test acc.</cell></row><row><cell>ERM</cell><cell cols="2">86.4 ? 0.2 14.0 ? 0.7</cell><cell>71.0 ? 0.7</cell></row><row><cell>IRM</cell><cell cols="2">71.0 ? 0.5 65.6 ? 1.8</cell><cell>66.1 ? 0.2</cell></row><row><cell cols="3">V-REx 71.7 ? 1.5 67.2 ? 1.5</cell><cell>68.6 ? 2.2</cell></row><row><cell>Fishr</cell><cell></cell><cell></cell></row></table><note>? 69.6 ? 0.9 71.2 ? 1.1 70.2 ? 0.7 Fishr? 71.0 ? 0.9 69.5 ? 1.0 70.2 ? 1.1 Fishr? 65.6 ? 1.3 73.8 ? 1.0 70.0 ? 0.9</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>DomainBed benchmark. We format first, second and worse than ERM results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy (?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ranking (?)</cell></row><row><cell>Algorithm</cell><cell cols="2">CMNIST RMNIST</cell><cell>VLCS</cell><cell>PACS</cell><cell cols="4">OfficeHome TerraInc DomainNet Avg</cell><cell>Arith. mean</cell><cell>Geom. mean</cell><cell>Median</cell></row><row><cell>ERM</cell><cell>57.8 ? 0.2</cell><cell cols="3">97.8 ? 0.1 77.6 ? 0.3 86.7 ? 0.3</cell><cell>66.4 ? 0.5</cell><cell>53.0 ? 0.3</cell><cell>41.3 ? 0.1</cell><cell>68.7</cell><cell>9.1</cell><cell>8.1</cell><cell>8</cell></row><row><cell>IRM</cell><cell>67.7 ? 1.2</cell><cell cols="3">97.5 ? 0.2 76.9 ? 0.6 84.5 ? 1.1</cell><cell>63.0 ? 2.7</cell><cell>50.5 ? 0.7</cell><cell>28.0 ? 5.1</cell><cell>66.9</cell><cell>14.7</cell><cell>12.4</cell><cell>16</cell></row><row><cell>GroupDRO</cell><cell>61.1 ? 0.9</cell><cell cols="3">97.9 ? 0.1 77.4 ? 0.5 87.1 ? 0.1</cell><cell>66.2 ? 0.6</cell><cell>52.4 ? 0.1</cell><cell>33.4 ? 0.3</cell><cell>67.9</cell><cell>8.6</cell><cell>7.5</cell><cell>8</cell></row><row><cell>Mixup</cell><cell>58.4 ? 0.2</cell><cell cols="3">98.0 ? 0.1 78.1 ? 0.3 86.8 ? 0.3</cell><cell>68.0 ? 0.2</cell><cell>54.4 ? 0.3</cell><cell>39.6 ? 0.1</cell><cell>69.0</cell><cell>5.3</cell><cell>3.9</cell><cell>4</cell></row><row><cell>MLDG</cell><cell>58.2 ? 0.4</cell><cell cols="3">97.8 ? 0.1 77.5 ? 0.1 86.8 ? 0.4</cell><cell>66.6 ? 0.3</cell><cell>52.0 ? 0.1</cell><cell>41.6 ? 0.1</cell><cell>68.7</cell><cell>9.1</cell><cell>8.2</cell><cell>9</cell></row><row><cell>CORAL</cell><cell>58.6 ? 0.5</cell><cell cols="3">98.0 ? 0.0 77.7 ? 0.2 87.1 ? 0.5</cell><cell>68.4 ? 0.2</cell><cell>52.8 ? 0.2</cell><cell>41.8 ? 0.1</cell><cell>69.2</cell><cell>4.6</cell><cell>3.4</cell><cell>3</cell></row><row><cell>MMD</cell><cell>63.3 ? 1.3</cell><cell cols="3">98.0 ? 0.1 77.9 ? 0.1 87.2 ? 0.1</cell><cell>66.2 ? 0.3</cell><cell>52.0 ? 0.4</cell><cell>23.5 ? 9.4</cell><cell>66.9</cell><cell>7.0</cell><cell>4.9</cell><cell>6</cell></row><row><cell>DANN</cell><cell>57.0 ? 1.0</cell><cell cols="3">97.9 ? 0.1 79.7 ? 0.5 85.2 ? 0.2</cell><cell>65.3 ? 0.8</cell><cell>50.6 ? 0.4</cell><cell>38.3 ? 0.1</cell><cell>67.7</cell><cell>11.9</cell><cell>9.6</cell><cell>15</cell></row><row><cell>CDANN</cell><cell>59.5 ? 2.0</cell><cell cols="3">97.9 ? 0.0 79.9 ? 0.2 85.8 ? 0.8</cell><cell>65.3 ? 0.5</cell><cell>50.8 ? 0.6</cell><cell>38.5 ? 0.2</cell><cell>68.2</cell><cell>9.6</cell><cell>7.4</cell><cell>10</cell></row><row><cell>MTL</cell><cell>57.6 ? 0.3</cell><cell cols="3">97.9 ? 0.1 77.7 ? 0.5 86.7 ? 0.2</cell><cell>66.5 ? 0.4</cell><cell>52.2 ? 0.4</cell><cell>40.8 ? 0.1</cell><cell>68.5</cell><cell>8.4</cell><cell>7.8</cell><cell>7</cell></row><row><cell>SagNet</cell><cell>58.2 ? 0.3</cell><cell cols="3">97.9 ? 0.0 77.6 ? 0.1 86.4 ? 0.4</cell><cell>67.5 ? 0.2</cell><cell>52.5 ? 0.4</cell><cell>40.8 ? 0.2</cell><cell>68.7</cell><cell>8.0</cell><cell>7.2</cell><cell>6</cell></row><row><cell>ARM</cell><cell>63.2 ? 0.7</cell><cell cols="3">98.1 ? 0.1 77.8 ? 0.3 85.8 ? 0.2</cell><cell>64.8 ? 0.4</cell><cell>51.2 ? 0.5</cell><cell>36.0 ? 0.2</cell><cell>68.1</cell><cell>9.9</cell><cell>7.5</cell><cell>12</cell></row><row><cell>V-REx</cell><cell>67.0 ? 1.3</cell><cell cols="3">97.9 ? 0.1 78.1 ? 0.2 87.2 ? 0.6</cell><cell>65.7 ? 0.3</cell><cell>51.4 ? 0.5</cell><cell>30.1 ? 3.7</cell><cell>68.2</cell><cell>7.7</cell><cell>5.5</cell><cell>5</cell></row><row><cell>RSC</cell><cell>58.5 ? 0.5</cell><cell cols="3">97.6 ? 0.1 77.8 ? 0.6 86.2 ? 0.5</cell><cell>66.5 ? 0.6</cell><cell>52.1 ? 0.2</cell><cell>38.9 ? 0.6</cell><cell>68.2</cell><cell>9.9</cell><cell>9.4</cell><cell>9</cell></row><row><cell>AND-mask</cell><cell>58.6 ? 0.4</cell><cell cols="3">97.5 ? 0.0 76.4 ? 0.4 86.4 ? 0.4</cell><cell>66.1 ? 0.2</cell><cell>49.8 ? 0.4</cell><cell>37.9 ? 0.6</cell><cell>67.5</cell><cell>13.4</cell><cell>13.1</cell><cell>12</cell></row><row><cell cols="2">SAND-mask 62.3 ? 1.0</cell><cell cols="3">97.4 ? 0.1 76.2 ? 0.5 85.9 ? 0.4</cell><cell>65.9 ? 0.5</cell><cell>50.2 ? 0.1</cell><cell>32.2 ? 0.6</cell><cell>67.2</cell><cell>14.3</cell><cell>13.5</cell><cell>15</cell></row><row><cell>Fish</cell><cell>61.8 ? 0.8</cell><cell cols="3">97.9 ? 0.1 77.8 ? 0.6 85.8 ? 0.6</cell><cell>66.0 ? 2.9</cell><cell>50.8 ? 0.4</cell><cell>43.4 ? 0.3</cell><cell>69.1</cell><cell>8.4</cell><cell>6.6</cell><cell>7</cell></row><row><cell>Fishr</cell><cell>68.8 ? 1.4</cell><cell cols="3">97.8 ? 0.1 78.2 ? 0.2 86.9 ? 0.2</cell><cell>68.2 ? 0.2</cell><cell>53.6 ? 0.4</cell><cell>41.8 ? 0.2</cell><cell>70.8</cell><cell>3.9</cell><cell>2.8</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>summarizes the results on DomainBed using the 'Test-domain' model selection: the validation set (to select the best hyperparameters) follows the same distribution as the test domain. Appendix D.2 reports results with the 'Training-domain' model selection while results are detailed per dataset in Appendix D.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Fishr reaches state-of-the-art performances on DomainBed when samples from the test domain are available for model selection. Our experiments -reproducible with our open-source implementation -suggest that Fishr would consistently improve a deep classifier for real-world usages when dealing with data from multiple domains. We hope to pave the way towards new gradient-based regularization to improve the generalization abilities of deep neural networks. DeGrave, A. J., Janizek, J. D., and Lee, S.-I. Ai for radiographic covid-19 detection selects shortcuts over signal. Nature Machine Intelligence, 2021. (p. 1). Deutsch, D. The beginning of infinity: Explanations that transform the world. Penguin UK, 2011. (p. 4). Ding, Z. and Fu, Y. Deep domain generalization with structured low-rank constraint. In TIP, 2017. (p. 2). Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. In ICML, 2017. (p. Xu, Y., and Rockmore, D. N. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. (pp. 7, 20). Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In ICML, 2017. (p. 3). Fisher, R. A. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London., 1922. (pp. 2, 5). Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. Sharpness-aware minimization for efficiently improving generalization. In ICLR, 2021. (p. 5).Fort, S., Nowak, P. K., Jastrzebski, S., and Narayanan, S. Stiffness: A new perspective on generalization in neural networks. arXiv preprint, 2019. (p. 2).Frantar, E., Kurtic, E., and Alistarh, D. Efficient matrixfree approximations of second-order information, with applications to pruning and optimization. arXiv preprint, 2021. (p. 5). Tejani, A., and Husz?r, F. Faster gaze prediction with dense networks and fisher pruning. arXiv preprint, 2018. (p. 6).Thomas, V., Pedregosa, F., van Merri?nboer, B., Manzagol, P.-A., Bengio, Y., and Roux, N. L. On the interplay between noise and curvature and its effect on optimization and generalization. InAISTATS, 2020. (pp. 5, 6, 17).Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Deep domain confusion: Maximizing for domain invariance. InCoRR, 2014. (p. 3).Valle-Perez, G., Camargo, C. Q., and Louis, A. A. Deep learning generalizes because the parameter-function map is biased towards simple functions. In ICLR, 2019. (p. 1).Vapnik, V. N. An overview of statistical learning theory. InTNN, 1999. (pp. 2, 19). InAISTATS,  2018. (pp. 2, 3).Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning. In NeurIPS, 2020. (p. 3).Zhang, X., Cui, P., Xu, R., Zhou, L., He, Y., and Shen, Z.Deep stable learning for out-of-distribution generalization. InCVPR, 2021. (pp. 2, 21).Zhang, Y., Yu, W., and Turk, G. Learning novel policies for tasks. In ICML, 2019. (p. 3).</figDesc><table><row><cell>17). Du, Y., Czarnecki, W. M., Jayakumar, S. M., Farajtabar, M., Pascanu, R., and Lakshminarayanan, B. Adapting auxiliary losses using gradient similarity. arXiv preprint, 2018. (p. 3). Faghri, F., Duvenaud, D., Fleet, D. J., and Ba, J. A study of gradient variance in deep learning. arXiv preprint, 2020. (p. 6). chanathan, S. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. (pp. 7, 20). Wald, Y., Feder, A., Greenfeld, D., and Shalit, U. On calibration and out-of-domain generalization. In NeurIPS, Fang, C., Venkateswara, H., Eusebio, J., Chakraborty, S., and Pan-2021. (p. 21).</cell></row></table><note>Teney, D., Abbasnejad, E., and van den Hengel, A. Unshuf- fling data for improved generalization. arXiv preprint, 2020. (p. 3). Teney, D., Abbasnejad, E., Lucey, S., and van den Hengel, A. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior OOD general- ization. arXiv preprint, 2021. (p. 21). Theis, L., Korshunova, I.,Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laakso- nen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperpa- rameter tuning: Analysis of the black-box optimization challenge 2020. In NeurIPS, 2021. (p. 24).Wang, Y., Li, H., and Kot, A. C. Heterogeneous domain generalization via domain mixup. In ICASSP, 2020. (p. 2). Wu, Y., Inkpen, D., and El-Roby, A. Dual mixup regularized learning for adversarial domain adaptation. In ECCV, 2020. (p. 2). Xie, S. M., Kumar, A., Jones, R., Khani, F., Ma, T., and Liang, P. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In ICLR, 2021. (p. 2). Yan, S., Song, H., Li, N., Zou, L., and Ren, L. Improve un- supervised domain adaptation with mixup training. arXiv preprint, 2020. (p. 19). Yang, G. and Salman, H. A fine-grained spectral perspective on neural networks. arXiv preprint, 2019. (p. 16). Ye, N., Li, K., Hong, L., Bai, H., Chen, Y., Zhou, F., and Li, Z. Ood-bench: Benchmarking and understanding out-of- distribution generalization datasets and algorithms. arXiv preprint, 2021. (pp. 1, 2, 7, 9). Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ram- chandran, K., and Bartlett, P. Gradient diversity: a key ingredient for scalable distributed learning.Zhang, M., Marklund, H., Dhawan, N., Gupta, A., Levine, S., and Finn, C. Adaptive risk minimization: A meta- learning approach for tackling group distribution shift. arXiv preprint, 2020. (pp. 3, 20, 21).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performances comparison on the linear dataset from(Shi et al., 2021)    C. Colored MNIST in the IRM Setup</figDesc><table /><note>C.1. Description of the Colored MNIST experiment Colored MNIST is a binary digit classification dataset introduced in IRM (Arjovsky et al., 2019). Compared to the traditional MNIST (LeCun et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Colored MNIST experiments without label flipping.</figDesc><table><row><cell cols="2">Method Train acc.</cell><cell>Test acc.</cell><cell>Gray test acc.</cell></row><row><cell>ERM</cell><cell cols="2">99.0 ? 0.0 91.8 ? 0.2</cell><cell>95.0 ? 0.4</cell></row><row><cell>IRM</cell><cell cols="2">96.4 ? 0.2 82.2 ? 0.1</cell><cell>92.6 ? 0.2</cell></row><row><cell cols="3">V-REx 97.1 ? 0.2 95.3 ? 0.4</cell><cell>94.1 ? 0.4</cell></row><row><cell>Fishr ?</cell><cell cols="2">97.9 ? 0.2 93.6 ? 0.4</cell><cell>94.8 ? 0.4</cell></row><row><cell>Fishr ?</cell><cell cols="2">97.0 ? 0.2 95.3 ? 0.4</cell><cell>94.1 ? 0.4</cell></row><row><cell>Fishr ?</cell><cell cols="2">97.9 ? 0.1 93.5 ? 0.3</cell><cell>94.8 ? 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Colored MNIST experiments with different statistics matched. All hyperparameters were optimized for IRM.</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="2">25% label flipping</cell><cell></cell><cell>No label flipping</cell></row><row><cell>Gradients in</cell><cell>Name</cell><cell cols="2">Matched statistics Train acc.</cell><cell>Test acc.</cell><cell>Gray test acc.</cell><cell>Train acc.</cell><cell>Test acc.</cell><cell>Gray test acc.</cell></row><row><cell></cell><cell>Centered variance (= Fishr?)</cell><cell>Var(Ge)</cell><cell cols="2">71.0 ? 0.9 69.5 ? 1.0</cell><cell>70.2 ? 1.1</cell><cell>97.0 ? 0.2</cell><cell>95.3 ? 0.4</cell><cell>94.1 ? 0.4</cell></row><row><cell>?</cell><cell>Centered covariance</cell><cell>Ce</cell><cell cols="2">70.7 ? 1.0 69.1 ? 1.1</cell><cell>69.9 ? 1.1</cell><cell>97.0 ? 0.2</cell><cell>95.3 ? 0.4</cell><cell>94.0 ? 0.4</cell></row><row><cell></cell><cell>Uncentered variance</cell><cell>Diag( 1 neF e)</cell><cell cols="2">71.3 ? 0.9 69.5 ? 1.0</cell><cell>70.3 ? 1.0</cell><cell>97.0 ? 0.2</cell><cell>95.3 ? 0.4</cell><cell>94.1 ? 0.4</cell></row><row><cell></cell><cell>Centered variance (= Fishr?)</cell><cell>Var(Ge)</cell><cell cols="2">69.6 ? 0.9 71.2 ? 1.1</cell><cell>70.2 ? 0.7</cell><cell>97.9 ? 0.1</cell><cell>93.5 ? 0.3</cell><cell>94.7 ? 0.4</cell></row><row><cell>?</cell><cell>Centered covariance</cell><cell>Ce</cell><cell>Not</cell><cell>possible</cell><cell>for</cell><cell cols="2">computational (memory)</cell><cell>reasons</cell></row><row><cell></cell><cell>Uncentered variance</cell><cell>Diag( 1 neF e)</cell><cell cols="2">71.0 ? 0.8 70.0 ? 1.1</cell><cell>70.1 ? 0.9</cell><cell>97.9 ? 0.0</cell><cell>93.5 ? 0.3</cell><cell>94.8 ? 0.4</cell></row><row><cell></cell><cell>Centered variance (= Fishr?)</cell><cell>Var(Ge)</cell><cell cols="2">65.6 ? 1.3 73.8 ? 1.0</cell><cell>70.0 ? 0.9</cell><cell>97.9 ? 0.1</cell><cell>93.5 ? 0.3</cell><cell>94.8 ? 0.4</cell></row><row><cell>?</cell><cell>Centered covariance</cell><cell>Ce</cell><cell>Not</cell><cell>possible</cell><cell>for</cell><cell cols="2">computational (memory)</cell><cell>reasons</cell></row><row><cell></cell><cell>Uncentered variance</cell><cell>Diag( 1 neF e)</cell><cell cols="2">71.5 ? 0.8 69.1 ? 1.1</cell><cell>70.0 ? 1.0</cell><cell>97.9 ? 0.1</cell><cell>93.5 ? 0.3</cell><cell>94.8 ? 0.4</cell></row><row><cell cols="3">C.2.4. CENTERED OR UNCENTERED VARIANCE ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters, their default values and distributions for random search.</figDesc><table><row><cell>Condition</cell><cell>Parameter</cell><cell cols="2">Default value Random distribution</cell></row><row><cell>VLCS / PACS /</cell><cell>learning rate</cell><cell>0.00005</cell><cell>10 Uniform(?5,?3.5)</cell></row><row><cell>OfficeHome /</cell><cell>batch size</cell><cell>32</cell><cell>2 Uniform(3,5.5) if not DomainNet else 2 Uniform(3,5)</cell></row><row><cell>TerraIncognita /</cell><cell>weight decay</cell><cell>0</cell><cell>10 Uniform(?6,?2)</cell></row><row><cell>DomainNet</cell><cell>dropout</cell><cell>0</cell><cell>RandomChoice ([0, 0.1, 0.5])</cell></row><row><cell cols="2">Rotated MNIST / learning rate</cell><cell>0.001</cell><cell>10 Uniform(?4.5,?3.5)</cell></row><row><cell>Colored MNIST</cell><cell>batch size</cell><cell>64</cell><cell>2 Uniform(3,9)</cell></row><row><cell></cell><cell>weight decay</cell><cell>0</cell><cell>0</cell></row><row><cell>All</cell><cell>steps</cell><cell>5000</cell><cell>5000</cell></row><row><cell></cell><cell cols="2">regularization strength ? 1000</cell><cell>10 Uniform(1,4)</cell></row><row><cell>Fishr</cell><cell>ema ?</cell><cell>0.95</cell><cell>Uniform(0.9, 0.99)</cell></row><row><cell></cell><cell>warmup iterations</cell><cell>1500</cell><cell>Uniform(0, 5000)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>1.Colored MNIST (Arjovsky et al., 2019)  is a variant of the MNIST handwritten digit classification dataset<ref type="bibr" target="#b43">(LeCun et al., 2010)</ref>. As described previously in Appendix C.1, domain d ? {90%, 80%, 10%} contains a disjoint set of digits colored: the correlation strengths between color and label vary across domains. The dataset contains 70,000 examples of dimension (2, 28, 28) and 2 classes. Most importantly, the network, the hyperparameters, the image shapes, etc. are not the same as in the IRM setup from Section 4.1.2. Rotated MNIST<ref type="bibr" target="#b17">(Ghifary et al., 2015)</ref> is a variant of MNIST where domain d ? {0, 15, 30, 45, 60, 75} contains digits rotated by d degrees, with 70,000 examples of dimension (1, 28, 28) and 10 classes.</figDesc><table><row><cell>3. VLCS (Fang et al., 2013) includes photographic domains d ? {Caltech101, LabelMe, SUN09, VOC2007}, with 10,729</cell></row><row><cell>examples of dimension (3, 224, 224) and 5 classes.</cell></row><row><cell>4. PACS (Li et al., 2017) includes domains d ? {art, cartoons, photos, sketches}, with 9,991 examples of dimension</cell></row><row><cell>(3, 224, 224) and 7 classes.</cell></row><row><cell>5. OfficeHome (Venkateswara et al., 2017) includes domains d ? {art, clipart, product, real}, with 15,588 examples of</cell></row><row><cell>dimension (3, 224, 224) and 65 classes.</cell></row><row><cell>6. TerraIncognita (Beery et al., 2018) contains photographs of wild animals taken by camera traps at locations d ? {L100,</cell></row><row><cell>L38, L43, L46}, with 24,788 examples of dimension (3, 224, 224) and 10 classes.</cell></row><row><cell>7. DomainNet (Peng et al., 2019) has six domains d ? {clipart, infograph, painting, quickdraw, real, sketch}, with</cell></row><row><cell>586,575 examples of size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>DomainBed with 'Training-domain' model selection. We format first, second and worse than ERM results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy (?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ranking (?)</cell></row><row><cell>Algorithm</cell><cell cols="2">CMNIST RMNIST</cell><cell>VLCS</cell><cell>PACS</cell><cell cols="4">OfficeHome TerraInc DomainNet Avg</cell><cell>Arith. mean</cell><cell>Geom. mean</cell><cell>Median</cell></row><row><cell>ERM</cell><cell>51.5 ? 0.1</cell><cell cols="3">98.0 ? 0.0 77.5 ? 0.4 85.5 ? 0.2</cell><cell>66.5 ? 0.3</cell><cell>46.1 ? 1.8</cell><cell>40.9 ? 0.1</cell><cell>66.6</cell><cell>7.0</cell><cell>5.9</cell><cell>7</cell></row><row><cell>IRM</cell><cell>52.0 ? 0.1</cell><cell cols="3">97.7 ? 0.1 78.5 ? 0.5 83.5 ? 0.8</cell><cell>64.3 ? 2.2</cell><cell>47.6 ? 0.8</cell><cell>33.9 ? 2.8</cell><cell>65.4</cell><cell>10.7</cell><cell>8.5</cell><cell>14</cell></row><row><cell>GroupDRO</cell><cell>52.1 ? 0.0</cell><cell cols="3">98.0 ? 0.0 76.7 ? 0.6 84.4 ? 0.8</cell><cell>66.0 ? 0.7</cell><cell>43.2 ? 1.1</cell><cell>33.3 ? 0.2</cell><cell>64.8</cell><cell>11.3</cell><cell>8.4</cell><cell>14</cell></row><row><cell>Mixup</cell><cell>52.1 ? 0.2</cell><cell cols="3">98.0 ? 0.1 77.4 ? 0.6 84.6 ? 0.6</cell><cell>68.1 ? 0.3</cell><cell>47.9 ? 0.8</cell><cell>39.2 ? 0.1</cell><cell>66.7</cell><cell>5.7</cell><cell>4.2</cell><cell>3</cell></row><row><cell>MLDG</cell><cell>51.5 ? 0.1</cell><cell cols="3">97.9 ? 0.0 77.2 ? 0.4 84.9 ? 1.0</cell><cell>66.8 ? 0.6</cell><cell>47.7 ? 0.9</cell><cell>41.2 ? 0.1</cell><cell>66.7</cell><cell>8.0</cell><cell>7.0</cell><cell>8</cell></row><row><cell>CORAL</cell><cell>51.5 ? 0.1</cell><cell cols="3">98.0 ? 0.1 78.8 ? 0.6 86.2 ? 0.3</cell><cell>68.7 ? 0.3</cell><cell>47.6 ? 1.0</cell><cell>41.5 ? 0.1</cell><cell>67.5</cell><cell>3.6</cell><cell>2.5</cell><cell>2</cell></row><row><cell>MMD</cell><cell>51.5 ? 0.2</cell><cell cols="3">97.9 ? 0.0 77.5 ? 0.9 84.6 ? 0.5</cell><cell>66.3 ? 0.1</cell><cell>42.2 ? 1.6</cell><cell>23.4 ? 9.5</cell><cell>63.3</cell><cell>12.3</cell><cell>11.8</cell><cell>10</cell></row><row><cell>DANN</cell><cell>51.5 ? 0.3</cell><cell cols="3">97.8 ? 0.1 78.6 ? 0.4 83.6 ? 0.4</cell><cell>65.9 ? 0.6</cell><cell>46.7 ? 0.5</cell><cell>38.3 ? 0.1</cell><cell>66.1</cell><cell>10.3</cell><cell>8.8</cell><cell>12</cell></row><row><cell>CDANN</cell><cell>51.7 ? 0.1</cell><cell cols="3">97.9 ? 0.1 77.5 ? 0.1 82.6 ? 0.9</cell><cell>65.8 ? 1.3</cell><cell>45.8 ? 1.6</cell><cell>38.3 ? 0.3</cell><cell>65.6</cell><cell>11.1</cell><cell>10.7</cell><cell>10</cell></row><row><cell>MTL</cell><cell>51.4 ? 0.1</cell><cell cols="3">97.9 ? 0.0 77.2 ? 0.4 84.6 ? 0.5</cell><cell>66.4 ? 0.5</cell><cell>45.6 ? 1.2</cell><cell>40.6 ? 0.1</cell><cell>66.2</cell><cell>10.9</cell><cell>10.2</cell><cell>10</cell></row><row><cell>SagNet</cell><cell>51.7 ? 0.0</cell><cell cols="3">98.0 ? 0.0 77.8 ? 0.5 86.3 ? 0.2</cell><cell>68.1 ? 0.1</cell><cell>48.6 ? 1.0</cell><cell>40.3 ? 0.1</cell><cell>67.2</cell><cell>4.0</cell><cell>3.0</cell><cell>3</cell></row><row><cell>ARM</cell><cell>56.2 ? 0.2</cell><cell cols="3">98.2 ? 0.1 77.6 ? 0.3 85.1 ? 0.4</cell><cell>64.8 ? 0.3</cell><cell>45.5 ? 0.3</cell><cell>35.5 ? 0.2</cell><cell>66.1</cell><cell>8.7</cell><cell>5.6</cell><cell>9</cell></row><row><cell>V-REx</cell><cell>51.8 ? 0.1</cell><cell cols="3">97.9 ? 0.1 78.3 ? 0.2 84.9 ? 0.6</cell><cell>66.4 ? 0.6</cell><cell>46.4 ? 0.6</cell><cell>33.6 ? 2.9</cell><cell>65.6</cell><cell>8.3</cell><cell>7.7</cell><cell>8</cell></row><row><cell>RSC</cell><cell>51.7 ? 0.2</cell><cell cols="3">97.6 ? 0.1 77.1 ? 0.5 85.2 ? 0.9</cell><cell>65.5 ? 0.9</cell><cell>46.6 ? 1.0</cell><cell>38.9 ? 0.5</cell><cell>66.1</cell><cell>11.4</cell><cell>10.6</cell><cell>9</cell></row><row><cell>AND-mask</cell><cell>51.3 ? 0.2</cell><cell cols="3">97.6 ? 0.1 78.1 ? 0.9 84.4 ? 0.9</cell><cell>65.6 ? 0.4</cell><cell>44.6 ? 0.3</cell><cell>37.2 ? 0.6</cell><cell>65.5</cell><cell>13.6</cell><cell>12.7</cell><cell>15</cell></row><row><cell cols="2">SAND-mask 51.8 ? 0.2</cell><cell cols="3">97.4 ? 0.1 77.4 ? 0.2 84.6 ? 0.9</cell><cell>65.8 ? 0.4</cell><cell>42.9 ? 1.7</cell><cell>32.1 ? 0.6</cell><cell>64.6</cell><cell>13.4</cell><cell>12.7</cell><cell>13</cell></row><row><cell>Fish</cell><cell>51.6 ? 0.1</cell><cell cols="3">98.0 ? 0.0 77.8 ? 0.3 85.5 ? 0.3</cell><cell>68.6 ? 0.4</cell><cell>45.1 ? 1.3</cell><cell>42.7 ? 0.2</cell><cell>67.1</cell><cell>5.6</cell><cell>3.8</cell><cell>3</cell></row><row><cell>Fishr</cell><cell>52.0 ? 0.2</cell><cell cols="3">97.8 ? 0.0 77.8 ? 0.1 85.5 ? 0.4</cell><cell>67.8 ? 0.1</cell><cell>47.4 ? 1.6</cell><cell>41.7 ? 0.0</cell><cell>67.1</cell><cell>5.6</cell><cell>4.8</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Importance of the exponential moving average (ema) on DomainBed's Colored MNIST.</figDesc><table><row><cell cols="3">Model selection Algorithm ema</cell><cell>+90%</cell><cell>+80%</cell><cell>10%</cell><cell>Avg</cell></row><row><cell></cell><cell>ERM</cell><cell cols="4">N/A 71.8 ? 0.4 72.9 ? 0.1 28.7 ? 0.5 57.8</cell></row><row><cell>Test-domain</cell><cell>V-REx</cell><cell></cell><cell cols="3">72.8 ? 0.3 73.0 ? 0.3 55.2 ? 4.0 67.0 73.0 ? 0.2 73.0 ? 0.3 59.9 ? 2.6 68.6</cell></row><row><cell></cell><cell>Fishr</cell><cell></cell><cell cols="3">72.7 ? 0.3 72.8 ? 0.1 34.0 ? 4.5 59.8 74.1 ? 0.6 73.3 ? 0.1 58.9 ? 3.7 68.8</cell></row><row><cell></cell><cell>ERM</cell><cell cols="4">N/A 71.7 ? 0.1 72.9 ? 0.2 10.0 ? 0.1 51.5</cell></row><row><cell>Training-domain</cell><cell>V-REx</cell><cell></cell><cell cols="3">72.4 ? 0.3 72.9 ? 0.4 10.2 ? 0.0 51.8 72.6 ? 0.5 73.3 ? 0.1 9.8 ? 0.1 51.9</cell></row><row><cell></cell><cell>Fishr</cell><cell></cell><cell cols="3">71.1 ? 0.6 73.6 ? 0.1 10.1 ? 0.2 51.6 72.3 ? 0.9 73.5 ? 0.2 10.1 ? 0.2 52.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Importance of the exponential moving average (ema) on DomainBed's OfficeHome.syntheticColored MNIST in  Table 10, the ema is critical for Fishr -notably when training on E = {90%, 80%} and the dataset 10% is in test (from 34.0% to 58.9% in 'Test-domain'). V-REx also benefits from ema. On the 'real' dataset OfficeHome in</figDesc><table><row><cell cols="3">Model selection Algorithm ema</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>Avg</cell></row><row><cell></cell><cell>ERM</cell><cell cols="5">N/A 61.7 ? 0.7 53.4 ? 0.3 74.1 ? 0.4 76.2 ? 0.6 66.4</cell></row><row><cell>Test-domain</cell><cell>V-REx</cell><cell></cell><cell cols="4">59.6 ? 1.0 53.3 ? 0.3 73.2 ? 0.5 76.6 ? 0.4 65.7 59.0 ? 0.7 52.8 ? 0.8 74.6 ? 0.4 75.5 ? 0.3 65.5</cell></row><row><cell></cell><cell>Fishr</cell><cell></cell><cell cols="4">63.6 ? 0.4 53.2 ? 0.5 75.4 ? 0.5 77.8 ? 0.3 67.5 63.4 ? 0.8 54.2 ? 0.3 76.4 ? 0.3 78.5 ? 0.2 68.2</cell></row><row><cell></cell><cell>ERM</cell><cell cols="5">N/A 61.3 ? 0.7 52.4 ? 0.3 75.8 ? 0.1 76.6 ? 0.3 66.5</cell></row><row><cell>Training-domain</cell><cell>V-REx</cell><cell></cell><cell cols="4">60.7 ? 0.9 53.0 ? 0.9 75.3 ? 0.1 76.6 ? 0.5 66.4 59.2 ? 1.0 51.7 ? 0.5 75.2 ? 0.2 76.6 ? 0.3 65.7</cell></row><row><cell></cell><cell>Fishr</cell><cell></cell><cell cols="4">62.2 ? 1.0 53.5 ? 0.2 76.6 ? 0.2 77.8 ? 0.4 67.5 62.4 ? 0.5 54.4 ? 0.4 76.2 ? 0.5 78.3 ? 0.1 67.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Fishr (gradient variance) vs. IGA (gradient mean) on DomainBed's Colored MNIST.</figDesc><table><row><cell cols="2">Model selection Algorithm</cell><cell cols="3">Gradients in Warmup ema</cell><cell>+90%</cell><cell>+80%</cell><cell>10%</cell><cell>Avg</cell></row><row><cell></cell><cell>ERM</cell><cell>N/A</cell><cell>N/A</cell><cell cols="3">N/A 71.8 ? 0.4 72.9 ? 0.1 28.7 ? 0.5 57.8</cell></row><row><cell></cell><cell></cell><cell>? = ? ? ?</cell><cell></cell><cell></cell><cell cols="2">71.8 ? 0.5 73.0 ? 0.3 29.2 ? 0.5 58.0</cell></row><row><cell>Test-domain</cell><cell>IGA</cell><cell>? ? ?</cell><cell></cell><cell></cell><cell cols="2">72.4 ? 0.1 73.3 ? 0.2 29.3 ? 0.6 58.3 72.5 ? 0.2 73.3 ? 0.1 31.8 ? 0.7 59.2 72.6 ? 0.3 72.9 ? 0.2 50.0 ? 1.2 65.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">73.0 ? 0.3 73.2 ? 0.1 29.5 ? 1.1 58.6</cell></row><row><cell></cell><cell>Fishr</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">72.7 ? 0.3 72.8 ? 0.1 34.0 ? 4.5 59.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">74.1 ? 0.6 73.3 ? 0.1 58.9 ? 3.7 68.8</cell></row><row><cell></cell><cell cols="2">Fishr + IGA ?</cell><cell></cell><cell></cell><cell cols="2">73.3 ? 0.0 72.6 ? 0.5 66.3 ? 2.9 70.7</cell></row><row><cell></cell><cell>ERM</cell><cell>N/A</cell><cell>N/A</cell><cell cols="3">N/A 71.7 ? 0.1 72.9 ? 0.2 10.0 ? 0.1 51.5</cell></row><row><cell></cell><cell></cell><cell>? = ? ? ?</cell><cell></cell><cell></cell><cell cols="2">71.8 ? 0.3 73.2 ? 0.2 9.8 ? 0.0 51.6</cell></row><row><cell>Training-domain</cell><cell>IGA</cell><cell>? ? ?</cell><cell></cell><cell></cell><cell cols="2">71.8 ? 0.1 73.2 ? 0.2 10.1 ? 0.0 51.7 71.8 ? 0.2 73.1 ? 0.2 10.1 ? 0.0 51.7 72.5 ? 0.4 73.3 ? 0.2 10.1 ? 0.1 52.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">71.6 ? 0.1 73.2 ? 0.1 9.9 ? 0.0 51.6</cell></row><row><cell></cell><cell>Fishr</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">71.1 ? 0.6 73.6 ? 0.1 10.1 ? 0.2 51.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">72.3 ? 0.9 73.5 ? 0.2 10.1 ? 0.2 52.0</cell></row><row><cell></cell><cell cols="2">Fishr + IGA ?</cell><cell></cell><cell></cell><cell cols="2">72.4 ? 0.4 73.1 ? 0.1 10.1 ? 0.1 51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Fishr (gradient variance) vs. IGA (gradient mean) on DomainBed's OfficeHome.</figDesc><table><row><cell cols="2">Model selection Algorithm</cell><cell cols="3">Gradients in Warmup ema</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>Avg</cell></row><row><cell></cell><cell>ERM</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">N/A 61.7 ? 0.7 53.4 ? 0.3 74.1 ? 0.4 76.2 ? 0.6 66.4</cell></row><row><cell></cell><cell></cell><cell>? = ? ? ?</cell><cell></cell><cell></cell><cell cols="3">50.1 ? 2.5 49.6 ? 1.6 59.5 ? 6.7 68.5 ? 1.2 56.9</cell></row><row><cell>Test-domain</cell><cell>IGA</cell><cell>? ? ?</cell><cell></cell><cell></cell><cell cols="3">62.3 ? 0.3 53.9 ? 0.2 75.2 ? 0.4 77.4 ? 0.1 67.2 61.9 ? 0.4 52.6 ? 0.6 76.0 ? 0.8 77.5 ? 0.3 67.0 62.3 ? 1.0 53.4 ? 0.3 76.0 ? 0.7 77.0 ? 0.1 67.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">61.8 ? 0.9 53.8 ? 0.4 76.6 ? 0.6 77.7 ? 0.2 67.5</cell></row><row><cell></cell><cell>Fishr</cell><cell>?</cell><cell></cell><cell></cell><cell cols="3">63.6 ? 0.4 53.2 ? 0.5 75.4 ? 0.5 77.8 ? 0.3 67.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">63.4 ? 0.8 54.2 ? 0.3 76.4 ? 0.3 78.5 ? 0.2 68.2</cell></row><row><cell></cell><cell cols="2">Fishr + IGA ?</cell><cell></cell><cell></cell><cell cols="3">63.6 ? 1.0 54.6 ? 0.5 76.6 ? 0.2 78.4 ? 0.4 68.3</cell></row><row><cell></cell><cell>ERM</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">N/A 61.3 ? 0.7 52.4 ? 0.3 75.8 ? 0.1 76.6 ? 0.3 66.5</cell></row><row><cell></cell><cell></cell><cell>? = ? ? ?</cell><cell></cell><cell></cell><cell cols="3">51.7 ? 1.3 49.3 ? 1.5 58.6 ? 7.1 69.0 ? 1.1 57.1</cell></row><row><cell>Training-domain</cell><cell>IGA</cell><cell>? ? ?</cell><cell></cell><cell></cell><cell cols="3">61.9 ? 0.0 53.6 ? 0.9 75.7 ? 0.5 76.0 ? 0.1 66.8 61.2 ? 0.1 52.2 ? 0.5 76.1 ? 0.2 77.2 ? 0.3 66.7 61.7 ? 0.5 52.4 ? 0.7 75.9 ? 0.4 77.1 ? 0.2 66.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">63.8 ? 0.6 52.5 ? 0.5 76.7 ? 0.6 77.1 ? 1.0 67.5</cell></row><row><cell></cell><cell>Fishr</cell><cell>?</cell><cell></cell><cell></cell><cell cols="3">62.2 ? 1.0 53.5 ? 0.2 76.6 ? 0.2 77.8 ? 0.4 67.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">62.4 ? 0.5 54.4 ? 0.4 76.2 ? 0.5 78.3 ? 0.1 67.8</cell></row><row><cell></cell><cell cols="2">Fishr + IGA ?</cell><cell></cell><cell></cell><cell cols="3">63.3 ? 1.0 54.1 ? 0.3 76.5 ? 0.4 78.2 ? 0.6 68.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Impact of the ? distribution fromTable 8.</figDesc><table><row><cell cols="2">Model selection ? distribution</cell><cell>CMNIST RMNIST</cell><cell>VLCS</cell><cell>PACS</cell><cell cols="3">OfficeHome TerraInc DomainNet Avg</cell></row><row><cell></cell><cell cols="4">Constant(0) (= ERM) 57.8 ? 0.2 97.8 ? 0.1 77.6 ? 0.3 86.7 ? 0.3</cell><cell>66.4 ? 0.5</cell><cell>53.0 ? 0.3</cell><cell>41.3 ? 0.1</cell><cell>68.7</cell></row><row><cell>Test-domain</cell><cell>10 Uniform (1,4)</cell><cell cols="3">68.8 ? 1.4 97.8 ? 0.1 78.2 ? 0.2 86.9 ? 0.2</cell><cell>68.2 ? 0.2</cell><cell>53.6 ? 0.4</cell><cell>41.8 ? 0.1</cell><cell>70.8</cell></row><row><cell></cell><cell>10 Uniform (1,5)</cell><cell cols="3">68.7 ? 1.3 97.8 ? 0.0 78.7 ? 0.3 87.5 ? 0.1</cell><cell>68.0 ? 0.4</cell><cell>52.2 ? 0.5</cell><cell>42.0 ? 0.1</cell><cell>70.7</cell></row><row><cell></cell><cell cols="4">Constant(0) (= ERM) 51.5 ? 0.1 98.0 ? 0.0 77.5 ? 0.4 85.5 ? 0.2</cell><cell>66.5 ? 0.3</cell><cell>46.1 ? 1.8</cell><cell>40.9 ? 0.1</cell><cell>66.6</cell></row><row><cell>Training-domain</cell><cell>10 Uniform (1,4)</cell><cell cols="3">52.0 ? 0.2 97.8 ? 0.0 77.8 ? 0.1 85.5 ? 0.4</cell><cell>67.8 ? 0.1</cell><cell>47.4 ? 1.6</cell><cell>41.7 ? 0.0</cell><cell>67.1</cell></row><row><cell></cell><cell>10 Uniform (1,5)</cell><cell cols="3">51.8 ? 0.3 97.9 ? 0.0 77.9 ? 0.1 85.5 ? 0.6</cell><cell>67.4 ? 0.3</cell><cell>47.2 ? 1.0</cell><cell>41.8 ? 0.1</cell><cell>67.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>? 0.2 98.7 ? 0.1 98.9 ? 0.1 98.7 ? 0.2 98.9 ? 0.0 96.2 ? 0.2 97.</figDesc><table><row><cell cols="2">D.4.2. ROTATED MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Rotated MNIST. Model selection: 'Test-domain' validation set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>0</cell><cell>15</cell><cell>30</cell><cell>45</cell><cell>60</cell><cell>75</cell><cell cols="2">Avg Ranking</cell></row><row><cell>ERM</cell><cell cols="7">95.3 8</cell><cell>12</cell></row><row><cell>IRM</cell><cell cols="7">94.9 ? 0.6 98.7 ? 0.2 98.6 ? 0.1 98.6 ? 0.2 98.7 ? 0.1 95.2 ? 0.3 97.5</cell><cell>16</cell></row><row><cell>GroupDRO</cell><cell cols="7">95.9 ? 0.1 99.0 ? 0.1 98.9 ? 0.1 98.8 ? 0.1 98.6 ? 0.1 96.3 ? 0.4 97.9</cell><cell>5</cell></row><row><cell>Mixup</cell><cell cols="7">95.8 ? 0.3 98.7 ? 0.0 99.0 ? 0.1 98.8 ? 0.1 98.8 ? 0.1 96.6 ? 0.2 98.0</cell><cell>2</cell></row><row><cell>MLDG</cell><cell cols="7">95.7 ? 0.2 98.9 ? 0.1 98.8 ? 0.1 98.9 ? 0.1 98.6 ? 0.1 95.8 ? 0.4 97.8</cell><cell>12</cell></row><row><cell>CORAL</cell><cell cols="7">96.2 ? 0.2 98.8 ? 0.1 98.8 ? 0.1 98.8 ? 0.1 98.9 ? 0.1 96.4 ? 0.2 98.0</cell><cell>2</cell></row><row><cell>MMD</cell><cell cols="7">96.1 ? 0.2 98.9 ? 0.0 99.0 ? 0.0 98.8 ? 0.0 98.9 ? 0.0 96.4 ? 0.2 98.0</cell><cell>2</cell></row><row><cell>DANN</cell><cell cols="7">95.9 ? 0.1 98.9 ? 0.1 98.6 ? 0.2 98.7 ? 0.1 98.9 ? 0.0 96.3 ? 0.3 97.9</cell><cell>5</cell></row><row><cell>CDANN</cell><cell cols="7">95.9 ? 0.2 98.8 ? 0.0 98.7 ? 0.1 98.9 ? 0.1 98.8 ? 0.1 96.1 ? 0.3 97.9</cell><cell>5</cell></row><row><cell>MTL</cell><cell cols="7">96.1 ? 0.2 98.9 ? 0.0 99.0 ? 0.0 98.7 ? 0.1 99.0 ? 0.0 95.8 ? 0.3 97.9</cell><cell>5</cell></row><row><cell>SagNet</cell><cell cols="7">95.9 ? 0.1 99.0 ? 0.1 98.9 ? 0.1 98.6 ? 0.1 98.8 ? 0.1 96.3 ? 0.1 97.9</cell><cell>5</cell></row><row><cell>ARM</cell><cell cols="7">95.9 ? 0.4 99.0 ? 0.1 98.8 ? 0.1 98.9 ? 0.1 99.1 ? 0.1 96.7 ? 0.2 98.1</cell><cell>1</cell></row><row><cell>V-REx</cell><cell cols="7">95.5 ? 0.2 99.0 ? 0.0 98.7 ? 0.2 98.8 ? 0.1 98.8 ? 0.0 96.4 ? 0.0 97.9</cell><cell>5</cell></row><row><cell>RSC</cell><cell cols="7">95.4 ? 0.1 98.6 ? 0.1 98.6 ? 0.1 98.9 ? 0.0 98.8 ? 0.1 95.4 ? 0.3 97.6</cell><cell>15</cell></row><row><cell>AND-mask</cell><cell cols="7">94.9 ? 0.1 98.8 ? 0.1 98.8 ? 0.1 98.7 ? 0.2 98.6 ? 0.2 95.5 ? 0.2 97.5</cell><cell>16</cell></row><row><cell cols="8">SAND-mask 94.7 ? 0.2 98.5 ? 0.2 98.6 ? 0.1 98.6 ? 0.1 98.5 ? 0.1 95.2 ? 0.1 97.4</cell><cell>18</cell></row><row><cell>Fish</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97.9</cell><cell>11</cell></row><row><cell>Fishr</cell><cell cols="7">95.8 ? 0.1 98.3 ? 0.1 98.8 ? 0.1 98.6 ? 0.3 98.7 ? 0.1 96.5 ? 0.1 97.8</cell><cell>12</cell></row><row><cell cols="5">Rotated MNIST. Model selection: 'Training-domain' validation set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>0</cell><cell>15</cell><cell>30</cell><cell>45</cell><cell>60</cell><cell>75</cell><cell cols="2">Avg Ranking</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">are not available.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>D.4.3. VLCS VLCS. Model selection: 'Test-domain' validation set ? 0.3 67.9 ? 0.7 70.9 ? 0.2 74.0 ? 0.6 77.6 12 IRM 97.3 ? 0.2 66.7 ? 0.1 71.0 ? 2.3 72.8 ? 0.? 0.4 64.3 ? 1.2 73.5 ? 0.7 76.8 ? 2.6 78.1 5 SAND-mask 98.5 ? 0.3 63.6 ? 0.9 70.4 ? 0.8 77.1 ? 0.8 77.4 ? 1.0 81.3 ? 0.6 96.2 ? 0.3 82.7 ? 1.1 86.7 ? 1.4 79.2 ? 2.0 96.9 ? 0.4 76.2 ? 1.4 84.4 14 SAND-mask 85.8 ? 1.7 79.2 ? 0.8 96.3 ? 0.2 76.9 ? 2.0 84.6 ? 0.7 53.4 ? 0.3 74.1 ? 0.4 76.2 ? 0.6 66.4 ? 1.4 51.4 ? 0.3 74.8 ? 1.1 75.1 ? 1.3 65.5 16 ANDMask 59.5 ? 1.2 51.7 ? 0.2 73.9 ? 0.4 77.1 ? 0.2 65.6 15 SAND-mask 60.3 ? 0.5 53.3 ? 0.7 73.5 ? 0.7 76.2 ? 0.3 65.8 Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization D.4.6. TERRAINCOGNITA TerraIncognita. Model selection: 'Test-domain' validation set</figDesc><table><row><cell>Algorithm ERM GroupDRO Mixup MLDG CORAL MMD DANN CDANN MTL SagNet ARM V-REx RSC AND-mask SAND-mask 97.6 ? 0.3 64.5 ? 0.6 69.7 ? 0.6 73.0 ? 1.2 76.2 C L S V Avg Ranking 97.6 4 76.9 16 97.7 ? 0.2 65.9 ? 0.2 72.8 ? 0.8 73.4 ? 1.3 77.4 15 97.8 ? 0.4 67.2 ? 0.4 71.5 ? 0.2 75.7 ? 0.6 78.1 4 97.1 ? 0.5 66.6 ? 0.5 71.5 ? 0.1 75.0 ? 0.9 77.5 14 97.3 ? 0.2 67.5 ? 0.6 71.6 ? 0.6 74.5 ? 0.0 77.7 10 98.8 ? 0.0 66.4 ? 0.4 70.8 ? 0.5 75.6 ? 0.4 77.9 6 99.0 ? 0.2 66.3 ? 1.2 73.4 ? 1.4 80.1 ? 0.5 79.7 2 98.2 ? 0.1 68.8 ? 0.5 74.3 ? 0.6 78.1 ? 0.5 79.9 1 97.9 ? 0.7 66.1 ? 0.7 72.0 ? 0.4 74.9 ? 1.1 77.7 10 97.4 ? 0.3 66.4 ? 0.4 71.6 ? 0.1 75.0 ? 0.8 77.6 12 97.6 ? 0.6 66.5 ? 0.3 72.7 ? 0.6 74.4 ? 0.7 77.8 7 98.4 ? 0.2 66.4 ? 0.7 72.8 ? 0.1 75.0 ? 1.4 78.1 4 98.0 ? 0.4 67.2 ? 0.3 70.3 ? 1.3 75.6 ? 0.4 77.8 7 98.3 ? 0.3 64.5 ? 0.2 69.3 ? 1.3 73.4 ? 1.3 76.4 17 18 Fish 77.8 7 Fishr 97.6 ? 0.7 67.3 ? 0.5 72.2 ? 0.9 75.7 ? 0.3 78.2 3 VLCS. Model selection: 'Training-domain' validation set Algorithm C L S V Avg Ranking ERM 97.7 ? 0.4 64.3 ? 0.9 73.4 ? 0.5 74.6 ? 1.3 77.5 10 IRM 98.6 ? 0.1 64.9 ? 0.9 73.4 ? 0.6 77.3 ? 0.9 78.5 3 GroupDRO 97.3 ? 0.3 63.4 ? 0.9 69.5 ? 0.8 76.7 ? 0.7 76.7 18 Mixup 98.3 ? 0.6 64.8 ? 1.0 72.1 ? 0.5 74.3 ? 0.8 77.4 13 MLDG 97.4 ? 0.2 65.2 ? 0.7 71.0 ? 1.4 75.3 ? 1.0 77.2 15 CORAL 98.3 ? 0.1 66.1 ? 1.2 73.4 ? 0.3 77.5 ? 1.2 78.8 1 MMD 97.7 ? 0.1 64.0 ? 1.1 72.8 ? 0.2 75.3 ? 3.3 77.5 10 DANN 99.0 ? 0.3 65.1 ? 1.4 73.1 ? 0.3 77.2 ? 0.6 78.6 2 CDANN 97.1 ? 0.3 65.1 ? 1.2 70.7 ? 0.8 77.1 ? 1.5 77.5 10 MTL 97.8 ? 0.4 64.3 ? 0.3 71.5 ? 0.7 75.3 ? 1.7 77.2 15 SagNet 97.9 ? 0.4 64.5 ? 0.5 71.4 ? 1.3 77.5 ? 0.5 77.8 6 ARM 98.7 ? 0.2 63.6 ? 0.7 71.3 ? 1.2 76.7 ? 0.6 77.6 9 V-REx 98.4 ? 0.3 64.4 ? 1.4 74.1 ? 0.4 76.2 ? 1.3 78.3 4 RSC 97.9 ? 0.1 62.5 ? 0.7 72.3 ? 1.2 75.6 ? 0.8 77.1 17 AND-mask 97.8 13 Fish 77.8 6 Fishr 98.9 ? 0.3 64.0 ? 0.5 71.5 ? 0.2 76.8 ? 0.7 77.8 6 PACS. Model selection: 'Test-domain' validation set Algorithm A C P S Avg Ranking ERM 86.5 8 IRM 84.2 ? 0.9 79.7 ? 1.5 95.9 ? 0.4 78.3 ? 2.1 84.5 18 GroupDRO 87.5 ? 0.5 82.9 ? 0.6 97.1 ? 0.3 81.1 ? 1.2 87.1 3 Mixup 87.5 ? 0.4 81.6 ? 0.7 97.4 ? 0.2 80.8 ? 0.9 86.8 6 MLDG 87.0 ? 1.2 82.5 ? 0.9 96.7 ? 0.3 81.2 ? 0.6 86.8 6 CORAL 86.6 ? 0.8 81.8 ? 0.9 97.1 ? 0.5 82.7 ? 0.6 87.1 3 MMD 88.1 ? 0.8 82.6 ? 0.7 97.1 ? 0.5 81.2 ? 1.2 87.2 1 DANN 87.0 ? 0.4 80.3 ? 0.6 96.8 ? 0.3 76.9 ? 1.1 85.2 17 CDANN 87.7 ? 0.6 80.7 ? 1.2 97.3 ? 0.4 77.6 ? 1.5 85.8 14 MTL 87.0 ? 0.2 82.7 ? 0.8 96.5 ? 0.7 80.5 ? 0.8 86.7 8 SagNet 87.4 ? 0.5 81.2 ? 1.2 96.3 ? 0.8 80.7 ? 1.1 86.4 10 ARM 85.0 ? 1.2 81.4 ? 0.2 95.9 ? 0.3 80.9 ? 0.5 85.8 14 V-REx 87.8 ? 1.2 81.8 ? 0.7 97.4 ? 0.2 82.1 ? 0.7 87.2 1 RSC 86.0 ? 0.7 81.8 ? 0.9 96.8 ? 0.7 80.4 ? 0.5 86.2 12 AND-mask 86.4 ? 1.1 80.8 ? 0.9 97.1 ? 0.2 81.3 ? 1.1 86.4 10 SAND-mask 86.1 ? 0.6 80.3 ? 1.0 97.1 ? 0.3 80.0 ? 1.3 85.9 13 Fish 85.8 14 Fishr 87.9 ? 0.6 80.8 ? 0.5 97.9 ? 0.4 81.1 ? 0.8 86.9 5 PACS. Model selection: 'Training-domain' validation set Algorithm A C P S Avg Ranking ERM 84.7 ? 0.4 80.8 ? 0.6 97.2 ? 0.3 79.3 ? 1.0 85.5 3 IRM 84.8 ? 1.3 76.4 ? 1.1 96.7 ? 0.6 76.1 ? 1.0 83.5 17 GroupDRO 83.5 ? 0.9 79.1 ? 0.6 96.7 ? 0.3 78.3 ? 2.0 84.4 14 Mixup 86.1 ? 0.5 78.9 ? 0.8 97.6 ? 0.1 75.8 ? 1.8 84.6 10 MLDG 85.5 ? 1.4 80.1 ? 1.7 97.4 ? 0.3 76.6 ? 1.1 84.9 8 CORAL 88.3 ? 0.2 80.0 ? 0.5 97.5 ? 0.3 78.8 ? 1.3 86.2 2 MMD 86.1 ? 1.4 79.4 ? 0.9 96.6 ? 0.2 76.5 ? 0.5 84.6 10 DANN 86.4 ? 0.8 77.4 ? 0.8 97.3 ? 0.4 73.5 ? 2.3 83.6 16 CDANN 84.6 ? 1.8 75.5 ? 0.9 96.8 ? 0.3 73.5 ? 0.6 82.6 18 MTL 87.5 ? 0.8 77.1 ? 0.5 96.4 ? 0.8 77.3 ? 1.8 84.6 10 SagNet 87.4 ? 1.0 80.7 ? 0.6 97.1 ? 0.1 80.0 ? 0.4 86.3 1 ARM 86.8 ? 0.6 76.8 ? 0.5 97.4 ? 0.3 79.3 ? 1.2 85.1 7 V-REx 86.0 ? 1.6 79.1 ? 0.6 96.9 ? 0.5 77.7 ? 1.7 84.9 8 RSC 85.4 ? 0.8 79.7 ? 1.8 97.6 ? 0.3 78.2 ? 1.2 85.2 6 AND-mask 85.3 10 Fish 85.5 3 Fishr OfficeHome. Model selection: 'Test-domain' validation set ERM 61.7 8 IRM 56.4 ? 3.2 51.2 ? 2.3 71.7 ? 2.7 72.7 ? 2.7 63.0 18 GroupDRO 60.5 ? 1.6 53.1 ? 0.3 75.5 ? 0.3 75.9 ? 0.7 66.2 3 Mixup 63.5 ? 0.2 54.6 ? 0.4 76.0 ? 0.3 78.0 ? 0.7 68.0 6 MLDG 60.5 ? 0.7 54.2 ? 0.5 75.0 ? 0.2 76.7 ? 0.5 66.6 6 CORAL 64.8 ? 0.8 54.1 ? 0.9 76.5 ? 0.4 78.2 ? 0.4 68.4 3 MMD 60.4 ? 1.0 53.4 ? 0.5 74.9 ? 0.1 76.1 ? 0.7 66.2 1 DANN 60.6 ? 1.4 51.8 ? 0.7 73.4 ? 0.5 75.5 ? 0.9 65.3 17 CDANN 57.9 ? 0.2 52.1 ? 1.2 74.9 ? 0.7 76.2 ? 0.2 65.3 14 MTL 60.7 ? 0.8 53.5 ? 1.3 75.2 ? 0.6 76.6 ? 0.6 66.5 8 SagNet 62.7 ? 0.5 53.6 ? 0.5 76.0 ? 0.3 77.8 ? 0.1 67.5 10 ARM 58.8 ? 0.5 51.8 ? 0.7 74.0 ? 0.1 74.4 ? 0.2 64.8 14 V-REx 59.6 ? 1.0 53.3 ? 0.3 73.2 ? 0.5 76.6 ? 0.4 65.7 1 RSC 61.7 ? 0.8 53.0 ? 0.9 74.8 ? 0.8 76.3 ? 0.5 66.5 12 AND-mask 60.3 ? 0.5 52.3 ? 0.6 75.1 ? 0.2 76.6 ? 0.3 66.1 10 SAND-mask 59.9 ? 0.7 53.6 ? 0.8 74.3 ? 0.4 75.8 ? 0.5 65.9 13 Fish 66.0 12 Fishr 63.4 ? 0.8 54.2 ? 0.3 76.4 ? 0.3 78.5 ? 0.2 68.2 5 OfficeHome. Model selection: 'Training-domain' validation set Algorithm A C P R Avg Ranking ERM 61.3 ? 0.7 52.4 ? 0.3 75.8 ? 0.1 76.6 ? 0.3 66.5 7 IRM 58.9 ? 2.3 52.2 ? 1.6 72.1 ? 2.9 74.0 ? 2.5 64.3 18 GroupDRO 60.4 ? 0.7 52.7 ? 1.0 75.0 ? 0.7 76.0 ? 0.7 66.0 11 Mixup 62.4 ? 0.8 54.8 ? 0.6 76.9 ? 0.3 78.3 ? 0.2 68.1 3 MLDG 61.5 ? 0.9 53.2 ? 0.6 75.0 ? 1.2 77.5 ? 0.4 66.8 6 CORAL 65.3 ? 0.4 54.4 ? 0.5 76.5 ? 0.1 78.4 ? 0.5 68.7 1 MMD 60.4 ? 0.2 53.3 ? 0.3 74.3 ? 0.1 77.4 ? 0.6 66.3 10 DANN 59.9 ? 1.3 53.0 ? 0.3 73.6 ? 0.7 76.9 ? 0.5 65.9 12 CDANN 61.5 ? 1.4 50.4 ? 2.4 74.4 ? 0.9 76.6 ? 0.8 65.8 13 MTL 61.5 ? 0.7 52.4 ? 0.6 74.9 ? 0.4 76.8 ? 0.4 66.4 8 SagNet 63.4 ? 0.2 54.8 ? 0.4 75.8 ? 0.4 78.3 ? 0.3 68.1 3 ARM 58.9 ? 0.8 51.0 ? 0.5 74.1 ? 0.1 75.2 ? 0.3 64.8 17 V-REx 60.7 ? 0.9 53.0 ? 0.9 75.3 ? 0.1 76.6 ? 0.5 66.4 8 RSC 60.7 13 Fish 68.6 2 Fishr 62.4 ? 0.5 54.4 ? 0.4 76.2 ? 0.5 78.3 ? 0.1 67.8 5 88.4 ? 0.2 78.7 ? 0.7 97.0 ? 0.1 77.8 ? 2.0 85.5 D.4.5. OFFICEHOME D.4.4. PACS Algorithm A C P R Avg Ranking Algorithm L100 L38 L43 L46 Avg Ranking</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Sorbonne Universit?, CNRS, LIP6, Paris, France 2 Valeo.ai. Correspondence to: Alexandre Ram? &lt;alexandre.rame@sorbonneuniversite.fr&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was granted access to the HPC resources of IDRIS under the allocation A0100612449 made by GENCI. We acknowledge the financial support by the ANR agency in the chair VISA-DEEP (ANR-20-CHIA-0022-01).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Systematic generalisation with group invariant predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Invariance principle meets information bottleneck for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving the convergence of back-propagation learning with second order methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist models summer school</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain generalization by marginal transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>JMLR, 2021. (pp. 8, 19, 21</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Input similarity from the neural network perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Felardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information and accuracy attainable in the estimation of statistical parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Bulletin of the Calcutta Mathematical Society</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Underspecification presents challenges for credibility in modern machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BackPACK: Packing more into backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2020" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Curvature access through the generalized gauss-newton&apos;s low-rank structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tatzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivit</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Domain-adversarial training of neural networks. JMLR, 2016. (pp. 1, 2, 8, 19</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multitask autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An investigation into neural net optimization via hessian eigenvalue density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">In search of lost domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICLR, 2021. (pp. 1, 2, 3, 6, 7, 19</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Out-ofdistribution prediction with invariant risk minimization: The limitation and an effective fix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gradient descent happens in a tiny subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On &quot;natural&quot; learning and pruning in multilayered perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selfchallenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning robust representations with score invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Idnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML UDL Workshop</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Three factors influencing minima in SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Catastrophic fisher explosion: Early phase fisher matrix impacts generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Astrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Kerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Support and invertibility in domain-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Does invariant risk minimization capture invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tangella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<title level="m">Pathological spectra of the fisher information metric and its variants in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization Karakida,</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PNAS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">A benchmark of in-the-wild distribution shifts</title>
		<imprint/>
	</monogr>
	<note>arXiv preprint, 2020. (pp. 2, 3</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kopitkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Indelman</surname></persName>
		</author>
		<title level="m">Neural spectrum alignment: Empirical study</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization with maximal invariant predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamaguchi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Out-ofdistribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICML, 2021. (pp. 1, 3, 4, 5, 6, 8, 15, 20, 21</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Limitations of the empirical fisher approximation for natural gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving first and second-order methods by modeling uncertainty. Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Solla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hessian based analysis of sgd for deep nets: Dynamics and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Domain generalization via conditional invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Group fisher pruning for practical network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On transfer learning via linearized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Best sources forward: domain generalization through sourcespecific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Domain generalization via gradient surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mansilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Echeveste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Milone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">New insights and perspectives on the natural gradient method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reducing domain gap by reducing style bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>p</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning from failure: De-biasing classifier from biased classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2020. (pp. 8, 21</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning explanations that are hard to vary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICLR, 2021. (pp. 2, 3, 4, 5, 6, 20, 24</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization Paszke</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Gradient starvation: A learning proclivity in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-O</forename><surname>Kaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lajoie</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2021. (pp. 8, 16</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Diverse weight averaging for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirchmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rahier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Common pitfalls and recommendations for using machine learning to detect and prognosticate for covid-19 using chest radiographs and ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Driggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ursprung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Etmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The risks of invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Distributionally robust neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020a</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An investigation of why overparameterization exacerbates spurious correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Empirical analysis of the hessian of over-parametrized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">U</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The impact of neural network overparameterization on gradient confusion and stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Sankararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">No more pesky learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fast curvature matrix-vector products for second-order gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The pitfalls of simplicity bias in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tamuly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Sand-mask: An enhanced gradient masking strategy for the discovery of invariances in domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shahtalebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Gagnon-Audet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML UDL Workshop</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>pp. 3, 4, 19, 20, 24</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Gradient matching for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>arXiv preprint, 2021. (pp. 2, 3, 6, 8, 16, 20, 22, 24</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Woodfisher: Efficient secondorder approximation for neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ECCV, 2016. (pp. 1, 3, 19</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Model selection: &apos;Test-domain&apos; validation set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Model selection: &apos;Training-domain&apos; validation set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnist</forename><surname>Colored</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Model selection: &apos;Training-domain&apos; validation set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terraincognita</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
