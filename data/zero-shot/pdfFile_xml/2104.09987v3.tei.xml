<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Model Compression via Pseudo Quantization Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
							<email>defossez@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
							<email>adiyoss@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Meta AI</orgName>
								<orgName type="institution" key="instit2">FAIR Team</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Meta AI, FAIR Team</orgName>
								<address>
									<settlement>Tel-Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Meta AI, FAIR Team</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Model Compression via Pseudo Quantization Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published in Transactions on Machine Learning Research (09/2022) Reviewed on OpenReview: https: // openreview. net/ forum? id= DijnKziche</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose DiffQ a differentiable method for model compression for quantizing model parameters without gradient approximations (e.g., Straight Through Estimator). We suggest adding independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. DiffQ is differentiable both with respect to the unquantized weights and the number of bits used. Given a single hyper-parameter balancing between the quantized model size and accuracy, DiffQ optimizes the number of bits used per individual weight or groups of weights, in end-to-end training. We experimentally verify that our method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the ImageNet dataset, DiffQ compresses a 12 layers transformerbased model by more than a factor of 8, (lower than 4 bits precision per weight on average), with a loss of 0.3% in model accuracy. Code is available at github.com/facebookresearch/diffq. * Equal contribution.</p><p>Published in Transactions on Machine Learning Research (09/2022) (2016) and later Krishnamoorthi (2018) propose to use a gradient Straight-Through-Estimator (STE) (Bengio  et al., 2013)  in order to provide a non-zero gradient to the original weights. This allows the model to adapt to quantization during training and reduces the final degradation of performance. However, <ref type="bibr" target="#b10">Fan et al. (2021)</ref> noticed instability and bias in the learned weights, as STE is not the true gradient to the function.</p><p>The nature of quantization noise has been extensively studied as part of Analog-to-Digital Converters (ADC). In particular, a useful assumption to facilitate the design of post-processing filters for ADC is the independence of the input value and the "Pseudo Quantization Noise" (PQN), as formalized by <ref type="bibr" target="#b50">Widrow et al. (1996)</ref>. In this work, we show that it also applies to deep learning model quantization, and provides a simple framework in which the output and the quantized model size are both differentiable, without any use of STE. This allows to optimally set the number of bits used per individual weight (or group of weights) to achieve a trade-off between size and accuracy, in a single training and at almost no extra cost. Even when the number of bits to use is fixed, we show that unlike STE, using independent pseudo quantization noise does not introduce bias in the gradient and achieves higher performance. Although PQN has been proposed before for quantization (Baskin et al., 2018a;b), it has never been used on its own without any need for STE or other quantization methods, while achieving state-of-the-art performance.</p><p>Our Contribution: (i) With DiffQ, we propose to use pseudo quantization noise only to approximate quantization at train time, as a differentiable alternative to STE, both with respect to the unquantized weights and number of bits used. (ii) We provide a differentiable model size estimate, so that given a single penalty level ?, DiffQ optimizes the number of bits per weight or group of weights to achieve a given trade-off between model size and accuracy. (iii) We provide extensive experimental validation using various models (ConvNets and Transformers) and domains (image classification, language modeling, audio source separation). We demonstrate the efficiency of DiffQ both in providing small footprint models with comparable performance to the uncompressed ones, together with easy and stable optimization, using only one sensitive hyper-parameter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important factor in the adoption of a deep learning model for real-world applications is how easily it can be pushed to remote devices. It has been observed that larger models usually lead to better performance, for instance with larger ResNets <ref type="bibr" target="#b12">(He et al., 2016)</ref> achieving higher accuracies than smaller ones. In response, the community has worked toward smaller, and more efficient models <ref type="bibr" target="#b41">(Tan &amp; Le, 2019</ref>). Yet an EfficientNet-B3 is still almost 50MB, a considerable amount if the model is to be included in online applications, or updated with limited network capabilities. For other applications, such as language modeling <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> or source separation <ref type="bibr" target="#b2">(D?fossez et al., 2019)</ref>, the typical model size is closer to 1GB, ruling out any kind of mobile usage. Efficient model compression is thus important for on device adoption of deep learning models. Thus, we focus in the present work on reducing model size, rather than achieving computational gains.</p><p>The simplest method to reduce model size consists in decreasing the number of bits used to encode individual weights. For instance, using 16 bits floating point numbers halves the model size, while retaining a sufficient approximation of the set of real numbers, R, to train with first-order optimization methods <ref type="bibr" target="#b29">(Micikevicius et al., 2018)</ref>. When considering lower precision, for instance, 8 or 4 bits, the set of possible values is no longer a good approximation of R, hence preventing the use of first-order optimization methods. Specifically, uniform quantization requires using the round function, which has zero gradients wherever it is differentiable.</p><p>Quantization can be done as a post-processing step to regular training. However, errors accumulate in a multiplicative fashion across layers, with a possibly uncontrolled decrease in the model accuracy. Courbariaux et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early network quantization methods focused on low-bitwidth networks such as BinaryNet <ref type="bibr">Courbariaux et al. (2015;</ref>, XOR-Nets <ref type="bibr" target="#b37">Rastegari et al. (2016)</ref>, or Ternary networks <ref type="bibr" target="#b22">Li et al. (2016)</ref>; <ref type="bibr" target="#b52">Wu et al. (2018)</ref>. Although these methods produce highly quantized models, their performance is not on par with uncompressed ones. To improve accuracies, higher bitwidth quantization methods were studied <ref type="bibr" target="#b18">Jung et al. (2019)</ref>; <ref type="bibr" target="#b56">Zhang et al. (2018a)</ref>; <ref type="bibr" target="#b30">Mishra et al. (2017)</ref>. These methods followed the STE approach <ref type="bibr">Bengio et al. (2013)</ref>. STE allows the gradients to be backpropagated through the quantizers and, thus, the network weights can be adapted with gradient descent <ref type="bibr">Courbariaux et al. (2016)</ref>.</p><p>Variational approaches were used to make the categorical distribution over quantized weights differentiable. <ref type="bibr" target="#b27">Louizos et al. (2019)</ref> uses a Gumbel-softmax <ref type="bibr" target="#b17">(Jang et al., 2017)</ref> but requires 2 hyper-parameters and has no bitwidth tuning. DiffQ has a single hyper-parameter and supports automatic bitwidth tuning. <ref type="bibr" target="#b38">Shayer et al. (2018)</ref> relies on a Central Limit Theorem (CLT) application, however this prevents weights from converging to a deterministic value, which would break the assumptions of the CLT. With DiffQ, weights are free to converge to any optimal value. Finally <ref type="bibr" target="#b45">Ullrich et al. (2017)</ref> uses a gaussian mixture model trained on top of the weights, adding significant complexity both in terms of code, and computation. In contrast, DiffQ adds only one penalty term to the loss, optimized along the rest of the model in an end-to-end fashion.</p><p>An alternative is to use a smoothed version of the quantization operator, possibly with a trained meta-network <ref type="bibr">(Chen et al., 2019)</ref>, however as the smoothed operator converges to the true one, gradients will eventually be zero almost everywhere. <ref type="bibr" target="#b11">Gong et al. (2019)</ref> use a meta-network to provide gradients despite quantization. However, their implementation for training the meta-network still relies on STE.</p><p>Additive noise injection has been studied by <ref type="bibr">Baskin et al. (2018a)</ref>, although only during the first few epochs, after which STE based approximation is used. This work was extented to non uniform quantization <ref type="bibr">(Baskin et al., 2018b)</ref>. In contrast, DiffQ uses only noise injection, and as demonstrated in Results Section, achieves a better accuracy for an equivalent compression level than both methods. Non uniform quantization was also studied by <ref type="bibr" target="#b35">Polino et al. (2018)</ref>, but without differentiability with respect to the weights, with worse performance than DiffQ. Additive noise was also studied in the context of image compression <ref type="bibr" target="#b1">(Ball? et al., 2017;</ref><ref type="bibr" target="#b18">Choi et al., 2019)</ref> in order to provide a differentiable pseudo-quantization operator. However, those work rely on an explicit estimation of the quantized values entropy, in particular with respect to a distribution of images. This formalism breaks down when having to quantize a single model, not a distribution, and DiffQ uses a simpler approach where the bitwidth is directly tuned. More recently, <ref type="bibr" target="#b33">Park et al. (2022)</ref> extended our method for activation quantization.</p><p>An important contribution from DiffQ is the automatic tuning of the bitwidth using mixed-precision. Other mixed-precision quantization methods are based on Reinforcement Learning <ref type="bibr" target="#b48">(Wang et al., 2019;</ref><ref type="bibr" target="#b7">Elthakeb et al., 2020;</ref><ref type="bibr" target="#b23">Liu et al., 2021)</ref>, second-order optimization <ref type="bibr" target="#b4">(Dong et al., 2019;</ref><ref type="bibr" target="#b53">Yao et al., 2021)</ref>, and differentiable quantization methods <ref type="bibr" target="#b44">(Uhlich et al., 2020;</ref><ref type="bibr" target="#b49">Wang et al., 2020)</ref>. Comparing to DiffQ, such methods are more complex (e.g., require plenty of parameter tuning), more computationally heavy, and most importantly based on STE approximations. <ref type="bibr" target="#b48">Wang et al. (2019)</ref>; <ref type="bibr" target="#b6">Elthakeb et al. (2019)</ref> suggested learning a bitwidth assignment policy using reinforcement learning methods. In contrast, our method select bitwidth along training, using only first order optimization. Jain et al. <ref type="formula" target="#formula_1">(2019)</ref>; <ref type="bibr">Bhalgat et al. (2020)</ref> proposed learning the quantizer step-size or dynamic-range using STE, but do not allow to select the bitdwidth. Our experiments show that DiffQ outperforms  (LSQ) both on most vision and natural language tasks. <ref type="bibr" target="#b44">Uhlich et al. (2020)</ref> proposed a re-parametrization that allows to select the bitwidth for each layer through first order optimization, while also relying on STE. The re-parametrization is more complex than the additive noise used in DiffQ, and suffers from the biased gradient of STE. Results suggest that DiffQ achieves similar or better trade-offs between model size and accuracy. Besides, in the present work we explore setting a bitwidth for individual groups of weights within each layer, rather than layer-wise.</p><p>The limitations of STE methods for quantization were first noticed by <ref type="bibr" target="#b25">Liu &amp; Mattina (2019)</ref>. They recommend using a linear combination of the unquantized and quantized weight, with the gradient flowing only through the unquantized contribution. In a similar spirit, <ref type="bibr" target="#b10">Fan et al. (2021)</ref> sample for each layer and iteration whether to use the quantized or unquantized weight. Both methods reduce the bias from STE, but also remove some of the quantization noise during training. In contrast our method allows to keep a full pseudo quantization noise without the STE bias. <ref type="bibr" target="#b24">Liu et al. (2022)</ref> proposed the Generalized STE method to deal with gradient instabilities by calculating the expectation of the stochastic quantization during the backward phase. Finally, <ref type="bibr" target="#b31">Nagel et al. (2022)</ref> extend the analysis we present in Section 3.3 on the oscillations of weights when using STE and suggest tracking the weight oscillations in order to freeze them when needed, as an ad-hoc solution.</p><p>A last line of related work is Product Quantization (PQ) <ref type="bibr" target="#b39">Stock et al. (2019)</ref>, where code words are being learned to quantize blocks of weights rather than single weights. This method achieves a higher compression level than per-weight quantization but also requires carefully choosing the size of the codebooks for each layer. In contrast, our method requires only choosing a single hyper-parameter to balance between model size and accuracy. Besides, as noted by <ref type="bibr" target="#b10">Fan et al. (2021)</ref>, per-weight quantization and PQ can be combined. We compare with PQ on vision and language tasks: while PQ can reach smaller model size than DiffQ, it can also suffer from unacceptable accuracy loss, in particular for language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Let us consider a weight vector w ? R d , where d ? N, typically the weights of convolution or linear layer. Each entry of the vector is typically coded over 32 bits with floating-point precision. We aim to reduce the number of possible states to 2 B , where B 32 is the number of bits of precision. First, we assume w i ? [0, 1] for all 1 ? i ? d. In practice, one would first normalize w a?</p><formula xml:id="formula_0">w = w ? min(w) max(w) ? min(w) ,</formula><p>and provide the tuple (min(w), max(w)) separately as a 32 bits IEEE float. Given that for typical deep learning models d 1, storing this range has a negligible cost. For readability, we describe the method for scalar values w ? [0, 1], however, this can be easily extended to vectors w ? R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uniform quantization</head><p>The simplest quantization methods consist of taking 2 B points evenly spaced in the range [0, 1] and round each entry of w to the nearest point. One can then store the rounded value by its index, which requires only B bits. Formally, we quantize a number w ? [0, 1] over B bits as</p><formula xml:id="formula_1">?w ? [0, 1], B ? N * , Q(w, B) = round w ? (2 B ? 1) 2 B ? 1 .<label>(1)</label></formula><p>While the intuitive definition of quantization is for an integer number of bits, we can extend the previous definitions to a real-valued number of bits B ? R * + . Note that variants of this scheme exist, for instance, symmetric uniform quantization, which enforces that 0 is always exactly represented <ref type="bibr" target="#b20">(Krishnamoorthi, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization of the quantized weights</head><p>The weight vector w is typically obtained through the process of training a predictor function parameterized by w, denoted as f w , to minimize a loss function L,</p><formula xml:id="formula_2">min w?R d L(f w ),<label>(2)</label></formula><p>where L(f w ) is the empirical risk over a given dataset. The process of quantizing a vector w over B bits introduces a quantization noise N(w, B) = Q(w, B) ? w, which is unaware of the training objective L. Even if w is close to the optimum, Q(w, B) might deteriorate arbitrarily the performance of the predictor.</p><p>Thus, given a fixed budget of bits B, one would ideally like to minimize the empirical risk when considering the quantization process, min</p><formula xml:id="formula_3">w?R d L(f Q(w,B) ),<label>(3)</label></formula><p>where f Q(w,B) is the predictor function using the quantized model parameters.</p><p>Unfortunately, the gradients of Q(w, B) are zero over its definition domain because of the rounding operation, and as a result, it cannot be optimized using first-order optimization methods such as SGD or Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref>. One possible solution is to replace the Jacobian of Q(?, B) with the identity matrix during the backward phase, as suggested in the STE method <ref type="bibr">(Bengio et al., 2013)</ref>. The STE method was popularized for quantization as the Quantization Aware Training (QAT) technique by <ref type="bibr" target="#b20">Krishnamoorthi (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The instability and bias in STE</head><p>As described by <ref type="bibr" target="#b10">Fan et al. (2021)</ref>, following the STE approach can cause instability during training and bias in the models' gradients and weights. As a result optimization will fail to converge to the optimal value even on simple cases. To demonstrate that, consider the following 1D least-mean-square problem, where B ? N * , the optimal weight w * ? [0, 1] such that Q(w * , B) = w * , and Q(w * , B) ? (0, 1). Given a random variable X ? R with ? 2 = E X 2 such that 0 &lt; ? 2 &lt; ?, we would like to minimize the following using STE based QAT:</p><formula xml:id="formula_4">min w?[0,1] L(w) := E 1 2 (XQ(w, B) ? Xw * ) 2 .<label>(4)</label></formula><p>We immediately have that the optimum is achieved for Q(w, B) = Q(w * , B). Let us try to optimize equation 4 using SGD with STE starting from w 0 = w * , with w n the sequence of iterates. We call w ? and w + the quantized values just under and above w * , and we assume without loss of generality that Q(w * , B) = w + . The expected gradient with STE at iteration n is given by</p><formula xml:id="formula_5">G n = ? 2 (Q(w n , B) ? w * ).<label>(5)</label></formula><p>In particular, G 0 = ? 2 (w + ? w * ) &gt; 0, and G n will stay positive until Q(w n , B) = w ? . At this point, we will have G n &lt; 0, and will stay so until Q(w n , B) = w + . Thus, we observe that using STE, Q(w n , B) will oscillate between w ? and w + , while the optimal value is w + . The pattern of oscillation will depend on the learning rate and relative position of w * within the segment [w ? , w + ]. Taking a smaller step size will reduce the amplitude of the oscillations of w n , but not of Q(w n , B), which is what interests us. Indeed, w n oscillations are centered at the boundary (w + + w ? )/2. We provide one example of those oscillations on <ref type="figure" target="#fig_0">Figure 1</ref> with w * = 0.11, B = 4, X = 1 a.s. and a step size of 0.5.</p><p>Extrapolating to a model with millions of parameters, at any point in time, a significant fraction of the weights could be quantized to a suboptimal value due to the oscillations implied by the STE method. We conjecture that this behavior explains the oscillations of the accuracy observed when training an EfficientNet-b3 with QAT using 4 bits per weight on ImageNet (see <ref type="figure" target="#fig_0">Figure 1</ref>(b)). In the following section, we introduce DiffQ, a method based on independent additive pseudo quantization noise, that does not suffer from such a bias, while approximating well enough quantization noise to perform efficient quantization aware training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Pseudo quantization noise. A classical assumption in digital signal processing when working with quantized signals is that the quantization noise is approximated by independents uniform variables over [??/2, ?/2] with ? = 1 2 B ?1 the quantization step. This approximation was studied in depth by <ref type="bibr" target="#b50">Widrow et al. (1996)</ref> as Pseudo Quantization Noise (PQN). Following this assumption, we define the pseudo quantization function Q for all x ? R and B ? R + * as</p><formula xml:id="formula_6">Q(x, B) = x + ? 2 ? U[?1, 1],<label>(6)</label></formula><p>with U[?1, 1] an independent sample from the uniform distribution over <ref type="bibr">[?1, 1]</ref>. This pseudo quantization function is differentiable with respect to x and B. Unlike QAT, this differentiability does not require an STE. It also provides a meaningful gradient with respect to the number of bits used B (extended to be real-valued).</p><p>If we look back at the example from <ref type="figure" target="#fig_0">Figure 1</ref>, using now equation 6 instead of STE, the expected gradients for SGD become</p><formula xml:id="formula_7">G n = E x ? w n + ? 2 ? U[?1, 1] x ? w * x = ? 2 (w n ? w * ),<label>(7)</label></formula><p>which cancels out for w n = w * , so that at convergence we indeed have Q(w n , B) = Q(w * , B), i.e. the gradient estimate is unbiased and converges to the right solution.</p><p>Mixed precision. We used a common precision B for all the entries of the weight vector w. One can instead use different values for different entries. Formally, the entries in w are grouped by considering w ? R g?d/g with g the group size and d/g the number of groups. We can then extend the definition of Q(w, B) given by equation 1 and equation 6 to use a number of bits b s for the group s, with b ? R * + d/g .</p><p>Training objective. Given w ? R g?d/g with g groups of d/g entries, and a number of bits b ? N g * , we define the model size, expressed in MegaBytes (1MB = 8 ? 2 20 bits)</p><formula xml:id="formula_8">M(b) = g 2 23 d/g s=1 b s .<label>(8)</label></formula><p>A typical objective of quantization is to achieve the best possible performance within a given model size budget or to achieve the smallest model size that reaches a given performance, i.e. we want to minimize with b ? N d/g * , and w ? R g?d/g either,</p><formula xml:id="formula_9">min w,b L(f Q(w,b) ), s.t. M(b) ? m. or min w,b M(b), s.t. L(f Q(w,b) ) ? l.<label>(9)</label></formula><p>We can relax b to be real valued, and replace Q by our differentiable pseudo quantization function Q. Then, following the exact penalty method (Bertsekas (1997), Section 4.2, Bertsekas (2014), Chapter 4), there is ?(m) &gt; 0 (or ?(l) for the right hand side problem), such that the left hand size problem is equivalent to</p><formula xml:id="formula_10">min w,b L(f Q(w,b) ) + ?(m)M(b),<label>(10)</label></formula><p>which is fully differentiable with respect to w and b and can be optimized with first order optimization.</p><p>Parametrization. In practice, the number of bits used for each group b ? R g * + is obtained from a logit parameter l ? R g , so that we have</p><formula xml:id="formula_11">b = b min + ?(l)(b max ? b min ),<label>(11)</label></formula><p>with ? is the sigmoid function, and b min and b max the minimal and maximal number of bits to use. The trainable parameter l is initialized so that b = b init . We set b init = 8.</p><p>Evaluation and noise distribution. At evaluation time, we round the value b obtained from equation 10 asb = round(b) and quantize w as Q(w,b). Thus, the amount of quantization noise at evaluation can be larger than the amount of noise injected at train time. We observed that using a noise distribution with larger support, such as Gaussian noise with unit variance (i.e. 3 times the variance of U([?1, 1])), makes the model more robust to this operation. An empirical comparison between uniform and Gaussian noise can be found in <ref type="table" target="#tab_11">Table B</ref>.7 in the Appendix. Thus in the rest of the paper, we always use Gaussian noise at train time.</p><p>True model size. The mode size given by equation 8 is used at train time but does not account for part of the true model size. At evaluation time, we represent each weight by the integer obtained from the rounding operation in equation 1. For each layer in the network, we also store two 32 bits float numbers for the minimum and maximum scale. Finally, the actual value ofb must be coded, as it is no longer a fixed constant. For each layer, we compute the maximum value of C s = log 2 (1 +b s ? b min ) over all groups s ? {1, . . . , d/g}. We encode once the value max(C) as an 8-bit integer, and for each group, we encode b s ? b min over max(C) bits. The true size for one layer, expressed in MegaBytes, is thus given b? </p><formula xml:id="formula_12">M(b) = 1 2 23 ? ? 2 ? 32 + 8 + d g max(C) + g d/g s=1 b s ? ? .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We present experimental results for language modeling, audio source separation, and image classification. We show that DiffQ can often provide a model with comparable performance to the uncompressed one while producing a model with a smaller footprint than the baseline methods (STE based). We provide a finer analysis of different aspects of DiffQ hyper-parameters and their impact on quantized models in next Section. Finally, we discuss limitations of DiffQ in the Limitation Section. Both experimental code, and a generic framework usable with any architecture in just a few lines, is available on our Github github.com/facebookresearch/diffq. All hyper-parameters for optimization and model definition are detailed in the Appendix. In all tables, ? (resp. ?) indicates that highest is best (resp. lowest is best). All results referred to as "QAT" are obtained using the formula given by equation 1 with a layer-wise min-max scaling of the weights. When using DiffQ, we use the same per layer min-max scaling. When also doing activation quantization, we use per-channel min-max scaling of the activations. All DiffQ experiments use Gaussian noise as explained in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to related work</head><p>On  quantization method by <ref type="bibr" target="#b35">(Polino et al., 2018)</ref>, which only optimizes the non uniform quantization points, not the pre-quantization weights. Following their practice, we report numbers after Huffman coding. We achieve a model almost half as small, with a gap of 25% in accuracy, proving that optimizing pre-quantization weights is more important than tuning a non uniform quantization grid.  <ref type="table" target="#tab_11">Table B</ref>.1 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language Modeling</head><p>We trained a 16 layers transformer <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> based language model on the Wikitext-103 text corpus <ref type="bibr" target="#b28">(Merity et al., 2016)</ref>, following , and using the Fairseq framework <ref type="bibr" target="#b32">(Ott et al., 2019)</ref>. Results are presented in <ref type="table" target="#tab_2">Table 2</ref>. We compare to the Quant-Noise method by <ref type="bibr" target="#b10">Fan et al. (2021)</ref>, but use a reduced layer-drop  of 0.1 instead of 0.2. This both improves the baseline, as well as the performance of DiffQ models. For DiffQ, we explicitly set the gradient for the number of bits parameters to zero for all layers that have been dropped. In order to test the compatibility of DiffQ with efficient int8 kernels, we further quantize the activations to 8 bits using PyTorch native support <ref type="bibr" target="#b34">(Paszke et al., 2019</ref>).</p><p>The transformer model has some tied parameters (e.g. word embedding in the first and pre-softmax layer). It is important to detect such tied parameters with DiffQ. We use a single shared bits parameter when a parameter tensor is reused multiple times, and for each forward, we sample a single pseudo quantization noise per group of shared weights and reuse it appropriately. Failure to do so led to a significant worsening of the performance at validation time.</p><p>While QAT breaks down when trying to get to 4 bits precision (perplexity of 29.9), using DiffQ allows to achieve a lower model size (113MB vs. 118 MB for QAT 4 bits) with a perplexity closer to the uncompressed one (18.6, vs. 18.1 uncompressed). We also tried fine-tuning a pre-trained model with LSQ . While this works better than QAT, LSQ reaches a worst perplexity for a slightly larger model size  <ref type="bibr" target="#b10">(Fan et al., 2021)</ref> improves on QAT but performs worse than DiffQ, even when using more than twice as many bits. With just 4.4 bits per weight on average, DiffQ achieve the same perplexity as the baseline. We also compare to PQ <ref type="bibr" target="#b39">(Stock et al., 2019)</ref>, as reported by <ref type="bibr" target="#b10">Fan et al. (2021)</ref>. While PQ achieves higher compression levels, with just 38MB, its perplexity is the worst of all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Music Source Separation</head><p>We use the Demucs architecture by <ref type="bibr" target="#b2">D?fossez et al. (2019)</ref> with 64 initial hidden channels. The model is trained on the standard MusDB benchmark <ref type="bibr" target="#b36">(Rafii et al., 2017)</ref>, for 180 epochs, and evaluated with the Signal-To-Distortion Ratio (SDR) metric <ref type="bibr" target="#b47">(Vincent et al., 2006)</ref>. The unquantized model is 1GB. We compare DiffQ with QAT training with either 5 or 4 bits, with the results presented in <ref type="table" target="#tab_4">Table 3</ref>. With 5 bits, QAT is able to replicate almost the same performance as the uncompressed model. When trying to further compress the model to 4 bits per weight, QAT leads to a sharp decrease of the SDR, losing 0.3dB, for a 130MB model. DiffQ achieves a model size of 120MB, with only a drop of 0.03dB of SDR compared to the uncompressed baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Image Classification</head><p>Next, we evaluated three image classification benchmarks: We compare DiffQ, QAT and LSQ (without activation quantization) using 2, 3, and 4 bits quantization.</p><p>Performance of the uncompressed model is additionally presented as an upper-bound. To better understand the effect of the penalty level ? on both model size and accuracy, we train models with DiffQ using different penalty levels. Exact results are presented in   ImageNet -EfficientNet. We evaluate the performance of DiffQ on the memory-efficient EfficientNet-B3 model. Results are depicted on <ref type="figure">Figure B</ref>.1 (c) as well as in <ref type="table" target="#tab_11">Table B</ref>.5, both in the Appendix. Both QAT 8 bits and DiffQ achieves similar accuracy (QAT 81.3 %, DiffQ 81.5%) but with a smaller model size for DiffQ (8.5MB vs. 12MB for QAT). When considering QAT 4 bits, DiffQ produces a smaller model with a significantly better accuracy level (80.8%). For QAT 4, we noticed considerable instability close to the end of the training, see <ref type="figure">Figure B</ref>.1 (b) in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis</head><p>Bits Histogram. <ref type="figure">Figure 3 presents</ref>  Group size. We additionally evaluate the affect of the group-size, g, on model size and accuracy, by optimizing DiffQ models using g ? {1, 4, 8, ?}. When g=?, we use a single group for the entire layer. Results for ResNet-18 using CIFAR-100 are depicted in <ref type="figure" target="#fig_0">Figure 1(a)</ref> in the Appendix. Interestingly, we observed that increasing g, yields in a smaller model size on the expense of a minor decrease in performance. However, when setting g=? model performance (model size and accuracy) is comparable to g=8 for this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Limitations</head><p>The model size given by equation 12 is obtained with a traditional encoding of the quantized model. However, more efficient coding techniques exist when the entropy of the data is low, such as Huffman coding <ref type="bibr" target="#b14">(Huffman, 1952)</ref>. Using the ZLib library, we obtain an estimate of the Huffman compressed model size after quantization. For instance, for the language model described in <ref type="table" target="#tab_2">Table 2</ref>, the QAT 8 model gets further compressed from 236MB to 150MB, showing that the entropy of its quantized weight is significantly lower than the maximal one for 8 bits integers. However, the DiffQ model naive size is 113MB, and after compression by ZLib, gets to 122MB. This is a sign that the entropy is close to its maximal value, with ZLib adding only overhead for no gain. In equation 10, we only penalize the naive number of bits used, while asking for the best possible accuracy. In that case, the model maximally use the entropy capabilities for a given number of bits. An interesting line of research would be to replace the model size equation 8 to account for the actual entropy of the data, for instance with differentiable kernel density estimation. We leave that for further research.</p><p>Another limitation of DiffQ is that it can make training up to twice as slow, due to the extra parameters to optimize for and the more complex gradient calculation graph. Besides, in order to achieve a specific model size or accuracy, one has to tune the ? penalty parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We presented DiffQ, a novel and simple differentiable method for model quantization via pseudo quantization noise addition to models' parameters. Given a single hyper-parameter that quantifies the desired trade-off between model size and accuracy, DiffQ can optimize the number of bits used for each trainable parameter or group of parameters during model training. We conduct expensive experimental evaluations on various domains using different model architectures. Results suggest that DiffQ is superior to the baseline methods on several benchmarks from various domains. On ImageNet, Wikitext-103, and MusDB, we achieve a model size that is smaller than a 4 bits quantized model, while retaining the same performance as the unquantized baseline. For future work, we consider adapting the model size penalty to account for Huffman encoding, which could allow to further reduce the model size when it is gzipped. Another line of work would be using PQN to improve activation quantization, enabling 4-bits kernels for a larger number of tasks.  <ref type="figure">Figure 3</ref>: We group layers of a given architecture into 11 groups (group 0 being closest to the input, and 10 closest to the output), and report for each group its contribution to the model size. We compare the baseline EfficientNet-B3 (above) and DeiT (below) models (floating point 32 bits) and the quantized models with DiffQ (?=5e?3 for EfficientNet-B3, ?=1e?2 for DeiT). For quantized model, we also report the distribution over each bitwidth within each group of layers. Scale is logarithmic across layers, and linear inside each one.</p><p>Finally, "overhead" shows the capacity needed to encode the bitwidth used for each group of weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differentiable Model Compression via Pseudo Quantization Noise</head><p>We provide in Section A all the details on the exact hyper-parameters, models, and datasets used for the results in Section 5 of the main paper. Then, we provide supplementary results in Section B, in particular tables for the scatter plots given on <ref type="figure" target="#fig_0">Figures 1(b)</ref> and 2(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed experimental setup</head><p>All experiments are conducted using NVIDIA V100 GPUs with either 16GB or 32GB RAM, depending on the applications (with language modeling requiring larger GPUs) on an internal cluster. For all models trained with QAT or DiffQ, we do not quantize tensors with a size under 0.01 MB (0.1 MB for the DeiT model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DiffQ hyper-parameters</head><p>For all experiments, we use b min = 2, b max = 15, b init = 8 and Gaussian noise. We observed on most models that taking b min &lt; 2 is unstable, with the notable exception of Resnet-20. We use a separate Adam optimizer <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> for the logit parameters controlling the number of bits used, with a default momentum ? 1 = 0.9 and decay ? 2 = 0.999. We use the default learning rate ? = 1e?3 for all task, except language modeling where we use ? = 1e?2. The remaining hyper-parameters are ?, the amount of penalty applied to the model size, and g, the group size. When g is not mentioned, it is set to the default value g = 8, which we found to be the best trade-off between the model freedom and the overhead from storing the number of bits used for each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Music Source Separation</head><p>We train a Demucs source separation model <ref type="bibr" target="#b2">(D?fossez et al., 2019</ref>) (MIT license) with a depth of 6 and 64 initial hidden channels, on the MusDB dataset <ref type="bibr" target="#b36">(Rafii et al., 2017</ref>) 2 , which is released under mixed licensing 3 . All the training details are exactly as in <ref type="bibr" target="#b2">(D?fossez et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modeling</head><p>We trained a 16 layers transformer <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> based language model on the Wikitext-103 text corpus <ref type="bibr" target="#b28">(Merity et al., 2016)</ref> 4 released under the CC-BY-SA license, following , and using the Fairseq framework <ref type="bibr" target="#b32">(Ott et al., 2019)</ref>, released under the MIT license. We used the hyper-parameters and the script provided by <ref type="bibr" target="#b10">(Fan et al., 2021)</ref> in the Fairseq repository 5 , however, and unlike what they mention in their paper, this script does not include layer drop . For DiffQ, we tried the penalty levels ? in {1, 5, 10}, with group size 8, as well as ? = 10 and g = 16. For LSQ, we used the same training hyper-parameters as DiffQ, except we initialized the model to a pre-trained model and used a learning rate 10 time smaller for fine tuning. Without this initialization, LSQ was failing to get under 40 of perplexity.</p><p>Tied weights and DiffQ. The model we trained was configured so that the word embedding in the first layer and the weight of the adaptive softmax are bound to the same value. It is important to detect such bounded parameters with DiffQ, as otherwise, a different number of bits could be used for what is in fact, the very same tensor. Not only do we use a single bits logit parameter when a parameter tensor is reused multiple times, but for each forward, we make sure that the pseudo quantization noise is sampled only once and reused appropriately. Failure to do so led to a significant worsening of the performance at validation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image classification</head><p>CIFAR10/100. On the CIFAR10/100 datasets, we train 3 different models: <ref type="bibr">MobileNet-v1 (Howard et al., 2017)</ref>, ResNet-18 <ref type="bibr" target="#b12">(He et al., 2016)</ref>, and a Wide-ResNet with 28x10 depth and width levels respectively <ref type="bibr" target="#b55">(Zagoruyko &amp; Komodakis, 2016)</ref>. All experiments are conducted on a single GPU with a batch size of 128, SGD with a learning rate of 0.1, momentum of 0.9, weight decay of 5e?4. The learning rate is decayed by a factor of 0.2 every 60 iterations. To generate <ref type="figure" target="#fig_2">Figure 2(b)</ref>, we evaluated DiffQ for ? in {0.01, 0.05, 0.1, 0.5, 1, 5} and the group size g in {4, 8, 16}.</p><p>For LSQ , we initialize from a pre-trained model. We tested both training for 90 epochs with a cosine schedule as originally done, or keeping the original learning rate schedule, only reducing the initial learning rate from 0.1 to 0.01. The second option achieved better overall results and that is the one we report. We also tried training with LSQ from a randomly initialized model, but that performed the worst of all approaches.</p><p>The dataset has been obtained from the torchvision package 6 . The input images are augmented with a random crop of size 32 with padding of 4, and a random horizontal flip. The RGB pixel values are normalized to mean 0 and standard deviation 1. We use the default split between train and valid as obtained from the torchvision package.</p><p>CIFAR-10 -Resnet 20. We use the implementation of Resnet 20 from <ref type="bibr" target="#b15">Idelbayev (2018)</ref>. We train for 600 epochs with a batch size of 128, with a learning rate of 0.1, momentum of 0.9, weight decay of 2e?4, and decrease the learning rate by a factor of 10, every 200 epochs. We quantize all parameters except biases, we set the minimum number of bits to b min = 1 as we observe this was stable for this particular task, and lower the maximum number of bits to b max = 10. We use a group size g = 16 and a penalty ? = 10 and a learning rate of 2e?4, for the separate Adam optimizer used for the bits parameters, which allows stay stable while going under 2 bits per weight.</p><p>ImageNet. We train an EfficientNet as implemented by <ref type="bibr" target="#b51">(Wightman, 2019)</ref> (Apache license), as well as a DeiT vision transformer <ref type="bibr" target="#b43">(Touvron et al., 2020</ref>) (MIT license) on the ImageNet dataset <ref type="bibr">Deng et al. (2009) 7</ref> . We use the original dataset split between train and valid. The images go through a random resize crop to 300px, a random horizontal flip, and pixel RGB values are normalized to have zero mean and unit variance.</p><p>ImageNet -EfficientNet. We trained for 100 epochs, using <ref type="bibr">RMSProp Tieleman &amp; Hinton (2012)</ref> as implemented in the timm package 8 with a learning rate of 0.0016, a weight decay of 1e ? 5 and a momentum of 0.9. The learning rate is decayed by a factor of 0.9875 with every epoch. As a warmup, the learning rate is linearly scaled from 0 to 0.0016 over the first 3 epochs. Following <ref type="bibr" target="#b51">(Wightman, 2019)</ref>, we evaluate with an exponential moving average of the weights of the model, with a decay of 0.99985. We use the random erase augmentation from <ref type="bibr" target="#b51">(Wightman, 2019)</ref>, as well as cutmix <ref type="bibr" target="#b54">(Yun et al., 2019)</ref>, with a probability of 0.2 and parameter to the beta distribution of 0.2. All the models are trained on 8 GPUs. For DiffQ, we used the penalties ? in {5e?4, 1e?3, 5e?3, 1e?2, 5e?2, 0.1, 0.5} and the default group size g = 8.</p><p>ImageNet -DeiT. We use the official DeiT implementation by <ref type="bibr">Touvron et al. (2020) 9</ref> , with the default training parameters, but without exponential moving averaging of the weights. More precisely, we trained for 300 epochs over 16 GPUs, with a batch size per GPU of 64, AdamW <ref type="bibr" target="#b26">(Loshchilov &amp; Hutter, 2019)</ref>, a weight decay of 0.05, learning rate of 5e?4, cosine learning rate scheduler, a learning rate warmup from 1e?6 over 5 epochs and label smoothing <ref type="bibr" target="#b40">(Szegedy et al., 2016)</ref>. As data augmentation, we used color-jitter, random erase, and either cutmix or mixup <ref type="bibr">(Zhang et al., 2018b)</ref>.</p><p>For DiffQ, we tested the penalty ? in {1e?3, 1e?2, 0.1, 0.5, 1, 5}, and group size g in {1, 4, 8}. We use a minimum number of bits of 3, instead of 2, as this led to better stability. We use Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> to optimize the bits parameters, with a learning rate of 5e?4.</p><p>ImageNet -ResNet18 and ResNet50. We trained all models (DiffQ and LSQ) for 400 epochs over 4 GPUs, with a batch size per GPU of 256, using <ref type="bibr">RMSProp Tieleman &amp; Hinton (2012)</ref> as implemented in the timm package 10 (also experimented with SGD however RMSProp provides better results), a weight decay of 0.05, learning rate of 5e?4, where we multiply the learning by 0.9875 after every epoch. We used a learning rate warmup from 1e?6 over 3 epochs and label smoothing <ref type="bibr" target="#b40">(Szegedy et al., 2016)</ref> with smoothing factor of 0.3. As data augmentation, we used color-jitter, random erase and cutmix using ? = 0.2 with probability of 0.3.</p><p>For DiffQ, we tested the penalty ? in {1e?2, 3e?2, 4e?2, 5e?2, 8e?2, 0.1}, and group size g in {8, 16}. We use a minimum number of bits of 2. We use Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> to optimize the bits parameters, with a learning rate of 5e?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Supplementary results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head><p>On Results marked with * are the ones reported in  using slightly better uncompressed model. For fair comparison we reported both numbers. Notice, DiffQ achieves comparable and even superior results over LSQ also under considering this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10/100</head><p>We report on <ref type="table" target="#tab_11">Table B</ref>.2 the results on the CIFAR10/100 datasets, which are shown for CIFAR100 in <ref type="figure" target="#fig_2">Figure 2</ref>(b) in the main paper. Results are presented using MobileNet-v1, ResNet-18, and WideResNet. For CIFAR100 the presented results used for creating <ref type="figure" target="#fig_2">Figure 2</ref>(b) in the main paper. As we cannot show all the DiffQ runs, we selected for each model and dataset two versions: v1 is the smallest model that has an accuracy comparable to the baseline (accuracy is greater than 1 ? 1/100 times the baseline accuracy), while v2 is the model with the highest accuracy that is comparable in size with the QAT 2 bits model (size must be smaller than 1 + 1/100 times the baseline size, except for MobileNet, for which we had to allow a 4% relative increase in size. The penalty and group size selected with this procedure is displayed on <ref type="table" target="#tab_11">Table B</ref>.3.</p><p>Looking first at v1 models, we achieve on all tasks and datasets a model that is competitive with the baseline (sometimes even better), with a model size that is smaller than a QAT 4 bits model (for instance more than 2MB saved on a ResNet-18 trained on CIFAR-10 compared to QAT 4 bits, for the same accuracy). Now for v2, first note that as the minimum number of bits used by DiffQ is exactly 2, it is not possible here to make a model smaller than QAT 2 bits. However, even with as little as 0.01 MB extra, DiffQ can get up to 30% increase in accuracy compared to QAT 2 bits (for a Wide ResNet). On all architectue and datasets, the gain from DiffQ over QAT 2 bits is at least 10% accuracy. This confirms in practice the bias of STE-based methods when the number of bits is reduced, a bias that we already demonstrated in theory in Section 3.3. In particular, it is interesting that the largest improvement provided by DiffQ is for the Wide ResNet model, which should be the easiest to quantize. But having the largest number of weights, it also likely the one that is the most sensitive to the oscillations of QAT quantized weights described in Section 3.3.</p><p>Ablation. <ref type="table" target="#tab_11">Table B</ref>.4 summarizes the results of comparing QAT against DiffQ for model quantization using a fixed number of bits using MobileNet, ResNet-18, and WideResNet on both CIFAR10 and CIFAR100. DiffQ outperforms QAT, where this is especially noticeable while using 2 bits quantization, in which training is less stable for QAT.</p><p>Next, we evaluated the affect of the group-size, g, on model size and accuracy, by optimizing DiffQ models using g ? {1, 4, 8, ?}. When g = ?, we use a single group for the entire layer. Results for ResNet-18 using CIFAR-100 are summarized in <ref type="figure" target="#fig_0">Figure B.1 (a)</ref>. Interestingly, we observed that increasing g, yields in a smaller model size on the expanse of a minor decrease in performance. However, when setting g = ? model performance (model size and accuracy) is comparable to g = 8 for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EfficientNet-b3 on ImageNet</head><p>On <ref type="table" target="#tab_11">Table B</ref>.5 we report the results for training EfficientNet-b3 Tan &amp; Le (2019) on the ImageNet dataset, matching the results reported on <ref type="figure" target="#fig_0">Figure 1(b)</ref>.</p><p>As previously, we selected two versions of DiffQ, one matching the size of QAT 8bits, and one smallest than QAT 4 bits. At 8 bits, DiffQ achieves the same accuracy as the uncompressed baseline, for a slightly smaller model than QAT 8bits. As we lower the number of bits, we again see a clear advantage for DiffQ, with both a smaller model (5.7MB against 6.1MB) than QAT 4bits, and significantly higher accuracy (76.8% vs. 57.3%).</p><p>The lower accuracy for QAT4 on ImageNet led us to take a closer look at the model performance. <ref type="figure">Figure B</ref>.1 (b) depicts the model accuracy as a function of the number of epochs for both QAT4 and DiffQ. Notice, similarly to the toy example presented in Section 3.3 training with QAT4 creates instability in the model optimization (especially near model convergence), which leads to significant differences in performance across adjacent epochs. When considering DiffQ, model optimization is stable and no such differences are observed. <ref type="table" target="#tab_11">Table B</ref>.2: Detailed results of QAT and DiffQ on the CIFAR-10/100 datasets. For each architecture and dataset, we provide the performance of the baseline, QAT models with 2 to 4 bits, and two DiffQ runs: v1. is the smallest model that is within a small range of the baseline performance, v2. is the best model of comparable size with QAT 2 bits, selected from the pool of candidates described in Section A. For Wide-ResNet, we report a single variant of DiffQ, as it is both the smallest and the one with the best accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation Quantization for Language Modeling</head><p>In <ref type="table" target="#tab_11">Table B</ref>.6 we report language modeling results for a 16-layers Transformer models while applying activation quantization. Unlike the results in <ref type="table" target="#tab_2">Table 2</ref> where we used per-channel activation quantization, here we report results with a histogram quantizer. Additionally when considering histogram quantizer, results suggest DiffQ is superior to both QAT and QN when considering both model size and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uniform Noise vs. Gaussian Noise</head><p>In <ref type="table" target="#tab_11">Table B</ref>.7 we provide an empirical comparison between uniform noise and Gaussian noise using ResNet-18 on CIFAR10 for DiffQ. We found that using Gaussian noise acheives the same model size with better accuracy levels. <ref type="table" target="#tab_11">Table B</ref>.4: A comparison between QAT and DiffQ while we consider a fixed number of bits for all model parameters, specifically using 2, 3, and 4 bits. Results are reported for CIFAR-10 and CIFAR-100 using MobileNet-v1, ResNet-18. and WideResNet. We report Accuracy (Acc.) and Model Size (M.S.). <ref type="table" target="#tab_11">Table B</ref>.5: Image classification results for the ImageNet benchmark. Results are presented for DiffQ and QAT using 4 and 8 bits using the EfficientNet-b3 model <ref type="bibr" target="#b41">(Tan &amp; Le, 2019)</ref>. We report Top-1 Accuracy (Acc.) together with Model Size (M.S.).  ImageNet results using EfficientNet-B3 model. We plot the model size vs. model accuracy using different penalty levels. We additionally, present the uncompressed models (uncomp.) and Quantization Aware Training (QAT) using 4 and 8 bits. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Using STE and SGD to optimize the 1D least-mean-square problem given by equation 4 (with B = 4 and X = 1 a.s.). Q(w n , B) oscillates between the quantized value just above (w + ) and just under (w ? ) the unquantized ground truth w * , while w n oscillates around the boundary (w + + w ? )/2. (b) Model accuracy vs. epochs for ImageNet using EfficientNet-b3. Results are presented for both QAT over 4 bits and DiffQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ImageNet Deng et al. (2009), CIFAR-10 and CIFAR-100 Krizhevsky et al. (2009). For CIFAR-10 and CIFAR-100 results are reported for MobileNet-v1 Howard et al. (2017), ResNet-18 He et al. (2016), and Wide-ResNet with 28x10, depth and width levels respectively Zagoruyko &amp; Komodakis (2016). ImageNet results are reported using EfficientNet-B3 Tan &amp; Le (2019) and DeiT-B Touvron et al. (2020) models. More details regarding hyper-parameters and augmentations used can be found in the Appendix. CIFAR10 &amp; CIFAR-100. Results for CIFAR10 and CIFAR100 are depicted in Figures 2(a) and 2(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Model accuracy and size on CIFAR10 (a) and CIFAR100 (b) using MobileNet, ResNet-18, and WideResNet (WRN) models for various penalty levels using DiffQ, QAT, LSQ, and the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>%) ? M. S. (MB) ? Acc. (%) ? M. S. (MB) ? Acc. (%) ? M. S. (MB) ? CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>B.1: (a): DiffQ results with various groups sizes (g ? {1, 4, 8, ?}). g = ? refers to a single group for the entire layer. For reference, we report the accuracy of the uncompressed model (42.8 MB). Models are Resnet-18 trained on CIFAR-100. (b):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of DiffQ against baselines presented in the Related Work section. Sizes marked with ? are reported after Huffman coding, following<ref type="bibr" target="#b35">Polino et al. (2018)</ref>. Accuracies marked with * are the best rather than last one to match previous practices.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="2">Top-1 Acc. (%) M.S. (MB)</cell></row><row><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell></row><row><cell>ResNet-18</cell><cell>Uncompressed</cell><cell>95.3</cell><cell>42.7</cell></row><row><cell>ResNet-18</cell><cell>UNIQ Baskin et al. (2018b)</cell><cell>89.1</cell><cell>2.7</cell></row><row><cell>ResNet-18</cell><cell>NICE Baskin et al. (2018a)</cell><cell>92.7</cell><cell>2.7</cell></row><row><cell>ResNet-18</cell><cell>DiffQ (Ours)</cell><cell>93.9</cell><cell>2.7</cell></row><row><cell>ResNet-20</cell><cell>Uncompressed</cell><cell>92.7*</cell><cell>1.48</cell></row><row><cell>ResNet-20</cell><cell>DQ Uhlich et al. (2020)</cell><cell>91.4*</cell><cell>0.07</cell></row><row><cell>ResNet-20</cell><cell>DiffQ (Ours)</cell><cell>91.6*</cell><cell>0.06</cell></row><row><cell></cell><cell>CIFAR100</cell><cell></cell><cell></cell></row><row><cell cols="2">Wide-ResNet Uncompressed</cell><cell>76.2</cell><cell>139.4</cell></row><row><cell cols="2">Wide-ResNet DiffQuant Polino et al. (2018)</cell><cell>49.3</cell><cell>7.9</cell></row><row><cell cols="2">Wide-ResNet DiffQ (Ours)</cell><cell>75.6</cell><cell>4.7</cell></row><row><cell></cell><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell>ResNet-18</cell><cell>Uncompressed</cell><cell>70.9*</cell><cell>44.6</cell></row><row><cell>ResNet-18</cell><cell>Meta-Quant Chen et al. (2019)</cell><cell>60.3</cell><cell>1.3</cell></row><row><cell>ResNet-18</cell><cell>DQ Uhlich et al. (2020)</cell><cell>70.1*</cell><cell>5.4</cell></row><row><cell>ResNet-18</cell><cell>LSQ 4 bits Esser et al. (2020)</cell><cell>70.7*</cell><cell>5.6</cell></row><row><cell>ResNet-18</cell><cell>DiffQ (Ours)</cell><cell>71.1*</cell><cell>5.3</cell></row><row><cell>ResNet-50</cell><cell>Uncompressed</cell><cell>77.1*</cell><cell>97.5</cell></row><row><cell>ResNet-50</cell><cell>LSQ 4 bits Esser et al. (2020)</cell><cell>76.2*</cell><cell>12.3</cell></row><row><cell>ResNet-50</cell><cell>LSQ 3 bits Esser et al. (2020)</cell><cell>75.6*</cell><cell>9.3</cell></row><row><cell>ResNet-50</cell><cell>DiffQ (Ours)</cell><cell>76.6*</cell><cell>10.5</cell></row><row><cell>ResNet-50</cell><cell>DiffQ (Ours)</cell><cell>76.3*</cell><cell>8.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>, we compare DiffQ to some of the related work presented in Section 2. Compared with the NICE (Baskin et al., 2018a) and UNIQ (Baskin et al., 2018b) methods, which also rely on additive noise, DiffQ achieves significantly better accuracy for the same model size. We then compare to the differentiable</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization. We compared DiffQ to QAT and Quant-Noise (QN) method proposed by<ref type="bibr" target="#b10">Fan et al. (2021)</ref> (models with ? were trained with a layer-drop of 0.2). Activations are quantized over 8 bits, with a per-channel scaling.</figDesc><table><row><cell>Weights</cell><cell cols="3">Activation PPL ? M. S. (MB) ?</cell></row><row><cell>Uncompressed</cell><cell>-</cell><cell>18.1</cell><cell>942</cell></row><row><cell>8 bits</cell><cell>8 bits</cell><cell>18.3</cell><cell>236</cell></row><row><cell>QAT 8bits</cell><cell>8 bits</cell><cell>19.7</cell><cell>236</cell></row><row><cell>QAT 4bits</cell><cell>8 bits</cell><cell>29.9</cell><cell>118</cell></row><row><cell cols="2">LSQ 4 bits (Esser et al., 2020) 8 bits</cell><cell>18.9</cell><cell>118</cell></row><row><cell>DiffQ (?=5, g=16)</cell><cell>8 bits</cell><cell>18.1</cell><cell>130</cell></row><row><cell>DiffQ (?=10, g=16)</cell><cell>8 bits</cell><cell>18.6</cell><cell>113</cell></row><row><cell>Uncompressed  ?</cell><cell>-</cell><cell>18.3</cell><cell>942</cell></row><row><cell>QN 8 bits ? Fan et al. (2021)</cell><cell>QN 8 bits</cell><cell>18.7</cell><cell>236</cell></row><row><cell>QN 4 bits ? Fan et al. (2021)</cell><cell>QN 8 bits</cell><cell>19.5</cell><cell>118</cell></row><row><cell>PQ ? Fan et al. (2021)</cell><cell>-</cell><cell>20.7</cell><cell>38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Meta-Quant (Chen et al., 2019)  achieves smaller model size than DiffQ, with 1 bit per weight, a regime where the PQN assumption breaks down, at the price of losing nearly 10% of accuracy. Finally, compared with two quantization methods: DQ by<ref type="bibr" target="#b44">Uhlich et al. (2020)</ref> and LSQ by. When considering DQ, DiffQ achieves slightly smaller model size and better accuracy on ImageNet using ResNet-18, and a 15% smaller model with sightly better accuracy for a Resnet-20 trained on CIFAR-10. Comparing to LSQ 1 , DiffQ achieves better accuracy with smaller model size on ImageNet using both ResNet-18 and ResNet-50. Additional comparison between DiffQ and LSQ for higher compression rates can be on</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Music source separation results for the Demucs model<ref type="bibr" target="#b2">(D?fossez et al., 2019)</ref>. We report Signal-to-Distortion Ration (SDR) together with Model Size (M.S.).SDR (dB) ? M. S. (MB) ?</figDesc><table><row><cell>Uncompressed</cell><cell>6.31</cell><cell>1014</cell></row><row><cell>QAT 4bits</cell><cell>5.99</cell><cell>130</cell></row><row><cell>QAT 5bits</cell><cell>6.27</cell><cell>162</cell></row><row><cell>DiffQ (?=3e?4)</cell><cell>6.28</cell><cell>120</cell></row></table><note>than DiffQ (18.9 perplexity for 118 MB). Similarly, Quant-Noise</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table B.2, in the Appendix, together with a detailed analysis.Results suggest DiffQ models reach comparable performance to the LSQ and outperforms QAT models while producing models with a smaller footprint. When considering 2 bits quantization, QAT is always worse than both LSQ and DiffQ. While LSQ works well for Resnet18, it suffers from large drops in accuracies for MobileNet and WideResNet, failing entirely to train for MobileNet on CIFAR10, despite initialization from a pre-trained model.</figDesc><table /><note>ImageNet -DeiT. Results for ImageNet using DeiT-B model are presented in Table 4. We compared DiffQ to QAT when training with 4 and 8 bits. Both QAT with 8 bits and DiffQ reach comparable performance to the uncompressed model, while DiffQ yields a model almost half of the size as QAT, however still bigger than QAT with 4 bits. When we increase ?, we get a smaller model-size than QAT with 4 bits but with better accuracy levels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Uncompressed</cell><cell>81.8</cell><cell>371.4</cell></row><row><cell>QAT 4bits</cell><cell>79.2</cell><cell>41.7</cell></row><row><cell>QAT 8bits</cell><cell>81.6</cell><cell>82.9</cell></row><row><cell>DiffQ (?=1e?2)</cell><cell>82.0</cell><cell>45.7</cell></row><row><cell>DiffQ (?=0.1)</cell><cell>81.5</cell><cell>33.02</cell></row></table><note>Image classification results for the ImageNet benchmark. Results are presented for DiffQ and QAT using 4 and 8 bits using the DeiT model (Touvron et al., 2020). We report Top-1 Accuracy (Acc.) together with Model Size (M.S.).Top-1 Acc. (%) ? M.S. (MB) ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>DiffQ outperforms QAT, with a gap especially noticeable for 2 bits models, a regime where QAT becomes unstable, as we described in previous section.</figDesc><table><row><cell>the weight bitwidth assignment over layer groups for the EfficientNet-B3</cell></row><row><cell>Tan &amp; Le (2019) and DeiT Touvron et al. (2020) models trained on ImageNet. The capacity distribution</cell></row><row><cell>over depth for ConvNets (EfficientNet-B3) and Transformers (DeiT) are different (fp32 shows uncompressed</cell></row><row><cell>capacity). Notice, that the quantization trends are different too: for the ConvNet, smaller bitwidths are used</cell></row></table><note>for deeper layers of the model while large bitwidth is more common in the first layers (except for the last linear layer which seems to need some precision). For the Transformer, this effect of varying quantization by layer is similar but less pronounced, due to the more symmetric nature of the architecture. Fixed bitwidth. On Table B.4 in the Appendix, we compare QAT to DiffQ using a fixed number of bits, i.e. comparing strictly PQN to STE. On MobileNet, ResNet-18, and WideResNet for both CIFAR10 and CIFAR100,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Constrained optimization and Lagrange multiplier methods. Academic press, 2014. Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 696-697, 2020. Shangyu Chen et al. Metaquant: Learning to quantize by learning to penetrate non-differentiable quantization. In Advances in Neural Information Processing Systems, 2019. Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Variable rate deep image compression with a conditional autoencoder. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3146-3154, 2019. Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In Proc. of the International Conference on Learning Representations, 2018b.</figDesc><table><row><cell>arXiv preprint arXiv:1810.00162, 2018a. Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes, Alex M Bronstein, and Avi Mendelson. Uniq: Uniform noise injection for non-uniform quantization of neural networks. arXiv preprint arXiv:1804.10969, 2018b. Yoshua Bengio, Nicholas L?onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334-334, 1997. Dimitri P Bertsekas. Hongyi</cell></row></table><note>Chaim Baskin, Natan Liss, Yoav Chai, Evgenii Zheltonozhskii, Eli Schwartz, Raja Giryes, Avi Mendelson, and Alexander M Bronstein. Nice: Noise injection and clamping estimation for neural network quantization.Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. arXiv preprint arXiv:1511.00363, 2015. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table B.1 results are reported for DiffQ and LSQ Esser et al. (2020) using ResNet-18 and ResNet-50 on ImageNet dataset. We compared different model sizes and compression rates. Results suggest that DiffQ is superior both in terms of accuracy and smaller model size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table B</head><label>B</label><figDesc>.1: Additional comparison between DiffQ and LSQ for different model sizes and compression rates. Results marked with * are the ones reported in using slightly better uncompressed baseline model. For fair comparison we reported both numbers. All results reported are for the best model accuracy.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="2">Top-1 Acc. (%) M.S. (MB)</cell></row><row><cell></cell><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell>ResNet-18</cell><cell>Uncompressed</cell><cell>70.9</cell><cell>44.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table B.3: Penalty ? and group size g for the v1 and v2 DiffQ models reported onTable B.2</figDesc><table><row><cell></cell><cell>Uncompressed</cell><cell>90.9</cell><cell>12.3</cell><cell>95.3</cell><cell></cell><cell></cell><cell>42.7</cell><cell>95.3</cell><cell>139.2</cell></row><row><cell></cell><cell>QAT 2bits</cell><cell>78.1</cell><cell>0.88</cell><cell>87.2</cell><cell></cell><cell></cell><cell>2.70</cell><cell>70.8</cell><cell>8.81</cell></row><row><cell></cell><cell>QAT 3bits</cell><cell>88.2</cell><cell>1.26</cell><cell>94.0</cell><cell></cell><cell></cell><cell>4.03</cell><cell>94.3</cell><cell>13.16</cell></row><row><cell></cell><cell>QAT 4bits</cell><cell>90.1</cell><cell>1.64</cell><cell>95.0</cell><cell></cell><cell></cell><cell>5.36</cell><cell>94.4</cell><cell>17.50</cell></row><row><cell></cell><cell>LSQ 2 bits</cell><cell>10.0</cell><cell>0.88</cell><cell>95.0</cell><cell></cell><cell></cell><cell>2.70</cell><cell>81.9.</cell><cell>8.81</cell></row><row><cell></cell><cell>LSQ 3 bits</cell><cell>90.8</cell><cell>1.26</cell><cell>95.3</cell><cell></cell><cell></cell><cell>4.03</cell><cell>88.8</cell><cell>13.16</cell></row><row><cell></cell><cell>LSQ 4 bits</cell><cell>90.9</cell><cell>1.64</cell><cell>95.2</cell><cell></cell><cell></cell><cell>5.36</cell><cell>89.9</cell><cell>17.50</cell></row><row><cell></cell><cell>DiffQ v1</cell><cell>90.3</cell><cell>0.94</cell><cell>94.9</cell><cell></cell><cell></cell><cell>3.17</cell><cell>94.1</cell><cell>8.81</cell></row><row><cell></cell><cell>DiffQ v2</cell><cell>87.9</cell><cell>0.91</cell><cell>93.9</cell><cell></cell><cell></cell><cell>2.71</cell><cell>94.1</cell><cell>8.81</cell></row><row><cell>CIFAR-100</cell><cell>Uncompressed QAT 2bits QAT 3bits QAT 4bits LSQ 2 bits</cell><cell>68.1 10.9 59.7 66.9 64.9</cell><cell>12.6 0.91 1.29 1.69 0.91</cell><cell>77.9 58.7 73.7 77.3 77.5</cell><cell></cell><cell></cell><cell>42.8 2.72 4.05 5.39 2.72</cell><cell>76.2 46.5 75.0 75.5 40.9</cell><cell>139.4 8.83 13.18 17.53 8.82</cell></row><row><cell></cell><cell>LSQ 3 bits</cell><cell>67.7</cell><cell>1.29</cell><cell>77.7</cell><cell></cell><cell></cell><cell>4.05</cell><cell>55.6</cell><cell>13.18</cell></row><row><cell></cell><cell>LSQ 4 bits</cell><cell>68.5</cell><cell>1.69</cell><cell>77.8</cell><cell></cell><cell></cell><cell>5.39</cell><cell>56.5</cell><cell>17.53</cell></row><row><cell></cell><cell>DiffQ v1</cell><cell>68.5</cell><cell>1.10</cell><cell>77.6</cell><cell></cell><cell></cell><cell>4.82</cell><cell>75.3</cell><cell>8.83</cell></row><row><cell></cell><cell>DiffQ v2</cell><cell>64.6</cell><cell>0.94</cell><cell>71.7</cell><cell></cell><cell></cell><cell>2.72</cell><cell>75.6</cell><cell>8.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">MobileNet ResNet-18 WideResNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>g</cell><cell>?</cell><cell>g</cell><cell>?</cell><cell>g</cell></row><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>DiffQ v1 1 DiffQ v2 5</cell><cell>16 8</cell><cell>0.1 5</cell><cell>8 4</cell><cell>5 5</cell><cell>16 16</cell></row><row><cell></cell><cell></cell><cell>CIFAR-100</cell><cell>DiffQ v1 1 DiffQ v2 5</cell><cell>16 16</cell><cell>0.05 5</cell><cell>4 8</cell><cell>5 1</cell><cell>16 16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Top-1 Acc. (%) ? M.S. (MB) ?</figDesc><table><row><cell></cell><cell>77.9</cell><cell></cell></row><row><cell></cell><cell>77.8</cell><cell>1e-2</cell></row><row><cell>Accuracy (%)</cell><cell cols="2">Model Size (MB) 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 77.2 77.3 77.4 77.5 77.6 77.7 5e-2 5e-2 1e-2 Uncomp. g = 1 g = 4 g = 8 g =</cell></row><row><cell></cell><cell>(a)</cell><cell></cell></row><row><cell></cell><cell>Uncompressed</cell><cell>81.6</cell><cell>46.7</cell></row><row><cell></cell><cell>QAT 4bits</cell><cell>57.3</cell><cell>6.3</cell></row><row><cell></cell><cell>QAT 8bits</cell><cell>81.3</cell><cell>12.0</cell></row><row><cell></cell><cell>PQ (Fan et al., 2021)</cell><cell>80.0</cell><cell>3.1</cell></row><row><cell></cell><cell>DiffQ (?=0.05)</cell><cell>80.8</cell><cell>6.0</cell></row><row><cell></cell><cell>DiffQ (?=0.01)</cell><cell>81.5</cell><cell>8.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table B .</head><label>B</label><figDesc>6: Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization using a histogram quantizer. We compared DiffQ to QAT and Quant-Noise (QN) method proposed by Fan et al. (2021) (models with ? were trained with layer drop of 0.2 Fan et al. (2019), and 0.1 for the others.).Table B.7: Empirical comparison between uniform noise and Gaussian noise using ResNet-18 on CIFAR10 for DiffQ.</figDesc><table><row><cell>Weights</cell><cell cols="3">Activation PPL ? M. S. (MB) ?</cell></row><row><cell cols="2">Uncompressed (Ours) -</cell><cell>18.1</cell><cell>942</cell></row><row><cell>QAT 8bits</cell><cell>-</cell><cell>18.2</cell><cell>236</cell></row><row><cell>QAT 4bits</cell><cell>-</cell><cell>28.8</cell><cell>118</cell></row><row><cell>DiffQ (?=1, g=16)</cell><cell>-</cell><cell>18.0</cell><cell>182</cell></row><row><cell>DiffQ (?=10, g=16)</cell><cell>-</cell><cell>18.5</cell><cell>113</cell></row><row><cell>8 bits</cell><cell>8 bits</cell><cell>19.5</cell><cell>236</cell></row><row><cell>QAT 8bits</cell><cell>8 bits</cell><cell>26.0</cell><cell>236</cell></row><row><cell>QAT 4bits</cell><cell>8 bits</cell><cell>34.6</cell><cell>118</cell></row><row><cell>DiffQ (?=1, g=16)</cell><cell>8 bits</cell><cell>19.1</cell><cell>182</cell></row><row><cell>DiffQ (?=10, g=16)</cell><cell>8 bits</cell><cell>19.2</cell><cell>113</cell></row><row><cell>Uncompressed  ?</cell><cell>-</cell><cell>18.3</cell><cell>942</cell></row><row><cell>QN 8 bits ?</cell><cell>QN 8 bits</cell><cell>18.7</cell><cell>236</cell></row><row><cell>QN 4 bits ?</cell><cell>QN 8 bits</cell><cell>20.5</cell><cell>118</cell></row><row><cell>Uniform</cell><cell>86.9</cell><cell></cell><cell>2.7</cell></row><row><cell>Gaussian</cell><cell>93.6</cell><cell></cell><cell>2.7</cell></row></table><note>Noise distribution Top-1 Acc. (%) ? M.S. (MB) ?</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We used our own LSQ implementation, with only weight quantization, since no official code is available. Comparison with the results reported in can be found onTable B.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sigsep.github.io/datasets/musdb.html 3 https://github.com/sigsep/website/blob/master/content/datasets/assets/tracklist.csv 4 https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/ 5 https://github.com/pytorch/fairseq/tree/master/examples/quant_noise</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/pytorch/vision 7 http://www.image-net.org/ 8 https://github.com/rwightman/pytorch-image-models 9 https://github.com/facebookresearch/deit</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/rwightman/pytorch-image-models</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end optimized image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hawq: Hessian aware quantization of neural networks with mixed-precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hawq-v2: Hessian aware trace-weighted quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyaan</forename><surname>Arfeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18518" to="18529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Releq: An automatic reinforcement learning approach for deep quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elthakeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannoy</forename><surname>Pilligundla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS ML for Systems workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Releq: A reinforcement learning approach for automatic deep quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannoy</forename><surname>Ahmed T Elthakeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemehsadat</forename><surname>Pilligundla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepika</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rathinakumar</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmendra S</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training with quantization noise for extreme model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>ICLR 2021</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Differentiable soft quantization: Bridging full-precision and low-bit neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
		<idno type="DOI">10.1109/JRPROC.1952.273898</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Proper ResNet implementation for CIFAR10/CIFAR100 in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yerlan</forename><surname>Idelbayev</surname></persName>
		</author>
		<ptr target="https://github.com/akamaster/pytorch_resnet_cifar10" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Sambhav R Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to quantize deep networks by optimizing quantization intervals with task loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangil</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyong</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4350" to="4359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Ternary weight networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sharpness-aware quantization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12273</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4942" to="4952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning low-precision neural networks without straight-through estimator (ste)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01061</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relaxed quantization for discretized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Reisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wrpn: Wide reduced-precision networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eriko</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debbie</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Overcoming oscillations in quantization-aware training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Fournarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelysei</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11086</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Nipq: Noise injection pseudo quantization for automated dnn optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sein</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheol</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00820</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Polino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The musdb18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
	<note>Published in Transactions on Machine Learning Research (09/2022</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning discrete weights using the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Shayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">And the bit goes down: Revisiting the quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mixed precision dnns: All you need is a good parametrization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cardinaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Yoshiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Alonso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Soft weight-sharing for neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Information Processing Systems</title>
		<meeting>of Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Differentiable joint pruning and quantization for hardware efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="259" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Statistical theory of quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istvan</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Chang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="361" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training and inference with integers in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hawq-v3: Dyadic neural network quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangcheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11875" to="11886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
	<note>ResNet-18 LSQ 8 bits Esser et al.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
		<title level="m">ResNet-18 LSQ 4 bits Esser et</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esser</surname></persName>
		</author>
		<idno>ResNet-18 LSQ 3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esser</surname></persName>
		</author>
		<idno>ResNet-50 LSQ 8</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esser</surname></persName>
		</author>
		<idno>ResNet-50 LSQ 3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wideresnet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
