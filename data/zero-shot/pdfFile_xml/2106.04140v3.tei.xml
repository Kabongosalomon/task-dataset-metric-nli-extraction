<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Broadcasted Residual Learning for Efficient Keyword Spotting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonggeun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Qualcomm AI Research ?</orgName>
								<orgName type="institution" key="instit2">Qualcomm Korea YH</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simyung</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Qualcomm AI Research ?</orgName>
								<orgName type="institution" key="instit2">Qualcomm Korea YH</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Lee</surname></persName>
							<email>jinkyu@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Qualcomm AI Research ?</orgName>
								<orgName type="institution" key="instit2">Qualcomm Korea YH</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dooyong</forename><surname>Sung</surname></persName>
							<email>dooysung@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Qualcomm AI Research ?</orgName>
								<orgName type="institution" key="instit2">Qualcomm Korea YH</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Broadcasted Residual Learning for Efficient Keyword Spotting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: keyword spotting</term>
					<term>speech command recognition</term>
					<term>deep neural network</term>
					<term>efficient neural network</term>
					<term>residual learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Keyword spotting is an important research field because it plays a key role in device wake-up and user interaction on smart devices. However, it is challenging to minimize errors while operating efficiently in devices with limited resources such as mobile phones. We present a broadcasted residual learning method to achieve high accuracy with small model size and computational load. Our method configures most of the residual functions as 1D temporal convolution while still allows 2D convolution together using a broadcasted-residual connection that expands temporal output to frequency-temporal dimension. This residual mapping enables the network to effectively represent useful audio features with much less computation than conventional convolutional neural networks. We also propose a novel network architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted residual learning and describe how to scale up the model according to the target device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7% top-1 accuracy on Google speech command datasets v1 and v2, respectively, and consistently outperform previous approaches, using fewer computations and parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing efficient architecture is an important topic in neural speech processing. In particular, for keyword spotting (KWS), which aims to detect a predefined keyword, network efficiency is essential because it is usually performed in edge devices while requiring low latency. Recent efficient CNNs <ref type="bibr">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> are usually made up of repeated blocks of the same structure and are based on residual learning <ref type="bibr" target="#b4">[5]</ref> and depthwise separable convolutions <ref type="bibr" target="#b5">[6]</ref>. This trend continues in CNN-based KWS approaches, and they use either 1D temporal or 2D frequency?temporal convolutions with pros and cons. In the case of using temporal convolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, they require less computing than 2D approaches. However, the convolution's internal biases, such as translation equivariance, cannot be obtained for the frequency dimension. On the other hand, the approaches based on 2D convolution still require more computations than 1D methods despite efficient designs like using depthwise separable convolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In this paper, we introduce broadcasted residual learning to address these problems of 1D or 2D convolution. Instead of processing all features in 1D or 2D, frequency-wise convolution performs on the 2D features. Then, we average the 2D features by frequency to get temporal features. After some temporal operations, we can apply residual mapping to the input 2D feature by broadcasting the 1D residual information. This learning * equal contribution ? Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. <ref type="figure">Figure 1</ref>: Model Size vs. Google speech command dataset v1 Test Accuracy. The proposed BC-ResNets significantly outperform other KWS approaches. The smallest BC-ResNet-1 achieves 96.6% accruacy with less than 10k parameters. We scale the BC-ResNet-1 by channel width with a factor of 8, and BC-ResNet-8 achieves the state-of-the-art 98.0%. The details are in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>method enables convolutional processing in the frequency direction to obtain the advantage of 2D CNNs while minimizing computational cost. Based on this residual learning method, we propose a novel network named broadcasting-residual network (BC-ResNet). BC-ResNet achieves top-1 accuracy 96.6% and 96.9%, respectively on the Google speech command datasets, v1 and v2 <ref type="bibr" target="#b11">[12]</ref> with less than 10k parameters. And by scaling up BC-ResNet, our method achieves state-of-the-art performance with a much smaller memory footprint than other keyword spotting methods, as shown in <ref type="figure">Figure 1</ref>.</p><p>Our contributions are summarized as follows:</p><p>(1) We introduce a brand new framework termed, broadcasted residual learning, which utilizes the advantage of 1D temporal and 2D convolution while minimizing the increase of computation.</p><p>(2) We propose a novel model architecture, BC-ResNet, based on broadcasted residual learning and obtain a family of networks, BC-ResNets, by increasing the model width.</p><p>(3) The comprehensive experiments show our method's effectiveness in keyword spotting, and our model achieves stateof-the-art accuracy while reducing model parameters and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head><p>In this session, we introduce broadcasted residual learning that can obtain the advantages of 1D and 2D convolution while minimizing an increase of computation. First, we define the broadcasted residual learning and describe a novel network architecture, the broadcasting-residual network (BC-ResNet). Follow- , where x ? R c?h?w with number of channels c. Right, BC-ResBlock. The BC-ResNet block contains a frequency-depthwise convolution with a SubSpectralNorm. Then the feature is averaged by frequency followed by temporal-depthwise separable convolution. Temporal feature is broadcasted to 2D features at residual connection. In a transition block, we have an additional 1x1 convolution on the front to change the number of channel without identity shortcut.</p><p>ing, we introduce a family of models, BC-ResNets, by channel scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Broadcasted Residual Learning</head><p>A typical residual block <ref type="bibr" target="#b4">[5]</ref> can be expressed as y = x + f (x), where x and y are input and output features and function f computes the residuals. Here the identity shortcut x and residual f (x) are usually in same dimension and summed by simple addition. To utilize both 1D and 2D features together, we decompose the function f into f1 and f2 which are the temporal and 2D operations, respectively. We average the 2D features after f2 by frequency to get temporal features and expand the temporal feature back to the 2D shape after f1. We repeatedly do the averaging and expanding at each residual block and propose Broadcasted residual learning. Broadcasted residual learning employs a residual block of the form, y = x + BC(f1(avgpool(f2(x)))),</p><p>(1) as depicted in <ref type="figure" target="#fig_0">Figure 2</ref> left, where BC, the Broadcasting implies the expanding operation to frequency dimension, and avgpool is average pooling by frequency dimension. In this way, broadcasted residual learning expands and adds a residual information to bigger dimension of identity shortcut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BC-ResNet Block</head><p>The overall architecture is depicted in Figure 2 right. In equation 1, we ignore batch and channel dimensions for clarity, and the input feature x is in R h?w , where h and w correspond to the frequency and time dimensions, respectively. The 2D feature part, f2 consists of a 3x1 frequencydepthwise convolution and SubSpectral Normalization (SSN) <ref type="bibr" target="#b12">[13]</ref> which splits the input frequency into multi-groups to separately normalize them. Here we use SSN instead of Batchnorm (BN) <ref type="bibr" target="#b13">[14]</ref> to achieve frequency-aware temporal features. After averaging by frequency, we get features in R 1?w . The f1 is a composite of a 1x3 temporal depthwise convolution followed by BN, swish activation <ref type="bibr" target="#b14">[15]</ref>, 1x1 pointwise convolution, and channel-wise dropout of dropout rate p. The broadcasting (BC) operation expands the feature in R 1?w to R h?w . To be frequency convolution aware over the blocks, we add an auxiliary 2D residual connection from 2D features. In summary, the proposed BC-ResBlock becomes y = x + f2(x) + BC(f1(avgpool(f2(x)))).</p><p>(2) </p><formula xml:id="formula_0">Input Operator n c s d 1 ? 40 ? W conv2d 5x5 -BN -ReLU - 16 (2,1) 1 16 ? 20 ? W BC-ResBlock 2 8 1 1 8 ? 20 ? W BC-ResBlock 2 12 (2,1) (1,2) 12 ? 10 ? W BC-ResBlock 4 16 (2,1) (1,4) 16 ? 5 ? W BC-ResBlock 4 20 1 (1,8) 20 ? 5 ? W DWconv 5x5 - 20 1 1 20 ? 1 ? W conv2d 1x1 -BN -ReLU - 32 1 1 32 ? 1 ? W avgpool - - - - 32 ? 1 ? 1 conv2d 1x1 - 12 - -</formula><p>We also define a transition block (Where the number of In/Out channels are different) with two additional modifications; (a) add pointwise convolution where channel changing occurs followed by BN and ReLU activation. (b) No identity shortcut. Using the proposed block, we can achieve an efficient KWS design while keeping 2D features. In a tiny network, pointwise convolution takes the most computations <ref type="bibr" target="#b2">[3]</ref>. We perform the temporal depthwise and the pointwise convolution on temporal features and reduce their computing load by a factor of h compared to that of 2D depthwise separable convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network Architecture</head><p>We design the base model, BC-ResNet-1, with parameters less than 10k as shown in <ref type="table" target="#tab_0">Table 1</ref>. The model has a 5x5 convolution on the front for downsampling by frequency with a BN and a non-linearity and followed by a total of 12 BC-ResBlocks. We split the blocks into four stages which stand for a sequence of BC-ResBlocks whose activations are the same width. Inspired by <ref type="bibr" target="#b3">[4]</ref>, we explore the several choices and get the combination, 2, 2, 4, and 4 blocks for each stage, which implies that the model focuses more on performing high-level features. If the channel width c is different from the input width, the first block of a stage is a transition block as in <ref type="figure" target="#fig_0">Figure 2</ref> right. To do residual learning, we keep the frequency and temporal dimension by using zero-padding for each depthwise convolution. After the BC-ResBlocks, there is a 5x5 depthwise convolution without zero-padding in frequency dimension followed by a pointwise convolution that increases the number of channels before average pooling. Here we add the 5x5 depthwise convolution to reduce the computations of the pointwise convolution behind it.</p><p>Many CNN-based KWS approaches use dilated convolutions to achieve required receptive fields <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref>, and the proposed BC-ResNet also utilizes dilated convolutions. We empirically found that it is beneficial to keep the temporal dimension. Therefore we used stride s in the frequency direction and dilation d in the temporal dimension. Model Scaling Previous KWS works usually scale their models by changing depth and width together <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref> which makes it difficult to fit each computational or memory constraint. We explore compound <ref type="bibr" target="#b3">[4]</ref>, depth only, and width only scaling and decide to scale up the base model, BC-ResNet-1, by increasing the channel width ? times to get a BC-ResNet-? . Therefore the model is easy to scale with any predefined resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Works</head><p>Efficient CNN-based KWS While NASNet <ref type="bibr" target="#b15">[16]</ref> and Amoe-baNet <ref type="bibr" target="#b16">[17]</ref> introduced automatic ways to optimize networks, there are successful handcrafted CNN designs such as Mo-bileNets [1, 2] and ShuffleNet <ref type="bibr" target="#b2">[3]</ref> which utilize depthwise separable convolution, inverted bottleneck blocks, and channel shuffle. Inspired by the designs, there have been various CNNbased KWS approaches. <ref type="bibr" target="#b9">[10]</ref> uses residual learning and DS-ResNet <ref type="bibr" target="#b10">[11]</ref> adds depthwise separable convolutions upon <ref type="bibr" target="#b9">[10]</ref>. TC-ResNet <ref type="bibr" target="#b6">[7]</ref> uses temporal convolution and treats frequency dimension as channel for better efficiency and TENet <ref type="bibr" target="#b7">[8]</ref> and MatchBoxNet <ref type="bibr" target="#b8">[9]</ref> improve further with depthwise separable convolutions. While the approaches use either 1D or 2D convolution, the BC-ResNets suggest combining both.</p><p>Other Approaches There are other approaches that are not fully CNNs. Some of them use CNNs at the front and perform high-level features with recurrent neural networks (RNNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. MHAtt-RNN <ref type="bibr" target="#b19">[20]</ref> utilizes multi-head attention over it. On the other hand, there are automatic speech recognition based approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Note that the approaches are successful but typically are not efficient in terms of the number of parameters compared to CNN-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets We evaluate the performance of proposed BC-ResNets on Google speech commands datasets v1 and v2 <ref type="bibr" target="#b11">[12]</ref>. Version 1 contains 64,727 utterances from 1,881 speakers. There are total thirty words and we use ten classes of "Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop", and "Go" with two additional classes "Unknown Word (remaining twenty words)" and "Silence (no speech detected)" following the settings of <ref type="bibr" target="#b11">[12]</ref>. Version 2 has 105,829 utterances from 2,618 speakers. There are 35 words and split into 12 classes as version 1. Each utterance is 1 sec long, and the sampling rate is 16 kHz. We divide the dataset into training, validation and testing set based on the validation and testing file lists <ref type="bibr" target="#b11">[12]</ref>. We re-balance the "Unknown Word" and "Silence" with the average number of utterances in the remaining ten classes following common settings of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref> and especially we use the standard testing sets of v1 and v2 that the Google speech commands dataset offers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use input features of 40dimensional log Mel spectrograms with a 30ms window size and a 10ms frame shift. We followed the data augmentation settings of <ref type="bibr" target="#b9">[10]</ref>, time shift in the range of -100 to 100ms, background noise <ref type="bibr" target="#b11">[12]</ref> with the probability of 0.8, and SpecAugment <ref type="bibr" target="#b22">[23]</ref> with two time and two frequency masks except time warping. We also found it beneficial to provide stronger augmentation as the model capacity increases. In specific, the smallest model, BC-ResNet-1 does not use SpecAugment and BC-ResNet-{1.5, 2, 3, 6, 8} use SpecAugment with {1, 3, 5, 7, 7} of frequency mask parameters, respectively with fixed temporal mask parameter of 20 <ref type="bibr" target="#b22">[23]</ref>. Dropout rate is always p = 0.1. Also, we use SSN with five sub-bands <ref type="bibr" target="#b12">[13]</ref>. All models are trained for 200 epochs with stochastic gradient descent (SGD) optimizer with momentum to 0.9, weight decay to 0.001, minibatch size to 100, and a learning rate which linearly increases from zero to 0.1 over the first five epochs as warmup <ref type="bibr" target="#b23">[24]</ref> before decaying to zero with cosine annealing <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of Broadcasted Residual Learning</head><p>We compare BC-ResNet with fully 1D (ResNet-1D) and fully 2D (ResNet-2D) models to verify our method's effectiveness. These models consist of residual blocks with depthwise separable convolution instead of BC-ResBlock while maintaining the basic network architecture of BC-ResNet. ResNet-2D uses depthwise separable convolution with a 3x3 kernel, and ResNet-1D uses a 1x3 kernel. ResNet-1D requires a 1.1M multiply-accumulate (MAC) operation at the same width, which is smaller than BC-ResNet-1, so we scaled up the model by doubling its width for a fair comparison. <ref type="table" target="#tab_1">Table 2</ref> shows the comparison results of these baselines and our method.</p><p>ResNet-1D has about three times more parameters, but it is still more than 1% less accurate than our method. ResNet-2D has about 16% fewer parameters than BC-ResNet by using batch normalization (BN) between depthwise convolution and pointwise convolution, and it requires a higher amount of computation due to more 2D operations. This 2D model shows about 2% lower accuracy than our method. When we apply SSN to ResNet-2D instead of BN, ResNet-2D w/ SSN, the model size is similar to BC-ResNet-1, and it can obtain 0.7% accuracy improvement without an increase in computation. This result shows that we can effectively apply SSN even in 2D CNNs. However, BC-ResNet still outperforms these baselines by a large margin. As shown in this result, broadcasted residual learning reduces the computation and can represent more discriminative information in keyword spotting. BC-ResNet-Attn  <ref type="table" target="#tab_2">Table 3</ref>.</p><p>denotes the model using attention instead of broadcasted residual mapping like <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Sigmoid is applied to f1 of equation 2, then element-wise multiplication is performed. It can be considered temporal-channel attention since the output of f1 is a temporal feature. This model performs better than 1D and 2D baselines, but BC-ResNet is still more than 0.6% accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study of BC-ResBlock</head><p>Broadcasted residual learning plays a crucial role in BC-ResNet, but BC-ResBlock consists of other core components; auxiliary 2D residual connection and SSN for frequency awareness of BC-ResBlocks. We evaluate how these components affect the model's performance. BC-ResNet without the auxiliary 2D residual shows accurate results. However, with this additional connection, BC-ResNet can obtain about 0.4% performance improvement without increasing the model size. And the shortcut connection of the identity also contributes slightly to performance.</p><p>'w/o SSN' shows the result when BN is used in BC-ResBlock instead of SSN, which eliminates inter-frequency deflection and provides frequency awareness. We can reduce some parameters by removing the SSN, but a more than 0.4% accuracy drop occurs. We also compare the model that uses the base channel's size 9, 'w/o SSN (w=1.125)', to compensate for the loss in parameters due to BN. This model requires about 20% more computation but still has lower performance than BC-ResNet. 'w/o 2D residual and SSN' is the model without using both the 2D residual and SSN in BC-ResBlock. These two components help reduce information loss due to eliminating the frequency dimension. Removing all of them increases the error more significantly than when removing them one by one. These results show that the two components play an essential role in broadcasted residual learning. We also evaluate the model,'w/ Freq MaxPool,' which uses max-pooling as a frequency dimension reduction method instead of average pooling. The model records a slightly lower accuracy than BC-ResNet-1. However, it still shows much higher accuracy than fully 1D and 2D models. It means that broadcasted residual learning can be effective with other frequency dimension reduction functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Baseline</head><p>We compare the efficiency of KWS models in two aspects: performance per parameter and performance per MAC. In <ref type="figure">Figure 1</ref>, BC-ResNets are consistently efficient than other approaches in terms of accuracy per parameter on Google speech command dataset v1. We also compare MAC-accuracy curves on the datasets v2 as depicted in <ref type="figure" target="#fig_1">Figure 3</ref>. BC-ResNets achieve higher accuracy than 1D-based MatchboxNets <ref type="bibr" target="#b8">[9]</ref> and state-of-the-art MHAtt-RNN <ref type="bibr" target="#b19">[20]</ref> by a large margin while doing smaller computations. In the case of MatchboxNet, it shows better efficiency with dataset v1 compared to its v2 results as in <ref type="table" target="#tab_2">Table 3</ref>, but still, BC-ResNets require x2.6 smaller number of parameters while achieving higher accuracy. The details of the figures are in <ref type="table" target="#tab_2">Table 3</ref>. We get the average performances of BC-ResNets in 10 random seeds each. The smallest model, BC-ResNet-1 matches the performance of 1D convolution-based approaches, TC-ResNet14-1.5 <ref type="bibr" target="#b6">[7]</ref> and TeNet12 <ref type="bibr" target="#b7">[8]</ref> with x10.9 smaller number of parameters while using 1D approach-level number of multiplies. BC-ResNet-3 works better than the state-of-the-art method, MHAtt-RNN <ref type="bibr" target="#b19">[20]</ref> with x13.7 smaller number of parameters. The biggest model achieves the new state-of-the-art accuracy, 98.0 % and 98.7 % on Google speech command dataset v1 and v2 respectively, and is still x2.3 smaller than MHAtt-RNN <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Existing CNN-based KWS approaches usually process all features by 1D or 2D convolutions with pros and cons. Using 1D convolution enables efficient design in terms of both the number of parameters and amount of computation, but it lacks the characteristics such as translation equivariance in the frequency direction. On the other hand, 2D convolution requires more computing compared to 1D approaches. To address the issues, we propose broadcasted residual learning that allows 1D and 2D features together. Broadcasted residual learning repeatedly averages 2D features to 1D features and expands 1D features back to the 2D. Leveraging the broadcasted residual learning and simple scaling by width, we design a family of networks called BC-ResNets and surpass state-of-the-art on Google speech command dataset v1 and v2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left, Broadcasted Residual Learning described in Equation 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>MACs vs. Google speech command dataset v2 Accuracy. Details are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[1] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, "Mobilenets: Efficient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>BC-ResNet-1. Each row is a sequence of one or more identical modules repeated n times with input shape of channel?frequency?time, total time steps W , and the number of output channels c. Changes in number of channels and down- sampling by stride s belong to the first block of each sequence of BC-ResBlocks. The temporal convolutions in all BC-ResBlocks use dilation of d.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Impact of Broadcasted Residual Learning and Ablation Study. We demonstrate how each component affects the base model, BC-ResNet-1, on Google speech command datasets v1 and v2. We show mean and standard deviation of Top-1 test accuracy (%). (averaged over 5 seeds).</figDesc><table><row><cell>Model</cell><cell>v1</cell><cell>v2</cell><cell cols="2">#Param #Mult</cell></row><row><cell>ResNet-1D (w=2)</cell><cell cols="2">95.0 ? 0.25 95.7 ? 0.46</cell><cell>27.3k</cell><cell>3.2M</cell></row><row><cell>ResNet-2D (w=1)</cell><cell cols="2">94.8 ? 0.42 94.9 ? 0.32</cell><cell>7.9k</cell><cell>5.9M</cell></row><row><cell>ResNet-2D (w=1) w/ SSN</cell><cell cols="2">95.5 ? 0.22 95.6 ? 0.24</cell><cell>9.4k</cell><cell>5.9M</cell></row><row><cell>BC-ResNet-Attn</cell><cell cols="2">96.0 ? 0.14 96.2 ? 0.24</cell><cell>9.2k</cell><cell>3.1M</cell></row><row><cell>BC-ResNet-1</cell><cell cols="2">96.6 ? 0.21 96.9 ? 0.30</cell><cell>9.2k</cell><cell>3.1M</cell></row><row><cell cols="3">w/o auxiliary 2D residual 96.2 ? 0.20 96.5 ? 0.10</cell><cell>9.2k</cell><cell>3.1M</cell></row><row><cell>w/o shortcut</cell><cell cols="2">96.4 ? 0.34 96.8 ? 0.18</cell><cell>9.2k</cell><cell>3.1M</cell></row><row><cell>w/o SSN</cell><cell cols="2">96.1 ? 0.11 96.5 ? 0.12</cell><cell>7.8k</cell><cell>3.1M</cell></row><row><cell>w/o SSN (w=1.125)</cell><cell cols="2">96.2 ? 0.26 96.7 ? 0.12</cell><cell>9.1k</cell><cell>3.7M</cell></row><row><cell cols="3">w/o 2D residual and SSN 95.4 ? 0.29 95.7 ? 0.32</cell><cell>7.9k</cell><cell>3.1M</cell></row><row><cell>w/ Freq MaxPool</cell><cell cols="2">96.3 ? 0.25 96.8 ? 0.11</cell><cell>9.2k</cell><cell>3.1M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>BC-ResNets Top-1 test accuracy (%) on Google speech command datasets v1 and v2. Each BC-ResNet is scaled up with a coefficient of ? in Section 4.3 and KWS approaches are grouped by accuracy for easier comparison.</figDesc><table><row><cell>Model</cell><cell>v1</cell><cell>v2</cell><cell>#Param</cell><cell>#Mult</cell></row><row><cell>Att-RNN [18]</cell><cell cols="2">95.6 96.9</cell><cell>202k</cell><cell>22.3M</cell></row><row><cell>ResNet-15 [10]</cell><cell>95.8</cell><cell>-</cell><cell>238k</cell><cell>894M</cell></row><row><cell>DS-ResNet14 [11]</cell><cell>95.9</cell><cell>-</cell><cell>15.2k</cell><cell>15.7M</cell></row><row><cell>TC-ResNet14-1.5 [7]</cell><cell>96.6</cell><cell>-</cell><cell>305k</cell><cell>6.7M</cell></row><row><cell>TENet12 [8]</cell><cell>96.6</cell><cell>-</cell><cell>100k</cell><cell>2.9M</cell></row><row><cell>BC-ResNet-1</cell><cell cols="2">96.6 96.9</cell><cell>9.2k</cell><cell>3.1M</cell></row><row><cell>DS-ResNet18 [11]</cell><cell>96.7</cell><cell>-</cell><cell>72 k</cell><cell>285 M</cell></row><row><cell>BC-ResNet-1.5</cell><cell cols="2">97.2 97.6</cell><cell>17.2k</cell><cell>5.5M</cell></row><row><cell cols="3">MatchboxNet-3x1x64 [9] 97.2 96.9</cell><cell>77k</cell><cell>9.3M</cell></row><row><cell>Embedding+head [29]</cell><cell>-</cell><cell>97.7</cell><cell>385k</cell><cell>-</cell></row><row><cell>BC-ResNet-2</cell><cell cols="2">97.3 97.8</cell><cell>27.3k</cell><cell>8.5M</cell></row><row><cell cols="3">MatchboxNet-3x2x64 [9] 97.5 97.2</cell><cell>93k</cell><cell>11.3M</cell></row><row><cell>MatchboxNet-6x2x64 [9]</cell><cell>-</cell><cell>97.4</cell><cell>140k</cell><cell>17.1M</cell></row><row><cell>MHAtt-RNN [20]</cell><cell cols="2">97.2 98.0</cell><cell>743k</cell><cell>22.7M</cell></row><row><cell>BC-ResNet-3</cell><cell cols="2">97.6 98.2</cell><cell>54.2k</cell><cell>16.2M</cell></row><row><cell>BC-ResNet-6</cell><cell cols="2">97.9 98.6</cell><cell>188k</cell><cell>53.1M</cell></row><row><cell>BC-ResNet-8</cell><cell cols="2">98.0 98.7</cell><cell>321k</cell><cell>89.1M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal convolution for real-time keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3372" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting with multi-scale temporal convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA, 2020</title>
		<imprint>
			<biblScope unit="page" from="1987" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matchboxnet: 1d time-channel separable convolutional neural network architecture for speech commands recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA, 2020</title>
		<imprint>
			<biblScope unit="page" from="3356" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5484" to="5488" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depthwise separable convolutional resnet with squeeze-and-excitation blocks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA, 2020</title>
		<imprint>
			<biblScope unit="page" from="2547" to="2551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Subspectral normalization for neural audio data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="850" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI. AAAI Press</publisher>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L D S</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno>abs/1808.08929</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Orthogonality constrained multi-head attention for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASRU</title>
		<imprint>
			<biblScope unit="page" from="86" to="92" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Streaming keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laurenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA, 2020</title>
		<imprint>
			<biblScope unit="page" from="2277" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unrestricted vocabulary keyword spotting using LSTM-CTC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="938" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Query-byexample on-device keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU. IEEE</publisher>
			<biblScope unit="page" from="532" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Urnet: User-resizable residual networks with conditional gating module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4569" to="4576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Broadcasting convolutional network for visual relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training keyword spotters with limited and synthesized speech data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7474" to="7478" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
