<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Star algorithm for NN ensembling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zinchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Novosibirsk State University Russian Federation</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lishudi</surname></persName>
							<email>dlishudi@hse.ru</email>
							<affiliation key="aff1">
								<orgName type="institution">HSE University</orgName>
								<address>
									<settlement>Russian Federation</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Star algorithm for NN ensembling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network ensembling is a common and robust way to increase model efficiency. In this paper, we propose a new neural network ensemble algorithm based on Audibert's empirical star algorithm. We provide optimal theoretical minimax bound on the excess squared risk. Additionally, we empirically study this algorithm on regression and classification tasks and compare it to most popular ensembling methods.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been successfully applied to many types of problems and has reached the stateof-the-art performance. Deep learning models have shown good results in regression analysis and time series forecasting <ref type="bibr" target="#b14">[Qiu+14]</ref>, computer vision <ref type="bibr" target="#b17">[He+16]</ref>, as well as in natural language processing <ref type="bibr" target="#b27">[OMK20]</ref> and other areas. In many complex problems, such as the Imagenet competition <ref type="bibr" target="#b8">[Den+09]</ref>, the best results are achieved by ensembles of neural networks, that is, it is often useful to combine the predictions of multiple neural networks to create a new one. The easiest way to ensemble multiple neural networks is to average their predictions <ref type="bibr" target="#b1">[Dru+94]</ref>. As shown in work <ref type="bibr" target="#b19">[Kaw16]</ref>, the number of local minima grows exponentially with the number of parameters. And since modern neural network training methods are based on stochastic optimization, two identical architectures optimized with different initializations will probably converge to different solutions. Such a technique for obtaining neural networks with subsequent construction of an ensemble by majority voting or averaging is used, for example, in article <ref type="bibr" target="#b5">[Car+04]</ref>.</p><p>In addition to the fact that the class of deep neural networks has a huge number of local minima, it is also non-convex. It was shown in work <ref type="bibr" target="#b9">[LM09]</ref> that for the procedure of minimizing the empirical risk in a non-convex class of functions, the order of convergence is not optimal. In fact, most modern neural network training methods do just that: they minimize the mean value of some error function on the training set. J.-Y. Audibert proposed the star procedure method, which has optimal rate of convergence of excess squared risk <ref type="bibr" target="#b7">[Aud07]</ref>. Motivated by this observation and the huge success of ensembles of neural networks, we propose a modification of the star procedure that will combine the advantages of both methods. In short, the procedure we propose can be described as follows: we run d independent learning processes of neural networks, obtaining empirical risk minimizers g 1 , . . . , g d , freeze their weights, then we initialize a new model and connect all d + 1 models with a layer of convex coefficients, after that we start the process of optimizing all non-frozen parameters. This whole procedure can be viewed as a search for an empirical minimizer in all possible d-dimensional simplices spanned by d-minimizers and a class of neural networks. As is known, the minimization of the empirical risk with respect to the convex hull is not optimal in the same way as with respect to the original class of functions. Our method, however, minimizes over some set intermediate between the original class of functions and its convex hull, allowing us to combine the advantages of model ensembling and the star procedure.</p><p>One can look at this procedure as a new way to train one large neural network with a block architecture, as well as a new way of aggregating models. In this work, we carry out a theoretical analysis of the behavior of the proposed algorithm for solving the regression problem with a class of sparse neural networks, and also check the operation of the algorithm in numerical experiments on classification and regression problems.</p><p>In addition to this, we take into account that it is impossible to achieve a global minimum in the class of neural networks, and we consider the situation of imprecise minimization.</p><p>The main results of our work can be formulated as follows:</p><p>? A multidimensional modification of the star procedure is proposed. ? We prove that the resulting estimate satisfies the exact oracle inequality. It follows from this estimate that the order of convergence of the algorithm (in terms of sample size n) for a fixed neural network architecture is optimal. Our results improve over the imprecise oracle inequality in <ref type="bibr" target="#b28">[Sch20]</ref>. ? We give an upper bound on the generalization error for the case of approximate empirical risk minimizers, which implies the stability of our algorithm against minimization errors. ? Based on our algorithm, we propose a new method for training block architecture neural networks, which is quite universal in terms of procedures. We also propose a new way to solve the aggregation problem. ? We illustrate the efficiency of our approach with numerical experiments on real-world datasets.</p><p>The rest of this paper is organized as follows.</p><p>In Section 2, we make an overview of neural network ensembling methods and briefly discuss the advantages of the star algorithm. In Section 3, following the Schmidt-Hieber notation <ref type="bibr" target="#b28">[Sch20]</ref>, we define a class of sparse fully connected neural networks and formulate a number of statements from which it follows that the algorithm we proposed has a fast rate of convergence. All proofs are attached in additional materials. In Section 4, we discuss the implementation of our algorithm, point out a number of possible problems, and suggest several modifications to fix them. It also describes the conditions for conducting numerical experiments and presents some of their results. At the end, we offer two possible views on our procedure: a new way to train block neural networks and a fairly flexible model aggregation procedure.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ensemble strategies</head><p>The main idea of the ensemble is to train several predictors and build a good metamodel on them. There are many techniques for its construction. We present some of them. A more detailed review can be found in the work <ref type="bibr" target="#b30">[Gan+21]</ref>.</p><p>Bagging.</p><p>The first of two stages is the generation of several samples with the same distribution as the training one. The next stage is training multiple models and aggregate their predictions. There are cases when the predictions of the constructed models are transferred to another model as new features <ref type="bibr" target="#b3">[Kim+02]</ref>. But still, most often, aggregation is performed either by majority voting or by averaging. For example, Kyoungnam Ha et al. showed that the ensemble method for perceptrons based on bagging works better than the individual neural network <ref type="bibr" target="#b6">[HCM05]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boosting</head><p>Another approach to construct ensembles is boosting. The idea is to build one strong model from several weak models by stepwise additive modeling. It was first applied to random trees to construct a so-called random forest. But it has also been applied to deep learning models as well. For example, in the task of recognizing facial expressions <ref type="bibr" target="#b13">[Liu+14]</ref>, or to improve the predictions of convolutional neural networks <ref type="bibr" target="#b21">[Mog+16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Snapshots</head><p>The main problem in aggregation of deep learning models is the cost of training. Training even one modern model requires a lot of resources, and the ensemble needs a lot. A snapshot technique <ref type="bibr" target="#b23">[Hua+17]</ref> and their modification <ref type="bibr" target="#b26">[Gar+18]</ref> have been created to combat this problem. In short, during the learning process, the step length in the gradient descent is cyclically changed. This allows a learner to get into various local minima (parameters of which are stored for subsequent aggregation) and, as a consequence, to build an ensemble using a computational budget comparable with the cost of training one model.</p><p>Later, Zhang et al. tried combining this idea together with boosting in <ref type="bibr" target="#b29">[Zha+20]</ref>. The novelty of this work is that the difference between the models is achieved by re-weighting the objects from the training sample, the step length parameter is changed adaptively, and for aggregation, the best model from the entire training cycle is taken, and not the last one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implicit ensembles</head><p>In this approach a single model is trained in such a way as to behave like an ensemble. But it requires a much smaller computational budget for training. This is achieved due to the fact that in implicit ensembles the parameters of the models are shared, and their averaging is returned as predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Star algorithm</head><p>Unlike the ensemble problem, the aggregation problem focuses on building a good predictor in a situation where there are already several ready-made models. The reader is referred to [Nem00; Tsy03] for different types of aggregation. It is important to mention that, in contrast to the two-stage star procedure of Audibert <ref type="bibr" target="#b7">[Aud07]</ref>, the usual empirical risk minimization procedure among the class of known predictors (or their convex hull) does not necessarily lead to the optimal rate of convergence <ref type="bibr" target="#b9">[LM09]</ref>. This result was further developed in <ref type="bibr" target="#b16">[LRS15]</ref>, where the authors extend the theoretical analysis of the star algorithm to the case of infinite classes using the offset Rademacher's complexity technique. It was also shown in the [Vij21] that these results can be generalized to other loss functions.</p><p>In particular, this means that the star procedure can be applied to more than just regression problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical results</head><p>In this chapter, we will formulate our theoretical results for solving the regression problem with a class of sparse fully connected neural networks using our proposed Star d procedure.</p><p>We</p><formula xml:id="formula_0">have a S n = (X i ,Y i ) n i=1 sample of i.i.d. input-output pairs (X i ,Y i ) ? X ?Y distributed</formula><p>according to some unknown distribution P. We also chose a certain family of solutions F. Our goal is to build a new predictor f minimizing the excess risk</p><formula xml:id="formula_1">E( g) := E( g ? Y ) 2 ? inf f ?F E(f ? Y ) 2 .</formula><p>Let E denote the empirical expectation operator</p><formula xml:id="formula_2">E(f ) := 1 n n i=1 f (X i )</formula><p>and call g ? F a ?-empirical risk minimizer in F if the following inequality holds</p><formula xml:id="formula_3">E( g ? Y ) 2 ? min f ?F E(f ? Y ) 2 + ?.</formula><p>We suggest the next two step procedure. In the first, calculate g i d i=1 -different ? 1 -empirical risk minimizers in F. And then look for a ? 2 -empirical risk minizer in the next set:</p><formula xml:id="formula_4">Star d F, g 1 , . . . g d := d i=1 ? i g i + 1 ? d i=1 ? i ? f ? i , ? ? [0, 1]; f ? F .</formula><p>(1)</p><p>We will call the found function f = f (F, d, ? 1 ,? 2 ) as Star d estimator. The main result of our work is the proof that the proposed estimator has an optimal excess risk convergence rate in the case when F is a class of fully connected neural networks. It is defined by the choice of the activation function ? : R ? R and the network architecture. We study neural network with activation function ReLu:</p><formula xml:id="formula_5">?(x) := max(x,0)</formula><p>.</p><formula xml:id="formula_6">For v = (v 1 , . . . ,v r ) ? R r define shifted activation function ? v : R r ? R r : ? v (x) := ?(x i ? v i ) r i=1 .</formula><p>The network architecture (L,p) consists of a positive integer L called the number of hidden layers or depth and a width vector p = (p 0 , . . . ,p L+1 ) ? N L+2 . A neural network with network architecture (L, p) is then any function of the form</p><formula xml:id="formula_7">f (x) = W L ? v L W L?1 ? v L?1 . . . W 1 ? v1 W 0 x,<label>(2)</label></formula><p>where W j is a p j+1 ? p j matrix and v i ? R pi is a shift vector.</p><p>We will focus on the case when the model parameters satisfy some constraint. Denote W j ? the maximum-entry norm of W j , W j 0 the number of non-zero/active entries of W j then the space of network functions with given network architecture and network parameters bounded by one is</p><formula xml:id="formula_8">F(L, p) := f of the form (2) : max j=0,...,L W j ? ? v j ? ? 1</formula><p>and the s-sparse networks are given by</p><formula xml:id="formula_9">F(L, p, s) := f ? F(L, p) : L j=0 W j 0 + v j 0 ? s .</formula><p>Let's denote by N ? (F, ?), N 2 (F, ?) the size of the ?-net of F in the metric space L ? and L 2 , respectively. Then from Lemma 5 in <ref type="bibr" target="#b28">[Sch20]</ref> we have</p><formula xml:id="formula_10">?f ? F(L, p, s) : f ? ? V (L + 1)<label>(3)</label></formula><p>and</p><formula xml:id="formula_11">log N 2 (F(L, p, s),?) ? log N ? (F(L, p, s),?) ? (s + 1) log(2? ?1 (L + 1)V 2 ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_12">V := L+1 l=0 (p l + 1).<label>(5)</label></formula><p>Also let define the risk-minimizer in F and some sets:</p><formula xml:id="formula_13">Hull d F := d i=1 ? i (f i ? f ) ? i ? [0, 1]; d i=1 ? i ? 1; f, f 1 . . . f d ? F ,<label>(6)</label></formula><formula xml:id="formula_14">f * := arg min f ?F E(f (X) ? Y ) 2 , H := F ? f * + Hull d (F).<label>(7)</label></formula><p>Notice, that Star d estimator f lies in H + f * . With the introduced notation, one of our main results is stated as follows.</p><p>Theorem 3.1. Let f be a Star d estimator and H be the set defined in 7 for F = F(L, p, s). The following expectation bound on excess loss holds:</p><formula xml:id="formula_15">E E(f ) ? 2(F + V (L + 1)) ? K n + M ? c A.8 d s log V L n d n + 4[? 1 + ? 2 ], where C = min c A.2 4F , c A.2 4V (L + 1)(2 + c A.2 ) , F = sup F |Y ? f | ? , K := 2 ? ? n i=1 ? 2 /n + C ? ? , M := sup h?H\{0} 4 n i=1 h(X i ) 2 ? 2 i C n i=1 h(X i ) 2 .<label>(8)</label></formula><p>Thus, the expectation of excess risk for Star d estimator is limited for a fixed architecture neural network to a value of the order</p><formula xml:id="formula_16">O s d log(V L n) n + ? 1 + ? 2 .</formula><p>In order to formulate an upper bound for the excess risk, performed with a high probability, we need to impose some constraints on the class of functions. Definition 3.2 (Lower Isometry Bound). Class F satisfies the lower isometry bound with some parameters 0 &lt; ? &lt; 1 and</p><formula xml:id="formula_17">0 &lt; ? &lt; 1 if P inf f ?F \{0} 1 n n i=1 f 2 (X i ) E f 2 ? 1 ? ? ? 1 ? ? for all n ? n 0 (F,?, ?),</formula><p>where n 0 (F,?, ?) depends on the complexity of the class.</p><p>Now we are ready to formulate the main theoretical result of the paper. </p><formula xml:id="formula_18">Let ? i = Y i ? f * (X i ). Define A := sup h?H E h 4 (E h 2 ) 2 and B := sup X,Y E ? 4 .</formula><p>Then there exist 3 absolute constants c A.10 ,c A.10 , c A.10 &gt; 0 (which only depend on c A.2 ), such that</p><formula xml:id="formula_19">P E( f ) &gt; 4(D + ? 1 + ? 2 ) ? 4(? lib + ?)</formula><p>as long as n &gt;</p><formula xml:id="formula_20">16(1?c A.10 ) 2 A c 2 A.10 ? n 0 (H, ? lib , c A.10 /4), where K := ? ? n i=1 ? 2 /n + 2c A.10 ? ? , M := sup h?H\{0} n i=1 h(X i ) 2 ? 2 ? c A.10 n i=1 h(X i ) 2 , D := max K n + M ? c A.8 d s log V L n d + log 1 ? n , 32 ? AB c A.10 1 n and c A.8 is an independent constant.</formula><p>That is, with some assumptions on the class of neural networks F, we again obtained the order O log nN n of convergence of the excess risk. Note that in the general case for an infinite class functions such an asymptotic rate with respect to the sample size n is unimprovable (for example, see [RST17, Theorem 3 and Section 5]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Realization</head><p>Algorithm 1: Star d algorithm Data: class of functions F and parameter d Result: estimator f g 1 ? calculate_erm(set = F, seed = seed 1 );</p><formula xml:id="formula_21">. . . g d ? calculate_erm(set = F, seed = seed d ); f ? calculate_erm(set = Star d , seed = seed); return f</formula><p>The proposed Star d procedure can be represented by the following pseudocode (see Algorithm 1). Since in practice we will not be able to give an infinite number of predictors, in fact the algorithm takes the architecture of the neural network as F, without fixing its parameters. The calculate_erm procedure is some optimization process that reduces the empirical risk. Also, in practice, it is impossible to search for a global empirical risk minimizer in the space of neural networks, which is why we introduced the concept of ?-minimizers. As follows from our results, the more accurate the optimization is at each step of the algorithm, the higher the accuracy guarantee of the final predictor f . Despite the fact that the second step of the star algorithm requires an optimization procedure over some complex set Star d , this is fairly easy to implement in practice (see <ref type="figure" target="#fig_1">Figure 1</ref>). Suppose that we have fixed some architecture of estimator (black block), then in the first step we independently optimize the weights of the blocks g i , freeze them and in the second step we add a new block f , connecting all of them by convex layer ? i (red elements) and optimize them. This actually iterates over all possible simplices, optimizing the weights of the lower block, and all possible points within the simplex, optimizing the convex weights. To ensure the convexity coefficients in practice we took softmax from the parameter vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>Boston House Pricing We conduct the first experiment on the Boston House Pricing dataset <ref type="bibr" target="#b0">[HR78]</ref>, whose task is to predict the value of real estate according to some characteristics. The ratio of training and test samples is equal to 7 : 3. Standard scaler was used as preprocessing, batch size is 32. A small fully connected ReLu neural network of 4 layers was chosen as the architecture of the neural network, the number of neurons on the first layer is 128, then with the growth of the layer it decreases by 2 times, DropOut with parameter p and batch normalization are applied between the layers. For warming up a new model, 40% of the budget for block training is spent. Training was done using Adam <ref type="bibr" target="#b12">[KB14]</ref> on the CPU Apple M1. Error results on the test set were averaged over 5 runs. Some of the data obtained in the course of numerical experiments we present in the <ref type="table">Table 1</ref>. Hereinafter, we highlight in bold the best results with a fixed parameter of the number of models d.</p><p>It is worth adding that without the use of the DropOut technique, the usual Ensemble with some parameters proved to be quite good (e.g. 10.236 MSE after 100 epochs with d = 5, p = 0.0, lr = 0.01). In total, about 100 different sets of parameters were tried on this launch, we attach part of the results in Appendix B, and the full versions can be found in the repository. Fashion Mnist The second experiment was carried out on the Fashion Mnist dataset <ref type="bibr" target="#b25">[XRV17]</ref>, which consists of 70,000 images (28 ? 28 pixels). It is required to classify images by clothing classes. The ratio of training and test samples is equal to 6 : 1. No scaler is used, batch size is 64. A simple convolutional network LeNet was chosen as a solution to this task. For warming up a new model, 40% of epochs is again spent. Training was done using Adam on the CPU Apple M1. Error results on the test set were averaged over 3 runs.</p><p>As can be seen from the results (see <ref type="table">Table 2</ref>), the algorithm proposed by us is again in the lead. We also did one relatively hard run with parameters d = 5 and 25 epochs for train, the Classic Star (new warm-up) won with 92.2% accuracy. At the moment, they take 10 ? 11th place in the leaderboard 3 for this dataset. For comparison, the model from the 10th place has more than 380K parameters. It is worth noting that we used fairly simple models (about 266K parameters) and training took about 40 minutes on a regular laptop. The results of training by our method also exceed the indicators Resnet18 4 with over 2M parameters and 92% accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">General patterns</head><p>From the obtained results it follows that the best results are achieved by the proposed algorithm with the warm-up of the last model. We believe that the reason for this is that, with a weakly trained last model, it is easier for the algorithm to nullify the contribution of the last model and focus on the predictions of well-trained d estimators. Although results of star algorithm with snapshot technique are mediocre for high number of epochs, it shows quite good results for smaller number of epochs (see Tables with a small number of epochs in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and limitations</head><p>The proposed algorithm performs well in the classification problem with cross-entropy loss, although this paper only presents a theoretical analysis of regression with squared loss. The practical implementation of the algorithm can be sensitive to the initialization of the last block, as shown by the warm-up method. And experiments on larger models were not carried out due to high computational costs. Some solutions to these problems are offered in Section 4.2.</p><p>In fact, the star estimator we proposed is a multidimensional analogue of the Audibert's algorithm. It combines optimal orders as a solution to the aggregation problem of model selection, and at the same time behaves like an ensemble method. This decision can be viewed from two sides at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Star d algorithm as a new learning algorithm for neural networks of block architecture</head><p>It is worth noting that if we spend a fixed amount of computing resources B for each call to the optimization process calculate_erm, then the total budget of our algorithm is about (d + 1) ? B. But the surprising fact is that the result obtained is able to compete with other methods for training the final large neural network from d + 1 blocks, although our theoretical analysis guarantees optimality only in comparison with the best single block architecture model. Thus, the procedure we proposed can be perceived as a new method for training neural networks with block architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Star d algorithm as a new way of model aggregation</head><p>Also note that the predictors g i need not be trained in the first step. Then the Star d algorithm can be perceived as an algorithm for aggregating these models. It will consist of the following: a new predictive model f is added, a connecting layer, and the process of optimization by a parameter is started. At the same time, generally speaking, it is not necessary to have all blocks be of the same architecture. As intuition suggests, the main thing is that the expressive abilities of those classes of solutions to which the predictors given to us will relate should be approximately equal. Then it will be possible to formally consider the union of those decision classes to which each of the predictors belongs, and consider them as ?-minimizers from the following class F = i F i , where</p><p>given predictors g i ? F i (which may be heterogeneous).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>We are grateful to Nikita Puchkin for essential comments and productive discussions, and also to Alexander Trushin for help with the design of the work. The article was prepared within the framework of the HSE University Basic Research Program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>The combination of the following 2 Lemmas is a generalization of the geometric inequality proved by Liang et al. <ref type="bibr" target="#b16">[LRS15]</ref>. In many respects the scheme of the proof is similar.</p><p>Lemma A.1. (Geometric inequality for the exact Star d estimator in the second step) Let? 1 . . .? d be ? 1 -empirical risk minimizers from the first step of the Star d procedure, f be the exact minimizer from the second step of the Star d procedure. Then, for c A.1 = 1 18 the following inequality holds:</p><formula xml:id="formula_22">E(h ? Y ) 2 ? E( f ? Y ) 2 ? c A.1 E( f ? h) 2 ? 2? 1 .<label>(9)</label></formula><p>Proof. For any function f, g we denote the empirical 2 distance to be f n := E f 2 1 2</p><p>, empirical product to be f, g n := E [f g] and the square of the empirical distance between F and Y as r 1 . By definition of Star d estimator for some ? ? [0; 1] we have:</p><formula xml:id="formula_23">f = (1 ? ?) g + ?f,</formula><p>where g lies in a convex hull of ? 1 -empirical risk minimizers</p><formula xml:id="formula_24">{ g i } d i=1 . Denote the balls centered at Y to be B 1 := B(Y, ? r 1 ), B 1 := B(Y, g ? Y n ) and B 2 := B(Y, f ? Y n ).</formula><p>The corresponding spheres will be called S 1 , S 1 , S 2 . We have B 2 ? B 1 and B 2 ? B 1 . Denote by C the conic hull of B 2 with origin g and define the spherical cap outside the cone C to be S = S 1 \ C.</p><p>First, f ? B 2 and it is a contact point of C and S 2 . Indeed, f is necessarily on a line segment between g and a point outside B 1 that does not pass through the interior of B 2 by optimality of f . Let K be the set of all contact points of C and S 2 -potential locations of f .</p><p>Second, for any h ? F, we have h ? Y n ? ? r 1 i.e. any h ? F is not in the interior of B 1 .</p><p>Furthermore, let C be bounded subset cone C cut at K.</p><formula xml:id="formula_25">Thus h ? (intC) c ? (B 1 ) c or h ? T , where T := (intC ) ? (B 1 ) c .</formula><p>For any h ? F consider the two dimensional plane L that passes through three points?, Y, h, depicted in <ref type="figure" target="#fig_2">Figure 2</ref>. Observe that the left-hand side of the desired inequality (9) is constant as f ranges over K. The maximization of h ? f 2 n over f ? K is achieved by f ? K ? L. Hence, to prove the desired inequality, we can restrict our attention to the plane L and f . Let h ? be the projection of h onto the shell L ? S 1 . By the geometry of the cone and triangle inequality we have:</p><formula xml:id="formula_26">f ? g n ? 1 2 g ? h ? n ? 1 2 ( f ? h ? n ? f ? g n ) ,</formula><p>and, hence, f ? g n ? f ? h ? n /3. By the Pythagorean theorem,</p><formula xml:id="formula_27">h ? ? Y 2 n ? f ? Y 2 n = g ? Y 2 n ? f ? Y 2 n = f ? g 2 n ? 1 9 f ? h ? 2 n .</formula><p>We can now extend this claim to h. Indeed, due to the geometry of the projection h ? h ? and the fact that h ? (intC) c ? (intB 1 ) c or h ? T there are 2 possibilities:</p><formula xml:id="formula_28">a) h ? (B 1 ) c . Then h ? ? Y, h ? ? h n ? 0; b) h ? B 1 . Then, since h ? (B 1 ) c , we have h ? ? Y, h ? ? h n ? h ? Y + h ? h ? h ? h ? ? h ? ? Y 2 n ? h ? Y 2 n ? ? 1 .</formula><p>In both cases, the following inequality is true </p><formula xml:id="formula_29">h ? Y 2 n ? f ? Y 2 n = h ? ? h 2 n ? 2 h ? ? Y, h ? ? h n + ( h ? ? Y 2 n ? f ? Y 2 n ) ? h ? ? h 2 n ? 2? 1 + 1 9 f ? Y 2 n ? 1 18 f ? h 2 n ? 2? 1 .</formula><formula xml:id="formula_30">E(h ? Y ) 2 ? E( f ? Y ) 2 ? c A.2 E( f ? h) 2 ? 2(1 + c A.2 )[? 1 + ? 2 ].</formula><p>Combining everything together, we get the required result for the constant c A.2 = c A.1 2 = 1 36 :</p><formula xml:id="formula_31">E(h ? Y ) 2 ? E( f ? Y ) 2 ? c A.2 ? E( f ? h) 2 ? 2(1 + c A.2 )[? 1 + ? 2 ].</formula><p>For convenience, we introduce a ?-excess risk</p><formula xml:id="formula_32">E ? ( g) := E( g ? Y ) 2 ? inf f ?F E(f ? Y ) 2 ? 2(1 + c A.2 )[? 1 + ? 2 ],</formula><p>then the following 2 statements are the direct consequences of the corresponding statements from the article <ref type="bibr" target="#b16">[LRS15]</ref>. The only difference is that in our case the geometric inequality has terms on the right side with minimization errors ? 1 , ? 2 . Also our definition of the set H is different, but all that was needed from it was the property that f lies in H + f * . For brevity, we will not repeat the proofs, but only indicate the numbers of the corresponding results in the titles of the assertions. We will also proceed for statements the proofs for which we slightly modify or use without changes.</p><formula xml:id="formula_33">Corollary A.3 (Corollary 3). Conditioned on the data {(X i , Y i ) : 1 ? i ? n},</formula><p>we have a deterministic upper bound for the Star d estimator:</p><formula xml:id="formula_34">E ? ( f ) ? ( E ? E)[2(f * ? Y )(f * ? f )] + E(f * ? f ) 2 ? (1 + c A.2 ) ? E(f * ? f ) 2 .</formula><p>Theorem A.4 (Theorem 4). The following expectation bound on excess loss of the Star d estimator holds:</p><formula xml:id="formula_35">E E ? ( f ) ? (2F + F (2 + c A.2 )/2) ? E ? sup h?H 1 n n i=1 2? i h(X i ) ? c A.4 h(X i ) 2 ,</formula><p>where ? 1 , . . . ? n are independent Rademacher random variables, Then there exist two absolute constants c A.5 ,c A.5 &gt; 0 (which only depend on c A.2 ), such that</p><formula xml:id="formula_36">c A.4 = min c A.2 4F , c A.2 4F (2+c A.2 ) , F = sup f ?F |f | ? and F = sup F |Y ? f | ? almost surely.</formula><formula xml:id="formula_37">P E ? ( f ) &gt; 4u ? 4? lib + 4 P sup h?H 1 n n i=1 ? i ? i h(X i ) ?c A.5 h(X i ) 2 &gt; u for any u &gt; 32 ? AB c A.5</formula><p>1 n as long as n &gt;</p><formula xml:id="formula_38">16(1?c A.5 ) 2 A c 2 A.5 ? n 0 (H, ? lib , c A.2 /4).</formula><p>Lemma A.6 (Lemma 15). The offset Rademacher complexity for H is bounded as:</p><formula xml:id="formula_39">E ? sup H 1 n n i=1 2? i ? i h(X i ) ? Ch(X i ) 2 ? K(C)? + M (C) ? log N 2 (H, ?) n</formula><p>and with probability at least 1 ? ?</p><formula xml:id="formula_40">sup H 1 n n i=1 2? i ? i h(X i ) ? Ch(X i ) 2 ? K(C)? + M (C) ? log N 2 (H, ?) + log 1 ? n ,</formula><p>where </p><formula xml:id="formula_41">K(C) := 2 ? ? n i=1 ? 2 /n + C ? ? , M (C) := sup h?H\{0} 4 n i=1 h(X i ) 2 ? 2 i C n i=1 h(X i ) 2 .<label>(10</label></formula><formula xml:id="formula_42">1 n n i=1 2? i ? i h(X i ) ? Ch(X i ) 2 ? 1 n n i=1 2? i ? i (h(X i ) ? v[h](X i )) + C v[h] 2 (X i )/2 ? h 2 (X i ) + 1 n sup v?N2(H,?) n i=1 2? i ? i v(X i ) ? C 2 v(X i ) 2 ? 2? ? ? n i=1 ? 2 i /n + C ? ? + 1 n sup v?N2(H,?) n i=1 2? i ? i v(X i ) ? C 2 v(X i ) 2 .</formula><p>The right summarand is supremum over set of cardinality not more than N 2 (H,?). By using Lemma A.11, we acquire the expected estimates.</p><p>We have now obtained, using the offset Rademacher complexity technique, the upper bound on excess risk in terms of the coverage size of the set H. To get the desired result, we need to obtain an upper bound on the size of the cover H in terms of the size of the cover F. Lemma A.7. For any scale ? &gt; 0, the covering number of F ? V (L + 1) ? B 2 (where B 2 is a sphere of radius one in space with norm ? n ) and that of H are bounded in the sense:</p><formula xml:id="formula_43">log N 2 (F,?) ? log N 2 (H, ?) ? (d + 2) log N 2 F, ? 3(d + 1) + log 6(d + 1)V (L + 1) ? .</formula><p>Proof. If we define as N (F, ?) the ?-net cardinality no more then N (F, ?), then the following is true: N (F 1 , ? 1 ) + N (F 1 , ? 2 ) is (? 1 + ? 2 )-net for F 1 + F 2 . Hence, N (F 1 + F 2 , ? 1 + ? 2 ) ? N (F 1 , ? 1 ) ? N (F 2 , ? 2 ) . With this we can obtain the following upper bound</p><formula xml:id="formula_44">N 2 (H, ?) ? N 2 (F + Hull d , ?) ? N 2 F, ? 3 ? N 2 Hull d , 2?</formula><p>3 .</p><p>But since Hull d is the sum of d + 1 functions from F with coefficients in [?1; 1], by the inequaility (3), we can cover this with a net of size no more than</p><formula xml:id="formula_45">N 2 F, ? 3(d + 1) ? 6(d + 1)V (L + 1) ? d+1</formula><p>.</p><p>Note that to obtain the required orders, we only need coverage with ? = 1/n. Proof. By lemma A.7 and inequality 4, we have log N 2 (H, 1/n) ? (d + 2) log N 2 F(L, p, s), 1 3n(d + 1) + log 6n(d + 1)V (L + 1) ? (d + 2) (s + 1) log 2V 2 (L + 1)(3n(d + 1)) + log (6n(d + 1)V (L + 1)) .</p><p>We are now fully prepared to prove the two main results.</p><p>Theorem A.9. Let f be a Star d estimator and H be the set defined in 7 for F = F(L, p, s). The following expectation bound on excess loss holds:</p><p>E E ? (f ) ? 2(F + V (L + 1)) ? K(C) n + M (C) ? c A.8 d s log (V L n d) n ,</p><p>where K(C), M (C) defined in (10) for constants</p><formula xml:id="formula_46">C = min c A.2 4F , c A.2 4V (L + 1)(2 + c A.2 ) , F = sup F |Y ? f | ? .</formula><p>Proof. By using Theorem A.4 and inequality 3 we have</p><formula xml:id="formula_47">E E ? (f ) ? (2F + V (L + 1)(2 + c A.2 )/2) ? E ? sup h?H 1 n n i=1 2? i h(X i ) ? Ch(X i ) 2 , where C = min c A.2 4F , c A.2</formula><p>4V (L+1)(2+c A.2 ) , F = sup F |Y ? f | ? almost surely. By using Lemma A.6 and corollary A.8 we get desired result By using Lemmas A.6 and A.8 we have with probability no more than ? for any C &gt; 0 :</p><formula xml:id="formula_48">E ? sup H 1 n n i=1 2? i ? i h(X i ) ? Ch(X i ) 2 ? K(C) n + M (C) ? c A</formula><formula xml:id="formula_49">sup H 1 n n i=1 ? i ? i h(X i ) ? C 2 h(X i ) 2 ? K(C) 2 ? + M (C) 2 ? log N 2 (H, ?) + log 1 ? n ,</formula><p>where K(C), M (C) are defined in (10). Combining this inequality for C = 2c A.10 = 2c A.5 and c A.10 = c A.5 , c A.10 = c A.2 we get the required result.</p><p>Lemma A.11 (Lemma 9). Let V ? R n be a finite set, |V | = N . Then, for any C &gt; 0 :</p><formula xml:id="formula_50">E ? max v?V 1 n n i=1 ? i ? i v(X i ) ? Cv(X i ) 2 ? M log N n .</formula><p>For any ? &gt; 0:</p><formula xml:id="formula_51">P ? max v?V 1 n n i=1 ? i ? i v(X i ) ? Cv(X i ) 2 &gt; M log N + log 1 ? n ? ?, where M := sup v?V \{0} n i=1 v(X i ) 2 ? 2 i 2C n i=1 v(X i ) 2 .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 3. 3 .</head><label>3</label><figDesc>Let f be a Star d estimator and let H be the set defined in 7 for F = F(L, p, s). Assume for H the lower isometry bound in Definition 3.2 holds with ? lib = c A.2 /4 and some ? lib &lt; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustrate Star d algorithm on NN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The cut surface L Lemma A.2 (Geometric Inequality for ?-empirical minimizers). Let? 1 . . .? d be ? 1 -empirical risk minimizers from the first step of the Star d procedure, and f be the ? 2 -empirical risk minimizer from the second step of the Star d procedure. Then, for any h ? F and c A.2 = 1 36 the following inequality holds:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Theorem A.5 (Theorem 7). Assume the lower isometry bound in Definition 3.2 holds with ? lib = c A.2 /4 and some ? lib &lt; 1 and H is the set defined in 7. Let ? i = Y i ? f * (X i ). Define A := sup h?H E h 4 (E h 2 ) 2 and B := sup X,Y E ? 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Corollary A.8. Let H defined in 7 for F = F (L, p, s), then for V defined in 5 holds log N 2 H, 1 n ? c A.8 d s log V L n d , where c A.8 is an indepedent constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 A. 5 ?</head><label>25</label><figDesc>.8 d s log V L n d n .Theorem A.10. Let f be a Star d estimator and let H be the set defined in 7 for F = F(L, p, s). Assume for H the lower isometry bound in Definition 3.2 holds with ? lib = c A.2 /4 and some ? lib &lt; 1.Let ? i = Y i ? f * (X i ). Define A := sup h?H E h 4 (E h 2 ) 2 and B := sup X,Y E ? 4 .Then there exist 3 absolute constants c A.10 ,c A.10 , c A.10 &gt; 0 (which only depend on c A.2 ), such that P E ? ( f ) &gt; 4D ? 4(? lib + ?) as long as n &gt; 16(1?c A.10 ) 2 A c 2 A.10 ? n 0 (H, ? lib , c A.10 /4), where c A.8 d s log V L n d + log 1 c A.8 is an independent constant.Proof. By using Theorem A.5 for any u &gt; 32? AB c A.5 1 n we have P E ? ( f ) &gt; 4u ? 4? lib + 4 P sup h?H 1 n n i=1 ? i ? i h(X i ) ?c A.5 h(X i ) 2 &gt; uas long as n &gt; 16(1?c A.5 ) 2 A c n 0 (H, ? lib , c A.2 /4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>:</head><label></label><figDesc>BOSTON HOUSE PRICING. Part of results at 200 epochs, p = 0.1, lr = 0.01</figDesc><table><row><cell>Name</cell><cell>d MSE</cell><cell>MAE R 2</cell><cell cols="2">TRAIN MSE TIME (sec)</cell></row><row><cell>Snap Star (shot warm-up)</cell><cell cols="3">5 11.155?0.967 2.185 0.866 1.232</cell><cell>52.4</cell></row><row><cell>Snap Star (new warm-up)</cell><cell cols="3">5 10.677?1.034 2.148 0.872 1.306</cell><cell>49.2</cell></row><row><cell>Snap Ensemble</cell><cell cols="3">5 11.123?0.615 2.206 0.866 1.728</cell><cell>43.4</cell></row><row><cell>Ensemble</cell><cell cols="3">5 10.172?0.621 2.164 0.878 2.133</cell><cell>43.2</cell></row><row><cell>Classic Star (no warm-up)</cell><cell>5 9.650?0.911</cell><cell cols="2">2.092 0.884 1.373</cell><cell>52.0</cell></row><row><cell cols="2">Classic Star (new warm-up) 5 9.664?0.324</cell><cell cols="2">2.086 0.884 1.374</cell><cell>45.8</cell></row><row><cell>Big NN</cell><cell cols="3">5 12.117?1.601 2.326 0.854 2.160</cell><cell>28.4</cell></row><row><cell>Snap Star (shot warm-up)</cell><cell cols="3">4 10.745?0.640 2.154 0.871 1.192</cell><cell>43.0</cell></row><row><cell>Snap Star (new warm-up)</cell><cell cols="3">4 10.862?0.537 2.162 0.870 1.401</cell><cell>43.0</cell></row><row><cell>Snap Ensemble</cell><cell cols="3">4 11.169?0.558 2.205 0.866 1.679</cell><cell>36.0</cell></row><row><cell>Ensemble</cell><cell>4 9.967?0.244</cell><cell cols="2">2.144 0.880 1.833</cell><cell>35.8</cell></row><row><cell>Classic Star (no warm-up)</cell><cell>4 9.887?0.501</cell><cell cols="2">2.111 0.881 1.326</cell><cell>43.6</cell></row><row><cell cols="2">Classic Star (new warm-up) 4 9.741?0.790</cell><cell cols="2">2.114 0.883 1.400</cell><cell>38.2</cell></row><row><cell>Big NN</cell><cell cols="3">4 11.856?0.429 2.323 0.858 1.937</cell><cell>24.0</cell></row><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Million Song For the last experiment we use UCI<ref type="bibr" target="#b22">[DG17]</ref> subset of Million Song dataset<ref type="bibr" target="#b10">[Ber+11]</ref>, containing 515345 songs, with 90 features each. Dataset's features are timbre average and covariance for every song, target is the year the song was released. Dataset is divided intro train and test in proportion 9 : 1, where test is the last 10% of the dataset. Features and targets were standard normalized prior to model training, batch size is 128. Base model for this task is fully-connected 4-layer ReLu neural network with layer dimensions 90, 120, 20, 1. After each hidden layer, batch</figDesc><table><row><cell>Name</cell><cell></cell><cell cols="2">d accuracy</cell><cell>entropy</cell><cell>TIME (sec)</cell></row><row><cell cols="2">Snap Star (shot warm-up)</cell><cell cols="4">3 0.902?0.001 0.354?0.017 1021.3</cell></row><row><cell cols="2">Snap Star (new warm-up)</cell><cell cols="4">3 0.902?0.001 0.348?0.015 940.3</cell></row><row><cell>Snap Ensemble</cell><cell></cell><cell cols="4">3 0.904?0.002 0.274?0.006 819.7</cell></row><row><cell>Ensemble</cell><cell></cell><cell cols="4">3 0.910?0.003 0.247?0.004 818.7</cell></row><row><cell cols="2">Classic Star (no warm-up)</cell><cell cols="4">3 0.913?0.002 0.245?0.007 1014.7</cell></row><row><cell cols="6">Classic Star (new warm-up) 3 0.913?0.004 0.243?0.008 855.3</cell></row><row><cell>Big NN</cell><cell></cell><cell cols="4">3 0.905?0.004 0.297?0.011 643.0</cell></row><row><cell cols="2">Snap Star (shot warm-up)</cell><cell cols="4">2 0.906?0.004 0.292?0.015 751.0</cell></row><row><cell cols="2">Snap Star (new warm-up)</cell><cell cols="4">2 0.907?0.005 0.284?0.012 697.0</cell></row><row><cell>Snap Ensemble</cell><cell></cell><cell cols="4">2 0.908?0.004 0.266?0.011 616.0</cell></row><row><cell>Ensemble</cell><cell></cell><cell cols="4">2 0.912?0.004 0.251?0.004 614.3</cell></row><row><cell cols="2">Classic Star (no warm-up)</cell><cell cols="4">2 0.910?0.000 0.245?0.003 747.3</cell></row><row><cell cols="6">Classic Star (new warm-up) 2 0.911?0.002 0.244?0.004 613.3</cell></row><row><cell>Big NN</cell><cell></cell><cell cols="4">2 0.902?0.001 0.303?0.007 498.3</cell></row><row><cell cols="6">Table 2: FASHION MNIST. Part of results at 15 epochs, lr = 0.001</cell></row><row><cell>Name</cell><cell cols="2">d MSE</cell><cell cols="2">MAE R 2</cell><cell>Train MSE TIME (sec)</cell></row><row><cell>Snap Star (shot warm-up)</cell><cell cols="2">5 76.70 ? 0.28</cell><cell>5.97</cell><cell cols="2">0.358 68.05</cell><cell>2369</cell></row><row><cell>Snap Star (new warm-up)</cell><cell cols="2">5 76.16 ? 0.15</cell><cell>5.96</cell><cell cols="2">0.363 69.39</cell><cell>2076</cell></row><row><cell>Snap Ensemble</cell><cell cols="2">5 76.48 ? 0.18</cell><cell>6.01</cell><cell cols="2">0.360 68.15</cell><cell>1766</cell></row><row><cell>Ensemble</cell><cell cols="2">5 75.74 ? 0.06</cell><cell>5.99</cell><cell cols="2">0.366 68.99</cell><cell>1955</cell></row><row><cell>Classic Star (no warm-up)</cell><cell cols="2">5 75.97 ? 0.20</cell><cell>6.02</cell><cell cols="2">0.365 70.88</cell><cell>2150</cell></row><row><cell cols="4">Classic Star (new warm-up) 5 75.62 ? 0.06 5.96</cell><cell cols="2">0.367 70.29</cell><cell>2176</cell></row><row><cell>Big NN</cell><cell cols="2">5 76.23 ? 0.19</cell><cell>5.95</cell><cell cols="2">0.362 71.70</cell><cell>1166</cell></row><row><cell>Snap Star (shot warm-up)</cell><cell cols="2">4 76.74 ? 0.20</cell><cell>5.96</cell><cell cols="2">0.358 68.44</cell><cell>2026</cell></row><row><cell>Snap Star (new warm-up)</cell><cell cols="2">4 76.22 ? 0.18</cell><cell>5.97</cell><cell cols="2">0.362 69.74</cell><cell>1753</cell></row><row><cell>Snap Ensemble</cell><cell cols="2">4 76.48 ? 0.19</cell><cell>6.01</cell><cell cols="2">0.360 68.41</cell><cell>1473</cell></row><row><cell>Ensemble</cell><cell cols="2">4 75.78 ? 0.07</cell><cell>5.99</cell><cell cols="2">0.366 69.01</cell><cell>1636</cell></row><row><cell>Classic Star (no warm-up)</cell><cell cols="2">4 76.02 ? 0.09</cell><cell>6.01</cell><cell cols="2">0.364 71.11</cell><cell>1811</cell></row><row><cell cols="4">Classic Star (new warm-up) 4 75.76 ? 0.17 5.96</cell><cell cols="2">0.366 70.41</cell><cell>1823</cell></row><row><cell>Big NN</cell><cell cols="2">4 76.29 ? 0.29</cell><cell>5.96</cell><cell cols="2">0.362 71.74</cell><cell>980</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MILLIION SONG. Part of results at 30 epochs normalization and DropOut with probability of 0.2 are applied. Snapshots used SGD optimizer starting at learning rate of 0.1. All other models used Adam optimizer with learning rate of 0.001. In contrast to previous experiments, for warming up a new model, 10% of epochs is spent and shot warm-up used next fully trained snapshot instead of a last shot (that is, the total training budget is (d + 2) ? epochs). Training was done on the NVIDIA GeForce GTX 1060 GPU (6gb) / Intel Core i5-7500. Error results were averaged over 5 runs. Some results are shown in theTable 3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>)</head><label></label><figDesc>Proof. Let N 2 (H,?) be the ?-net of the H of size at most N 2 (H,?) and v[h] be the closest point from this net for function h ? H, i.e. h ? v[h] 2 ? ?. By using the inequality v[h] 2 i ? 2 h 2 i + (v[h] i ? h i ) 2 , we can get next upper bound:</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">.2 Competitors and other detailsLooking at the structure of the proposed star procedure (let us call it Classic Star (no warm-up)), a desire naturally arises to compare it with the following models. First, with the model where all d + 1 blocks together with the connecting linear layer are trained simultaneously (Big NN), and second, where all d + 1 blocks are trained independently, and then their predictions are averaged (Ensemble).But the star procedure has a number of possible problems. Training even one neural network is a rather complicated process, while in our algorithm it is required to train d + 1 predictors. To solve this problem, we propose to train the d + 1 models in the ensemble method and the first d models in the Classic Star procedure using the snapshot technique, calling such approaches Snap Ensemble and Snap Star, respectively.Additionally, at the last step of the star procedure, when optimizing the weights of the last model and convex coefficients, the minimizer has the opportunity to set the contribution of the last model to zero and focus on the aggregation of already trained d models. We offer variations of the star algorithm that try to deal with these. It is possible to train the last snapshot (model d) from the Snap Star as a d + 1 model, this is what we call Snap Star (shot warm-up), also can add a new model to the Snap Star or Classic Star and spend part of the computational budget on training only its weights, the so-called warm-up, and get Snap Star (new warm-up) and Classic Star (new warm-up), respectively. We compare all these methods with each other on a number of tasks. The amount of the computational budget for training a separate block we will denote as epochs, in the snapshot technique learning rate decreases with cyclical cosine annealing<ref type="bibr" target="#b20">[LH16]</ref>. Architecture of neural networks and hardware for training are be reported for each task separately. We ran all experiments using the PyTorch 1 library. For the purposes of reproducing the results, the code and extended tables with results are publicly available at repository 2 . 1 https://pytorch.org/library 2 https://github.com/mordiggian174/star-ensembling</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://paperswithcode.com/sota/image-classification-on-fashion-mnist 4 https://github.com/kefth/fashion-mnist</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://paperswithcode.com/sota/image-classification-on-fashion-mnist 6 https://github.com/mordiggian174/star-ensembling</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. Since Lemma A.1 was actually proven for any f ? K, let f be the closest point to f from K. For this f the inequality (9) holds. Similarly to Lemma A.1, there are 2 options: either f ? (intC) c , or f ? T . <ref type="figure">Figure 2</ref>, L is the two dimensional plane which passes through f , g, Y ):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>then by the cosine theorem (as depicted on</head><p>Then we have:</p><p>Lemma A.1 states:</p><p>By using the triangle inequality and the convexity of the quadratic function, we can get the following bound</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Result Tables</head><p>Here we additionally present tables with the results of numerical experiments. Particularly for runs with a small number of epochs. It can be observed that the SnapStar algorithm is quite good with a strong budget constraint. The results also include a relatively large run for the FASHION MNIST dataset. At the moment, ClassicStar (new warm-up) takes 10 ? 11th place in the leaderboard 5 for this dataset. Full versions of the following tables can be found in the repository 6 .  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hedonic housing prices and the demand for clean air</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harrison</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel L Rubinfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of environmental economics and management</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boosting and other ensemble methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1289" to="1301" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Topics in non-parametric statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lectures on probability theory and statistics</title>
		<meeting><address><addrLine>Saint-Flour</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="85" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support vector machine ensemble with bagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun-Chul</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on support vector machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimal Rates of Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>COLT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Response models based on bagging neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungnam</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Maclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Interactive Marketing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Progressive mixture rules are deviation suboptimal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aggregation via empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lecu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probability theory and related fields</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="591" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Music Information Retrieval</title>
		<meeting>the 12th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<idno>PMLR. 2013</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for regression and time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueheng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE symposium on computational intelligence in ensemble learning (CIEL)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with square loss: Localization through offset rademacher complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1260" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boosted convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Moghimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: BMVC</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1704.00109</idno>
		<idno>free. 2017. DOI: 10.48550/ ARXIV.1704.00109</idno>
		<ptr target="https://arxiv.org/abs/1704.00109" />
		<title level="m">Snapshot Ensembles: Train 1, get M for</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Empirical entropy, minimax regret and minimax risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre B</forename><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="789" to="824" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1802.10026</idno>
		<ptr target="https://arxiv.org/abs/1802.10026" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of the usages of deep learning for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="604" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonparametric regression using deep neural networks with ReLU activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Schmidt-Hieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1875" to="1897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Snapshot boosting: a fast ensemble framework for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ensemble deep learning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ganaie</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2104.02395</idno>
		<ptr target="https://arxiv.org/abs/2104.02395" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Localization, Convexity, and Star Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhas</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Snap Star (shot warm-up) 2 0</title>
		<idno>894?0.007 0.294?0.020 248.667</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Snap Star (new warm-up) 2 0</title>
		<idno>892?0.001 0.294?0.006 230.333</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Classic Star (no warm-up) 2 0</title>
		<idno>889?0.003 0.304?0.009 249.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Classic Star (new warm-up)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Table 6: FASHION MNIST</title>
		<idno>lr = 0.001</idno>
		<imprint/>
	</monogr>
	<note>Part of results at 5 epochs</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Name d accuracy entropy TIME (sec)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Table 7: FASHION MNIST. All of results at 25 epochs</title>
		<idno>lr = 0.001</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
