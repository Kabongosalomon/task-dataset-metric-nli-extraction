<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EAutoDet: Efficient Architecture Search for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE &amp; MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong OPPO Mobile Telecommunications Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE &amp; MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong OPPO Mobile Telecommunications Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE &amp; MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong OPPO Mobile Telecommunications Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanping</forename><surname>Zhao</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
							<email>xkyang@sjtu.edu.cnzhaojuanping1325@oppo.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE &amp; MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong OPPO Mobile Telecommunications Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EAutoDet: Efficient Architecture Search for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training CNN for detection is time-consuming due to the large dataset and complex network modules, making it hard to search architectures on detection datasets directly, which usually requires vast search costs (usually tens and even hundreds of GPU-days). In contrast, this paper introduces an efficient framework, named EAutoDet, that can discover practical backbone and FPN architectures for object detection in 1.4 GPU-days. Specifically, we construct a supernet for both backbone and FPN modules and adopt the differentiable method. To reduce the GPU memory requirement and computational cost, we propose a kernel reusing technique by sharing the weights of candidate operations on one edge and consolidating them into one convolution. A dynamic channel refinement strategy is also introduced to search channel numbers. Extensive experiments show significant efficacy and efficiency of our method. In particular, the discovered architectures surpass state-of-the-art object detection NAS methods and achieve 40.1 mAP with 120 FPS and 49.2 mAP with 41.3 FPS on COCO test-dev set. We also transfer the discovered architectures to rotation detection task, which achieve 77.05 mAP50 on DOTA-v1.0 test set with 21.1M parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Handcrafted neural architectures that require large amounts of trials and errors of experts to design have achieved promising performance across classification, detection, and segmentation tasks. Automated architecture search methods have been recently explored, including reinforcement learning , evolutionary algorithm <ref type="bibr" target="#b26">(Real et al. 2019)</ref>, Bayesian optimization <ref type="bibr" target="#b37">(White, Neiswanger, and Savani 2021)</ref>, as well as the more cost-effective one-shot NAS <ref type="bibr" target="#b0">(Bender et al. 2018</ref>) that builds a supernet as the surrogate model to predict the performance of candidate architectures. DARTS <ref type="bibr" target="#b19">(Liu, Simonyan, and Yang 2019)</ref> further introduces a differentiable method that reduces the search cost to a few GPU-days. However, DARTS requires vast GPU memory since it has to train an over-parameterized supernet, making it impossible to search on large datasets or complex tasks. Some works <ref type="bibr" target="#b35">(Wang et al. 2020b;</ref><ref type="bibr" target="#b39">Xu et al. 2020;</ref><ref type="bibr" target="#b4">Dong and Yang 2019)</ref>   Though NAS has achieveed great success on classification tasks, it is still an open question on how to directly search architectures for detection tasks with two major difficulties: 1) It is time-consuming to train detection models from scratch on a detection dataset due to its complex architecture compared to those for classification, which consists of multiple modules, including backbone and feature pyramid network <ref type="bibr">(FPN)</ref>. So that many works <ref type="bibr" target="#b29">(Girshick 2015;</ref><ref type="bibr" target="#b1">Bochkovskiy, Wang, and Liao 2020;</ref><ref type="bibr" target="#b22">Liu et al. 2016)</ref> pre-train the backbone on ImageNet; 2) Training a detection model requires vast GPU memory cost, especially for those NAS works <ref type="bibr" target="#b2">(Chen et al. 2019;</ref><ref type="bibr" target="#b8">Guo et al. 2020</ref>) that need to build an over-parameterized supernet. They even have to pre-train it on ImageNet, further increasing the search cost. To simplify the supernet and lower the search difficulty, they usually restrict the search space by either searching backbone <ref type="bibr" target="#b2">(Chen et al. 2019;</ref><ref type="bibr" target="#b5">Du et al. 2020;</ref><ref type="bibr" target="#b12">Jiang et al. 2020)</ref> or FPN <ref type="bibr" target="#b6">(Ghiasi, Lin, and Le 2019;</ref><ref type="bibr" target="#b38">Xu et al. 2019;</ref><ref type="bibr" target="#b34">Wang et al. 2020a)</ref>, which, however, actually ignores the relationship between the two modules. In contrast, this paper introduces kernel reusing technique and dynamic channel refinement to speed up the convergence to train a supernet and reduce GPU memory requirement. We thus propose an efficient search method, named EAutoDet, which can jointly search architectures of backbone and FPN on MS-COCO <ref type="bibr" target="#b17">(Lin et al. 2014</ref>) detection dataset in a few GPU-days, with no need to pre-train a supernet on ImageNet.</p><p>Additionally, prior NAS detection methods are based on RetinaNet (one-stage) or Faster-RCNN (two-stage) framework that adopts ResNet-like architectures. Few have explored to search for YOLO (one-stage) framework that could leverage its known fast speed and outstanding performance. The handcrafted YOLO models even outperform many NAS methods in similar inference speed (shown in <ref type="table">Table 1</ref>), which implies the potential of a combination of NAS and YOLO. Nevertheless, a vanilla combination is undesirable. On the one hand, the handcrafted architecture of YOLO is subtle and elaborate. Such a nearly impeccable baseline puts forward higher requests for the ability of search method discovering the optimal architecture. On the other hand, it is better to absorb the knowledge of those well-designed architectures to design a sophisticated and large search space, which further asks for a flexible and efficient search method. However, our method offers a practical solution thanks to its low memory requirement and rapid convergence rate to train a supernet. Besides, our technique ameliorates the computation of convolutions in the supernet rather than restricting sub-architectures of supernet, making our method flexible to suit various search spaces. Experiments show the outstanding performance of our method on MS-COCO and DOTA-v1.0. Our contributions are summarized as follows:</p><p>1) Efficient Architecture Search Method for Object Detection. We propose kernel reusing and dynamic channel refinement techniques for the fine-grained search of backbone and FPN modules in 1.4 GPU-days on a single V100 GPU, significantly outperforming prior NAS methods, e.g., 28 GPU-days <ref type="bibr" target="#b34">(Wang et al. 2020a</ref>) and 44 GPU-days <ref type="bibr" target="#b2">(Chen et al. 2019)</ref>. Unlike other NAS methods <ref type="bibr" target="#b2">(Chen et al. 2019;</ref><ref type="bibr" target="#b43">Yao et al. 2020;</ref><ref type="bibr" target="#b8">Guo et al. 2020</ref>) that have to pre-train an over-parameterized supernet on the ImageNet, our supernet is trained from scratch on MS-COCO thanks to the property of our method: low memory requirement and rapid convergence rate to train a supernet.</p><p>2) Sophisticated and Large Search Space for Object Detection. By absorbing the knowledge of well-designed YOLO models, we design a complex search space for detection, including convolution types, channel numbers, and connection of layers for backbone and FPN. It puts forward higher requests for the flexibility and efficiency of search methods. This paper offers a solution, and to the best of our knowledge, it is the first NAS method that outperforms YOLO models with competitive high inference speed. Notice that our method can be easily applied to other detectors, e.g., Faster-RCNN  and RetinaNet .</p><p>3) Strong Performance and Fast Speed. Our design allows for direct search and evaluation without pre-training. The discovered architectures achieve outstanding performance on classic horizontal, as well as rotation detection tasks: 40.1 mAP with 120 FPS on MS-COCO where the bounding box is always assumed horizontal, and 77.05 mAP 50 on DOTA which is a dominant rotation detection benchmark but has not been used in NAS literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object Detection. Existing detection frameworks usually consist of four modules: backbone, feature fusion neck, region proposal network (in two-stage detectors), and detection head. For real-time detection, <ref type="bibr" target="#b22">(Liu et al. 2016;</ref><ref type="bibr" target="#b28">Redmon et al. 2016;</ref><ref type="bibr" target="#b16">Lin et al. 2020;</ref><ref type="bibr" target="#b1">Bochkovskiy, Wang, and Liao 2020;</ref><ref type="bibr" target="#b32">Wang, Bochkovskiy, and Liao 2021)</ref> design efficient architectures for the four modules. There are also emerging manually-designed detectors for rotation detection whereby different loss functions are carefully devised ranging from regression <ref type="bibr" target="#b42">Yang et al. 2021b</ref>) to classification <ref type="bibr" target="#b40">(Yang et al. 2021a;</ref><ref type="bibr" target="#b41">Yang and Yan 2020)</ref> models. Unlike the above works requiring massive trials and expert experience to design CNNs, we aim to search architectures for detection automatically.</p><p>Neural Architecture Search. Researchers have been dedicated to efficient search algorithms for neural architectures in recent years. NASNet ) utilizes reinforcement learning (RL) and proposes to generate candidate architectures by an RNN controller. <ref type="bibr" target="#b26">(Real et al. 2019)</ref> adopt evolutionary algorithms (EA) to derive new architectures by crossover and mutation. Besides the above timeconsuming methods, one-shot NAS <ref type="bibr" target="#b0">(Bender et al. 2018</ref>) is introduced and can reduce the search cost to a few GPUdays. DARTS <ref type="bibr" target="#b19">(Liu, Simonyan, and Yang 2019)</ref> regards NAS as a bi-level optimization problem and proposes to solve it by a differentiable method. This paper adopts the differentiable method in DARTS due to its efficacy and high efficiency.</p><p>NAS for Object Detection. Recent NAS methods for object detection can be briefly categorized into three streams: 1) Search backbone architecture and fix FPN, e.g. Det-NAS <ref type="bibr" target="#b2">(Chen et al. 2019</ref>) and SP-NAS <ref type="bibr" target="#b12">(Jiang et al. 2020)</ref>. 2) Search FPN architecture and fix backbone, e.g. NAS-FPN <ref type="bibr" target="#b6">(Ghiasi, Lin, and Le 2019)</ref>, Auto-FPN <ref type="bibr" target="#b38">(Xu et al. 2019)</ref>, and NAS-FCOS <ref type="bibr" target="#b34">(Wang et al. 2020a)</ref>. 3) Jointly search backbone and FPN, e.g., SM-NAS <ref type="bibr" target="#b43">(Yao et al. 2020</ref>), Hit-Detector ) and our method. Unlike SM-NAS and Hit-Detector that require vast GPUs to search, this work introduces kernel reusing and dynamic channel refinement techniques that can significantly reduce the GPU memory requirement during the search process. Specifically, our EAutoDet can search under a more extensive search space on MS-COCO dataset directly on a single V100 GPU in 1.4 days. Moreover, unlike many NAS detection methods <ref type="bibr" target="#b2">(Chen et al. 2019;</ref><ref type="bibr" target="#b30">Tan, Pang, and Le 2020;</ref><ref type="bibr" target="#b43">Yao et al. 2020;</ref><ref type="bibr" target="#b8">Guo et al. 2020</ref>) that need to pre-train supernets on the Ima-geNet, our EAutoDet trains the supernet from scratch on the MS-COCO during the search process, demonstrating its outstanding convergence ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed EAutoDet</head><p>Referring to DARTS <ref type="bibr" target="#b19">(Liu, Simonyan, and Yang 2019)</ref> that regards NAS as a bi-level optimization task, we also build a supernet and define architecture parameters ? to denote the importance of candidate operations. However, it is intractable to search on detection dataset directly by DARTS since the supernet is an over-parameterized model, making 1: Our method enables authentic fine-grained search w.r.t. operations, number of channels, and connections between layers, and is much faster than SM-NAS and Hit-Detector thanks to our kernel reusing and dynamic channel refinement techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Space Method</head><p>Backbone alone DetNAS <ref type="bibr" target="#b2">(Chen et al. 2019)</ref>, SpineNet <ref type="bibr" target="#b5">(Du et al. 2020</ref>  it much more challenging to train it from scratch on detection datasets, e.g., MS-COCO. We illustrate one super-edge in the supernet of DARTS in <ref type="figure" target="#fig_2">Fig. 2 (left)</ref>, which contains all independent candidate operations and thus requires massive GPU memory during the search stage. To address the above memory explosion issue, we introduce two techniques to reduce memory requirement and computational cost: Kernel Reusing Technique to search operation types, and Dynamic Channel Refinement to search channel numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Kernel Reusing for Operation Type Search</head><p>Each edge in the supernet contains multiple convolutions with various kernel sizes. Suppose X and Z are the input and output of convolutions on one edge, then Z = o?O ? o X ? ? o , where '?' denotes convolution operation, O is the candidate set of convolutions, and ? is the kernel weights.</p><p>To reduce the parameters, we reuse the weights of different convolutions as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (right), that is, kernels of all convolutions can be extracted from unified weights by a binary mask M . Moreover, since convolutions are linear operations, the weighted sum of multiple convolutions on the same edge can be compounded into one convolution. Therefore, the output of each edge can be simplified as Eq. 1, where ? is the unified weights, and kernels of candidate convolution o can be obtained by M o ? ?.</p><formula xml:id="formula_0">Z = o?O ? o X ? [M o ? ?] |O| convolutions = X ? ? ? o?O ? o M o One convolution (1)</formula><p>Advantage of Our Kernel Reusing Technique. Apart from our kernel reusing technique, sampling-based methods <ref type="bibr" target="#b4">Dong and Yang 2019)</ref> are also popular to Figure 3: MAP of supernets in the search process that are trained for 50 epochs. The search cost of each supernet on a V100 GPU is given in the legend. EAutoDet-s-sample denotes search based on the sampling-based method <ref type="bibr" target="#b4">(Dong and Yang 2019)</ref>. The horizontal dash line is the final performance of EAutoDet-s-sample.</p><p>reduce the memory and computational cost for classification tasks. However, they involve a dynamic network structure by sampling a sub-network of the supernet at each iteration, which will affect the supernet training. Though such an issue can be tolerated on classification tasks, it worsens on detection tasks. In contrast, our kernel reusing technique holds a stable network structure and reduces GPU memory and computational cost by compounding multiple kernels into one without interfering with the supernet training.</p><p>We illustrate the mAP of supernets on MS-COCO validation set during the search stage in <ref type="figure">Fig. 3</ref>. EAutoDets/m/l/x denotes four supernets under various search spaces (details are introduced in Sec. 3.3). They are built based on our kernel reusing technique. EAutoDet-s-sample denotes that an s-level supernet is built without kernel reusing and trained by sampling operation based on Gumbel reparameterization technique at each iteration <ref type="bibr" target="#b4">(Dong and Yang 2019)</ref>. We observe that: 1) Our four supernets can converge to 30% mAP; 2) Our kernel reusing technique converges better and faster than sampling-based method, which confirms the above analysis on better convergence property of our method. Notice that <ref type="figure">Fig. 3</ref> shows mAP of supernets during 50 epochs' training in the search stage. The ultimate performance of the discovered models is evaluated by training them from scratch for 300 epochs and is reported in <ref type="table">Table 1</ref>.</p><formula xml:id="formula_1">??? ??? ! ? ! (1 ? ! ) ? ! ! ? ! ??? ??? !"# ? !"# ! ? ! Features in l-1 layer</formula><p>The search cost is also illustrated in the legend. Specifically, an effective s-level model can be discovered in 1.4 GPUdays on a single V100 GPU, showing the remarkable efficiency of our method. Difference from the Prior Works. Unlike the prior NAS works <ref type="bibr" target="#b29">(Stamoulis et al. 2019;</ref><ref type="bibr" target="#b35">Wang et al. 2020b</ref>) that focus on classification tasks, our method aims to search detection models that are more complex and requires more computation resources. Our kernel reusing technique can significantly reduce the memory and computational cost, making it possible to discover an effective architecture in a few GPU-days. Apart from NAS works, RepVGG ) is also related to our approach, which introduces a reparam strategy to merge skip-connection, 3 ? 3 and 1 ? 1 convolutions for a plain inference-time model. However, the motivation of RepVGG is to stabilize the training of VGG, and the merge process is applied after the training stage. In contrast, our kernel reusing technique is applied during the search process and aims to reduce the memory and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Channel Refinement for Channel Number Search</head><p>Layer channels are essential hyperparameters for neural network architectures, which affect the model size and FLOPs. Unfortunately, few differentiable NAS works have explored to search channel numbers, especially for detection tasks.</p><p>In this work, we introduce the dynamic channel refinement technique based on Gumbel reparameterization technique <ref type="bibr" target="#b7">(Gumbel 1954)</ref> to search for optimal expansion rates for layers. Specifically, we sample an expansion rate for each layer at every iteration and refine the operation weights ? dynamically to fit the changeable channel numbers. Sampling for Expansion Rate. Expansion rate for one layer can be sampled by Gumbel-argmax technique:</p><formula xml:id="formula_2">E = one hot arg max i (log? i e + g i ) ,<label>(2)</label></formula><p>where? e = softmax(? e ) is the normalized weights for candidate expansion rates, and g i are random variables sampled from Gumbel(0, 1) distribution. To make E differentiable w.r.t. ? e , we adopt Gumbel-softmax to relax the sampled vector as follows, where ? is a gradually decayed tempera-</p><formula xml:id="formula_3">ture.? = exp (log? i e + g i )/? j exp (log? j e + g j )/? ,<label>(3)</label></formula><p>Therefore, we adopt the one-hot vector E (Eq. 2) to activate one candidate expansion rate during the forward pass and utilize the relaxed vector? ( Eq. 3) to obtain gradients for ? e during the back-propagation, which is a popular reparameterization technique that has been widely-use in many recent works <ref type="bibr" target="#b4">(Dong and Yang 2019;</ref><ref type="bibr" target="#b31">Wan et al. 2020)</ref>.</p><p>Dynamic Channel Refinement for Operation Weights. After sampling an expansion rate e l for layer l with base channel number C l , the output channel becomes e l C l . The operation weights on layer l can be refined by preserving the first e l C l filters. Besides, it affects the input channel of convolution on layer l + 1. Generally, sampling channels on one layer will affect the operation weights on the current and next layer. We, therefore, dynamically refine channels for weights of adjacent layers, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. FBNet-V2 <ref type="bibr" target="#b31">(Wan et al. 2020</ref>) is related to our method, which also utilizes the Gumbel technique to search for classification model architectures. However, FBNet-V2 has to pad zero on channels to obtain a unified dimension due to short-cut connections, resulting in useless computation. In contrast, we discard the short-cut connection and dynamically refine the channel numbers for each layer at every iteration. There is no useless computation resulting from padding zeros on channels.</p><p>Transformation for Concatenation Layers. The channel number for each layer alters dynamically during the search process, which brings difficulty to refine weights for convolutions after a concatenation layer. Specifically, suppose two features with expansion rates and base channels (e 1 , C 1 ) and (e 2 , C 2 ) are concatenated. A convolution is applied after the concatenation layer, then the activated input channels of weight are separated (0 ? e 1 C 1 and C 1 ? C 1 + e 2 C 2 ), making it hard to extract. To this end, we first give the following proposition, which is proved in the supplementary material.</p><p>Proposition 1 The output of a concatenation layer followed by a convolution layer is equivalent to the sum of separate convolutions on the inputs. Therefore, we can transfer concatenation layers to sum layers without any loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Detection-oriented Search Space Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-down Fusion Block</head><p>Bottom-up Fusion Block  EAutoDet-s 7.9 ? 10 11 9.8 ? 10 24 7.7 ? 10 36 EAutoDet-m 3.8 ? 10 18 5.2 ? 10 30 2.0 ? 10 49 EAutoDet-l 1.8 ? 10 25 2.8 ? 10 36 5.0 ? 10 61 EAutoDet-x 8.7 ? 10 31 1.5 ? 10 42 1.3 ? 10 74 tasks, YOLO models are specifically designed for detection tasks by experts considering both speed and performance. Therefore, we would like to absorb the knowledge of those elaborate architectures and thus design a sophisticated and large detection-oriented search space. In particular, our method ameliorates the computation of convolutions in a supernet rather than restricting architectures of submodels, making it flexible to suit such complex and large search spaces. Specifically, we resort to YOLOv5 <ref type="bibr" target="#b13">(Jocher 2020)</ref> and PANet <ref type="bibr" target="#b21">(Liu et al. 2018</ref>) and construct four types of supernets with various widths and depths, denoted as s (small), m (medium), l (large), and x (extra large), whose details are in the supplementary. In the following, we sepa-rately introduce the search spaces for backbone and FPN for their fundamentally different roles in the detection pipeline. The size of search space is also given in <ref type="table" target="#tab_2">Table 2</ref>, and the details are shown in the supplementary. Search Space for Backbone. We propose to search operation types and channel numbers for down-sampling operators and structure of blocks. The supernet is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. 1) Down-sampling operators have four candidates: {1x1 conv, 3x3 conv, 5x5 conv, 3x3 dilated conv} and three choices of expansion rate for output channel: {0.5, 0.75, 1.0}; 2) Bottleneck cell consists of two convolutions with three choices of expansion rate: {0.5, 0.75, 1.0}, and the second convolution have three candidates: {3x3 conv, 5x5 conv, 3x3 dilated conv}; 3) C3-block has two 1x1 convolutions with two choices of expansion rates: {0.75, 1.0}. Architectures of different layers are independently searched.</p><formula xml:id="formula_4">Mix-up 1x1Conv ( ) 1x1Conv (e) Conv ( , , ) Bottleneck (? ) 1x1Conv ( ) C3-block 1x1Conv ( )</formula><p>Search Space for Feature Pyramid Network. The supernet for top-down and bottom-up fusion blocks is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, which enables to search connections of features in three spatial scales, operation types and channel numbers of each connection, and the structure of C3-blocks. 1) Nodes indicate feature maps and connect with all their predecessors in the supernet. After the search stage, only two predecessors will be selected for each node; 2) Each red edge contains four candidate operations: {1x1 conv, 3x3 conv, 5x5 conv, 3x3 dilated conv} with three possible expansion rates for the output channel: {0.5,0.75,1.0}; 3) C3blocks, whose architectures are also searched, are concate- </p><formula xml:id="formula_5">ture z j = i&lt;j ? (i,j) e ? o?O? (i,j) o ? o(x i ) ,</formula><p>where O is the candidate operation set, x i is the features of predecessors. After the search stage, top-2 edges are preserved for each node according to ? e , and one operation is selected on each edge according to ? o .</p><p>Deriving the Final Architecture. Referring to DARTS, we utilize the magnitude of architecture parameters as the importance estimation for candidate operations. The final backbone architecture is derived by preserving the best operation and removing others. While for the feature pyramid network, nodes in each fusion block preserve top-2 connections, and each connection will preserve the best operation. <ref type="figure" target="#fig_6">Fig. 6</ref> compares the architecture of PANet, our supernet, and the discovered FPN module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on two popular detection tasks: classic detection and rotation detection. The former is typical in many detection contests to locate common objects; The latter has been widely used in aerial images and aims to locate the ground object instances with an oriented bounding box (OBB). In this work, we first search on a large and general dataset for classic detection tasks and then evaluate the performance of our discovered architectures on both classic and rotation detection tasks.</p><p>Benchmarks. For the classic detection task, we adopt MS-COCO 2017 benchmark, which contains 118K training images, 5K validation images, and 41K test images in 80 common object categories. For the rotation detection task, we adopt DOTA-v1.0 benchmark, which is one of the largest aerial image detection benchmarks. The fully annotated DOTA benchmark contains 15 object categories: Plane (PL), Baseball diamond (BD), Bridge (BR), Ground field track (GTF), Small vehicle (SV), Large vehicle (LV), Ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC).</p><p>Search Settings. We search on MS-COCO benchmark that has sufficient common objects under various scenes. The training set is divided into two parts to train architecture parameters and network weights. A supernet is built in which the architectures of blocks are independently searched. The final architecture is derived after alternately optimizing architecture parameters and network weights for 50 epochs by an SGD optimizer. Evaluation Settings. We adopt mean average precision (mAP) as the evaluation metric to evaluate the performance of our models. Firstly, the discovered architectures are trained on MS-COCO training set from scratch for 300 epochs by an SGD optimizer and evaluated on its validation and test sets. We directly utilize the hyper-parameters provided by YOLOv5 for a fair comparison. To fairly compare the speed (FPS) with YOLO methods, we convert the trained models to the style of YOLOv4 <ref type="bibr" target="#b1">(Bochkovskiy, Wang, and Liao 2020)</ref> and evaluate the FPS on the Darknet platform <ref type="bibr">(Redmon 2013</ref><ref type="bibr" target="#b28">(Redmon -2016</ref>, which is written in C and CUDA. Secondly, we train the architectures on rotation detection task on DOTA-v1.0 training set from scratch for 300 epochs and evaluate them on the validation and test sets. Notice that we train and test on a single input scale unlike previous works <ref type="bibr" target="#b42">Yang et al. 2021b</ref>) that adopt multi-scale training technique and random rotation augmentation. All our experiments are trained and tested on the V100 GPU, and our models are trained on PyTorch platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study of Backbone and FPN Search</head><p>We compare the performance of joint and independent search for backbone and FPN in <ref type="table" target="#tab_3">Table 3</ref>, where 'default' indicates that we directly adopt the architecture of YOLOv5, and 'searched' indicates that we search for the architectures. We observe that 1) Joint search achieves the best performance, showing the effectiveness of our algorithm and the necessity of joint search for detection models. 2) The search performance is much more sensitive to the architecture of FPN compared to the backbone module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Classic Detection Benchmark:</head><p>MS-COCO <ref type="table" target="#tab_4">Table 4</ref> reports the performance of our methods and compares with other state-of-the-art works on the COCO testdev dataset. Different blocks indicate models with various inference speeds and prediction performance. We observe  <ref type="table" target="#tab_4">Table 4</ref>). The discovered architecture outperforms YOLOv4-csp by 0.3% mAP with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transferablity Evaluation</head><p>We transfer the discovered x-level model to s, m, and l levels to evaluate the transferability of the discovered architecture. <ref type="table" target="#tab_5">Table 5</ref> compares the performance of the discovered models and the transferred models on the validation set of MS-COCO. We observe that: 1) Though transferred slevel and m-level can outperform baselines (YOLOv5), significant gaps exist between them and the directly searched models; 2) The transferred l-level model and the searched  <ref type="figure">Figure 7</ref>: FPN architecture of our searched models. 'k' denotes kernel size and 'd' denotes dilation ratio. Note that s-level and m-level have the same topology, while l-level and x-level discard the 32? down-sampled features (dashed node), which are the output of spatial pyramid pooling layer (SPP). In (d), we use blue lines and nodes to highlight the discarded SPP layer. We further analyze the effect of SPP by replacing one of the blue lines with the edge between dashed and blue nodes. one achieve competitive performance. We attribute it to the different architecture preferences for small and large neural networks. By comparing the discovered architectures, we find that they have similar backbone structures but rather different FPN structures, as shown in <ref type="figure">Fig. 7</ref>. Specifically, s-level and m-level have the same FPN topology but differ in operation types (kernel size and dilation ratio of convolutions). Besides, both l-level and x-level discard the 32? down-sampled features, which is the output of SPP.</p><p>To verify our analysis, we transfer the searched m-level model to s-level since they have similar FPN structures. <ref type="table" target="#tab_5">Table 5</ref> shows that the s-level model transferred from m achieves 39.5% mAP, significantly surpassing the one transferred from x by more than 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion on Spatial Pyramid Pooling</head><p>Spatial pyramid pooling (SPP) ) is designed to integrate various receptive fields and extract multi-scale features with the same spatial size. Most recent manuallydesigned detectors adopt it by default, including YOLOv4 and YOLOv5. However, our experiments show that the value of SPP degrades when the network gets deeper. As shown in <ref type="figure">Fig. 7</ref>, models on l-level and x-level discard the SPP layer and choose to enlarge the receptive field by 5?5 convolution. While models on s-level and m-level still prefer the SPP layer.</p><p>In our analysis, SPP is vital for shallow networks as it can increase the receptive field to extract global information. However, the receptive field is enough for deep networks, making the SPP layer dispensable with the increment of network depth. Results in <ref type="table" target="#tab_5">Table 5</ref> supports our analysis: When transferring x-level models to s-level, the performance degrades significantly. To verify the effectiveness of SPP for shallow networks, we manually add SPP for the transferred s-level model. In <ref type="figure">Fig. 7(d)</ref>, the blue node connects with 16? and 8? down-sampled features. We construct two models by removing one of the connections (blue lines) and connecting the blue node with the dashed node (output of SPP layer), whose performance on the COCO validation set is reported in <ref type="table" target="#tab_6">Table 6</ref>. We observe that after recovering the SPP layer, the performance of transferred model can be improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on Rotation Detection Benchmark: DOTA</head><p>We utilize Circular Smooth Label (CSL) technique <ref type="bibr" target="#b41">(Yang and Yan 2020)</ref> to obtain robust angular prediction through classification without suffering boundary conditions. The baseline adopts RetinaNet    <ref type="bibr" target="#b11">(He et al. 2016</ref>) and H-104 denotes Hourglass-104 <ref type="bibr" target="#b24">(Newell, Yang, and Deng 2016)</ref>  and EAutoDet-m surpasses YOLOv5-m by over 0.7%. Table 8 further details the comparison between our models with YOLOv5. The above results show the generalization of our discovered architectures, which also verify the effectiveness of our search method. Notice that the discovered architectures are not limited to the specific CSL as tested in our experiment, as the search paradigm is agnostic to the choice of rotation detection loss, e.g., GWD <ref type="bibr" target="#b42">(Yang et al. 2021b</ref>) and BBAVectors <ref type="bibr" target="#b44">(Yi et al. 2021)</ref>, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces kernel and dynamic channel refinement techniques and proposes a fast and memory-efficient search method for detection. We also design a sophisticated and large search space for detection by absorbing the knowledge of well-designed architectures of YOLO models. Our method can discover light-weighted models in 1.4 GPU-days, achieving 40.1 mAP on COCO test-dev with 120 FPS surpassing state-of-the-art NAS methods. Besides, our ablation studies suggest that the SPP plays a more vital role in shallow models than in deep models in the hope of facilitating future network design for detection. Moreover, the discovered architecture archives 77.05% mAP 50 on DOTA-v1.0 benchmark, outperforming most of the manually-designed models e.g. CSL (Yang and Yan 2020) (76.24%), further verifying the effectiveness of our search method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Rationality of Transformation for Concatenation Layers</head><p>To explain the rationality of transformation for concatenation layers, we can prove the following proposition.</p><p>Proposition 2 The output of a concatenation layer followed by a convolution layer is equivalent to the sum of separate convolutions on the inputs.</p><formula xml:id="formula_6">Suppose we have M features X m ? R N ?Cm?H?W , m ? [1, M ].</formula><p>By concatenating theses features along channel dimension, we can obtain a new feature X ? R N ?( M m=1 Cm)?H?W . A convolution with weights ? ? R Co?( M m=1 Cm)?K?K is then applied on the concatenated feature X. Im2col operation transfers convolution to matrix multiplication, and we get the input and weight matrixX,?, as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>.X ?? = <ref type="table">Table 9</ref>: Structures of the four supernet built based on YOLOv5. 'C' is the number of output channels (width), and 'M' denotes the number of bottleneck cells in the C3 block (depth). The last line indicates the statistics of the supernet structures used to calculate the size of the search space. Specifically, L D , L C , L B is the number of down-sampling layers, C3-blocks and Bottlenecks in the backbone, and K B is the number of Bottlenecks in each fusion block in the FPN module. </p><formula xml:id="formula_7">Statistics L D =4,L C =3,L B =7,K B =3 L D =4,L C =3,L B =14,K B =6 L D =4,L C =3,L B =21,K B =9 L D =4,L C =3,L B =28,K B =12</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of our Search Spaces</head><p>Marco Architectures. To abosrb the knowledge of architectures of YOLO models that are well designed by experts, we refer to YOLOv5 <ref type="bibr" target="#b13">(Jocher 2020)</ref> and build four supernets with various depths and widths, denoted as s (small), m (medium), l (large), and x (extra large). The depth and width of various types of supernet are illustrated in <ref type="table">Table 9</ref>, where 'C' is the number of base output channels indicating the supernet width, and 'M' is the number of bottleneck cells in the C3 block, affecting the supernet depth. The details of our search space for the backbone and FPN modules are introduced as follows. Backbone Search Space. 1) For the down-sampling operator, we design four candidates: {1x1 convolution, 3x3 convolution, 5x5 convolution, 3x3 dilated convolution} and three candidate expansion rates for output channel: {0.5, 0.75, 1.0}; 2) For the bottleneck cell, which consists of two convolutions, we search output channels for both convolutions with candidates {0.5, 0.75, 1.0}, and kernel settings for the second convolution with candidates {3x3 convolution, 5x5 convolution, 3x3 dilated convolution}; 3) For the C3-block, which consists of M bottleneck cells, we search for the expansion rate for its output channels among two candidates: {0.75, 1.0}. We adopt macro search space in this work, where architectures for bottlenecks and C3-blocks from different layers are independently searched. Suppose a backbone contains L D down-sampling layers, L C C3blocks and L B Bottlenecks, the size of search space is (4 ? 3) L D ? 2 L C ? (3 ? 3) L B .</p><p>FPN Search Space. This work extracts three scales of features and builds supernets for both Top-down and Bottom-up fusion blocks. Each node in the supernet indicates a feature map connecting with all its predecessors, and each edge owns four candidate operations: {1x1 convolution, 3x3 convolution, 5x5 convolution, 3x3 dilated convolu-tion} with three possible expansion rates for output channel: {0.5,0.75,1.0}. To derive the final architecture, each node in top-down and bottom-up fusion blocks preserves top-2 connections, and each connection will only preserve the best operation. Since we have three feature scales, there are 6 connections in each fusion block, leading to (4 ? 3) 6 ? 3 2 ? 4 2 ? 5 2 ? 5.4 ? 10 8 candidate connection types for each feature fusion block in total. Furthermore, we concatenate a C3 block with K B Bottlenecks after each feature fusion block, which has 2 ? (3 ? 3) K B . Since we have two fusion blocks: Top-down and Bottom-up block, the size of search space for FPN is {(4 ? 3) 6 ? 3 2 ? 4 2 ? 5 2 ? 2 ? (3 ? 3) K B } 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Experimental Settings</head><p>We search on MS-COCO 2017 detection dataset <ref type="bibr" target="#b17">(Lin et al. 2014)</ref> and evaluation on the test set of MS-COCO and DOTA-v1.0 benchmark. All our models are trained from scratch without pre-training on ImageNet.</p><p>Search Settings. We construct a supernet and define architecture parameters to represent the importance of candidate operations and connections. Unlike DARTS that shares the same cell structure, we independently search architectures for each C3 block and Bottleneck. The training set of MS-COCO is divided into two parts for training architecture parameters and network weights, respectively. The final architecture is derived after alternately optimizing architecture parameters and network weights for 50 epochs by an SGD optimizer. Evaluation Settings. The discovered architectures are trained from scratch for 300 epochs by an SGD optimizer. We directly utilize the hyper-parameters provided by YOLOv5 for a fair comparison. Our experiments are conducted on V100 GPU. To fairly compare the speed (FPS) with YOLO methods, we convert the trained models to the style of YOLOv4 <ref type="bibr" target="#b1">(Bochkovskiy, Wang, and Liao 2020)</ref> and evaluate the FPS on the Darknet platform <ref type="bibr">(Redmon 2013</ref><ref type="bibr" target="#b28">(Redmon -2016</ref>, which is written in C and CUDA. Besides, we evaluate the generalization of the discovered architectures by transferring them to rotation detection task. Specifically, we train models on the training set of DOTA-v1.0 from scratch for 300 epochs and evaluate on the validation and test sets. Notice that we train and test on a single input scale unlike previous works <ref type="bibr" target="#b42">Yang et al. 2021b</ref>) that adopt multi-scale training technique and random rotation augmentation. D Comparison to YOLOv5 models. <ref type="table" target="#tab_12">Table 10</ref> shows the detailed information of our models and YOLOv5 series models, including number of parameters, FLOPs, and mAP on the test test of MS-COCO. We observe that our models archieve better performance than YOLOv5 with similar parameters and FPS, showing the effectiveness of our search method. We will open-source our search and evaluation codes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>are dedicated to reducing the memory. * Correspondence author is Junchi Yan. The work is in part supported by China Major State Research Development Program (2020AAA0107600), NSFC (U19B2035), and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>mAP and FPS of various detection models. Solid or dashed lines indicate NAS or handcrafted architectures. ? : results obtained by our experiments, otherwise from the references.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Compared to DARTS, the kernel reusing technique compounds multiple convolutions into a single 5x5 convolution, which can reduce the memory cost and enables efficient search for backbone and FPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FeaturesFigure 4 :</head><label>4</label><figDesc>Refinement for convolution weights ? on adjacent layers in our dynamic channel refinement technique. e indicates the sampled expansion rate. Dotted blocks in light gray indicate the inactivated channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Unlike ResNet-like and Mobile-like backbones in RetinaNet and Faster-RCNN that are transferred from classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The architecture of supernet, containing all candidate operations and connections in the search space. A red edge indicates candidate operations compounded by the kernel reusing technique, which is illustrated inFig. 2. In backbone, C3block, and SPP module, parentheses under 'Conv' indicate hyper-parameters to search: kernel size k, dilation ratio d, expansion rate of output channels e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Architectures of PANet, supernet of FPN and our discovered model. Nodes on three rows denote feature maps on three spatial sizes. Red edges indicate multiple operations are compounded on that edge. nated at the end of each fusion block to independently extract multi-scale features, as shown inFig. 5. For each fusion block, we introduce architecture parameters ? e and ? o to denote the importance of edges and operations. Suppos? ? = softmax(?) is the normalized weight. The fused fea-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of an input and weight matrixX,?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Search space size: small, medium, large, extra large. Total size equals the multiplication of the backbone and FPN space sizes. Four supernets in various depth/width are designed. Details of the four search spaces are in the supplementary.</figDesc><table><row><cell>Supernet</cell><cell cols="2">Size of Search Space</cell></row><row><cell></cell><cell>Backbone</cell><cell>FPN</cell><cell>Total</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of joint and independent search for backbone and FPN on MS-COCO validation set.</figDesc><table><row><cell>Model</cell><cell cols="2">Architecture</cell><cell>mAP</cell><cell>?</cell></row><row><cell></cell><cell>Backbone</cell><cell>FPN</cell><cell></cell></row><row><cell>YOLOv5-s</cell><cell>default</cell><cell cols="3">default 36.9 +0.0</cell></row><row><cell>EAutoDet-s</cell><cell>searched</cell><cell cols="3">default 37.4 +0.5</cell></row><row><cell>EAutoDet-s</cell><cell>default</cell><cell cols="3">searched 38.9 +2.0</cell></row><row><cell>EAutoDet-s</cell><cell>searched</cell><cell cols="3">searched 40.1 +3.2</cell></row><row><cell>YOLOv5-m</cell><cell>default</cell><cell cols="3">default 44.0 +0.0</cell></row><row><cell cols="2">EAutoDet-m searched</cell><cell cols="3">default 44.6 +0.6</cell></row><row><cell cols="2">EAutoDet-m defualt</cell><cell cols="3">searched 45.0 +1.0</cell></row><row><cell cols="2">EAutoDet-m searched</cell><cell cols="3">searched 45.5 +1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with prior works on the COCO test-dev. FPS for YOLOv5 and our method are calculated on a single V100 GPU, and results for other methods are directly obtained from their papers. Different blocks indicate models with various inference speeds and prediction performance. ' ? ': The results are obtained by our experiments. '-': The value is not provided by the original paper. ' ': The unit of search cost is TPU-days, while the unit of other methods is GPU-days. ' ? ': SPNet<ref type="bibr" target="#b12">(Jiang et al. 2020)</ref> shows the search cost on VOC is 26 GPU-days, and is six times lower than that on COCO. AP 50 AP 75 AP S AP M AP</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone FPN</cell><cell cols="2">Resolution FPS</cell><cell cols="6">#Params mAP L Search (M) (%) (%) (%) (%) (%) (%) Cost</cell></row><row><cell>YOLOv4 (2020)</cell><cell>CD-53</cell><cell>PAN</cell><cell>416</cell><cell cols="2">96 -</cell><cell cols="5">41.2 62.8 44.3 20.4 44.4 56.0 -</cell></row><row><cell>YOLOv5s  ? (2020)</cell><cell>YOLOv5</cell><cell>PAN</cell><cell>640</cell><cell cols="2">113 7.3</cell><cell cols="5">36.9 56.0 40.0 19.9 41.1 46.0 -</cell></row><row><cell>EfficientDet-D0 (2020)</cell><cell cols="2">Efficient-B0 BiFPN</cell><cell>512</cell><cell cols="2">98 3.9</cell><cell cols="5">33.8 52.2 35.8 12.0 38.3 51.2 -</cell></row><row><cell>NAS-FPN (2019)</cell><cell>Res50</cell><cell cols="2">Searched 640</cell><cell cols="2">24 60.3</cell><cell>39.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-333</cell></row><row><cell cols="2">NAS-FCOS@128 (2020a) Res50</cell><cell cols="3">Searched 1333?800 -</cell><cell>27.8</cell><cell>37.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-28</cell></row><row><cell>SpineNet-49S (2020)</cell><cell>Searched</cell><cell>FPN</cell><cell>640</cell><cell>-</cell><cell>11.9</cell><cell cols="5">39.5 59.3 43.1 20.9 42.2 54.3 -</cell></row><row><cell>SM-NAS:E2 (2020)</cell><cell></cell><cell></cell><cell cols="3">800?600 25 -</cell><cell cols="5">40.0 58.2 43.4 21.1 42.4 51.7 187</cell></row><row><cell>EAutoDet-s (ours)</cell><cell cols="3">Searched Searched 640</cell><cell cols="2">120 9.1</cell><cell cols="5">40.1 58.7 43.5 21.7 43.8 50.5 1.4</cell></row><row><cell cols="2">YOLOv3 + ASFF (2019) D-53</cell><cell>ASFF</cell><cell>416</cell><cell cols="2">54 -</cell><cell cols="5">40.6 60.6 45.1 20.3 44.2 54.1 -</cell></row><row><cell>YOLOv4 (2020)</cell><cell>CD-53</cell><cell>PAN</cell><cell>512</cell><cell cols="2">83 -</cell><cell cols="5">43.0 64.9 46.5 24.3 46.1 55.2 -</cell></row><row><cell>YOLOv4-csp (2021)</cell><cell>CD-53</cell><cell>PAN</cell><cell>512</cell><cell cols="2">80  ? 43</cell><cell cols="5">46.2 64.8 50.2 24.6 50.4 61.9 -</cell></row><row><cell>YOLOv5m  ? (2020)</cell><cell>YOLOv5</cell><cell>PAN</cell><cell>640</cell><cell cols="2">88 21.4</cell><cell cols="5">43.9 62.5 47.6 25.1 48.1 54.9 -</cell></row><row><cell>EfficientDet-D1 (2020)</cell><cell cols="2">Efficient-B1 BiFPN</cell><cell>640</cell><cell cols="2">74 6.6</cell><cell cols="5">39.6 58.6 42.3 17.9 44.3 56.0 -</cell></row><row><cell>DetNAS (2019)</cell><cell>Searched</cell><cell>FPN</cell><cell cols="2">1333?800 -</cell><cell>-</cell><cell cols="5">42.0 63.9 45.8 24.9 45.1 56.8 44</cell></row><row><cell>NAS-FPN (2019)</cell><cell>Res50</cell><cell cols="2">Searched 1024</cell><cell cols="2">13 60.3</cell><cell>44.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-333</cell></row><row><cell>Auto-FPN (2019)</cell><cell>Res50</cell><cell cols="2">Searched 800</cell><cell>-</cell><cell>32.6</cell><cell cols="5">40.5 61.5 43.8 25.6 44.9 51.0 16</cell></row><row><cell cols="2">NAS-FCOS@256 (2020a) R-101</cell><cell cols="3">Searched 1333?800 -</cell><cell>57.3</cell><cell>43.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-28</cell></row><row><cell>SpineNet-49 (2020)</cell><cell>Searched</cell><cell>FPN</cell><cell>640</cell><cell>-</cell><cell>28.5</cell><cell cols="5">42.8 62.3 46.1 23.7 45.2 57.3 -</cell></row><row><cell>SM-NAS:E3 (2020)</cell><cell></cell><cell></cell><cell cols="3">800?600 20 -</cell><cell cols="5">42.8 61.2 46.5 23.5 45.5 55.6 187</cell></row><row><cell>Hit-Detector (2020)</cell><cell>Searched</cell><cell cols="3">Searched 1200?800 -</cell><cell>27.1</cell><cell cols="5">41.4 62.4 45.9 25.2 45.0 54.1 -</cell></row><row><cell>OPA-FPN@64 (2021)</cell><cell>Res50</cell><cell cols="4">Searched 1333?800 22 29.5</cell><cell>41.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-4</cell></row><row><cell>EAutoDet-m (ours)</cell><cell cols="3">Searched Searched 640</cell><cell cols="2">70 28.1</cell><cell cols="5">45.2 63.5 49.1 25.7 49.1 57.3 2.2</cell></row><row><cell cols="2">YOLOv3 + ASFF (2019) D-53</cell><cell>ASFF</cell><cell>608</cell><cell cols="2">46 -</cell><cell cols="5">42.4 63.0 47.4 25.5 45.7 52.3 -</cell></row><row><cell>YOLOv4 (2020)</cell><cell>CD-53</cell><cell>PAN</cell><cell>608</cell><cell cols="2">62 -</cell><cell cols="5">43.5 65.7 47.3 26.7 46.7 53.3 -</cell></row><row><cell>YOLOv4-csp (2021)</cell><cell>CD-53</cell><cell>PAN</cell><cell>640</cell><cell cols="2">65  ? 53</cell><cell cols="5">47.5 66.2 51.7 28.2 51.2 59.8 -</cell></row><row><cell>EAutoDet-csp (ours)</cell><cell>CD-53</cell><cell cols="2">Searched 640</cell><cell cols="2">55 49.8</cell><cell cols="5">47.8 66.1 51.9 28.6 51.5 60.1 4.2</cell></row><row><cell>YOLOv5l  ? (2020)</cell><cell>YOLOv5</cell><cell>PAN</cell><cell>640</cell><cell cols="2">59 47.1</cell><cell cols="5">46.8 65.4 50.9 27.7 51.0 58.5 -</cell></row><row><cell>EfficientDet-D2 (2020)</cell><cell cols="2">Efficient-B2 BiFPN</cell><cell>768</cell><cell cols="2">57 8.1</cell><cell cols="5">43.0 62.3 46.2 22.5 47.0 58.4 -</cell></row><row><cell>SPNet(BNB) (2020)</cell><cell>Searched</cell><cell>FPN</cell><cell cols="3">1333?800 10 -</cell><cell cols="5">45.6 64.3 49.6 28.4 48.4 60.1 156  ?</cell></row><row><cell>SM-NAS:E5 (2020)</cell><cell></cell><cell></cell><cell cols="2">1333?800 9</cell><cell>-</cell><cell cols="5">45.9 64.6 48.6 27.1 49.0 58.0 187</cell></row><row><cell>OPA-FPN@160 (2021)</cell><cell>Res50</cell><cell cols="4">Searched 1333?800 13 60.6</cell><cell>47.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-4</cell></row><row><cell>EAutoDet-l (ours)</cell><cell cols="3">Searched Searched 640</cell><cell cols="2">59 34.4</cell><cell cols="5">47.9 66.3 52.0 28.3 52.0 59.9 4.5</cell></row><row><cell cols="2">YOLOv3 + ASFF (2019) D-53</cell><cell>ASFF</cell><cell>800</cell><cell cols="2">29 -</cell><cell cols="5">43.9 64.1 49.2 27.0 46.6 53.4 -</cell></row><row><cell>YOLOv5x  ? (2020)</cell><cell>YOLOv5</cell><cell>PAN</cell><cell>640</cell><cell cols="2">43 87.8</cell><cell cols="5">49.1 67.5 53.6 30.2 53.4 61.4 -</cell></row><row><cell>EfficientDet-D3 (2020)</cell><cell cols="2">Efficient-B3 BiFPN</cell><cell>896</cell><cell cols="2">35 12</cell><cell cols="5">45.8 65.0 49.3 26.6 49.4 59.8 -</cell></row><row><cell>SPNet(XB) (2020)</cell><cell>Searched</cell><cell>FPN</cell><cell cols="2">1333?800 6</cell><cell>-</cell><cell cols="5">47.4 65.7 51.9 29.6 51.0 60.4 156  ?</cell></row><row><cell>EAutoDet-x (ours)</cell><cell cols="3">Searched Searched 640</cell><cell cols="2">41 86.0</cell><cell cols="5">49.2 67.5 53.6 30.4 53.4 61.5 22</cell></row><row><cell cols="4">that our discovered models (EAutoDet) achieve the best per-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">formance. Specifically, EAutoDet-s achieves 40.1 mAP with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">120 FPS, outperforming EfficientDet-D0 by 6.3% mAP with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">similar inference speed. Compared to manually-designed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">detectors (YOLO series), our method has competitive and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">even better performance. EAutoDet-m achieves 45.2% mAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">with 69.9 FPS, surpassing YOLOv4 by 2.2% mAP. More-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">over, we compare to YOLOv4-csp (Wang, Bochkovskiy, and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Liao 2021) by inheriting its backbone CD-53 and searching</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FPN architecture ('EAutoDet-csp' in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the transferred models and directly searched models on the COCO validation set.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params mAP</cell><cell>?</cell></row><row><cell>s (YOLOv5)</cell><cell>7.3M</cell><cell cols="2">36.9 +0.0</cell></row><row><cell>s (transferred from x)</cell><cell>8.0M</cell><cell cols="2">37.4 +0.5</cell></row><row><cell>s (transferred from m)</cell><cell>8.0M</cell><cell cols="2">39.5 +2.6</cell></row><row><cell>s (searched)</cell><cell>9.1M</cell><cell cols="2">40.1 +3.2</cell></row><row><cell>m (YOLOv5)</cell><cell>21.4M</cell><cell cols="2">44.0 +0.0</cell></row><row><cell>m (transferred from x)</cell><cell>21.8M</cell><cell cols="2">44.6 +0.6</cell></row><row><cell>m (searched)</cell><cell>28.1M</cell><cell cols="2">45.5 +1.5</cell></row><row><cell>l (YOLOv5)</cell><cell>47.1M</cell><cell cols="2">47.0 +0.0</cell></row><row><cell>l (transferred from x)</cell><cell>48.6M</cell><cell cols="2">47.3 +0.3</cell></row><row><cell>l (searched)</cell><cell>42.2M</cell><cell cols="2">47.9 +0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">: Study of SPP on MS-COCO validation set. 's-16-8'</cell></row><row><cell cols="4">is the original transferred model from x-level, while 's-32-</cell></row><row><cell cols="4">8' and 's-32-16' are the modified models by adding connec-</cell></row><row><cell>tions from the SPP layer.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">w/ SPP mAP</cell><cell>?</cell></row><row><cell>YOLOv5-s</cell><cell></cell><cell cols="2">36.9 +0.0</cell></row><row><cell>s-16-8 (transferred from x)</cell><cell>?</cell><cell cols="2">37.4 +0.5</cell></row><row><cell>s-32-16 (transferred from x</cell><cell></cell><cell cols="2">39.0 +2.1</cell></row><row><cell>s-32-8 (transferred from x)</cell><cell></cell><cell cols="2">38.8 +1.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison on the test set of oriented bounding boxes (OBB) task in DOTA-v1.0 benchmark. R152 denotes ResNet-152</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>. ? : Models are trained with multi-scale training techniques, while others are not. : two-stage detection framework, while others belong to one-stage framework. Method PL BD BR GTF SV LV SH TC BC ST SBF RA HA SP HC mAP 50 ReDet(ReR50) (2021) 88.79 82.64 53.97 74.00 78.13 84.06 88.04 90.89 87.78 85.75 61.76 60.39 75.96 68.07 63.39 76.25 CenterMap(R101) (2021) 89.83 84.41 54.60 70.25 77.66 78.32 87.19 90.66 84.89 85.27 56.46 69.23 74.13 71.56 66.06 76.03 CSL(R152) (2020) ? 90.13 84.43 54.57 68.13 77.32 72.98 85.94 90.74 85.95 86.36 63.42 65.82 74.06 73.67 70.08 76.24 O .24 53.86 62.35 81.19 84.72 88.57 90.77 80.98 88.09 53.92 61.90 75.99 80.43 68.59 76.33 CSL(EAutoDet-m) 89.04 86.61 53.83 63.05 81.51 85.12 88.46 90.77 88.21 87.93 56.04 60.96 75.96 82.13 66.18 77.05</figDesc><table><row><cell>2 -DNet(H104) (2020)</cell><cell>89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04</cell></row><row><cell>DAL(R101)(2021)</cell><cell>88.61 79.69 46.27 70.37 65.89 76.10 78.53 90.84 79.98 78.41 58.71 62.02 69.23 71.32 60.65 71.78</cell></row><row><cell>P-RSDet(R101) (2020)</cell><cell>88.58 77.83 50.44 69.29 71.10 75.79 78.66 90.88 80.10 81.71 57.92 63.03 66.30 69.77 63.13 72.30</cell></row><row><cell cols="2">BBAVectors(R101) (2021) 88.35 79.96 50.69 62.18 78.43 78.98 87.94 90.85 83.58 84.35 54.13 60.24 65.22 64.28 55.70 72.32</cell></row><row><cell>DRN(H104) (2020)  ?</cell><cell>89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23</cell></row><row><cell>DCL(R152) (2021a)  ?</cell><cell>89.10 84.13 50.15 73.57 71.48 58.13 78.00 90.89 86.64 86.78 67.97 67.25 65.63 74.06 67.05 74.06</cell></row><row><cell cols="2">PolarDet(R101) (2021)  ? 89.65 87.07 48.14 70.97 78.53 80.34 87.45 90.76 85.63 86.87 61.64 70.32 71.92 73.09 67.15 76.64</cell></row><row><cell>GWD(R152) (2021b)  ?</cell><cell>86.96 83.88 54.36 77.53 74.41 68.48 80.34 86.62 83.41 85.55 73.47 67.77 72.57 75.76 73.40 76.30</cell></row><row><cell cols="2">CSL(YOLOv5-s) (2020) 88.93 76.07 50.97 59.61 81.38 84.00 88.19 90.81 80.52 87.63 47.38 62.85 68.27 80.63 62.74 74.00</cell></row><row><cell>CSL(EAutoDet-s)</cell><cell>88.64 84.78 51.02 62.04 81.30 84.02 88.23 90.82 85.79 87.36 52.20 65.68 73.72 74.33 65.93 75.72</cell></row><row><cell cols="2">CSL(YOLOv5-m) (2020) 88.36 85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">: Comparison to YOLOv5 on the validation set of ori-</cell></row><row><cell cols="4">ented bounding box (OBB) task in DOTA-v1.0 benchmark.</cell></row><row><cell>Model</cell><cell cols="3">#Params FLOPs mAP 50 (%)</cell></row><row><cell>YOLOv5-s</cell><cell>7.6M</cell><cell>17.5G</cell><cell>73.71</cell></row><row><cell>YOLOv5-m</cell><cell>21.7M</cell><cell>50.6G</cell><cell>74.65</cell></row><row><cell>EAutoDet-s</cell><cell>8.7M</cell><cell>21.5G</cell><cell>74.09</cell></row><row><cell cols="2">EAutoDet-m 22.7M</cell><cell>50.8G</cell><cell>76.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Detailed comparison between the discovered architectures and original YOLOv5 models. MAP is tested on the test set of MS-COCO.</figDesc><table><row><cell>Model</cell><cell cols="4">#Params FLOPs FPS mAP</cell></row><row><cell>YOLOv5-s</cell><cell>7.3M</cell><cell>17.1G</cell><cell>113</cell><cell>36.9</cell></row><row><cell>YOLOv5-m</cell><cell>21.4M</cell><cell>51.4G</cell><cell>88</cell><cell>43.9</cell></row><row><cell>YOLOv5-l</cell><cell>47.1M</cell><cell>112.5G</cell><cell>59</cell><cell>46.8</cell></row><row><cell>YOLOv5-x</cell><cell>87.8M</cell><cell>219.0G</cell><cell>43</cell><cell>49.1</cell></row><row><cell>AutoYOLO-s</cell><cell>9.1M</cell><cell>24.9G</cell><cell>120</cell><cell>40.1</cell></row><row><cell>AutoYOLO-m</cell><cell>28.1M</cell><cell>60.8G</cell><cell>70</cell><cell>45.2</cell></row><row><cell>AutoYOLO-l</cell><cell>34.4M</cell><cell>115.4G</cell><cell>59</cell><cell>47.9</cell></row><row><cell>AutoYOLO-x</cell><cell>86.0M</cell><cell>225.3G</cell><cell>41</cell><cell>49.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">M m=1X m ?? m , which is the sum of M multiplications of small matrix, whereX m ?? m is exactly the convolution on input feature X m . Consequently, Proposition 2 is proved.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Detnas: Backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RepVGG: Making VGG-Style ConvNets Great Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SpineNet: Learning scalepermuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Girshick, R. 2015. Fast r-cnn. In ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Government Printing Office</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ReDet: A Rotation-Equivariant Detector for Aerial Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SP-NAS: Serial-to-parallel backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">YOLOv5 Documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<ptr target="https://docs.ultralytics.com/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Yolov5 for Oriented Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaixuan</surname></persName>
		</author>
		<ptr target="https://github.com/hukaixuan19970627/yolov5obb" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>Tuytelaars, T., eds., ECCV</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning spatial fusion for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09516</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Path Aggregation Network for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic Anchor Learning for Arbitrary-Oriented Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic Refinement Network for Oriented and Densely Packed Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11207" to="11216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularized Evolution for Image Classifier Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet/" />
		<title level="m">Darknet: Open Source Neural Networks in C</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neurips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Single-Path NAS: Designing Hardware-Efficient ConvNets in Less Than 4 Hours</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scaled-yolov4: Scaling cross stage partial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Center Probability Map for Detecting Objects in Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote. Sens</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NAS-FCOS: Fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MergeNAS: Merge Operations into One for Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BA-NANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Arbitrary-Oriented Object Detection with Circular Smooth Label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaopeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SM-NAS: structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Oriented object detection in aerial images with box boundary-aware vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2150" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Polardet: A fast, more precise detector for rotated target in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="5821" to="5851" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Arbitrary-Oriented Object Detection in Remote Sensing Images Based on Polar Coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="223373" to="223384" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
