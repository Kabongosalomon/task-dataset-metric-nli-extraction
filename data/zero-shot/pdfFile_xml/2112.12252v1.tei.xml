<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kiefer</surname></persName>
							<email>benjamin.kiefer@uni-tuebingen.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ott</surname></persName>
							<email>david.ott@uni-tuebingen.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
							<email>andreas.zell@uni-tuebingen.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Tuebingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Tuebingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Tuebingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acquiring data to train deep learning-based object detectors on Unmanned Aerial Vehicles (UAVs) is expensive, time-consuming and may even be prohibited by law in specific environments. On the other hand, synthetic data is fast and cheap to access. In this work, we explore the potential use of synthetic data in object detection from UAVs across various application environments. For that, we extend the open-source framework DeepGTAV to work for UAV scenarios. We capture various largescale high-resolution synthetic data sets in several domains to demonstrate their use in real-world object detection from UAVs by analyzing multiple training strategies across several models. Furthermore, we analyze several different data generation and sampling parameters to provide actionable engineering advice for further scientific research. The DeepGTAV framework is available at https://git.io/Jyf5j.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Unmanned Aerial Vehicles (UAVs) equipped with cameras are increasingly used as autonomous vision systems in a wide range of applications, such as traffic surveillance, search and rescue, agriculture and smart cities <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. In all of these scenarios, these systems rely on the robust detection of objects of interest. Although object detection on natural images taken from hand-held or car-mounted cameras has been studied intensively, object detection from UAVs trails behind in performance <ref type="bibr" target="#b4">[5]</ref>. This is partly due to the limited amount of annotated publicly available data sets. In turn, this is partly caused by the highly complex data generating missions, which are subject to permissions, UAV flying restrictions and environmental factors <ref type="bibr" target="#b5">[6]</ref>. Furthermore, there are more degrees of freedom in the UAV domain (camera angles, position), which account for objects from unnatural perspectives, e.g. small objects from above. These difficulties are on top of other common obstacles, such as high and enduring labeling costs.</p><p>With more publicly available data sets, object detection on UAVs could be improved. However, data collection and labeling are expensive and time-consuming. Furthermore, data set collection raises serious privacy (e.g. GDPR <ref type="bibr" target="#b6">[7]</ref> in Europe) <ref type="bibr" target="#b7">[8]</ref> and security concerns because specific locations demand a long and complicated approval procedure.</p><p>Moreover, currently published data sets suffer from large class and domain imbalances <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Both problems are inherently caused by a problematic capturing procedure, where many variables cannot be controlled. On the other hand, synthetically generated data for computer vision problems can help train data demanding visual perception systems because it is comparably fast and inexpensive to acquire. Furthermore, this data can easily be tailored to specific requirements. Several works address synthetic data generation in computer vision. However, most of them focus on driving, simulating constrained traffic situations <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Few works consider the generation of synthetic data captured from UAVs. However, these only focus on the capturing process of the sensors and are lacking in world, object and physics details, or do not feature them at all <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>While the use of synthetic data in the context of autonomous driving has been investigated thoroughly, it is not clear whether these findings can be applied in the UAV setting. As mentioned in the beginning, the problems for general UAV object detection also apply to synthetic data generation. Most simulation engines focus on autonomous driving, and therefore rendering is aimed at looking realistic for these scenarios. Even if these simulation engines could technically be adapted to the UAV setting, light conditions, shadows, visibility range, rendered resolution, and more may significantly affect the quality of the rendered footage. In turn, the generated data may be less valuable for transfer to the real world.</p><p>In this work, we consider the video game Grand Theft Auto V (GTAV) <ref type="bibr" target="#b15">[16]</ref> as a simulation platform. It offers numerous detailed object models that interact in a large world with realistic graphics and physics simulations. Building on previous works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, we extend the DeepGTAV data extraction tool to work for airborne scenarios by extending its functionalities regarding in-game agent and camera position and rotation, environment manipulation, object spawning and metadata extraction.</p><p>Using this simulation engine, we create three large-scale high-resolution (4K) synthetic object detection data sets in different application scenarios. Using these, we evaluate different training strategies and their transfer performances on three corresponding real-world data sets. We provide analytical insights and actionable advice on which settings to choose by doing extensive experiments and ablations.</p><p>In particular, our contributions are as follows:</p><p>? We modify and extend the DeepGTAV tool, in particular, to allow the production of airborne data. We discuss the specific improvements in Section II-A1. <ref type="bibr">?</ref> We provide three large-scale high-resolution metadata annotated data sets in different UAV application scenarios and make them publicly available. ? We evaluate the applicability of these data sets to improve real-world object detection and analyze the from-scratch performance. ? We analyze the influence of different parameters of the data generation, e.g. the graphics quality and the alignment of metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>This section reviews the literature regarding synthetic data generation engines, synthetic data taken from UAVs and the role of meta/environmental data in this context. 1) Synthetic Data Generation Engines: Many data generation engines focus on autonomous driving. AirSim <ref type="bibr" target="#b19">[20]</ref> and Carla <ref type="bibr" target="#b20">[21]</ref> leverage the Unreal Engine <ref type="bibr" target="#b21">[22]</ref> to create a simulation engine suitable for autonomous driving scenarios. While AirSim also provides support for UAV scenarios, it lacks a world to simulate objects and its physics implementation, which must be created and modeled first. Instead of laboriously creating simulation engines, some researchers use the computer game GTAV <ref type="bibr" target="#b15">[16]</ref> to generate data. With DeepGTAV <ref type="bibr" target="#b16">[17]</ref>, researchers leverage GTAV to gather data for autonomous driving. PreSIL <ref type="bibr" target="#b13">[14]</ref> builds upon this approach to refine the data acquisition process with their tool DeepGTAV-PreSIL. However, both systems lack the feature to modify the in-game agent and camera position and rotation, essentially limiting it to a single autonomous driving scenario. Further manipulations, such as spawning objects, manipulating the environment and extracting the metadata, are not possible either. The work at hand builds upon these two works, improving and extending them to open it for UAV research.</p><p>There are many more synthetic, simulated environments. Please see <ref type="bibr" target="#b22">[23]</ref> for an overview.</p><p>2) Data Sets Taken on UAVs: The first large-scale realworld object detection data sets taken on UAVs were VisDrone <ref type="bibr" target="#b23">[24]</ref> and UAVDT <ref type="bibr" target="#b8">[9]</ref>. Other data sets for object detection and tracking emerged with many different foci <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b33">[33]</ref>.</p><p>The need for data sets caused many researchers to focus on synthetic data. Mid-Air <ref type="bibr" target="#b14">[15]</ref> presents a synthetic data set for unstructured environments captured with the Unreal Engine <ref type="bibr" target="#b21">[22]</ref> in combination with AirSim <ref type="bibr" target="#b19">[20]</ref>. They laboriously built a world with landscape and streets for drone navigation. However, their world is lifeless and does not feature any objects of interest. The Synthinel-1 <ref type="bibr" target="#b34">[34]</ref> data set features synthetic data showing building footprints with segmentation masks.</p><p>An overview comparing the most important data sets is shown in <ref type="table" target="#tab_0">Table I</ref>.</p><p>3) Narrowing the Sim-to-Real Domain Gap: On the model level, many approaches apply synthetic-to-real domain adaptation <ref type="bibr" target="#b35">[35]</ref>- <ref type="bibr" target="#b42">[42]</ref>. By disentangling the features, these techniques aim to learn domain invariant features that lead to better transfer capabilities. A subset of these methods performs image stylization to make the synthetic images look more similar to their real-world counterparts <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, referred to as narrowing the appearance gap. Another domain gap arises from differences in content, called content gap <ref type="bibr" target="#b11">[12]</ref>. It depicts the layout and types of objects featured in the synthetic world instead of in the real world.</p><p>Orthogonal to these gaps, we focus on another gap called meta gap. Meta gap depicts the imaging conditions at the time of capture, such as altitude, viewing angle and time. It can be seen as a useful abstraction of the content gap. These metadata are freely available in the synthetic engine and on an actual UAV. We leverage these metadata to align the distributions of the synthetic and real-world data set, leading to better performance and data efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPERIMENTAL SETUP</head><p>A. DeepGTA-UAV 1) Tool Description and Improvements: The DeepGTAV framework as used in this work builds upon the DeepGTAV-PreSIL framework <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b47">[47]</ref>, built upon the DeepGTAV framework <ref type="bibr" target="#b48">[48]</ref>. Originally, DeepGTAV was built as a reinforcement learning environment for self-driving cars, providing functionality to interact with GTAV through a TCP-server and building upon the functionality of SkriptHookV <ref type="bibr" target="#b49">[49]</ref>. The Python library VPilot <ref type="bibr" target="#b50">[50]</ref> interacts with DeepGTAV, which runs in GTAV.</p><p>DeepGTA-PreSIL integrates DeepGTAV and GTAVisionExport <ref type="bibr" target="#b51">[51]</ref>, the technique presented in <ref type="bibr" target="#b16">[17]</ref>, to extract depth and stencil buffers from the rendering pipeline. With those and the world coordinates of objects extracted in GTAV, pixel-wise object segmentation data can be extracted. From those, object bounding boxes can be calculated. The DeepGTAV framework also allows extracting voxel-wise LiDAR segmentation data, which was not used in this work. The most notable improvements made in this work include: 1) Increasing the ease of use and the multitude of scenarios of which synthetic data can be generated by improving the VPilot interface of DeepGTAV. In particular, it is now possible to freely modify the in-game position, camera position and rotation, manipulate the environment (time of day, weather), spawn objects (e.g. pedestrians, cars) and specify their animations. 2) Improving the speed and reliability of the DeepGTAV framework (to obtain almost no overhead, compared to running GTAV natively). 3) Allowing the extraction of metadata of the generated data (like time of day, height, camera angle, ...). 4) Providing multiple easily comprehensible and modifiable data generation scripts for several airborne scenarios and different strategies of metadata distribution. 5) Modifications to capture 4k image data (although we did not analyze the influence of high resolution data). In this work, those adaptations were used to capture object detection data from a UAV perspective (in comparison to an autonomous car scenario in previous works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>).</p><p>2) Modifications of GTAV: For optimal use as a simulation environment, some modifications were made to the GTAV game files by installing the modifications Simple Increase Traffic (and Pedestrian) [52], No chromatic aberration lens distortion <ref type="bibr" target="#b52">[53]</ref> and Heap Limit Adjuster <ref type="bibr" target="#b53">[54]</ref>. Additionally, the automatic spawning of objects was modified to match the class distribution in VisDrone.</p><p>Furthermore, the bounding box quality was improved by setting the game resolution to 7680x4320DSR in NVIDIA GeForce Experience <ref type="bibr" target="#b54">[55]</ref>. This results in an upscaling of the rendering buffers to 4k, which is needed to obtain pixel-perfect object segmentation data for 4k images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Set Generation</head><p>To assess the usefulness of synthetic data as training data for real-world scenarios, particularly to assess the usefulness of DeepGTAV and its adaptability in this context, we examined three different object detection scenarios.</p><p>In choosing those three scenarios, we are bound to existing real-world data sets to assess the real-world performance. We find that the data sets VisDrone <ref type="bibr" target="#b55">[56]</ref>, SeaDronesSee <ref type="bibr" target="#b5">[6]</ref> and Cattle <ref type="bibr" target="#b46">[46]</ref> are very popular in their respective application Using the DeepGTAV framework, we generate synthetic training data for these scenarios by specifying VPilot data generation scripts for each scenario. In the following, we briefly describe the different capturing procedures employed in the VPilot scripts used to generate the synthetic data sets.</p><p>For the generation of all synthetic data sets, the game world of GTAV is systematically traversed. New images are captured with one frame per second to obtain mainly distinct images. We export the 4k images but discard the segmentation and depth maps to focus on pure object detection. Along with every frame, we export the corresponding ground truth bounding boxes and the meta labels, i.e. altitude, principal axes (yaw, pitch, roll of camera rotation), time of the day and weather state.</p><p>For the random traversals of the game world, the camera height and angles were varied. Additional in-game objects were spawned (e.g. vehicles, pedestrians). See <ref type="table" target="#tab_0">Table II</ref> for details. For example, in Cattle, every two seconds, four cows are spawned 50 ? 250m in front of the camera with a leftright offset of ?160 ? 160m. The objects were spawned in the in-game traffic and pathfinding and were despawned after 200 seconds. Additionally, a new random travel location in this area was chosen every 60 seconds to prevent the in-game navigation from getting stuck.</p><p>Concise descriptions of the data sets are given in <ref type="table" target="#tab_0">Tables I  and IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Models and Training setup</head><p>As object detection models, we take two one-stage realtime detectors, EfficientDet-D0 (E.-D0) <ref type="bibr" target="#b56">[57]</ref> and Yolov5 <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b61">[62]</ref>, both being on the forefront of real-time object detectors as measured by their performances on the COCO test set <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref>. In particular, EfficientDet-D0 is the state-of-the-art model for real-time detectors on large-scale UAVDT <ref type="bibr" target="#b8">[9]</ref> traffic surveillance data set <ref type="bibr" target="#b9">[10]</ref>. For that, we use the implementation from <ref type="bibr" target="#b65">[65]</ref> with an image size of 2176px width and anchor scales of (0.3 0.5 0.7).</p><p>Yolov5 <ref type="bibr" target="#b61">[62]</ref> is a state of the art implementation of the Yolo object detection model implemented with multiple improvements to the Yolo framework that have been found in recent years. In this work, we used the unmodified YOLOv5m6 implementation of Yolov5 in release v5.0 <ref type="bibr" target="#b66">[66]</ref> with an image size of 1280x1280px and a batchsize of 48. Unless otherwise specified, we used the provided weights pre-trained on COCO <ref type="bibr" target="#b63">[63]</ref>.</p><p>Furthermore, as a two-stage detector we take the best performing single-model (no ensemble) on VisDrone from the workshop report <ref type="bibr" target="#b4">[5]</ref> (DE-FPN), i.e. a Faster R-CNN (F.R.) with a ResNeXt-101 64-4d <ref type="bibr" target="#b67">[67]</ref> backbone (removing P6), which is trained using color jitter and random image cropping. The anchor sizes and strides are decreased to <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref> and <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b64">64)</ref>.</p><p>We measure the models performances on the popular mean average precision metric with overlap 0.5, i.e. mAP@0.5 <ref type="bibr" target="#b63">[63]</ref>. As we are interested in real-world performance, we test on the test set of the real-world data set unless indicated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL EVALUATION</head><p>First, we conducted different experiments to show that synthetic training data could yield good real-world performance or improve the performance of a real-world object detector. Then we conducted further ablation studies to examine different factors that could influence or modulate the positive effect of synthetic training data.</p><p>On a general level, we wanted to obtain actionable advice for an engineer using synthetic data to train an object detector, which factors should be examined with emphasis and which factors could be ignored. Such factors could be the graphics quality of the simulation environment or the alignment of synthetic and real height distributions.</p><p>In the following, those experimental conditions and their results will be described. For better clarity, we discuss the design of each ablation study and its result individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. General Benefit of Synthetic Data in UAV Object Detection</head><p>The main goal of this work is to examine the usefulness of synthetic training data to train object detectors from scratch or improve the performance of object detectors trained with real-world data.</p><p>From this goal, there naturally arise three conditions which we want to compare. First, as a baseline, we observe the performance of the object detector on the real-world data set. Second, we observe the performance of the object detector trained only on an entirely synthetically generated data set. Finally, we observe the performance of an object detector which is first pre-trained on a synthetic data set and then transfer-trained on the real-world data set. From preliminary experiments, we found these strategies to be superior to alternative strategies, such as combined training, similar to previous literature <ref type="bibr" target="#b68">[68]</ref>. For all three application scenarios, the corresponding realworld training set was split into a training, validation and test set. For fully synthetic training and synthetic pre-training the data set was split into a training and a validation set, as no testing is conducted on the synthetic data. See the sizes of the different complete sets in <ref type="table" target="#tab_0">Table IV</ref>.</p><p>Findings: <ref type="table" target="#tab_0">Table III</ref> shows that purely training on synthetic data can already provide minimal working solutions. While all object detectors perform well on the simple data set Cattle (29.2-64.2 mAP@50), the accuracies on VisDrone and SeaD-ronesSee are far lower. This is partly due to missing classes in the corresponding synthetic data sets ((awning-)tricycle in VisDrone and life jacket in SeaDronesSee). However, another apparent factor is the difference in the appearance of certain classes from the synthetic to the real data set. For example, see <ref type="figure" target="#fig_0">Figure 1</ref> to compare the same classes in the synthetic and real data set. The default appearances of classes vary in some cases significantly. This appearance gap may be narrowed by manually editing the appearance of classes to resemble realworld objects. Despite these challenges, synthetic pre-training with subsequent transfer training boosts performance on Vis-  Drone and SeaDronesSee significantly across all models. On the SeaDronesSee evaluation benchmark <ref type="bibr" target="#b69">[69]</ref>, we can even achieve state-of-the-art performance by surpassing the best model by +5.6 mAP@50. While the performance improvement is not as apparent on Cattle, pre-training on synthetic data helps for EfficientDet-D0 and Faster R-CNN. These experiments show that although synthetic data sets can not replace corresponding real-world data sets, they enhance the detection performance. Even the models from <ref type="bibr" target="#b5">[6]</ref> can be beaten just by synthetic pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Data Set Sizes</head><p>We wanted to test the influence that the data set sizes had on the performance in this context. The intuitive hypothesis was that we would observe a curve of diminishing returns for larger data sets, which is typical in machine learning. We hypothesized that we would observe such diminishing returns for the size of the real-world data set, as well as for the size of the synthetic (pre-training) data set. Furthermore, we hypothesized that the effect of synthetic pre-training would be more emphasized when using a smaller real-world data set.</p><p>To conduct this ablation, we varied the size of the realworld data set and of the synthetic data set for VisDrone and SeaDronesSee training.</p><p>Findings: <ref type="figure" target="#fig_2">Figure 2</ref> shows the effect of different data set sizes in synthetic pre-training. As hypothesized, we observe an improved mAP@50 with an increase of the real-world data set size, and an increase of the synthetic pre-training data set size with a diminishing return for larger data sets. Furthermore, we observe the hypothesized interactive effect, such that the improvement from using synthetic pre-training data is disproportionally larger for smaller real-world data sets.</p><p>Our found effects are consistent with the numbers of those reported in <ref type="bibr" target="#b13">[14]</ref>, that is, improvements of about +1.0 to +5.0 mAP@50 by using additional synthetic training data. We note that in <ref type="bibr" target="#b13">[14]</ref>, a 3D LiDAR object detection task was examined instead of a 2D object detection task. Compared to <ref type="bibr" target="#b13">[14]</ref>, in our work, those improvements are not only on one class but over the whole 10 and 6 classes of VisDrone and SeaDronesSee, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of using pre-trained weights</head><p>The use of initial weights pre-trained on large scale image data sets like ImageNet <ref type="bibr" target="#b70">[70]</ref> or COCO <ref type="bibr" target="#b63">[63]</ref> has become a standard for many vision machine learning tasks in recent years. We hypothesized that there could be an interaction between using such pre-trained weights and synthetic (pre-)training. For example, such pre-trained weights could encode invariance to image noise, which is prominent in real world images, but may be missing in synthetically generated images. The use of pre-trained initial weights would thereby allow a purely synthetically trained model to obtain the information it could not obtain from the synthetic data, thereby introducing an interactive effect. If this hypothesis were true, using pretrained weights would improve the performance of a purely synthetically trained model disproportionally more than it improves the performance of a model trained on real world data.</p><p>Findings: <ref type="table" target="#tab_3">Table V</ref> shows the effect of using COCO pretrained weights as initial weights for the training. Due to the small effect sizes, we would argue that our results are inconclusive at this time. We observe a more significant absolute improvement of the mAP@50 when using COCO pre-trained weights on pure DGTA-VisDrone training than on pure VisDrone training, which would speak for the hypothesized interactive effect. However, when observing the absolute obtained mAP@50 of those different conditions, this observed difference in improvements would also be consistent with an effect of diminishing returns of improvements for a larger mAP@50. This effect would be that starting from a weaker performance baseline, the same improvement (using COCO pre-trained weights) would yield a larger improvement than when starting from a more robust baseline.</p><p>So we conclude that there is at least no strong effect where using pre-trained weights trained on real-world imagery would disproportionally improve the performance of synthetic training. However, note that there might be other factors to consider, such as training time and stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of Good/Bad graphics settings</head><p>One of the parameters that one would intuitively look at when observing synthetic training data or when trying to improve the use of synthetic training data is the realism of this training data. This could be framed as closing the Sim-To-Real gap as discussed above. A similar but not fully congruent perspective on this coming from game development is trying to improve the realism of games by improving their graphics quality. In many cases, those improvements of graphics quality come with high computational demands (higher polygon models, higher resolution textures, demanding physics simulations like particle effects and lighting) and high amounts of labor, e.g. from graphics artists.</p><p>Therefore, from an engineering perspective, this raises the question, how well spent this effort is to obtain the final goal of increasing the synthetically trained model's performance.</p><p>To answer this question, we conducted experiments comparing the effects of synthetic training data obtained from GTAV either set to the highest or lowest possible graphics settings.</p><p>Findings: <ref type="table" target="#tab_0">Table VI</ref> shows the obtained model performance using synthetic data obtained from GTAV on the lowest and highest graphics settings, respectively. We observe that for purely synthetic training, the model trained with higher graphics setting images outperforms the one trained on lower graphics quality images. This effect, however, is small.</p><p>There is no observable effect in the synthetic pre-training condition with transfer training on the real-world data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Aligning Domain Distributions</head><p>Modifying the DeepGTAV tools to extract metadata allows us to automate parts of the data generation process by aligning the metadata distributions of the real and synthetic data sets. Instead of laboriously setting the correct metadata settings in the synthetic data generation process, one could fall back to the corresponding real data set to adapt the parameters automatically. Aligning the distributions may result in higher synthetic data quality or efficiency.</p><p>For instance, in SeaDronesSee, every image is annotated with the capture time stamp. By bootstrap sampling from the time distribution, we sample a new data set of 100k synthetic images with the correct time distribution. As before, we train a Yolov5 model and test it on the SeaDronesSee test set (Synthetic Only), and we transfer train it on SeaDronesSee (Synthetic-To-Real).</p><p>Findings: <ref type="table" target="#tab_0">Table VII</ref> shows that aligning the time helps in synthetic only training and synthetic pre-training by increasing the performance over the unaligned baseline by +4.1 and +0.2 mAP@50, respectively. The performance increase is mainly due to SeaDronesSee only featuring day-time images, such that sampling synthetic night images deteriorates the performance. Similarly, we can also automatically adjust the camera angle. The images in the Cattle data set have been taken from a downward-facing camera where a gimbal corrects for UAV angular movement. We sample from DGTA-Cattle only these images that fall into the range of these angles (with an error threshold of at most 20 degrees). This reduces the original 40k images to only roughly 10% of the images <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">954)</ref>. We train an EfficientDet-D0 on this subset.</p><p>Findings: Interestingly, the performance of the synthetic only training improves over the more extensive unaligned DGTA-Cattle training. This is likely due to the limited capacity of an EfficientDet-D0 model resulting in the model distributing its performance across many other angular viewpoints, which are unnecessary for the performance in this use-case. This experiment illustrates that less but more targeted data may be sufficient to reach the same performance while reducing the need to filter the data by only aligning the metadata distributions manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LIMITATIONS AND CONCLUSIONS</head><p>We demonstrated that synthetic data can be leveraged for object detection on UAVs. We can improve the performance over only real training by synthetic pre-training on multiple application scenarios. Synthetic-only training yields satisfactory results, but performances are not yet competitive to real training.</p><p>From our ablations, we conclude that the use of more synthetic training data improves the performance. The use of weights pre-trained on large scale image data sets constantly improves the performance, although we find no interactive effect with synthetic training. The graphics quality of the simulation engine appears to be important for purely synthetic training but not for synthetic pre-training. In general, metadata alignment is vital for the usefulness and data efficiency of synthetic training data.</p><p>We hope that the adaptation of the DeepGTAV tools helps cast light on object detection on UAVs via synthetically generated footage. In future works, the capabilities of the DeepGTAV framework to produce object segmentation data, LiDAR data and video could be leveraged.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Real (left) and synthetic (right) image samples in different applications scenarios with ground truth annotations. Representative objects are magnified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The effect of the real and synthetic data set size on the model performance for VisDrone and SeaDronesSee. The color shading specifies the size of the synthetic data set that was used for pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH THE MOST PROMINENT ANNOTATED AERIAL OBJECT DETECTION DATA SETS IN THREE DOMAINS.</figDesc><table><row><cell>Data Set</cell><cell>Domain</cell><cell cols="3">Data Type # Images Platform</cell><cell cols="3">Image Widths Altitude Angle Other meta</cell></row><row><cell>VisDrone [5]</cell><cell>traffic</cell><cell>real</cell><cell>10,209</cell><cell>UAV</cell><cell>960-2,000</cell><cell></cell><cell></cell></row><row><cell>AU-AIR [33]</cell><cell>traffic</cell><cell>real</cell><cell>32,823</cell><cell>UAV</cell><cell>1,920</cell><cell></cell><cell></cell></row><row><cell>PreSIL [14]</cell><cell>traffic</cell><cell>synthetic</cell><cell>40,000</cell><cell>car</cell><cell>1,920</cell><cell>-</cell><cell>-</cell></row><row><cell>DGTA-VisDrone</cell><cell>traffic</cell><cell>synthetic</cell><cell>50,000</cell><cell>UAV</cell><cell>3840</cell><cell></cell><cell></cell></row><row><cell>Airbus Ship [45]</cell><cell>maritime</cell><cell>real</cell><cell>40,000</cell><cell>satellite</cell><cell>768</cell><cell>-</cell><cell>-</cell></row><row><cell>SeaDronesSee [6]</cell><cell>maritime</cell><cell>real</cell><cell>5,630</cell><cell>UAV</cell><cell>3,840-5,456</cell><cell></cell><cell></cell></row><row><cell>DGTA-SeaDronesSee</cell><cell>maritime</cell><cell>synthetic</cell><cell>100,000</cell><cell>UAV</cell><cell>3,840</cell><cell></cell><cell></cell></row><row><cell>Cattle [46]</cell><cell>agriculture</cell><cell>real</cell><cell>670</cell><cell>UAV</cell><cell>4,000</cell><cell></cell><cell></cell></row><row><cell>DGTA-Cattle</cell><cell>agriculture</cell><cell>synthetic</cell><cell>50,000</cell><cell>UAV</cell><cell>3,840</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DATA</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">SET GENERATION SETTINGS.</cell><cell></cell></row><row><cell>DGTA-</cell><cell>Cattle</cell><cell>SeaDronesSee</cell><cell>VisDrone</cell></row><row><cell>GPS</cell><cell>[0, 17]?</cell><cell>[?28, ?18]?</cell><cell>[?12, 14]?</cell></row><row><cell>?10 3</cell><cell>[13, 22]</cell><cell>[?25, ?13]</cell><cell>[?22, 13]</cell></row><row><cell>Altitude</cell><cell>10-80m</cell><cell>0-80m</cell><cell>0-40m</cell></row><row><cell>Cam Pitch</cell><cell>20-90?20-90?20-90?M</cell><cell></cell><cell></cell></row><row><cell>anual spawn</cell><cell>4?cow@2s</cell><cell>4?ppl@2s 4?boat@2</cell><cell>3?bike@8 1?motor@8</cell></row><row><cell>Spawn y</cell><cell>50-250m</cell><cell>50-250m</cell><cell>50-150m</cell></row><row><cell>Spawn x</cell><cell>?160-160m</cell><cell>?160-160m</cell><cell>50-150m</cell></row></table><note>domain and therefore choose these. VisDrone aims for traf- fic surveillance in Asian cities with very crowded scenes. SeaDronesSee's application domain is search and rescue in open water featuring swimmers and boats, with the main challenges being reflective regions, shadows and waves or seafoam. Finally, Cattle aims to bring autonomous vision systems to agriculture (cattle detection).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF OBJECT DETECTORS FOR DIFFERENT TRAINING STRATEGIES GIVEN BY MAP@50.</figDesc><table><row><cell></cell><cell></cell><cell>Data set</cell><cell cols="3">Synthetic Real SyntheticToReal</cell></row><row><cell>E.-D0</cell><cell cols="2">Cattle SeaDronesSee VisDrone</cell><cell>29.2 10.3 1.2</cell><cell>78.4 36.3 24.6</cell><cell>85.8 38.8 27.2</cell></row><row><cell>F.R.</cell><cell cols="2">Cattle SeaDronesSee VisDrone</cell><cell>38.8 14.6 2.4</cell><cell>90.5 54.7 48.6</cell><cell>91.5 59.0 51.2</cell></row><row><cell>YOLO</cell><cell cols="2">Cattle SeaDronesSee VisDrone</cell><cell>64.2 10.5 10.2</cell><cell>88.8 55.8 43.9</cell><cell>86.9 60.3 45.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell></row><row><cell cols="6">NUMBER OF IMAGES AND CLASSES. NOTE THAT (AWNING-)TRICYCLE IS</cell></row><row><cell cols="6">ABBREVIATED AS (A.-)TRI., LIFE JACKET AS LF AND PEOPLE AS PPL.</cell></row><row><cell></cell><cell></cell><cell cols="3">Number of Images</cell></row><row><cell cols="2">Data Set</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell><cell>Classes</cell></row><row><cell cols="2">VisDrone</cell><cell>6471</cell><cell>548</cell><cell>1610</cell><cell>ppl.,bike,car,truck,van motor,tri.,a.-tri.,bus</cell></row><row><cell cols="2">Sea-DronesSee</cell><cell>2975</cell><cell>859</cell><cell>1796</cell><cell>swimmer,floater,boat swimmer  ? ,floater  ? ,LJ</cell></row><row><cell>Cattle</cell><cell></cell><cell>402</cell><cell>134</cell><cell>134</cell><cell>Cow</cell></row><row><cell cols="2">DGTA-VisDrone</cell><cell cols="2">40000 10000</cell><cell>-</cell><cell>ppl.,bike,truck car,motor,bus,van</cell></row><row><cell cols="2">DGTA-Sea-DronesSee</cell><cell cols="2">90000 10000</cell><cell>-</cell><cell>swimmer,floater,boat swimmer  ? ,floater  ?</cell></row><row><cell>DGTA-Cattle</cell><cell></cell><cell cols="2">40000 10000</cell><cell>-</cell><cell>Cow</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V EFFECT</head><label>V</label><figDesc>OF COCO PRE-TRAINED WEIGHTS ON PERFORMANCE GIVEN BY MAP@50 EVALUATED ON VISDRONE.</figDesc><table><row><cell></cell><cell cols="2">random initial weights Pre-trained on COCO</cell></row><row><cell>Real</cell><cell>43.1</cell><cell>43.9</cell></row><row><cell>Synthetic</cell><cell>7.5</cell><cell>10.2</cell></row><row><cell>SyntheticToReal</cell><cell>44.7</cell><cell>45.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>BETWEEN LOW QUALITY AND HIGH QUALITY IN-GAME SETTINGS (GIVEN BY MAP@50). WE COMPARE THE PERFORMANCE FOR ONLY REAL TRAINING ON VISDRONE AGAINST THE PERFORMANCE WITH SYNTHETIC DATA AS DGTA-VISDRONE.</figDesc><table><row><cell></cell><cell cols="3">Real Synthetic SyntheticToReal</cell></row><row><cell cols="2">Real data Baseline 43.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Low Quality</cell><cell>-</cell><cell>8.4</cell><cell>45.1</cell></row><row><cell>High Quality</cell><cell>-</cell><cell>10.2</cell><cell>45.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII EFFECT</head><label>VII</label><figDesc>OF TIME AND ANGLE ALIGNMENT ON PERFORMANCE (GIVEN AS MAP@50).</figDesc><table><row><cell>SeaDronesSee</cell><cell cols="2">Synthetic SyntheticToReal</cell></row><row><cell>Only real training</cell><cell>-</cell><cell>55.8</cell></row><row><cell>Unaligned Synthetic Pre-training</cell><cell>10.5</cell><cell>60.3</cell></row><row><cell>Time Aligned</cell><cell>14.6</cell><cell>60.5</cell></row><row><cell>Cattle</cell><cell></cell><cell></cell></row><row><cell>Only real training</cell><cell>-</cell><cell>78.4</cell></row><row><cell>Unaligned Synthetic Pre-training</cell><cell>29.2</cell><cell>85.8</cell></row><row><cell>Angle Aligned (Subset)</cell><cell>36.6</cell><cell>85.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperspectral imaging: A review on uav-based sensors, data processing and applications for agriculture and forestry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ad?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hru?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>P?dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Sousa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1110</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised human detection with an embedded vision system on a fully autonomous uav for search and rescue operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lygouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Santavas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taitzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tarchanidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">3542</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uav-based situational awareness system using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geraldes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Villerabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Prendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="122" to="583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uav delivery monitoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MATEC Web of Conferences</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">4011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visdrone-det2018: The vision meets drone object detection in image challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Seadronessee: A maritime benchmark for detecting humans in open water</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01922</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">General data protection regulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gdpr</surname></persName>
		</author>
		<ptr target="https://gdpr.eu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ms-celeb-1m privacy infringment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Exposingai</surname></persName>
		</author>
		<ptr target="https://exposing.ai/msceleb/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The unmanned aerial vehicle benchmark: Object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Leveraging domain labels for object detection from uavs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.12677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaining scale invariance in uav bird&apos;s eye view object detection by adaptive resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.12694</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-sim: Learning to generate synthetic datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cameracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusiniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4551" to="4560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi modal semantic segmentation using synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Hegde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Precise synthetic image and lidar (presil) dataset for autonomous vehicle perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hurl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2522" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mid-air: A multi-modal dataset for extremely low altitude drone flights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Grand theft auto v</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Games</surname></persName>
		</author>
		<ptr target="https://www.rockstargames.com/de/games/V" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unlimited road-scene synthetic annotation (ursa) dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Angus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elbalkini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A lidar point cloud generator: from a virtual world to autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="458" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Airsim: High-fidelity visual and physical simulation for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and service robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="621" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unrealcv: Connecting computer vision to unreal engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="909" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Synthetic data for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11512</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visdrone-sot2020: The vision meets drone single object tracking challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="728" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human trajectory prediction in crowded scene using social-affinity long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Drone-based object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-R</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4145" to="4153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kloeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2118" to="2125" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nature conservation drones for automatic localization and counting of animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Verschoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Epema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combining human computing and machine learning to make sense of big (aerial) data for disaster response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Briant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Millet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Au-air: A multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bozcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayacan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8504" to="8510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1814" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semantic-aware grad-gan for virtual-to-real urban scene adaption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured domain randomization: Bridging the reality gap by context-aware synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boochoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brophy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cameracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7249" to="7255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Enhancing photorealism enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04619</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Airbus ship dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Airbus</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/airbus-ship-detection" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cattle detection and counting in uav images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yoshihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="52" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deepgtav-presil code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hurl</surname></persName>
		</author>
		<ptr target="https://github.com/bradenhurl/DeepGTAV-PreSIL" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deepgtav code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruano</surname></persName>
		</author>
		<ptr target="https://github.com/aitorzip/DeepGTAV" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Script hook v</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blade</surname></persName>
		</author>
		<ptr target="http://www.dev-c.com/gtav/scripthookv/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deepgtav-vpilot code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruano</surname></persName>
		</author>
		<ptr target="https://github.com/aitorzip/VPilot" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Gtav mod simple increase traffic and pedestrians</title>
		<ptr target="https://de.gta5-mods.com/misc/simple-increase-traffic-and-pedestrian" />
		<imprint/>
	</monogr>
	<note>Gta vision export</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gtav mod -no chromatric aberration and lens distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venjam1n</surname></persName>
		</author>
		<ptr target="https://de.gta5-mods.com/misc/no-chromatic-aberration-lens-distortion-1-41" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Gtav mod -heap limit adjuster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Division</surname></persName>
		</author>
		<ptr target="https://de.gta5-mods.com/tools/heap-limit-adjuster-600-mb-of-heap" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Nvidia geforce experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/de-de/geforce/geforce-experience/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Vision meets drones: A challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Changyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sullivan</surname></persName>
		</author>
		<title level="m">ultralytics/yolov5: Initial Release</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<idno type="DOI">10.5281/zenodo.3908560</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3908560" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Object detection on coco test-dev</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Code</surname></persName>
		</author>
		<ptr target="https://paperswithcode.com/sota/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A pytorch implementation of efficientdet object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gmbh</surname></persName>
		</author>
		<ptr target="https://github.com/signatrix/efficientdet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanocode012</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Taoxie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Changyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laughing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexwang1900</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Diaconu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Defretin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lohia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milanko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fineran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khromov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ingham</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4679653</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4679653" />
		<title level="m">ultralytics/yolov5: v5.0 -YOLOv5-P6 1280 models, AWS, Supervise.ly and YouTube integrations</title>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Seadronessee benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Messmer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
