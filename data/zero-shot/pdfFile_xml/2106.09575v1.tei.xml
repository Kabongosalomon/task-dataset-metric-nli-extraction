<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotation Invariant Graph Neural Networks using Spin Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeesh</forename><surname>Kolluru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ulissi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rotation Invariant Graph Neural Networks using Spin Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress towards the energy breakthroughs needed to combat climate change can be significantly accelerated through the efficient simulation of atomic systems. Simulation techniques based on first principles, such as Density Functional Theory (DFT), are limited in their practical use due to their high computational expense. Machine learning approaches have the potential to approximate DFT in a computationally efficient manner, which could dramatically increase the impact of computational simulations on real-world problems. Approximating DFT poses several challenges. These include accurately modeling the subtle changes in the relative positions and angles between atoms, and enforcing constraints such as rotation invariance or energy conservation. We introduce a novel approach to modeling angular information between sets of neighboring atoms in a graph neural network. Rotation invariance is achieved for the network's edge messages through the use of a per-edge local coordinate frame and a novel spin convolution over the remaining degree of freedom. Two model variants are proposed for the applications of structure relaxation and molecular dynamics. Stateof-the-art results are demonstrated on the large-scale Open Catalyst 2020 dataset. Comparisons are also performed on the MD17 and QM9 datasets.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many of the world's challenges such as finding energy solutions to address climate change <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3]</ref> and drug discovery <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> are fundamentally problems of atomic-scale design. A notable example is the discovery of new catalyst materials to drive chemical reactions that are essential for addressing energy scarcity, renewable energy storage, and more broadly climate change <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref>. Potential catalyst materials are typically modeled using Density Functional Theory (DFT) that estimates the forces that are exerted on each atom and the energy of a system or structure of atoms. Unfortunately, the computational complexity of DFT limits the scale at which it can be applied. Efficient machine learning approximations to DFT calculations hold the potential to significantly increase the discovery rate of new materials for these important global problems.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> are a common approach to modeling atomic structures, where each node represents an atom and the edges represent the atom's neighbors <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref>. A significant challenge in designing models is utilizing relative angular information between atoms, while maintaining a model's invariance to system rotations. Numerous approaches have been proposed, such as only using the distance between atoms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>, or limiting equivariant angular representations to linear transformations to maintain equivariance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29]</ref>. One promising approach is the use of triplets of neighboring atoms to define local coordinate frames that are invariant to system rotations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. The relative angles between the three atoms may be used to update the GNN's messages while maintaining the network's invariance to rotations. It has been shown <ref type="figure">Figure 1</ref>: Illustration of projecting an atom? in the neighborhood of s onto a sphere in a local coordinate frame defined by atom s and t (left). For each projected atom, a corresponding latitude ? (inclination) and longitude ? (azimuth) is computed for its projection onto a 2D reference frame (middle). The spin convolution is done in the longitudinal direction, corresponding to a roll is 3D space. (right) Example channel filters that are learned using the grid-based approach for the first through third message blocks and the force block. that this additional angular information results in significantly improved accuracies on several tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>We propose encoding angular information using a local reference frame defined by only two atoms; the source and target atoms for each edge in a GNN. Using this reference frame, a spherical representation of the incoming messages to the source atom is created, <ref type="figure">Figure 1</ref>. The representation has the benefit of encoding all neighboring atom information, and not just information between atom triplets, which may result in higher-order information being captured. The complication is a reference frame defined by two atoms (or two 3D points) still has one remaining degree of freedom -the roll rotation about the axis defined by the two 3D points. If this final degree of freedom is not accounted for, the model will not be invariant to system rotations. Our solution is to perform a convolution on the spherical representation across this final rotation, called a "spin convolution". By globally pooling the convolution's features, the resulting SpinConv model maintains rotation invariance while enabling the capture of rich angular information. We describe two model variations that are used depending on the importance of energy conservation in the final application. We propose an energy-centric model that enforces energy conservation by calculating the forces using the negative partial derivative of the energy with respect to the atoms' positions <ref type="bibr" target="#b3">[4]</ref>. Our second approach is a force-centric model that directly estimates the atom forces that is not energy conserving. While the force-centric model's energy estimation is rotation invariant, the model's final force estimation layer is not strictly rotation equivariant, but through its architectural design it is encouraged to learn rotation equivariance during training.</p><p>Results are demonstrated on the Open Catalyst 2020 (OC20) dataset <ref type="bibr" target="#b2">[3]</ref> aimed at simulating catalyst materials that are useful for climate change related applications. The OC20 dataset contains over 130M training examples for approximating the DFT-estimated forces and energies. Our Spin-Conv model achieves state-of-the-art performance for both energy and force estimation. Notably, the force-centric variant, which is not energy conserving, outperforms the energy-centric models. Significant gains in accuracy are achieved for predicting relaxed energies from initial structures, by using the force-centric approach to predict the relaxed structure followed by its energy. Ablation studies are performed on numerous architectural choices, such as the choice of spherical representation and the size of the model. For completeness, we also evaluate our model on the MD17 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and QM9 <ref type="bibr" target="#b21">[22]</ref> datasets that measure accuracy for molecular dynamics and property prediction tasks respectively for small molecules. Results compare favorably with respect to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>We model a system or structure of atoms using a Graph Neural Network (GNN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref>, where the nodes represent atoms and the edges represent the atoms' neighbors. In this section, we describe both an energy-centric and force-centric model to estimating atomic forces, which vary in how they estimate forces and whether they are energy conserving. We begin by describing the components shared by each approach, followed by how these components are used. Code will be released upon acceptance under a permissive open-source license. The force block is only used in the force-centric model to estimate the per-atom forces after the message blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inputs and Outputs</head><p>The inputs to the network are the 3D positions x i and the atomic numbers a i for all i ? n atoms. The outputs are the per atom forces f i ? R 3 and the overall structure's energy E. The 3D distance offset between a pair of source and target atoms s and t respectively is x st = x s ? x t with a distance of d st = x st 2 . Directional information is encoded using the normalized unit vectorx st = x st /d st .</p><p>The graph neural network is constructed with each atom t as a node and the edges representing the atom's neighbors s ? N t , where N t contains all atoms s with d st &lt; ?. Each edge has a corresponding message m st that passes information from atom s to t. The output forces and energy are computed as a function of edge messages m st that we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Energy and force estimation</head><p>The energy-centric and force-centric models compute the structure's energy E as an output. Our GNN model updates for each edge an M -dimensional hidden message h </p><p>where F e is a single embedding block described later. As we also discuss later, the edge messages h st are invariant to system rotations, so the estimated energy E is also invariant.</p><p>The estimation of the forces varies for the energy-centric and force-centric models. The energy-centric model estimates the forces using the negative partial derivative of the energy with respect to the atom positions. This approach to force estimation has the benefit of enforcing energy conservation <ref type="bibr" target="#b3">[4]</ref>, i.e., the forces along any closed path sum to zero. The calculation of the partial derivative <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> requires an additional step similar to performing backpropagation when updating the network's weights:</p><formula xml:id="formula_1">f = ? ? ?x E(x, a)<label>(2)</label></formula><p>The force-centric model estimates forces directly for an atom t using:</p><formula xml:id="formula_2">f t = F f (a t ,x t , h (K) t ),<label>(3)</label></formula><p>where F f is the force block we describe later,x t are all the normalized unit vectors for the neighbors of t and h (K) t are all incoming messages to atom t. This has the benefit of improved efficiency since it does not require an extra backward pass to estimate the forces. The tradeoff is that it does not enforce energy conservation, i.e., the sum of the forces along a closed path may not equal zero. Depending on the application, an energy-centric or force-centric approach may be most suitable. In either model, losses may be applied to both the energy and force estimates with weights determined by the needs of the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Messages</head><p>The edge messages are iteratively updated to allow information from increasingly distant atoms to be captured. Each message is represented by a tuple, m st = {x st , d st , h k st }, where h k st is the message's hidden state at iteration k. Bothx st and d st are used to update the message's hidden state h st , which is itself rotation invariant due to the spin convolution that we describe later. The hidden state h st ? R M is updated using:</p><formula xml:id="formula_3">h (k+1) st = h (k) st + F h a s , a t , m (k) st , m (k) s ,<label>(4)</label></formula><p>where m (k) s is the set of messages coming into node s, i.e., all m? s with? ? N s . The form of F h is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. It contains three parts; the spin convolution that transforms a spherical projection of the messages into a rotation invariant representation, the distance block that encodes the distance d st between atoms, and the embedding block that incorporates information about the atoms' atomic numbers. The output of the spin convolution is passed through an embedding block, added to the output of the distance block and finally passed through another embedding block. We describe each of these parts in turn. The hidden messages are initialized using just a distance block followed by and embedding block, <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Spin Convolution</head><p>The spin convolution captures information about the neighbors? ? N s of atom s when updating the message hidden state h st . The spin convolution has three stages that we describe in turn; projection, convolution and pooling. The convolution captures the relative angular information between the neighboring atoms, and the pooling ensures the output D-dimensional feature representation is invariant to system rotations.</p><p>An important feature is the angular information of the neighboring atoms in N s relative to s and t. This information is encoded by creating a local reference frame in which atom s is the center (0, 0, 0) and the z-axis points from atom s to atom t. As shown in <ref type="figure">Figure 1</ref>(left), this fixes all degrees of freedom except the roll rotation about the vector from s to t. The spin convolution is performed across a discretized set of rotations about the roll rotation axis. At each rotation, the atoms? are projected onto a sphere centered on s and used to create a spherical representation of the hidden states h? s . Each atom? ? N s is projected using a polar coordinate frame (?, ?) where ? may be viewed as the latitude (inclination) and ? as the longitude (azimuth). The polar coordinates are computed in the local edge coordinate frame usingx? s = R stx?s where R st is a 3D rotation matrix that satisfies R stxst = (0, 0, 1). To capture the rich information encoded in the relative angular information between atoms, a set of filters is applied to the spherical representation ( <ref type="figure">Figure 1(right)</ref>), similar to how a filter is applied to an image patch with traditional CNNs.</p><p>We explore two potential spherical representations: spherical harmonics and a grid-based approach. Spherical harmonics represent a spherical function using a set of basis functions that are equivariant to rotations. The degree indicates the number of basis functions L = ( + 1) 2 used. The spherical representation of the incoming messages for each atom is R L ? R M , where M is the size of the message hidden states in h. The second approach uses the computed polar coordinates (?, ?) for al? s ? N s to create a grid-based representation, <ref type="figure">Figure 1</ref>(middle). The polar coordinates are discretized</p><formula xml:id="formula_4">creating a R ? ? R ? ? R M feature representation. Each message hidden state h (k)</formula><p>s? ? R M is added to the 3D feature representation using bilinear interpolation with its corresponding (?, ?).</p><p>A 1D convolution is performed with either spherical representation in the longitudinal direction. Filters have the same size as the feature representation, R L ? R M or R ? ? R ? ? R M for spherical harmonics and the grid-based approach respectively. Full coverage filters are used since the angular relationship between atoms at distant angles is important, e.g., the forces of atoms at exactly 180 ? from each other may cancel out. Large filters also enable the network to learn the complex relationships between numerous neighboring atoms. Rotations are performed using Wigner D-matrices for the spherical harmonic representation, while a simple translation is used for the grid-based representation. The result of the convolution is a R ? ? R D feature vector corresponding to D filters applied to each longitudinal orientation. To make the representation invariant to rotations, average pooling is performed in the longitudinal direction resulting in a final R D feature vector. Elements not in the OC20 dataset are marked with a light grey checkerboard pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Distance Block</head><p>The distance block encodes the distance between two atoms. The distance is encoded using a set of evenly distributed Gaussian basis functions G with means ? i and standard deviation ?. The means of the basis functions are evenly distributed from 0 to ? angstroms. Since the atomic radii of each element varies, the relative position of two atoms s and t is highly dependent on their atomic numbers a s and a t . To account for this, gain v asat and offset u asat scalars for the distance d st are learned for each potential pair of atomic numbers:</p><formula xml:id="formula_5">b i = G i (v asat d st + u asat ? ? i , ?)<label>(5)</label></formula><p>The resulting feature b is passed through a linear transformation to create a D-dimensional feature vector that is passed to the next block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Embedding Block</head><p>The embedding block incorporates the atomic number information a s and a t into the update of the message's hidden state. The embedding operation may be interpreted as a mixture of experts <ref type="bibr" target="#b17">[18]</ref> approach that computes B different variations of the input, which are weighted by an embedding computed from the atoms' atomic numbers. The block's inputs are used to compute B sets of hidden values V st ? R D ? R B . A one-hot embedding for the atomic numbers a s and a t are concatenated and used to compute an B dimensional vector, v st ? R B , for weighting the B different sets of hidden values. An illustration of the learned embeddings are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. v st is computed using a two layer network and softmax. The matrix V st is multiplied by vector v st resulting in a vector of length D. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the result is passed through an additional fully connected layer before being passed to the next block. The output of the block is either D if it is used in the message update. If the embedding block is used to compute the final energy, only the atomic number a t embedding is used, the input dimension is M instead of D, and the output is size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Force Block</head><p>The force block computes the per-atom 3D forces f from a t ,x t , and h (K) t using Equation <ref type="formula" target="#formula_2">(3)</ref>. The force block uses a similar spin convolution as the message block, except the sphere is centered on the target atom t and is orientated along the x, y and z axes to compute f x , f y and f z respectively. That is, the force block is used three times to compute the force magnitude in each orthogonal direction for each atom. The force block uses the same embedding blocks as message passing, <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The same weights are used to compute forces in each of the three directions, only the orientation of the sphere used to create the convolutional features changes. To add more robustness to the force estimation and encourage rotational equivariance, the overall structure may be randomly rotated several times and the forces estimated. The multiple estimates may then be rotated back to the original reference frame and averaged. For both training and testing, five random rotations are used. Empirically, this approach encourages the networks to learn an approximate rotation equivariant representation even though rotation equivariance is not strictly enforced.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we begin by presenting our primary results on the Open Catalyst 2020 (OC20) dataset <ref type="bibr" target="#b2">[3]</ref> and compare against state-of-the-art models. This is followed by results on the smaller datasets of MD17 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and QM9 <ref type="bibr" target="#b21">[22]</ref> for additional model comparison.</p><p>Implementation details. For all models, the edge messages have size M = 32 with K = 3 layers, the hidden dimension D = 256 and embedding dimension B = 8. Unless otherwise stated, the convolutional filters are of size 16x12 and 12x8 for the force-centric and energy-centric models respectively. A smaller filter size was used for the energy-centric model due to memory constraints. GroupNorm <ref type="bibr" target="#b31">[32]</ref> is applied after the spin convolution with group size 4. An L1 loss is used for all experiments. The force loss was weighed by 100 with respect to the energy loss, except for the force-only model where the energy loss is set to 0. All models were trained with Adam (amsgrad) to convergence with the learning rate multiplied by 0.8 when the validation error plateaus. Training was performed using batch sizes ranging from 64 to 96 samples across 32 Volta 32GB GPUs. The Swish <ref type="bibr" target="#b20">[21]</ref> function is used for all non-linear activation functions. The neighbors s ? N t of each atom t are found using a distance threshold of ? = 6?. If more than 30 atoms are within the distance threshold, only the closest 30 are used. The distance block uses 256 to 512 Gaussian basis functions with ?'s equal to three times the distance between Gaussian means.  <ref type="table">Table 3</ref>: Ablation studies for SpinConv model variations trained for 560k steps (32-48 batch size, 0.2 epochs) with 16 Volta 32 GB GPUs. Training time is in GPU days and the validation set is a 30k random sample of the OC20 ID Validation set. <ref type="figure">Figure 4</ref>: Performance of SpinConv ablations on OC20 Val ID 30k ( <ref type="table">Table 3</ref>). All models trained for 560k steps and plotted against wall-clock training time. Note force-centric models and grid-based approaches converge more quickly than energy-centric models and those using spherical harmonics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OC20</head><p>The OC20 dataset <ref type="bibr" target="#b2">[3]</ref> contains over 130 million structures used to train models for predicting forces and energies during structure relaxations that is released under a CC Attribution 4.0 License. Since the goal of a structure relaxation is to find a local energy minimum, energy conservation in optional for this task. We report results for the Structure to Energy and Forces (S2EF), the Initial Structure to Relaxed Energy (IS2RE) and the Initial Structure to Relaxed Structure (IS2RS) tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Structure to Energy and Forces (S2EF)</head><p>There are four metrics for the S2EF task, the energy and force Mean Absolute Error (MAE), the Force Cosine similarity, and the Energy and Forces within a Threshold (EFwT). The EFwT metric is meant to indicate the percentage of energy and force predictions that would be useful in practice. Results for three model variants are shown in <ref type="table" target="#tab_0">Table 1</ref> on the test set. The SpinConv force-centric approach has the lowest energy MAE and force MAE of all models. While still low in absolute terms, the SpinConv models are improving over other models on the EFwT metric. DimeNet++-large slightly out performs SpinConv on the force cosine metric. The training time for the SpinConv is significantly faster than DimeNet++, while being a little slower than ForceNet <ref type="bibr" target="#b11">[12]</ref> or SchNet <ref type="bibr" target="#b26">[27]</ref>.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we examine the performance of SpinConv across different test splits. Note that the energy prediction of SpinConv is signficantly better than SchNet or DimeNet++. Across all models the accuracy for the in domain split are highest and decline for the three Out of Domain (OOD Adsorbate, OOD Catalyst, OOD Both) splits. SpinConv outperforms all models on each of the different domain splits. When comparing energy-centric approaches trained with both force and energy losses (bottom rows), the SpinConv model does significantly better at predicting both. In fact, the energy-centric approach trained on forces and energy outperforms the DimeNet++ <ref type="bibr" target="#b13">[14]</ref> model when trained on only energy, or energy and forces.</p><p>We examine variations of the SpinConv model in <ref type="table">Table 3</ref> and <ref type="figure">Figure 4</ref> through ablation studies. We trained three variants of the energy-centric model and four variants of the force-centric model. The grid-based and spherical harmonic approaches produced similar accuracies. However, the grid-based approach was significantly faster to train, so it was used in the remaining experiments. Smaller models lead to reduced performance on the OC20 dataset, but we found for smaller datasets such as MD17 or QM9 smaller model sizes can be beneficial to avoid overfitting. Finally, we test the impact of not performing the convolution (no conv) and only applying the filter at a single rotation. Rotation invariance was maintained by orienting the filter based on the mean angle of the neighboring atoms   weighted by distance. The result of not performing the convolution is significantly reduced accuracy. However, its faster training time may make it suitable for some applications.</p><p>Finally, for the force-centric SpinConv model we explore results when varying the number of random rotations used in the force block. The force MAE when using a single random rotation is 0.0276 and improves slightly to 0.0270 when using 5 random rotations. Increasing the number of rotations beyond 5 leads to negligible gains. The standard deviation of the force estimates at different random rotations is 0.004 eV/?. This is equal to 15% of the force MAE, which indicates the amount of error due to the model not being strictly rotation equivariant is small relative to the overall error of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Initial Structure to Relaxed Energy (IS2RE)</head><p>The Initial Structure to Relaxed Energy (IS2RE) task takes an initial atomic structure and attempts to predict the energy of the structure after it has been relaxed. Two approaches may be taken to address this problem, the direct and relaxation approaches <ref type="bibr" target="#b2">[3]</ref>. The direct treats the task as a standard regression problem and directly estimates the relaxed energy from the initial structure. The relaxation approach computes the relaxed structure using the ML predicted forces to update the atom positions. Next, given the ML relaxed structure the energy is estimated. We show results for both approaches in the OC20 dataset using SpinConv in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>The results of the SpinConv model significantly outperform all previous approaches using the relaxation approach for both energy MAE and Energy within Threshold (EwT) metrics. DimeNet++ also shows improved results for the relaxation approach with the best approach using two models; DimeNet++-large for force estimation and DimeNet++ (energy-only) for the energy estimation. Note in contrast to other approaches, SpinConv shows good results across all test splits, including those with out of domain adsorbates and catalysts. Using the direct approach, SpinConv is comparable to DimeNet++'s direct approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Initial Structure to Relaxed Structure (IS2RS)</head><p>Our final results on the OC20 dataset are on the IS2RS task where predicted forces are used to relax an atom structure to a local energy minimum. The is performed by iteratively estimating the forces   that are in turn used to update the atoms positions. This process is repeated until convergence or 200 iterations. Results are shown in <ref type="table" target="#tab_5">Table 5</ref>. The suggested metrics are Average Distance within Threshold (ADwT) metric, which measures whether the atom positions are close to those found using DFT and Average Forces below Threshold (AFbT), which measures whether a true energy minimum was found (i.e., forces are close to zero). On the ADwT metric, SpinConv outperforms other approaches (53.62% averaged across splits). On the AFbT metric, DimeNet++-large outperforms SpinConv (21.82% vs. 16.67%), but is more than ?3x slower (814.6h vs. 263.2h) during inference. SpinConv outperforms all other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MD17</head><p>The MD17 dataset <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> contains molecular dynamic simulations for eight small molecules. Two training datasets are commonly used, one containing 1k examples and another containing 50k examples. We found the 1k training dataset to be too small for the SpinConv model, and may be more appropriate for approaches that incorporate prior chemistry knowledge, such as hand-coded features or force fields <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. The 50k dataset provides significantly more training data, but the remaining validation and test data are highly similar to those found in training, and may not guarantee independent samples in the test set <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, we report results on MD17 for comparison to prior work on the molecular dynamics task. Research in this domain would greatly benefit from the generation of a larger dataset.</p><p>Results are shown in <ref type="table" target="#tab_7">Table 6</ref>. SpinConv is on par or better for 7 of the 8 molecules when compared to DimeNet <ref type="bibr" target="#b14">[15]</ref>. Both SpinConv and DimeNet perform well with respect to the GDML <ref type="bibr" target="#b3">[4]</ref> and PhysNet <ref type="bibr" target="#b29">[30]</ref> models that take advantage of domain-specific information. Given the smaller dataset size, the SpinConv model uses a reduced 8x8 grid-based spherical representation. Other model parameters are the same as previously described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">QM9</head><p>Our final set of results are on the popular QM9 dataset <ref type="bibr" target="#b21">[22]</ref> that tests the prediction of numerous properties for small molecules. While the SpinConv model was designed to estimate energies and per-atom forces, we may use the same model to predict other proprieties. Results are shown in <ref type="table" target="#tab_8">Table 7</ref> on a random test split for an energy-centric 8x8 grid-based SpinConv model. The results of DimeNet++ and the recent SphereNet <ref type="bibr" target="#b16">[17]</ref> outperform those of others. However, DimeNet++, SphereNet and SpinConv perform well with respect to other approaches across many properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>A common approach to estimating molecular and atomic properties is the use of GNNs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref> where nodes represent atoms and edges connect neighboring atoms. One of the first approaches for force estimation was SchNet <ref type="bibr" target="#b24">[25]</ref>, which computed forces using only the distance between atoms without the use of angular information. Unlike previous approaches that used discrete distance filters <ref type="bibr" target="#b32">[33]</ref>, SchNet proposed the used of differentiable edge filters. This enabled the construction of an energy-conserving model for molecular dynamics that estimates forces by taking the negative gradient of the energy with respect to the atom positions <ref type="bibr" target="#b3">[4]</ref>. DimeNet extended this approach to also represent the angular information between triplets of atoms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. The more recent SphereNet further extends this by capturing dihedral angles <ref type="bibr" target="#b16">[17]</ref>. SpinConv is able to model relative angular relationships between all neighboring atoms, and not just triplets of atoms, due to the use of the spin convolutional filter. In parallel to invariant models, rotational equivariant networks are explored in depth by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24]</ref>. This was accomplished by decoupling the network-fed invariant information (distance), from the equivariant information (distance vector), followed by the careful combination via tensor products. The energy-centric SpinConv model is invariant to rotations due to the use of global pooling after the spin convolution. The final force block of the force-centric model is not strictly rotation equivariant, but is encouraged to learn rotation equivariance during training.</p><p>Another approach to force estimation is to directly regress the forces as an output of the network. This doesn't enforce energy conservation or rotational equivariance, but as shown by ForceNet <ref type="bibr" target="#b11">[12]</ref>, such models can still produce accurate force estimates.</p><p>Numerous approaches incorporate more domain specific information into machine learning models. These include GDML <ref type="bibr" target="#b3">[4]</ref> and PhysNet <ref type="bibr" target="#b29">[30]</ref> that use handcrafted features and force-fields respectively. OrbNet <ref type="bibr" target="#b19">[20]</ref> is a hybrid approach that utilizes proprietary orbital features that improves accuracy while achieving significant efficiency gains over DFT. While these approaches can lead to improved accuracy, they typically result in increased computational expense over ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>While the SpinConv model demonstrates improved performance, it still has significant limitations. Most notable is the force and energy estimates are still significantly lower than desired for practical applications. Further research is needed to improve accuracies, so that machine learning models can be widely adopted. Currently, the SpinConv model does not take advantage of domain specific information. Results could be significantly improved, especially for smaller datasets (e.g., MD17 1k), if more domain information was integrated into the model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20]</ref>. The use of the spin convolution becomes increasingly expensive as the size of the filter increases, since the number of convolutions is equal to the longitudinal dimension of the filter. If filters of higher resolution are needed, more computationally efficient approaches may be required.</p><p>In conclusion, we propose the SpinConv model that effectively captures the relative angular information of neighboring atoms, while maintaining the invariance of the energy estimation with respect to system rotations. This is enabled by utilizing a spin convolution over a spherical representation in a per-edge local reference frame, followed by global pooling. Two model variants are proposed based on whether energy conservation is enforced. Results demonstrate state-of-the-art results on the OC20 dataset, and strong results on both the MD17 and QM9 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Societal Impact</head><p>This work is motivated by the problems we face due to climate change <ref type="bibr" target="#b34">[35]</ref>, many of which require innovative solutions to reduce energy usage and replace traditional chemical feedstocks with renewable alternatives. For example, one of the most energy intensive chemical processes is the development of new electrochemical catalysts for ammonia fertilizer production that helped to feed the world's growing population during the 20th century <ref type="bibr" target="#b10">[11]</ref>. This is also an illustrative example of possible unintended consequences as advancements in chemistry and materials may be used for numerous purposes. As ammonia fertilization increased in use, its overuse in today's farming has led to ocean "dead zones" and its production is very carbon intensive. Knowledge and techniques used to create ammonia were also transferred to the creation of explosives during wartime. We hope to steer the use of ML for atomic simulations to societally-beneficial uses by training and testing our approaches on datasets, such as OC20, that were specifically designed to address chemical reactions useful for addressing climate change.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(left) Overall model diagram for energy-centric model taking atom positions x and atomic numbers a as input and estimating the energy E. (right) Diagram of the embedding and force blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>st ? R M for K iterations. The structure's energy E ? R is computed as a function of the final layer of the edge messages in the GNN: E(x, a) = t F e (a t ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of learned embeddings (weights on the one-hot embeddings) for the source a s and target a t atomic numbers plotted on a periodic table. A random sample of 12 values from each embedding are shown. Embeddings are from the first embedding block in the first message update. Note that neighboring atoms in the periodic table with similar properties have similar weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Energy MAE [eV] ? Force MAE [eV/?] ? Force Cos ? EFwT [%] ?Comparison of SpinConv to existing GNN models on the S2EF task. Average results across all four test splits are reported. We mark as bold the best performance and close ones, i.e., within 0.0005 MAE, which according to our preliminary experiments, is a good threshold to meaningfully distinguish model performance. Training time is in GPU days, and inference time is in GPU hours. Median represents the trivial baseline of always predicting the median training force across all the validation atoms.</figDesc><table><row><cell cols="6">Model time Median Train Inference Hidden #Msg #Params dim layers time ---</cell><cell>2.258</cell><cell></cell><cell>OC20 Test 0.08438</cell><cell>0.0156</cell><cell>0.005</cell></row><row><cell>SchNet[27, 3]</cell><cell>1024</cell><cell>5</cell><cell cols="2">9.1M 194d</cell><cell>0.8h</cell><cell>-</cell><cell></cell><cell>0.04903</cell><cell>0.3413</cell><cell>0</cell></row><row><cell>DimeNet++[14, 3]</cell><cell>192</cell><cell>3</cell><cell cols="2">1.8M 587d</cell><cell>8.5h</cell><cell cols="2">0.5343</cell><cell>0.04758</cell><cell>0.3560</cell><cell>0.05</cell></row><row><cell>DimeNet++ energy-only[14, 3]</cell><cell>192</cell><cell>3</cell><cell cols="2">1.8M 587d</cell><cell>8.5h</cell><cell cols="2">0.4802</cell><cell>0.3459</cell><cell>0.1021</cell><cell>0.0</cell></row><row><cell>DimeNet++ force-only[14, 3]</cell><cell>192</cell><cell>3</cell><cell cols="2">1.8M 587d</cell><cell>8.5h</cell><cell>-</cell><cell></cell><cell>0.03573</cell><cell>0.4785</cell><cell>-</cell></row><row><cell>DimeNet++-large[14, 3]</cell><cell>512</cell><cell>3</cell><cell cols="2">10.7M 1600d</cell><cell>27.0h</cell><cell>-</cell><cell></cell><cell>0.03275</cell><cell>0.5408</cell><cell>-</cell></row><row><cell>ForceNet[12]</cell><cell>512</cell><cell>5</cell><cell>11.3M</cell><cell>31d</cell><cell>1.3h</cell><cell>-</cell><cell></cell><cell>0.03432</cell><cell>0.4770</cell><cell>-</cell></row><row><cell>ForceNet-large[12]</cell><cell>768</cell><cell>7</cell><cell cols="2">34.8M 194d</cell><cell>3.5h</cell><cell>-</cell><cell></cell><cell>0.03113</cell><cell>0.5195</cell><cell>-</cell></row><row><cell>SpinConv (energy-centric)</cell><cell>256</cell><cell>3</cell><cell cols="2">6.1M 275d</cell><cell>22.7h</cell><cell cols="2">0.4114</cell><cell>0.03888</cell><cell>0.4299</cell><cell>0.16</cell></row><row><cell>SpinConv (energy-centric) force-only</cell><cell>256</cell><cell>3</cell><cell cols="2">6.1M 380d</cell><cell>22.7h</cell><cell>-</cell><cell></cell><cell>0.03258</cell><cell>0.4976</cell><cell>-</cell></row><row><cell>SpinConv (force-centric)</cell><cell>256</cell><cell>3</cell><cell cols="2">8.5M 275d</cell><cell>9.1h</cell><cell cols="2">0.3363</cell><cell>0.02966</cell><cell>0.5391</cell><cell>0.45</cell></row><row><cell>Model</cell><cell>ID</cell><cell cols="5">Energy MAE (eV) ? OOD Ads. OOD Cat. OOD Both</cell><cell>ID</cell><cell cols="3">Force MAE (eV/?) ? OOD Ads. OOD Cat. OOD Both</cell></row><row><cell>Median</cell><cell>2.043</cell><cell>2.420</cell><cell></cell><cell>1.992</cell><cell>2.577</cell><cell cols="2">0.0809</cell><cell>0.0801</cell><cell>0.0787</cell><cell>0.0978</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Energy Loss Only</cell><cell></cell><cell></cell></row><row><cell>SchNet</cell><cell>0.395</cell><cell>0.446</cell><cell></cell><cell>0.551</cell><cell>0.703</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DimeNet++</cell><cell>0.359</cell><cell>0.402</cell><cell></cell><cell>0.506</cell><cell>0.654</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Force Loss Only</cell><cell></cell><cell></cell></row><row><cell>SchNet</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">0.0443</cell><cell>0.0469</cell><cell>0.0459</cell><cell>0.0590</cell></row><row><cell>DimeNet++</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">0.0331</cell><cell>0.0341</cell><cell>0.0340</cell><cell>0.0417</cell></row><row><cell>DimeNet++-large</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">0.0281</cell><cell>0.0289</cell><cell>0.0312</cell><cell>0.0371</cell></row><row><cell>ForceNet</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">0.0313</cell><cell>0.0320</cell><cell>0.0331</cell><cell>0.0409</cell></row><row><cell>ForceNet-large</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">0.0278</cell><cell>0.0283</cell><cell>0.0309</cell><cell>0.0375</cell></row><row><cell>SpinConv (energy-centric)</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">0.0309</cell><cell>0.0321</cell><cell>0.0315</cell><cell>0.0393</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Energy and Force Loss</cell><cell></cell></row><row><cell>SchNet</cell><cell>0.443</cell><cell>0.491</cell><cell></cell><cell>0.529</cell><cell>0.716</cell><cell cols="2">0.0493</cell><cell>0.0527</cell><cell>0.0508</cell><cell>0.0652</cell></row><row><cell>DimeNet++</cell><cell>0.486</cell><cell>0.470</cell><cell></cell><cell>0.533</cell><cell>0.648</cell><cell cols="2">0.0443</cell><cell>0.0458</cell><cell>0.0444</cell><cell>0.0558</cell></row><row><cell cols="2">SpinConv (energy-centric) 0.351</cell><cell>0.367</cell><cell></cell><cell>0.411</cell><cell>0.517</cell><cell cols="2">0.0358</cell><cell>0.0374</cell><cell>0.0364</cell><cell>0.0460</cell></row><row><cell>SpinConv (force-centric)</cell><cell>0.261</cell><cell>0.275</cell><cell></cell><cell>0.350</cell><cell>0.459</cell><cell cols="2">0.0269</cell><cell>0.0277</cell><cell>0.0285</cell><cell>0.0356</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison of SpinConv to existing GNN models on different test splits. We mark as bold the best performance and close ones, i.e., within 0.0005 MAE, which according to our preliminary experiments, is a good threshold to meaningfully distinguish model performance. Training time is in GPU days, and inference time is in GPU hours. Median represents the trivial baseline of always predicting the median training force across all the validation atoms.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Energy MAE [eV] ? Force MAE [eV/?] ? Force Cos ? EFwT [%] ?</figDesc><table><row><cell cols="5">Model time Median Train Hidden #Msg #Params dim layers</cell><cell></cell><cell>OC20 Val ID 30k</cell><cell></cell><cell></cell></row><row><cell>Energy-Centric</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpinConv (grid 12x8)</cell><cell>128</cell><cell>2</cell><cell>1.3M</cell><cell>54d</cell><cell>-</cell><cell>0.0417</cell><cell>0.401</cell><cell>-</cell></row><row><cell>SpinConv (spherical harmonics, = 5)</cell><cell>256</cell><cell>3</cell><cell cols="2">6.4M 119d</cell><cell>-</cell><cell>0.0405</cell><cell>0.411</cell><cell>-</cell></row><row><cell>SpinConv (grid 12x8)</cell><cell>256</cell><cell>3</cell><cell>6.1M</cell><cell>87d</cell><cell>-</cell><cell>0.0406</cell><cell>0.426</cell><cell>-</cell></row><row><cell>Force-Centric</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpinConv (grid 12x8)</cell><cell>128</cell><cell>2</cell><cell>1.8M</cell><cell>54d</cell><cell>0.376</cell><cell>0.0370</cell><cell>0.436</cell><cell>0.15%</cell></row><row><cell>SpinConv (grid no conv 16x12)</cell><cell>256</cell><cell>3</cell><cell>8.5M</cell><cell>56d</cell><cell>0.341</cell><cell>0.0348</cell><cell>0.462</cell><cell>0.20%</cell></row><row><cell>SpinConv (spherical harmonics, = 5)</cell><cell>256</cell><cell>3</cell><cell cols="2">8.1M 113d</cell><cell>0.321</cell><cell>0.0328</cell><cell>0.484</cell><cell>0.22%</cell></row><row><cell>SpinConv (grid 16x12)</cell><cell>256</cell><cell>3</cell><cell>8.5M</cell><cell>76d</cell><cell>0.317</cell><cell>0.0326</cell><cell>0.484</cell><cell>0.20%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Initial Structure to Relaxed Energy (IS2RE) results on the OC20 test split as evaluated by the Energy MAE (eV) and Energy within Threshold (EwT)<ref type="bibr" target="#b2">[3]</ref> (see OC20 discussion board). Comparisons made for the direct and relaxation approaches using various models.</figDesc><table><row><cell>Model</cell><cell>Inference time ?</cell><cell cols="5">AFbT (%) ? ID OOD Ads. OOD Cat. OOD Both Average</cell><cell cols="4">ADwT (%) ? ID OOD Ads. OOD Cat. OOD Both Average</cell></row><row><cell>SchNet [25]</cell><cell cols="2">54.1h 5.28</cell><cell>2.82</cell><cell>2.62</cell><cell>2.73</cell><cell cols="2">3.36 32.49</cell><cell>28.59</cell><cell>30.99</cell><cell>35.08</cell><cell>31.79</cell></row><row><cell>DimeNet++ [14]</cell><cell cols="2">407.6h 17.52</cell><cell>14.67</cell><cell>14.32</cell><cell>14.43</cell><cell cols="2">15.23 48.76</cell><cell>45.19</cell><cell>48.59</cell><cell>53.14</cell><cell>48.92</cell></row><row><cell>DimeNet++-large [14]</cell><cell cols="2">814.6h 25.65</cell><cell>20.73</cell><cell>20.24</cell><cell>20.67</cell><cell cols="2">21.82 52.45</cell><cell>48.47</cell><cell>50.99</cell><cell>54.82</cell><cell>51.68</cell></row><row><cell>ForceNet [12]</cell><cell cols="2">75.1h 10.75</cell><cell>7.74</cell><cell>7.54</cell><cell>7.78</cell><cell cols="2">8.45 46.83</cell><cell>41.26</cell><cell>46.45</cell><cell>49.60</cell><cell>46.04</cell></row><row><cell>ForceNet-large [12]</cell><cell cols="2">186.9h 14.77</cell><cell>12.23</cell><cell>12.16</cell><cell>11.46</cell><cell cols="2">12.66 50.59</cell><cell>45.16</cell><cell>49.80</cell><cell>52.94</cell><cell>49.62</cell></row><row><cell>SpinConv (force-centric)</cell><cell cols="2">263.2h 21.10</cell><cell>15.70</cell><cell>15.86</cell><cell>14.01</cell><cell cols="2">16.67 53.68</cell><cell>48.87</cell><cell>53.92</cell><cell>58.03</cell><cell>53.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Relaxed structure from initial structure (IS2RS) results on the OC20 test split, as evaluated by Average Distance within Threshold (ADwT) and Average Forces below Threshold (AFbT). All values in percentages, higher is better. Results computed via the OCP evaluation server. Inference times are total across the 4 splits.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Forces MAE (kcal/mol?) on MD17 for models trained using 50k samples. Best results for models not using domain specific information are in bold. *The DimeNet results were trained in-house as the original authors did not use the 50k dataset. DimeNet was found to outperform DimeNet++ on this task.</figDesc><table><row><cell>Task</cell><cell>?</cell><cell>?</cell><cell>HOMO</cell><cell>LUMO</cell><cell>?</cell><cell>C ?</cell><cell>G</cell><cell>H</cell><cell>R 2</cell><cell>U</cell><cell cols="2">U 0 ZPVE</cell></row><row><cell>Units</cell><cell cols="12">bohr 3 meV meV meV D cal/mol K meV meV bohr 3 meV meV meV</cell></row><row><cell>NMP [9]</cell><cell cols="2">.092 69</cell><cell>43</cell><cell cols="2">38 .030</cell><cell>.040</cell><cell>19</cell><cell cols="3">17 .180 20</cell><cell cols="2">20 1.50</cell></row><row><cell>Schnet [25]</cell><cell cols="2">.235 63</cell><cell>41</cell><cell cols="2">34 .033</cell><cell>.033</cell><cell>14</cell><cell cols="3">14 .073 19</cell><cell cols="2">14 1.70</cell></row><row><cell>Cormorant [1]</cell><cell cols="2">.085 61</cell><cell>34</cell><cell cols="2">38 .038</cell><cell>.026</cell><cell>20</cell><cell cols="3">21 .961 21</cell><cell cols="2">22 2.03</cell></row><row><cell>L1Net [19]</cell><cell cols="2">.088 68</cell><cell>46</cell><cell cols="2">35 .043</cell><cell>.031</cell><cell>14</cell><cell cols="3">14 .354 14</cell><cell cols="2">13 1.56</cell></row><row><cell>LieConv [7]</cell><cell cols="2">.084 49</cell><cell>30</cell><cell cols="2">25 .032</cell><cell>.038</cell><cell>22</cell><cell cols="3">24 .800 19</cell><cell cols="2">19 2.28</cell></row><row><cell>TFN [29]</cell><cell cols="2">.223 58</cell><cell>40</cell><cell cols="2">38 .064</cell><cell>.101</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SE(3)-Tr. [8]</cell><cell cols="2">.142 53</cell><cell>35</cell><cell cols="2">33 .051</cell><cell>.054</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EGNN [24]</cell><cell cols="2">.071 48</cell><cell>29</cell><cell cols="2">25 .029</cell><cell>.031</cell><cell>12</cell><cell cols="3">12 .106 12</cell><cell cols="2">11 1.55</cell></row><row><cell cols="3">DimeNet++ [14] .044 33</cell><cell>25</cell><cell cols="2">20 .030</cell><cell>.023</cell><cell>8</cell><cell>7</cell><cell>.331</cell><cell>6</cell><cell>6</cell><cell>1.21</cell></row><row><cell cols="3">SphereNet [17] .047 32</cell><cell>24</cell><cell cols="2">19 .027</cell><cell>.022</cell><cell>8</cell><cell>6</cell><cell>.292</cell><cell>7</cell><cell>6</cell><cell>1.12</cell></row><row><cell>SpinConv</cell><cell cols="2">.058 47</cell><cell>26</cell><cell cols="2">22 .027</cell><cell>.028</cell><cell>12</cell><cell cols="3">12 .156 12</cell><cell cols="2">12 1.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Mean absolute error results for QM9 dataset [22] on 12 properties for small molecules.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04015</idno>
		<title level="m">Cormorant: Covariant molecular neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Se (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Mailoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mordechai</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Kozinsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03164</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Open Catalyst 2020 (oc20) dataset and community challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palizhati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ulissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Catalysis</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine learning of accurate energy-conserving molecular force fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poltavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1603015</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards exact molecular dynamics simulations with machine-learned force fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the role of gradients for machine learning of molecular energies and forces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45018</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3165" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Se (3)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The alchemy of air: a Jewish genius, a doomed tycoon, and the scientific discovery that fed the world but fueled the rise of Hitler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Broadway Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Forcenet: A graph neural network for large-scale quantum calculations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01436</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural message passing with edge updates for predicting properties of molecules and materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">Wedel</forename><surname>Peter Bj?rn J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and uncertainty-aware directional message passing for non-equilibrium molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankari</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">T</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-W</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bora</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05013</idno>
		<title level="m">Spherical message passing for 3d graph networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mixture of experts: a literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Relevance of rotationally equivariant convolutions for predicting molecular properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><forename type="middle">E</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>No?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08461</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Welborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas F</forename><surname>Frederick R Manby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orbnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08026</idno>
		<title level="m">Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tackling climate change with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Milojevic-Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waldman-Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05433</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Victor Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09844</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Schnet-a deep learning architecture for molecules and materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-J</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-R</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241722</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>??dek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Physnet: A neural network for predicting energies, forces, dipole moments, and partial charges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meuwly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02547</idno>
		<title level="m">Wouter Boomsma, and Taco Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An introduction to electrocatalyst design using machine learning for renewable energy storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palizhati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ulissi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09435</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
