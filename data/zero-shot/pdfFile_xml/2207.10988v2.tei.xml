<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Object Counting and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">VinAI Research</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">VinAI Research</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">VinAI Research</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">VinAI Research</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-shot Object Counting and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Few-shot Object Counting, Few-shot Object Detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle a new task of few-shot object counting and detection. Given a few exemplar bounding boxes of a target object class, we seek to count and detect all objects of the target class. This task shares the same supervision as the few-shot object counting but additionally outputs the object bounding boxes along with the total object count. To address this challenging problem, we introduce a novel twostage training strategy and a novel uncertainty-aware few-shot object detector: Counting-DETR. The former is aimed at generating pseudo ground-truth bounding boxes to train the latter. The latter leverages the pseudo ground-truth provided by the former but takes the necessary steps to account for the imperfection of pseudo ground-truth. To validate the performance of our method on the new task, we introduce two new datasets named FSCD-147 and FSCD-LVIS. Both datasets contain images with complex scenes, multiple object classes per image, and a huge variation in object shapes, sizes, and appearance. Our proposed approach outperforms very strong baselines adapted from few-shot object counting and few-shot object detection with a large margin in both counting and detection metrics. The code and models are available at https://github.com/VinAIResearch/Counting-DETR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper addresses a new task of Few-Shot object Counting and Detection (FSCD) in crowded scenes. Given an image containing many objects of multiple classes, we seek to count and detect all objects of a target class of interest specified by a few exemplar bounding boxes in the image. To facilitate fewshot learning, in training, we are only given the supervision of few-shot object counting, i.e., dot annotations for the approximate centers of all objects and a few exemplar bounding boxes for object instances from the target class. It is worth noting that the test classes may or may not be present in training classes. The problem setting is depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>FSCD is different from Few-Shot Object Counting (FSC) and Few-Shot Object Detection (FSOD). Compared to FSC, FSCD has several advantages: <ref type="bibr" target="#b0">(1)</ref> obtaining object bounding boxes "for free", which is suitable for quickly annotating bounding boxes for a new object class with a few exemplar bounding boxes;  (2) making the result of FSC more interpretable since bounding boxes are easier to verify than the density map. Compared to FSOD which requires bounding box annotation for all objects in the training phase of the base classes, FSCD uses significantly less supervision, i.e., only a few exemplar bounding boxes and dot annotations for all objects. This is helpful in crowded scenes where annotating accurate bounding boxes for all objects is ambiguously harder and significantly more expensive than the approximate dot annotation.</p><p>Consequently, FSCD is more challenging than both FSC and FSOD. FSCD needs to detect and count all the objects as for FSOD, but it is only trained with the supervision of FSC. This invalidates of most of available approaches used in these problems without significant changes in network architecture or loss function. Specifically, it is not trivial to extend the density map produced by FSC approaches to predict the object bounding boxes; and it is hard to train a few-shot object detector with few exemplar bounding boxes of the base classes.</p><p>A naive approach for FSCD is to extend FamNet <ref type="bibr" target="#b31">[32]</ref>, a density-map-based approach for FSC, whose counting number is obtained by summing over the predicted density map. To extend FamNet to detect objects, one can use a regression function on top of the features extracted from the peak locations (whose density values are highest in their respective local neighborhoods), the features extracted from the exemplars, and the exemplar boxes themselves. The process of this naive approach is illustrated in <ref type="figure">Fig. 2a</ref>. However, this approach has two limitations due to: 1) the imperfection of the predicted density map, and 2) the non-discriminative peak features. In the former, the density value is high in the environment locations whose color is similar to those of the exemplars, or the density map is peak-indistinguishable when the objects are packed in a dense region as depicted in <ref type="figure">Fig. 2b</ref>. In the latter, the extracted features are trained with counting objective (not object detection) so that they cannot represent for different shapes, sizes, and orientations, as illustrated in <ref type="figure">Fig. 2c</ref>.</p><p>To address the aforementioned limitations, we propose a new point-based approach, named Counting-DETR, treating objects as points. In particular, counting and detecting objects is equivalent to counting and detecting points, and the object bounding box is predicted directly from point features. Counting-DETR is based on an object detector, Anchor DETR <ref type="bibr" target="#b42">[43]</ref>, with improvements to better address FSCD. First, inspired by <ref type="bibr" target="#b4">[5]</ref> we adopt a two-stage training strategy: (1) Counting-DETR is trained to generate pseudo ground-truth (GT) bounding Limitations of a naive approach for FSCD by extending FamNet <ref type="bibr" target="#b31">[32]</ref> with a regression function for object detection. (a) Processing pipeline of this approach: a regressor takes as input exemplar boxes with their features, and features at peak density locations to predict bounding boxes for the peak locations. (b) Limitation 1: poor quality of the density map predicted by FamNet when the exemplars share similar appearance with background or densely packed region. The first row presents the input images with a few exemplars each, the second row presents the corresponding density map predicted by FamNet. (c) Limitation 2: Non-discriminative peak features cannot represent objects with significant differences in shape and size. The green boxes are predicted from the features extracted at the annotated dots.</p><p>boxes given the annotated points of training images; (2) Counting-DETR is further fine-tuned on the generated pseudo GT bounding boxes to detect objects on test images. Second, since the generated pseudo GT bounding boxes are imperfect, we propose to estimate the uncertainty for bounding box prediction in the second stage. The estimated uncertainty regularizes learning such that lower box regression loss is incurred on the predictions with high uncertainty.</p><p>The overview of Counting-DETR is illustrated in <ref type="figure">Fig. 3</ref>. In short, the contributions of our paper are: (1) we introduce a new problem of few-shot object counting and detection (FSCD); (2) we introduce two new datasets, FSCD-147 and FSCD-LVIS; (3) we propose a two-stage training strategy to first generate pseudo GT bounding boxes from the dot annotations, then use these boxes as supervision for training our proposed few-shot object detector; and (4) we propose a new uncertainty-aware point-based few-shot object detector, taking into account the imperfection of pseudo GT bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review some related work on object counting and detection.</p><p>Visual counting focuses on some predefined classes such as car <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15]</ref>, cell <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">46]</ref>, and human <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1]</ref>. The methods can be grouped into two types: density-map-based and detection-based. The former, e.g., <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b23">24]</ref>, predicts and sums over density map from input image to get the final results. The latter (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref>) counts the number of objects based on the detected boxes. The latter is better at justifying the counting number, however, it requires the GT points (stage 1)  <ref type="figure">Fig. 3</ref>. The overview of our two-step training approach: (1) Counting-DETR is first trained on a few pairs of dot and bounding boxes and then used to predict pseudo GT boxes for the annotated dots; (2) Counting-DETR is trained to predict the object bounding boxes, with the prediction target being the pseudo GT boxes from the first stage. Specifically, the input image is first forwarded through a CNN+FPN backbone to extract its feature map. The exemplar features are extracted from their boxes to integrate with the feature map producing the exemplars-integrated feature map. This feature map is then taken as input to the encoder-decoder transformer along with either the annotated dots in the first stage or the anchor points in the second stage for foreground/background classification and bounding box regression. In the second stage, the estimated uncertainty is used to regularize the training with a new uncertainty loss to account for the imperfection of the pseudo GT bounding boxes.</p><p>GT bounding boxes for training and its performance is exceeded by that of the former, especially for images of crowded scenes.</p><p>Few-shot counting (FSC) counts the number of objects in an image with some exemplar bounding boxes of a new object class. Since the number of exemplar boxes is so small that an object detector cannot be reliably learned, prior methods on FSC are all based on density-map regression. GMN <ref type="bibr" target="#b25">[26]</ref> formulates object counting as object matching in video tracking such that a class agnostic counting module can be pretrained on a large-scale video object tracking dataset (ImageNet VID <ref type="bibr" target="#b35">[36]</ref>). FamNet <ref type="bibr" target="#b31">[32]</ref> correlates the features extracted from a few exemplars with the feature map to obtain the density map for object counting. VCN <ref type="bibr" target="#b29">[30]</ref> improves upon <ref type="bibr" target="#b31">[32]</ref> by augmenting the input image with different styles to make the counting more robust. LaoNet <ref type="bibr" target="#b21">[22]</ref> combines self-attention and cross attention in the transformer to aggregate features from the exemplar to the image to facilitate density map prediction. ICFR <ref type="bibr" target="#b48">[48]</ref> proposes an iterative framework to progressively refine the exemplar-related features, thus producing a better density map than a single correlation in <ref type="bibr" target="#b31">[32]</ref>. However, these approaches do not output object bounding boxes. An extension for object detection from these approaches is depicted in <ref type="figure">Fig. 2a</ref>, but it has several limitations as illustrated <ref type="figure">Fig. 2b</ref> and <ref type="figure">Fig. 2c</ref>. Whereas, our approach Counting-DETR effectively predicts object bounding boxes along with the object count with only the supervision of FSC.</p><p>Object detection methods include anchor-based approaches such as Faster-RCNN <ref type="bibr" target="#b33">[34]</ref> and Retina Net <ref type="bibr" target="#b22">[23]</ref>, point-based approaches such as like FCOS <ref type="bibr" target="#b37">[38]</ref> and Center-Net <ref type="bibr" target="#b52">[52]</ref>, and transformer-based approaches such as DETR <ref type="bibr" target="#b2">[3]</ref>, Point DETR <ref type="bibr" target="#b4">[5]</ref> and Anchor DETR <ref type="bibr" target="#b42">[43]</ref>. DETR is the first approach to apply transformer architecture <ref type="bibr" target="#b38">[39]</ref> to object detection. Anchor DETR improves the convergence rate and performance of DETR by learnable anchor points representing the initial prediction of the objects in the image. However, these methods require thousands of bounding box annotations on some predefined classes for training and cannot generalize well on a new class in testing with a few box exemplars as in our few-shot setting. Point DETR <ref type="bibr" target="#b4">[5]</ref> alleviates this requirement using two separate detectors: teacher (i.e., Point-DETR) and student (i.e., FCOS). The former learns from a small set of fully annotated boxes to generate pseudo-GT bounding boxes of a large amount of point-annotated images. Then the latter is trained with these pseudo-GT boxes to predict the bounding boxes of the test images. This approach is complicated and does not take into account the imperfect pseudo-GT bounding boxes. In contrast, our Counting-DETR is a unified single network with the uncertainty-aware bounding prediction.</p><p>Few-shot object detection (FSOD) approaches are mostly based on Faster-RCNN <ref type="bibr" target="#b33">[34]</ref> and can be divided into two subgroups based on episodic training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b5">6]</ref> and fine-tuning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7]</ref>. The former leverages episodic training technique to mimic the evaluation setting in the training whereas the latter finetunes some layers while keeping the rest unchanged to preserve the knowledge learned from the training classes. However, all FSOD approaches require box annotations for all objects of the base classes in training. It is not the case in our setting where only a few exemplar bounding boxes are given in training.</p><p>To address this problem, we propose a two-stage training strategy wherein the first stage, the pseudo GT bounding boxes for all objects are generated from the given exemplar boxes and the dot annotations.</p><p>Object detection with uncertainty accounts for the uncertainty in the input image due to blurring or indistinguishable boundaries between objects and the background. Prior work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> assumes the object bounding boxes are characterized by a Gaussian distribution whose mean and standard deviation are predicted by a network trained with an uncertainty loss function derived from maximum the likelihood between the predicted distribution and the GT boxes. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> apply the uncertainty loss for 3D object detection. Our uncertainty loss shares some similarities with prior work, however, we use the uncertainty loss to account for the imperfection of the pseudo GT bounding boxes (not the input image) such that the Laplace distribution works significantly better than the prior Gaussian distribution as shown in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>Problem definition: In training, we are given a set of images containing multiple object categories. For each image I, a few exemplar bounding boxes B k , k = 1, . . . , K where K is the number of exemplars, and the dot annotations for all object instances of a target class are annotated. This kind of supervision is the same as in few-shot object counting. In testing, given a query image with bounding boxes for a few exemplar objects in the target class, our goal is to detect and count all instances of the target class in the query image.</p><p>To address this problem, we propose a novel uncertainty-aware point-based few-shot object detector, named Counting-DETR trained with a novel two-stage training strategy to first generate pseudo GT bounding boxes from dot annotations and then train Counting-DETR on the generated pseudo GT boxes to predict bounding boxes of a new object class defined by a few bounding box exemplars in testing. The overview of Counting-DETR is illustrated in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction and Feature Aggregation</head><p>Feature extraction: A CNN backbone is used to extract feature map F I ? R H?W ?D from the input image I where H, W, D are height, width, and number of channels of the feature map, respectively. We then extract the exemplar feature vectors f B k ? R 1?D , at the center of the exemplar bounding boxes B k . Finally, the exemplar feature vector f B ? R 1?D is obtained by averaging these feature vectors, or f</p><formula xml:id="formula_0">B = 1 K k f B k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature aggregation:</head><p>We integrate the exemplar feature f B to the feature map of the image F I to produce the exemplar-integrated feature map F A :</p><formula xml:id="formula_1">F A = W proj * [F I ; F I ? f B ],<label>(1)</label></formula><p>where * , ?, [?; ?] are the convolution, channel-wise multiplication, and concatenation operations, respectively. W proj ? R 2D?D is a linear projection weight. The first term in the concatenation preserves the original information of the feature map, while the second term aims at enhancing features at locations whose appearance are similar to those of the exemplars and suppressing the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Encoder-Decoder Transformer</head><p>Inspired by DETR <ref type="bibr" target="#b2">[3]</ref>, we design our transformer of Counting-DETR to take as input the exemplar-integrated feature map F A and M query points {p m } M m=1 and predict the bounding box b m for each query point p m . The queries are the 2D points representing the initial guesses for the object locations rather than the learnable embeddings to achieve a faster training rate as shown in <ref type="bibr" target="#b42">[43]</ref>. Thus, Counting-DETR is point-based approach that leverages the given dot annotations as the queries to predict the pseudo GT bounding boxes. Also, the transformer consists of two sub-networks: encoder and decoder. The former aims at enhancing features among the input set of features with the self-attention operation. The latter allows all the query points to interact with the enhanced features from the encoder with the cross-attention operation, thus capturing global information.</p><p>Next, the decoder is used to: (1) predict the classification score s representing the presence or absence of the object at a particular location, (2) regress the object's bounding box ? represented by the offset x, y from the GT object center to the query point along with its size w, h. Following <ref type="bibr" target="#b42">[43]</ref>, first, the Hungarian algorithm is used to match each of the GT bounding boxes with its corresponding predicted bounding boxes. Then for each pair of matched GT and predicted bounding boxes, the focal loss <ref type="bibr" target="#b22">[23]</ref> and the combination of L 1 loss and GIoU loss <ref type="bibr" target="#b34">[35]</ref> are used as training loss functions. In particular, at each query point, the following loss is computed:</p><formula xml:id="formula_2">L DETR = ? 1 Focal(s, s * ) + ? 2 L 1 (?, ? * ) + ? 3 GIoU(?, ? * ),<label>(2)</label></formula><p>where s * , ? * are the GT class label and bounding box, respectively. ? 1 , ? 2 , ? 3 are the coefficients of focal, L 1 , and GIoU loss functions, respectively. Notably, Counting-DETR also estimates the uncertainty ? when training under the supervision of the imperfect pseudo GT bounding boxes?. This uncertainty is used to regularize the learning of bounding box ? such that a lower loss is incurred at the prediction with high uncertainty. We propose to use the following uncertainty loss:</p><formula xml:id="formula_3">L uncertainty = 1 2 o?{x,y,w,h} |? o ?? o | ? o + log ? o ,<label>(3)</label></formula><p>where ? is the estimated uncertainty. This loss is derived from the maximum likelihood estimation (MLE) between the predicted bounding box distribution characterized by a Laplace distribution and the pseudo GT bounding box as evidence. Another option is the Gaussian distribution, however, in the experiments, we show that the Gaussian has the inferior performance to that of Laplace, and is even worse than the variant that does not employ uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Two-stage Training Strategy</head><p>The proposed few-shot object detector, Counting-DETR, can only be trained with the bounding box supervision for all objects. However, we only have bounding box annotation for a few exemplars and the point annotation for all objects as the setting of FSCD. Hence, we propose a two-stage training strategy as follows.</p><p>In Stage 1, we first pretrain Counting-DETR on a few exemplar bounding boxes with their centers as the query points (as described in Sec. 3.2). Subsequently, the pretrained network is used to predict the pseudo GT bounding boxes on the training images with the dot annotations as the query points. It is worth noting that, in this stage, we have the GT exemplar center as queries and their corresponding bounding boxes as supervision, so we do not use the Hungarian matching, uncertainty estimation, and uncertainty loss, i.e., we only use L DETR in Eq. (2) to train our Counting-DETR. The visualization of some generated pseudo-GT boxes is illustrated in <ref type="figure">Fig. 4</ref>.</p><p>In Stage 2, the generated pseudo GT bounding boxes on the training images are used to fine-tune the pretrained Counting-DETR. The fine-tuned model is then used to make predictions on the test images with the uniformly sampled anchor points as queries. Different from Stage 1, the supervision is the imperfect pseudo GT bounding boxes, hence, we additionally leverage the uncertainty estimation branch with uncertainty loss to train. Particularly, we use the following loss to train Counting-DETR in this stage:</p><formula xml:id="formula_4">L combine = L DETR + ? 4 L uncertainty ,<label>(4)</label></formula><p>where ? 4 is the coefficient of L uncertainty .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">New Datasets for Few-shot Counting and Detection</head><p>A contribution of our paper is the introduction of two new datasets for few-shot counting and detection. In this section, we will describe these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The FSCD-147 Dataset</head><p>The FSC-147 dataset <ref type="bibr" target="#b31">[32]</ref> was recently introduced for the few-shot object counting task with 6135 images across a diverse set of 147 object categories. In each image, the dot annotation for all objects and three exemplar boxes are provided. However, this dataset does not contain bounding box annotations for all objects. For evaluation purposes, we extend the FSC-147 dataset by providing bounding box annotations for all objects of the val and test sets. We name the new dataset FSCD-147. To be consistent with the counting, an object will be annotated with its bounding box only if it has a dot annotation. <ref type="figure">Fig. 5a</ref> shows some samples of the FSCD-147 dataset. It is worth noting that annotating bounding boxes for many objects in crowded scenes of FSC-147 is a laborious process, and this is a significant contribution of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The FSCD-LVIS Dataset</head><p>Although the FSC-147 dataset contains images with a large number of objects in each image, the scene of each image is rather simple. Each image of FSC-147 shows the target object class so clearly that one can easily know which object class to count without having to specify any exemplars as shown in <ref type="figure" target="#fig_1">Fig. 1</ref> and <ref type="figure">Fig. 5a</ref>. For real-world deployment of methods for few-shot counting and detection, we introduce a new dataset called FSCD-LVIS. Specifically, the scene is more complex with multiple object classes with multiple object instances each as illustrated in <ref type="figure">Fig. 5b</ref>. Without providing the exemplars for the target class, one cannot definitely guess which the target class is. The FSCD-LVIS dataset contains 6196 images and 377 classes, extracted from the LVIS dataset <ref type="bibr" target="#b11">[12]</ref>. For each image, we filter out all instances with an area smaller than 20 pixels, or a width or a height smaller than 4 pixels. The comparison between the FSCD-LVIS and FSC-147 datasets is shown in Tab. 1. The histogram of the number of labeled objects per image is illustrated in Tab. 2. The LVIS dataset has the box annotations for all objects, however, to be consistent with the setting of FSCD, we randomly choose three annotated bounding boxes of a selected object class as the exemplars for each image in the training set of FSCD-LVIS. is the number of test images, c * j and c j are GT and the predicted number of objects for image j, respectively. Unlike the absolute errors MAE and RMSE, the relative errors NAE and SRE reflect the practical usage of visual counting, i.e., with the same number of wrong objects counted (e.g., 10), it is more serious for images having a smaller number of objects (e.g., 20) than the ones having larger numbers of objects (e.g., 200).</p><p>For object detection, we use mAP and AP50. They are the average precision metrics with the IoU threshold between predicted and GT boxes for determining a correct prediction ranging from 0.5 to 0.95 for mAP and 0.5 for AP50. Implementation details. We implement our approach, baselines, and ablations in PyTorch <ref type="bibr" target="#b27">[28]</ref>. Our backbone network is ResNet-50 <ref type="bibr" target="#b12">[13]</ref> with the frozen Batch Norm layer <ref type="bibr" target="#b17">[18]</ref>. We extract exemplar features f B k from exemplar boxes B k from Layer 4 of the backbone. Our transformer network shares the same architecture as that of Anchor DETR <ref type="bibr" target="#b42">[43]</ref> with the new uncertainty estimation and is trained with additional uncertainty loss as described in Sec. 3.2, while keeping the rest intact with six layers for both encoder and decoder. We use AdamW optimizer <ref type="bibr" target="#b24">[25]</ref> with the learning rate of 10 ?5 for the backbone and 10 ?4 for the transformer to train Counting-DETR in 30 epochs with a batch size of one. We use the following training loss coefficients ? 1 = 2, ? 2 = 5, ? 3 = 2, ? 4 = 2, which were tuned based on the validation set. Also, the number of exemplar boxes is set to K = 3, as in FamNet <ref type="bibr" target="#b31">[32]</ref> for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>We conduct several experiments on the validation data of FSCD-147 to study the contribution of various components of our method.</p><p>Pseudo Box and Uncertainty Loss. From Tab. 3, we see that pseudo GT boxes (pseudo box) generated from our first stage are much better than the boxes generated by Ridge Regression (-6 in AP, +9 in MAE). Without using uncertainty loss (similar to Point DETR <ref type="bibr" target="#b4">[5]</ref>), the performance drops substantially (-3 in AP, +3 in MAE). That justifies the effectiveness of our uncertainty loss. Without using both of them, the performance gets worst (-7 in AP, +11 in MAE). These results demonstrate the important contribution of our proposed pseudo GT box generation and uncertainty loss.</p><p>Types of anchor points. As described in Sec. 3.2, we follow the design of Anchor DETR whose anchor points can either be learnable or fixed-grid. The results of these two types are shown in <ref type="table">Table 4</ref>, we can see that the fixed-grid anchor points are comparable to the learnable anchor points on counting metrics, but better on the detection metrics. Thus, the fixed-grid anchor points are chosen for the Counting-DETR.</p><p>Numbers of anchor points. Tab. 5 presents the results with different numbers of anchor points M . Both the detection and counting results increase as the number of anchor points increases, and they reach the highest points when the number of anchor points is M = 600. Hence, we choose 600 anchor points for Counting-DETR.</p><p>Types for the uncertainty loss. Instead of using the Laplace distribution as described in Sec. 3.2, we use Gaussian distribution to derive the uncertainty loss: L Gaussian uncertainty = 1 2 o?{x,y,w,h}</p><formula xml:id="formula_5">(?o?? * o ) 2 ? 2 o + log ? 2 o .</formula><p>This loss is similar to <ref type="bibr" target="#b13">[14]</ref>. The results are shown in Tab. 6. The uncertainty loss derived from the Gaussian distribution yields the worst results among the variants, even worse than the variant without using any uncertainty loss. On the contrary, our proposed uncertainty loss derived from the Laplace distribution gives the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to Prior Work</head><p>Since there is no existing method for the new FSCD task, we compare Counting-DETR with several strong baselines adapted from few-shot object counting and few-shot object detection: FamNet <ref type="bibr" target="#b31">[32]</ref>+RR, FamNet <ref type="bibr" target="#b31">[32]</ref>+MLP, Attention-RPN <ref type="bibr" target="#b5">[6]</ref>+RR, and FSDetView <ref type="bibr" target="#b45">[45]</ref>+RR. Other few-shot object detectors are not chosen due to the unavailability of the source code or the requirement for fine-tuning  On FSCD-147, our method significantly outperforms others with a large margin for object detection. For counting, compared to a density-based approach like FamNet, Counting-DETR achieves worse results in RMSE metric but with much better results in other counting metrics MAE, NAE, and SRE. FamNet+MLP seems to overfit to the exemplar boxes so it performs the worst in detection.</p><p>On FSCD-LVIS, our method outperforms all others for both detection and counting tasks. This is because the image in FSCD-LVIS is much more complicated than those in FSCD-147, i.e., multiple object classes per image and significant differences in object size and shape. Also, the class of interest is usually packed and occluded by other classes, so the density map cannot be reliably predicted as shown in <ref type="figure">Fig. 6</ref>. More interestingly, we also evaluate the performance of Counting-DETR and other baselines on a special test set of unseen classes of the FSCD-LVIS dataset to show their generalizability to unseen classes during training in Tab. 9. It can be seen that our approach performs the best while the FamNet+RR performs the worst. <ref type="figure">Fig. 7</ref> shows the qualitative comparison between our approach and the other methods, including FSDetView <ref type="bibr" target="#b45">[45]</ref>, Attention-RPN <ref type="bibr" target="#b5">[6]</ref>, and FamNet <ref type="bibr" target="#b31">[32]</ref>+RR. Our method can successfully detect the objects of interest while other methods cannot, as shown in the first four rows of <ref type="figure">Fig. 7</ref>. The last row is a failure case for all methods, due to object truncation, perspective distortion, and scale variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exemplar boxes</head><p>FSDetView Attention RPN FamNet+RR Counting-DETR <ref type="figure">Fig. 7</ref>. Qualitative comparison. Each row shows an example with the exemplars (red) and GT bounding boxes (blue) on the first column. The first four rows show the superior performance of ours over the others, while the last row is a failure case. In the first row, our method can distinguish between the target class and other foreground classes, while other methods confuse between different foreground classes. In the next three rows, exemplar objects either share similar color with environment or contain many background pixels. These conditions lead to either under detect or over detect where other methods are unsure about if detected objects are foreground or background. The last row shows a failure case for all methods due to the huge variation in the scale, distortion, and truncation of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced a new task of few-shot object counting and detection that shares the same supervision with few-shot object counting but additionally predict object bounding boxes. To address this task, we have collected two new datasets, adopted a two-stage training strategy to generate pseudo bounding boxes for training, and developed a new uncertainty-aware few-shot object detector to adapt to the imperfection of pseudo label. Extensive experiments on the two datasets demonstrate that the proposed approach outperforms strong baselines adapted from few-shot object counting and few-shot object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Equal contribution arXiv:2207.10988v2 [cs.CV] 28 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>We address the task of few-shot counting and detection in a novel setting: (a) in training, each training image contains dot annotations for all objects and a few exemplar boxes. (b) In testing, given an image with a few exemplar boxes defining a target class, our goal is to count and detect all objects of that target class in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2. Limitations of a naive approach for FSCD by extending FamNet [32] with a regression function for object detection. (a) Processing pipeline of this approach: a regressor takes as input exemplar boxes with their features, and features at peak density locations to predict bounding boxes for the peak locations. (b) Limitation 1: poor quality of the density map predicted by FamNet when the exemplars share similar appearance with background or densely packed region. The first row presents the input images with a few exemplars each, the second row presents the corresponding density map predicted by FamNet. (c) Limitation 2: Non-discriminative peak features cannot represent objects with significant differences in shape and size. The green boxes are predicted from the features extracted at the annotated dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Examples of pseudo GT bounding boxes generated by the 1-st stage our method.(a) FSCD-147 (b) FSCD-LVIS Sample images from our datasets and annotated bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison between the FSCD-147 and FSCD-LVIS datasets Number of images for each bin of the FSCD-LVIS dataset</figDesc><table><row><cell>Number of images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on each component's contribution to the final results</figDesc><table><row><cell cols="2">Combination</cell><cell></cell><cell cols="2">Counting</cell><cell></cell><cell cols="2">Detection</cell></row><row><cell cols="8">Pseudo box Uncertainty MAE (?) RMSE(?) NAE(?) SRE (?) AP(?) AP50(?)</cell></row><row><cell>?</cell><cell>?</cell><cell>20.38</cell><cell>82.45</cell><cell>0.19</cell><cell cols="3">3.38 17.27 41.90</cell></row><row><cell>?</cell><cell>?</cell><cell>29.74</cell><cell>104.04</cell><cell>0.26</cell><cell>4.44</cell><cell>11.37</cell><cell>29.98</cell></row><row><cell>?</cell><cell>?</cell><cell>23.57</cell><cell>93.54</cell><cell>0.21</cell><cell>3.77</cell><cell>14.19</cell><cell>36.34</cell></row><row><cell>?</cell><cell>?</cell><cell>31.36</cell><cell>105.76</cell><cell>0.27</cell><cell>4.60</cell><cell>10.81</cell><cell>28.76</cell></row><row><cell cols="8">Table 4. Performance of Counting-DETR with different types of anchor points</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Counting</cell><cell></cell><cell cols="2">Detection</cell></row><row><cell>Anchor type</cell><cell></cell><cell cols="6">MAE (?) RMSE(?) NAE(?) SRE (?) AP(?) AP50(?)</cell></row><row><cell>Learnable</cell><cell></cell><cell>25.20</cell><cell>81.94</cell><cell>0.25</cell><cell>3.92</cell><cell>16.46</cell><cell>38.34</cell></row><row><cell cols="3">Fixed grid (proposed) 20.38</cell><cell>82.45</cell><cell>0.19</cell><cell cols="3">3.38 17.27 41.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance of Counting-DETR with different numbers of anchor points</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Counting</cell><cell></cell><cell cols="2">Detection</cell></row><row><cell cols="7"># anchor points MAE (?) RMSE(?) NAE(?) SRE (?) AP(?) AP50(?)</cell></row><row><cell>100</cell><cell>30.22</cell><cell>113.24</cell><cell>0.24</cell><cell>4.55</cell><cell>11.26</cell><cell>28.62</cell></row><row><cell>200</cell><cell>26.76</cell><cell>103.11</cell><cell>0.22</cell><cell>4.15</cell><cell>14.06</cell><cell>34.33</cell></row><row><cell>300</cell><cell>23.57</cell><cell>93.54</cell><cell>0.21</cell><cell>3.77</cell><cell>14.19</cell><cell>36.34</cell></row><row><cell>400</cell><cell>22.62</cell><cell>88.45</cell><cell>0.21</cell><cell>3.69</cell><cell>14.91</cell><cell>37.30</cell></row><row><cell>500</cell><cell>21.72</cell><cell>85.20</cell><cell>0.20</cell><cell>3.52</cell><cell>16.03</cell><cell>39.66</cell></row><row><cell>600</cell><cell>20.38</cell><cell>82.45</cell><cell>0.19</cell><cell cols="3">3.38 17.27 41.90</cell></row><row><cell>700</cell><cell>21.19</cell><cell>83.70</cell><cell>0.22</cell><cell>3.47</cell><cell>15.10</cell><cell>37.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation study for the uncertainty loss Results of FamNet on the FSCD-LVIS dataset. The objects of interest for Row 1 and 2 are bananas and chairs, respectively. FamNet fails to output good density maps for images containing objects with huge variations in shape and size. on the whole novel classes together (see Sec. 2). It is different from our setting, where each novel class is processed independently in a separate image. Fam-Net+RR is a method that uses Ridge Regression on top of the density map predicted by FamNet as depicted inFig. 2a. FamNet+MLP is similar to Fam-Net+RR but replaces the ridge regression with a two-layer MLP with the Layer norm. Attention-RPN and FSDetView are detection-based methods, which require GT bounding boxes for all objects to train, thus, we generate the pseudo GT bounding boxes using either (1) the FamNet+RR with the features extracted from the dot annotations of training images instead of peak locations (called RR box) or (2) our first stage of training as described in Sec. 3.3 (called pseudo box). Tab. 7 and Tab. 8 show the comparison on the test sets of FSCD-147 and FSCD-LVIS, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Counting</cell><cell></cell><cell cols="2">Detection</cell></row><row><cell>Distribution type</cell><cell cols="7">MAE (?) RMSE(?) NAE(?) SRE (?) AP(?) AP50(?)</cell></row><row><cell>W/o uncertainty loss</cell><cell></cell><cell>23.20</cell><cell>92.87</cell><cell>0.21</cell><cell>3.77</cell><cell>13.86</cell><cell>35.67</cell></row><row><cell>Gaussian loss</cell><cell></cell><cell>24.46</cell><cell>94.20</cell><cell>0.22</cell><cell>3.84</cell><cell>14.03</cell><cell>34.91</cell></row><row><cell cols="3">Laplacian loss (proposed) 20.38</cell><cell>82.45</cell><cell>0.19</cell><cell cols="3">3.38 17.27 41.90</cell></row><row><cell cols="7">Table 7. Comparison with strong baselines on the FSCD-147 test set</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Counting</cell><cell></cell><cell cols="2">Detection</cell></row><row><cell>Method</cell><cell></cell><cell cols="6">MAE (?) RMSE (?) NAE(?) SRE (?) AP(?) AP50(?)</cell></row><row><cell>FamNet [32]+RR</cell><cell></cell><cell>22.09</cell><cell>99.55</cell><cell>0.44</cell><cell>6.45</cell><cell cols="2">9.44 29.73</cell></row><row><cell>FamNet [32]+MLP</cell><cell></cell><cell>22.09</cell><cell>99.55</cell><cell>0.44</cell><cell>6.45</cell><cell>1.21</cell><cell>6.12</cell></row><row><cell cols="2">Attention-RPN [6]+RR box</cell><cell>32.70</cell><cell>141.07</cell><cell>0.38</cell><cell cols="3">5.27 18.53 35.87</cell></row><row><cell>FSDetView [45]+RR box</cell><cell></cell><cell>37.83</cell><cell>146.56</cell><cell>0.48</cell><cell cols="3">5.47 13.41 32.99</cell></row><row><cell cols="3">Attention-RPN [6]+pseudo box 32.42</cell><cell>141.55</cell><cell>0.38</cell><cell cols="3">5.25 20.97 37.19</cell></row><row><cell cols="2">FSDetView [45]+pseudo box</cell><cell>37.54</cell><cell>147.07</cell><cell>0.44</cell><cell cols="3">5.40 17.21 33.70</cell></row><row><cell cols="2">Counting-DETR (proposed)</cell><cell>16.79</cell><cell>123.56</cell><cell>0.19</cell><cell cols="3">5.23 22.66 50.57</cell></row><row><cell>Exemplar boxes</cell><cell cols="2">GT boxes</cell><cell cols="2">Predicted boxes</cell><cell></cell><cell cols="2">Predicted density map</cell></row><row><cell>Fig. 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Comparison with strong baselines on the FSCD-LVIS test set</figDesc><table><row><cell>Counting</cell><cell>Detection</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localization in the crowd with topological constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abousamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting overlapping instances in microscopy images using extremal region trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models in Biomedical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Medical Image Analysis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Points as queries: Weakly semisupervised object detection by points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 5</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 5</meeting>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized few-shot object detection without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Locality-constrained spatial transformer network for video crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07911</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Labels are not perfect: Improving probabilistic object detection via label uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04168</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Labels are not perfect: Inferring spatial uncertainty in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Drone-based object counting by spatially regularized regional proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ambient sound helps: Audiovisual crowd counting in extreme conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision</title>
		<meeting>the european conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning(ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Localization uncertainty estimation for anchor-free object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15607</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Density map regression guided detection network for rgb-d crowd counting and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05993</idno>
		<title level="m">Object counting: You only need to look at one</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV</title>
		<meeting>the IEEE international conference on computer vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting heads using feature refine net and cascaded multi-scale architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vicinal counting networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to count everything</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 3, 4</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 3, 4</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Uncertainty estimation and sample selection for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained crowd counting: New dataset and benchmark method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision (ICCV)</title>
		<meeting>the IEEE/CVF international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distribution matching for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nwpu-crowd: A large-scale benchmark for crowd counting and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Anchor detr: Query design for transformerbased detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07107</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta r-cnn : Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08959</idno>
		<title level="m">Iterative correlation-based feature refinement for few-shot counting</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Meta-detr: Few-shot object detection via unified image-level meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11731</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
