<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Dong</surname></persName>
						</author>
						<title level="a" type="main">S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion recognition in conversation (ERC) has attracted much attention in recent years for its necessity in widespread applications. Existing ERC methods mostly model the self and inter-speaker context separately, posing a major issue for lacking enough interaction between them. In this paper, we propose a novel Speaker and Position-Aware Graph neural network model for ERC (S+PAGE), which contains three stages to combine the benefits of both Transformer and relational graph convolution network (R-GCN) for better contextual modeling. Firstly, a two-stream conversational Transformer is presented to extract the coarse self and inter-speaker contextual features for each utterance. Then, a speaker and position-aware conversation graph is constructed, and we propose an enhanced R-GCN model, called PAG, to refine the coarse features guided by a relative positional encoding. Finally, both of the features from the former two stages are input into a conditional random field layer to model the emotion transfer. Extensive experiments demonstrate that our model achieves state-of-the-art performance on three ERC datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion recognition in conversation (ERC), which aims to identify the emotion of each utterance in a conversation, is a task arousing increasing interests in many fields. With the prevalence of social media and intelligent assistants, ERC has great potential applications in several areas, such as emotional chatbots, sentiment analysis of comments in social media and healthcare intelligence, for understanding emotions in the conversation with emotion dynamics and generating emotionally coherent responses. ERC remains a challenge. Both lexicon-based <ref type="bibr" target="#b29">(Wu, Chuang, and Lin 2006;</ref><ref type="bibr" target="#b23">Shaheen et al. 2014</ref>) and deep learning-based <ref type="bibr" target="#b4">(Colneri? and Dem?ar 2018)</ref> text emotion recognition methods that treat each utterance individually fail in this task as these works ignore some conversation-specific characteristics.</p><p>Empirical results show that conversational context plays an important role in the ERC task . As demonstrated in <ref type="figure">Figure 1</ref>, the third utterance by speaker A will be assigned a wrong emotion label if the history conversation information is blind to the model. In the past few years, recurrent neural network (RNN)-based solutions, such as CMN <ref type="bibr" target="#b9">(Hazarika et al. 2018b</ref>), ICON <ref type="bibr" target="#b8">(Hazarika et al. 2018a</ref>) and DialogueRNN , have Figure 1: A dialogue from IEMOPCAP, in which the emotion of the last utterance by speaker A will be wrongly classified if the dialogue context is not taken into consideration. dominated this field due to the sequential nature of conversational context. Nonetheless, they share some inherent limitations: 1) RNN model performs poorly in grasping distant contextual information; 2) RNN-based methods are not capable of handling large-scale multiparty conversations.</p><p>With the rise of graph neural network (GNN) <ref type="bibr" target="#b30">(Wu et al. 2020</ref>) in many natural language processing (NLP) tasks, researchers pay increasing attention to GNN-based ERC methods recently. Instead of modeling only sequential data recurrently in RNN, GNN is designed to capture all kinds of graph structure information via various aggregation algorithms. Existing GNN-based ERC methods, such as Di-alogueGCN <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref>, RGAT <ref type="bibr" target="#b10">(Ishiwatari et al. 2020</ref>) and DAG-ERC <ref type="bibr" target="#b26">(Shen et al. 2021)</ref>, which are the state of the art, have demonstrated the superiority of GNN in modeling conversational context. A directed graph is constructed on each dialogue in these methods, where the nodes denote the individual utterances, and the edges indicate relationships between utterances. However, we notice that these methods do not work well on modeling speaker-specific context, which is also important in the ERC task. For example, in <ref type="figure">Figure 1</ref> the third utterance spoken by speaker A is more influenced by speaker A's prior utterances rather than the second utterance spoken by speaker B, even though the latter is closer. Thus, in contextual modeling, we should consider both the emotional influence that speakers have on themselves during a conversation, i.e., self-speaker context, and context on the entire conversation flow, i.e., inter-speaker context, as well as the interaction between them.</p><p>On the other hand, none of the existing methods actually exploit the fine-grained temporal information, i.e., the relative distance between utterances, because the original graph aggregation algorithms do not propagate distance-related message. We surmise that the relative distance is helpful to grasp more temporal information, and thus can further improve the performance.</p><p>In this paper, we propose a novel Speaker and Position-Aware GNN model for ERC (S+PAGE) to settle the above drawbacks of exisiting methods. Our model contains three stages to fully consider both self and inter-speaker context. Specifically, given a sequence of utterances in the same dialogue, we first leverage a Two-Stream Conversational Transformer (TSCT) with the attentive masking mechanism to get both the same speaker's and the whole dialogue's contextual features. Then, guided by the speaker identity and utterance order, we construct a speaker and positionaware conversation graph. The Position-Aware GNN model (PAG), which is an enhanced relational graph convolution network (R-GCN), is utilized to refine the contextual features with self and inter-speaker dependency. Particularly, we introduce relational relative positional encoding in the aggregation algorithm, in order to make PAG capable of capturing fine-grained temporal information. Finally, the global transfer of emotion labels is modeled by a conditional random field (CRF) layer with the features from both TSCT and PAG. Experimental results demonstrate the superiority of our model compared with state-of-the-art models. Ablation study illustrates the effectiveness of the proposed Transformer and position-aware graph structure in modeling the conversational context. To conclude, our contributions are as follows:</p><p>? We propose a novel graph neural network-based ERC method, called S+PAGE, to incorporate all kinds of conversational context information in the model at the same time. ? We present a two-stream conversational Transformer architecture to extract both conversational and speakerspecific context-aware features, which is also capable of handling multiparty conversations on large scale with high efficiency. ? We apply a relational relative positional encoding on the graph neural network to capture fine-grained temporal information in a conversation. ? We conduct extensive experiments on several ERC datasets, which demonstrate that our proposed model can significantly promote the performance.</p><p>2 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Emotion Recognition in Conversation</head><p>Emotion recognition in conversation is a popular area in NLP. Many ERC datasets have been scripted and annotated in the past few years, such as IEMOCAP <ref type="bibr" target="#b2">(Busso et al. 2008)</ref>, MELD , DailyDialog <ref type="bibr" target="#b14">(Li et al. 2017)</ref>, EmotionLines <ref type="bibr" target="#b3">(Chen et al. 2018)</ref> and EmoryNLP <ref type="bibr" target="#b32">(Zahiri and Choi 2018)</ref>. IEMOCAP, MELD, and EmoryNLP are multimodal datasets, containing acoustic, visual and textual information, while the remaining two datasets are textual.</p><p>In recent years, ERC solutions are mostly deep learningbased models. CMN <ref type="bibr" target="#b9">(Hazarika et al. 2018b</ref>) and ICON <ref type="bibr" target="#b8">(Hazarika et al. 2018a</ref>) utilize gated recurrent unit (GRU) and memory networks to capture the dialogue dynamics. In IAAN <ref type="bibr" target="#b31">(Yeh, Lin, and Lee 2019)</ref> and <ref type="bibr">DialgueRNN (Majumder et al. 2019)</ref>, attention mechanisms are applied to interact between the party state and global state. With the rise of Transformer and graph neural networks in NLP tasks, many works have also introduce them into the ERC task. <ref type="bibr" target="#b33">Zhong, Wang, and Miao (2019)</ref> propose KET, which is a structure of hierarchical Transformers assisted by external commonsense knowledge. DialogueXL <ref type="bibr" target="#b25">(Shen et al. 2020)</ref> applies dialogue-aware self-attention to deal with the multiparty structures. In DialogueGCN <ref type="bibr" target="#b6">(Ghosal et al. 2019</ref>) and RGAT <ref type="bibr" target="#b10">(Ishiwatari et al. 2020)</ref>, GCN <ref type="bibr" target="#b11">(Kipf and Welling 2016)</ref> and GAT <ref type="bibr" target="#b28">(Veli?kovi? et al. 2017</ref>) are applied to refine the features with speaker dependencies and temporal information. DAG-ERC <ref type="bibr" target="#b26">(Shen et al. 2021</ref>) applies a directed acyclic graph for conversation representation and it achieves the state-of-the-art performance on multiple ERC datasets. <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref> first propose Transformer for machine translation task, whose success subsequently has been proved in various down-stream NLP tasks. Self-attention mechanisms endow Transformer with the ability of capturing longer-range dependency among elements of an input sequence than the RNN structure. Beltagy, Peters, and Cohan (2020) propose a novel self-attention mechanism for feature extraction of long documents. Pre-trained models such as BERT <ref type="bibr" target="#b5">(Devlin et al. 2018</ref>) and GPT <ref type="bibr" target="#b1">(Brown et al. 2020)</ref> use Transformer encoder and decoder respectively to learn representations on large-scale datasets. In our model, we present a modified Transformer structure to encode the contextual information in a conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Neural Network</head><p>Graph neural network has attracted a lot of attention in recent years, which learns a target node's representation by propagating neighbor information in the graph. <ref type="bibr" target="#b11">Kipf and Welling (2016)</ref> propose a simple and well-behaved layer-wise propagation rule for neural network models and demonstrate its effectiveness in semi-supervised classification tasks. Better aggregation methods for large graphs are proposed in GAT <ref type="bibr" target="#b28">(Veli?kovi? et al. 2017</ref>) and Graph-Sage <ref type="bibr" target="#b7">(Hamilton, Ying, and Leskovec 2017)</ref>. <ref type="bibr" target="#b22">Schlichtkrull et al. (2018)</ref> propose R-GCN to deal with the highly multirelational data characteristic by assigning different aggregation structures for each relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The framework of our model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We decompose the emotion classification procedure into three stages, i.e., contextual modeling, speaker dependency modeling, and global consistency modeling. In the first stage, we present a novel conversation-specific Transformer to get coarse contextual features as well as cues of speaker information. Then, a graph neural network is proposed to re-</p><formula xml:id="formula_0">! (#) % (#) &amp; (') ( (') ) (#) * (#) Graph Layer N " # $ % &amp; ' TSCT Layer TSCT Layer ? ? " ? # ? $ ? % ? &amp; ? ' ! (#) % (#) &amp; (#) ( (#) ) (#) * (#)</formula><p>Graph Layer 0 fine the features with speaker dependency and temporal information guided by relational relative positional features. Subsequently, we employ conditional random field to model the context of global emotion consistency, i.e., the emotion transfer. The interaction between self and inter-speaker context is involved in both of the former two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Modeling Speaker Dependency Modeling Consistency Modeling</head><formula xml:id="formula_1">% (') ? " ?? ?? ? * (') ? ' ? C R F Labels ?? ??</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>The ERC task is to predict emotion labels (e.g., Happy, Sad, Neutral, Angry, Excited, and Frustrated) for utterances u 1 ; u 2 ; ? ? ? ; u N , where N denotes the number of utterances in a conversation. Let S be the number of speakers in a given dataset. P is a mapping function, and p s = P (u i ) denotes utterance u i uttered by speaker p s , where s ? {1, ? ? ? , S}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Utterance Encoding</head><p>Following previous works <ref type="bibr" target="#b6">(Ghosal et al. 2019;</ref><ref type="bibr" target="#b16">Majumder et al. 2019)</ref>, we use a simple architecture consisting of a single convolutional layer followed by a max-pooling layer and a fully connected layer to extract context-independent textual features of each utterance. The input of this network is the 300 dimensional pre-trained 840B GloVe vectors (Pennington, Socher, and Manning 2014). We use the output features, denoted as u i , as the representation of each utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contextual Modeling</head><p>Two-Stream Conversational Transformer We present a Two-Stream Conversational Transformer (TSCT) to better extract the contextual representation of each utterance in a conversation, which is also capable of handling multi-party conversations efficiently. The structure of TSCT is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The collection of utterance representations U = u 1 ; u 2 ; ? ? ? ; u N is taken as the input. We design a multi-head self-attention mechanism, composed of two streams, i.e., the inter-speaker self-attention stream and the intra-speaker selfattention stream.</p><p>Inter-Speaker Self-Attention The inter-speaker selfattention is the same as the self-attention in vanilla Transformer, in which each utterance can attend to all positions in the dialogue as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). It is calculated as:</p><formula xml:id="formula_2">q l i , k l i , v l i = h l?1 i W l iq , h l?1 i W l ik , h l?1 i W l iv (1) f l i = sof tmax( q l i (k l i ) T ? d )v l i (2)</formula><p>where W l iq , W l ik and W l iv are three learnable weight matrices for attention head i at layer l.</p><p>Intra-Speaker Self-Attention The intra-speaker selfattention models speaker-specific contextual information by only computing attention on the same speaker's utterances in a dialogue. In this way, the model is able to capture the emotional influence that speakers have on themselves during the conversation. It is implemented by the attentive masking strategy as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(c) and formulated as:</p><formula xml:id="formula_3">z l i = sof tmax( q l i (k l i ) T ? d + m)v l i<label>(3)</label></formula><p>where m ? R N ?N is the attentive masking matrix. The elements of m are set as below:</p><formula xml:id="formula_4">m ij = ?? P (u i ) = P (u j ) 0 otherwise<label>(4)</label></formula><p>where P (?) is the function that maps the utterance and its corresponding speaker.</p><p>Each attention head i of the l-th layer in TSCT, denoted as head l i , is the concatenation of the f i and z i , and the output of the multi-head attention can be formulated as follows:</p><formula xml:id="formula_5">M ultiHead l i = M i=1 head l i<label>(5)</label></formula><p>where denotes concatenation operation. M is the number of attention heads, while 1 ? i ? M . Following the structure of the original Transformer, the output of the TSCT layer can be generated by passing M ultiHead l i through a normalization layer followed by a feed-forward network: </p><formula xml:id="formula_6">h l = LayerNorm(FeedForward(M ultiHead l i )) (6) TSCT Layer ! " # $ % ! (#) ! (#) % (#) % (#) &amp; (#) &amp; (#) ' (#) ' (#) ( (#) ( (#) ? ? ? ? ? ? ! (#) ? % (#) ? &amp; (#) ? ' (#) ? ( (#) Q K, V Inter-Speaker Attention ? ! (#) ? % (#) ? &amp; (#) ? ' (#) ? ( (#) ! (!) (a) (b) (c) Attention Mask Attention Mask ! (!) ! (!) % (!) % (!) &amp; (!) &amp; (!) ' (!) ' (!) ( (!) ( (!) Q K, V Intra-Speaker Attention ? ! (#) ? % (#) ? &amp; (#) ? ' (#) ? (<label>(#)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Speaker Dependency Modeling</head><p>After extracting the contextual features, we construct a conversation graph based on the speaker identity and the order of utterances. We design a novel position-aware graph neural network to capture both speaker dependency and temporal information in a dialogue by introducing relative positional encoding.</p><p>Graph Architecture We construct a directed graph, G = (V, E, R, W), for each dialogue with N utterances. The nodes in the graph are the utterances in the conversation, i.e., V = {u 1 ; u 2 ; ? ? ? , u N }. (v i , v j , r ij ) ? E denotes a labeled edge (relation), where r ij ? R is a relation type, defined according to speaker identity and relative distance. W represents the set of edge weights.</p><p>Nodes Feature vector g i of each node v i is initialized as the output of the TSCT layer, i.e., h i . Feature vector g i is modified by the aggregation algorithm through the stacked graphical layers in GNN. The output feature is described as g l i , where l denotes the number of layers.</p><p>Edges Instead of only focusing on past utterances, we take converse influence into account <ref type="bibr" target="#b6">(Ghosal et al. 2019</ref>). We construct edges E with a sliding window for each utterance. The window sizes p and f denote the number of past and future utterances from the target utterance. Each utterance node v i has an edge with p utterances of the past:</p><formula xml:id="formula_7">v i?1 , v i?2 , ..., v i?p , f utterances of the future: v i+1 , v i+2 , ..., v i+f , and itself.</formula><p>Edge Types The relation type r ? R is determined by the speaker identity and relative distance. The maximum relative distance is clipped to the window sizes. For example, if we set both future and past window sizes to n, there are n relative distances. Thus, assuming there are m distinct speakers in a dialogue, there should be N e = m * n relation types in the constructed graph G. Two utterances share the same edge type only if they are uttered by the same speaker  <ref type="figure">Figure 4</ref>: An example of dialogue graph construction. Different types of arrows denote different edge types. The table shows the relative distances for each utterance to the target utterance u 3 . and have equal distances from the target utterance. For example, in <ref type="figure">Figure 4</ref> u 1 and u 5 belong to the same edge type, while u 1 and u 6 belong to different edge types.</p><p>Edge Weights Edge weights are computed by an attention mechanism. The particular attentional setup in our model closely follows the work of GAT <ref type="bibr" target="#b28">(Veli?kovi? et al. 2017)</ref>. The input of the attention module is a set of node features, i.e., g = g 1 , g 2 , ..., g N ? R F , where F is the dimension of each node feature. Motivated by <ref type="bibr" target="#b24">(Shaw, Uszkoreit, and Vaswani 2018)</ref>, which shows that absolute positional encoding is not effective for the model to capture the information of relative word order, we inject relative positional encoding into the attention mechanism. Fully expanded out, the edge weights, denoted as ? ij ? W, computed by the positionaware graph attention mechanism can be described as:</p><formula xml:id="formula_8">? ij = exp L ij k?N i exp L ik (7) L ij = LeakyReLU a T [W g i (W g j + ? ij )]<label>(8)</label></formula><p>where W ? R F ?F is a weight matrix applied to every node, and F is the dimension of ? ij . N i is the number of nodes linked with node i. a ? R 2F is a parametrized weight vector. ? T represents transposition, and is the concatenation operation. ? ij denotes the relative position representation between utterance i and utterance j in a dialogue, which is encoded by a learnable embedding matrix E p :</p><formula xml:id="formula_9">? ij = E p (o(u j ) ? o(u i ))<label>(9)</label></formula><p>where o(?) is a mapping function between utterance and its absolute position in the dialogue sequence. Notice that we use a signed relative position instead of the relative distance in definition of edge type to keep the temporal order. Thus, we have E p ? R w * F , where w = p + f denotes the whole window size.</p><p>PAG We introduce a novel graph neural network to propagate utterance features, named Position-Aware Graph neural network (PAG). Inspired by R-GCN <ref type="bibr" target="#b22">(Schlichtkrull et al. 2018)</ref>, we define the following aggregation algorithm to calculate the forward-pass update of a node in the graph:</p><formula xml:id="formula_10">g l i = ? ? ? r?R j?N r i ? ij c i,r W l r g l?1 i + ? ii W l o g l?1 i ? ? (10)</formula><p>where g l i is the hidden state of node i in the l-th layer. N r i denotes the set of neighbors of utterance i under the edge type r ? R. c i,r is a normalization constant, and we set c i,r = |N r i | in our experiment. W l r and W l o are learnable weight matrices, and ?(?) is an activation function. Different from R-GCN, we use edge weights calculated by Equation 7 to involve fine-grained temporal information in a conversation.</p><p>Furthermore, between the stacked layers of PAG, we employ a fusion function, which is formulated as:</p><formula xml:id="formula_11">g l i = F use(g l i , g l?1 i )<label>(11)</label></formula><p>where l ? 1. The fusion function is designed as a gated sum of two features:</p><formula xml:id="formula_12">F use(a, b) = z * a + (1 ? z) * b (12) z = sigmoid (W z [a; b; a * b; a ? b] + b z )<label>(13)</label></formula><p>where a and b denote the feature vectors, and W z is a trainable weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Consistency Modeling</head><p>Conditional Random Field To model the label consistency, i.e., the emotion transfer on a dialogue, a linear chain conditional random field is employed to yield final emotion tags of each utterance. Following the description of <ref type="bibr" target="#b12">(Lample et al. 2016)</ref>, for an input set of utterances U = u 1 , u 2 , ..., u n and a sequence of tag predictions y = y 1 , y 2 , .., y n , y i ? 1, ? ? ? , K (K is the number of emotion tags), the score of the sequence is defined as:</p><formula xml:id="formula_13">s(U, y) = n i=0 T yi,yi+1 + n i=1 Q i,yi<label>(14)</label></formula><p>where T ? R K?K is the trainable transition matrix of the labels, and Q ? R n?K denotes the emmision matrix. In our model, P is computed by the concatenation of the last two modules' output features, following a linear layer and a softmax function. The model is trained to maximize the log-probability of the correct tag sequence:</p><formula xml:id="formula_14">log(p(y | U)) = s(U, y) ? log ? ? ??Y e s(U,?) ? ? (15)</formula><p>where Y is the set of all possible tag sequences. In the evaluation procedure, Equation 15 is computed by the Viterbi algorithm <ref type="bibr" target="#b21">(Rabiner 1989)</ref> to get the maximumprobability label sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the datasets, baselines, metrics and experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our S+PAGE model on four benchmark datasets -IEMOCAP, MELD, DailyDialog and EmoryNLP. For this work, we only consider emotion recognition based on textual features. The statistic of them is shown in <ref type="table">Table 1</ref>.</p><p>IEMOCAP <ref type="bibr" target="#b2">(Busso et al. 2008</ref>) is a audiovisual dataset. It consists of dyadic conversations where actors perform improvisations or scripted scenarios. Each conversation is segmented into utterances, which are annotated with one of the six emotion labels, which are happy, sad, neutral, angry, excited, and frustrated.</p><p>DailyDialogue <ref type="bibr" target="#b14">(Li et al. 2017</ref>) is a human-written multiturn dyadic dialogue dataset, reflecting our daily communication way and covering various topics about our daily life. Emotion labels in it contain neutral, happiness, surprise, sadness, anger, disgust, and fear.</p><p>MELD ) is a multi-modal emotion classification dataset. It is a multi-party dialogue dataset created from scripts of the Friends TV series. Each utterance is annotated as one of the seven emotion classes: anger, disgust, sadness, joy, surprise, fear or neutral. <ref type="table" target="#tab_1">Train  Val  Test  Train  Val  Test  IEMOCAP  120  31  5810  1623  MELD  1039  114  280  9989 1109 2610  DailyDialog 11118 1000 1000 87170 8069 7740  EmoryNLP  713  99  85  9934 1344 1328   Table 1</ref>: The statistics of four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Conversations # Uterrances</head><p>EmoryNLP <ref type="bibr" target="#b32">(Zahiri and Choi 2018)</ref> is also collected from Friends TV series, but it is different from MELD in the choice of scenes and emotion labels. The emotion labels include neutral, sad, mad, scared, powerful, peaceful, and joyful.</p><p>For the evaluation metrics, we choose micro-averaged F1 for DailyDialog and weighted-average F1 for the other datasets, following previous works <ref type="bibr" target="#b10">(Ishiwatari et al. 2020;</ref><ref type="bibr" target="#b26">Shen et al. 2021;</ref><ref type="bibr" target="#b33">Zhong, Wang, and Miao 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>For a comprehensive performance evaluation, We compare S+PAGE with the following baselines:</p><p>CNN: A single layer convolutional neural network model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-based methods:</head><p>CNN+cLSTM <ref type="bibr" target="#b18">(Poria et al. 2017)</ref>, DialogueRNN  KET <ref type="bibr" target="#b33">(Zhong, Wang, and Miao 2019)</ref>: A transformer structure with hierarchical self-attention and external commonsense knowledge.</p><p>GNN-based methods: DialogurGCN <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref>, RGAT <ref type="bibr" target="#b10">(Ishiwatari et al. 2020</ref>) and DAG-ERC <ref type="bibr" target="#b26">(Shen et al. 2021</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>We set the initial learning rate of 1e-4 in the Transformer layers, 2e-3 in the PAG layers and 2e-2 in the CRF layer. AdamW optimizer is used under a scheduled learning rate following <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref>. The number of dimensions of the utterance representations and contextual embeddings is set to 300. We set the dropout rate and number of attention head in TSCT to be 0.1 and 8 repectively. 3-head attention is used during calculating the edge weights. We also conduct experiments with different window sizes and PAG layers. We choose the hyper-parameters that achieve the best score on each dataset by using development data. The training and testing process is run on a single Tesla P100 GPU with 32G memory. The reported results of our implemented models are all based on the average score of 5 random runs on the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head><p>We compare our model with the baseline methods, and the results are reported in <ref type="table" target="#tab_1">Table 2</ref>. We can note that our proposed S+PAGE has competitive results across the four datasets and  There are three main advantages that contribute to our performance: 1) contextual modeling with both self and inter-speaker dependency, 2) the presence of relative positional encoding in GNN, 3) consistency modeling of global emotion transfer.</p><p>We find that the improvement on MELD and EmoryNLP is not significant. These two datasets are constructed based on Friends TV series, which involve plenty of commonsense knowledge. Thus, pre-trained and knowledgeenhanced models, such as RGAT, DAG-ERC, KET have good results. However, our model still achieves competitive performance compared to these models as shown in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>For better understanding the contribution of each component in our proposed model, we conduct experiments by replacing TSCT with the vanilla Transformer, and removing PAG and CRF from our model respectively. The results on IEMO-CAP and MELD are shown in table 3. We can observe that when TSCT is removed, the weighted F1 score drop more on MELD than that on IEMOCAP. This shows the superiority of TSCT on contextual feature extraction of multi-party conversations, as there are more speakers in dialogues of MELD. Removal of PAG leads to significant drop on both datasets, which implies the importance of PAG to refine the contextual features with speaker dependency and temporal information. Meanwhile, after removing CRF layer, we observe the performance degradation. It implies that the modeling of label consistency is essential in the ERC task. To sum up, all of the three components contribute to the performance improvement of S+PAGE.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">PAG Model Depth</head><p>We further explore the relationship between model performance and the depth of PAG. Stacking too many layers of GNN leads to performance degradation due to the oversmoothing problems <ref type="bibr" target="#b13">(Li, Han, and Wu 2018)</ref>. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we set different number of layers of PAG on IEMOCAP, and compare the performance with Diaglog-GCN and DAG-ERC. As can be noted from <ref type="figure" target="#fig_3">Figure 5</ref>, all the models suffer from performance degradation when the model depth grows. However, the curve of PAG's F1 score descends more slowly than the other methods, which indicates the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Whether PAG outperforms other graph structures?</head><p>We conduct experiments on IEMOCAP by replacing PAG with the graph structures in DialogueGCN, RGAT and DAG-ERC respectively. As shown in <ref type="table" target="#tab_5">Table 4</ref>, S+PAGE still outperforms the other methods significantly. Notice that both DialogueGCN and RGAT with our contextual and consistency modeling perform better than their original versions. It proves the effectiveness of our none-graph parts. In addition, the performance declines when PAG is replaced by the DAG in DAG-ERC. We think the main reason is that we do not use RoBERTa <ref type="bibr" target="#b15">(Liu et al. 2019)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Window Size</head><p>We analyze the influence of past and future window sizes by conducting experiments with window size w of (4, 4), (6, 6), (8, 8), (10, 10), <ref type="bibr">(20,</ref><ref type="bibr">20)</ref>, (30, 30), (40, 40) on IEMOCAP dataset. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the F1 score of S+PAGE, RGAT and DialogueGCN significantly increase, when the window sizes expand from 4 to 10. The reason is that useful contextual information keeps growing with the increasing of w. However, after window sizes exceed 20, the F1 score drops for both DialogueGCN and RGAT. The reason is that the amount of useless long-range dependency increases when the window size continuously grows, which hinders the models from efficiently capturing crucial context. In contrast, the performance of S+PAGE fluctuates in a relatively narrow range, which shows the robustness of our model on varied window sizes. We can infer that the relative positional encoding endows capability of distinguishing critical contextual information to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel graph neural networkbased model, named S+PAGE, for emotion recognition in conversation (ERC). Specifically, S+PAGE contains three parts, i.e., contextual modeling, speaker dependency modeling, consistency modeling, to incorporate all kinds of contextual information. We present a new Transformer structure with two-stream attention mechanism to better capture the self and inter-speaker conversational context. In speaker dependency modeling, we introduce a novel GNN model, named PAG, to get fine-grained temporal information guided by the relative positional encoding. Furthermore, we use a CRF layer to model emotion transfer in the consistency modeling part. Experiments demonstrate that our model achieves competitive results on several ERC benchmarks. The effectiveness of the proposed two-stream conversational Transformer and position-aware graph neural network is also proved by extensive ablation study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The framework of S+PAGE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Structure of the two-stream conversational Transformer, (b) Inter-speaker self-attention, (c) Intra-speaker selfattention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Results of varying depths of GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Results of varying window sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall performance on the four datasets.</figDesc><table><row><cell>achieves a new state-of-the-art performance on the IEMO-</cell></row><row><cell>CAP, DailyDialog and EmoryNLP datasets.</cell></row><row><cell>As shown in the table, all GNN-based models outper-</cell></row><row><cell>form RNN-based models. Compared with existing GNN-</cell></row><row><cell>based models, our model even has considerable improve-</cell></row><row><cell>ment.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of ablation study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of replacing PAG with other graph structures.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emotionlines: An emotion corpus of multi-party conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08379</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition on twitter: Comparative study and training a unison model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Colneri?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dem?ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="446" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11540</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Icon: Interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics</title>
		<meeting>the conference. Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishiwatari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7360" to="7370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<title level="m">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dialoguernn: An attentive rnn for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
	<note>Long papers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Meld: A multimodal multiparty dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotion recognition from text based on automatically generated rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>El-Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbassuoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Selfattention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Di-alogXL: All-in-one XLNet for multi-party conversation emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08695</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Directed Acyclic Graph Network for Conversational Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12907</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emotion recognition from text using semantic labels and separable mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on Asian language information processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<title level="m">A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An interactionaware attention network for speech emotion recognition in spoken dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6685" to="6689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotion detection on tv show transcripts with sequence-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the thirty-second aaai conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Knowledgeenriched transformer for emotion detection in textual conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
