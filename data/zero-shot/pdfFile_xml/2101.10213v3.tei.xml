<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Trigger-Sense Memory Flow Framework for Joint Entity and Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>April 19-23, 2021. April 19-23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
							<email>maxinyin@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yechun</forename><surname>Tang</surname></persName>
							<email>tangyechun@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Trigger-Sense Memory Flow Framework for Joint Entity and Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Slovenia ? 2021 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. Proceedings of the Web Conference 2021 (WWW &apos;21)</title>
						<meeting> <address><addrLine>Ljubljana; Ljubljana, Slovenia; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">12</biblScope>
							<date type="published">April 19-23, 2021. April 19-23, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449895</idno>
					<note>ACM Reference Format: Yongliang Shen, Xinyin Ma, Yechun Tang, and Weiming Lu. 2021. A Trigger-Sense Memory Flow Framework for Joint Entity and Relation Extraction. * Corresponding author This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. In</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS ? Computing methodologies ? Information extraction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint entity and relation extraction framework constructs a unified model to perform entity recognition and relation extraction simultaneously, which can exploit the dependency between the two tasks to mitigate the error propagation problem suffered by the pipeline model. Current efforts on joint entity and relation extraction focus on enhancing the interaction between entity recognition and relation extraction through parameter sharing, joint decoding, or other ad-hoc tricks (e.g., modeled as a semi-Markov decision process, cast as a multi-round reading comprehension task). However, there are still two issues on the table. First, the interaction utilized by most methods is still weak and uni-directional, which is unable to model the mutual dependency between the two tasks. Second, relation triggers are ignored by most methods, which can help explain why humans would extract a relation in the sentence. They're essential for relation extraction but overlooked. To this end, we present a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and relation extraction. We build a memory module to remember category representations learned in entity recognition and relation extraction tasks. And based on it, we design a multi-level memory flow attention mechanism to enhance the bi-directional interaction between entity recognition and relation extraction. Moreover, without any human annotations, our model can enhance relation trigger information in a sentence through a trigger sensor module, which improves the model performance and makes model predictions with better interpretation. Experiment results show that our proposed framework achieves state-of-the-art results by improves the relation F1 to 52.44% (+3.2%) on SciERC, 66.49% (+4.9%) on ACE05, 72.35% (+0.6%) on CoNLL04 and 80.66% (+2.3%) on ADE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Entity recognition and relation extraction aim to extract structured knowledge from unstructured text and hold a critical role in information extraction and knowledge base construction. For example, given the following text: Ruby shot Oswald to death with the 0.38caliber Colt Cobra revolver in the basement of Dallas City Jail on Nov. <ref type="bibr" target="#b23">24,</ref><ref type="bibr">1963</ref>, two days after President Kennedy was assassinated., the goal is to recognize entities about People, Location and extract relations about Kill, Located in held between recognized entities. There are two things of interest to humans when carrying out this task. First, potential constraints between the relation type and the entity type, e.g., the head and tail entities of the Kill are of People type, and the tail entity of the Located in is of Location type. Second, triggers for relations, e.g. with words shot and death, the fact (Ruby, Kill, Oswald) can be easily extracted from the above example.</p><p>Current entity recognition and relation extraction methods fall into two categories: pipeline methods and joint methods. Pipeline methods label entities in a sentence through an entity recognition model and then predict the relation between them through a relation extraction model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>. Although it is flexible to build pipeline methods, there are two common issues with these methods. First, they are more susceptible to error prorogation wherein prediction errors from entity recognition can affect relation extraction. Second, they lack effective interaction between entity recognition and relation extraction, ignoring the intrinsic connection and dependency between the two tasks. To address these issues, many joint entity and relation extraction methods are proposed and have achieved superior performance than traditional pipeline methods. In these methods, an entity recognition model and a relation extraction model are unified through different strategies, including constraintbased joint decoding <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>, parameter sharing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>, cast as a reading comprehension task <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41]</ref> or hierarchical reinforcement learning <ref type="bibr" target="#b31">[32]</ref>. Current joint extraction models have made great progress, but the following issues still remain:</p><p>(1) Trigger information is underutilized in entity recognition and relation extraction. Before neural information extraction models, rule-based entity recognition and relation extraction framework were widely used. They were devoted to mine hard template-based rules or soft feature-based rules from text and match them with instances <ref type="bibr">[1-3, 13, 17, 19, 28]</ref>.</p><p>Such methods provide good explanations for the extraction work, but the formulation of rules requires domain expert knowledge or automatic discovery from a large corpus, suffering from tedious data processing and incomplete rule coverage. End-to-end neural network methods have made great progress in the field of information extraction in recent years. To exploit the rules, many works have begun to combine traditional rule-based methods by introducing a neural matching module <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref>. However, these methods still need to formulate seed rules or label seed relation triggers manually, and iteratively expand them. <ref type="bibr" target="#b1">(2)</ref> The interaction between entity recognition and relation extraction is insufficient and uni-directional. Entity recognition and relation extraction tasks are supposed to be mutually beneficial, but joint extraction methods do not take full advantage of dependency between the two tasks. Most joint extraction models are based on parameter sharing, where different task modules share input features or internal hidden layer states. However, these methods usually use independent decoding algorithms, resulting in a weak interaction between the entity recognition module and the relation extraction module. The joint decoding-based extraction model strengthens the interaction between modules, but it requires a trade-off between the richness of features for different tasks and joint decoding accuracy. Other joint extraction methods, such as modeling the task as a reading comprehension problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41]</ref> or a semi-Markov process <ref type="bibr" target="#b31">[32]</ref>, still suffer from a lack of bi-directional interaction due to the sequential order of subtasks. More specifically, if relation extraction follows entity recognition, the entity classification task will ignore the solution of the relation classification task. (3) There is no distinction between the syntactic and semantic importance of words in a sentence. We note that some words have a significant syntactic role but contribute little to the semantics of a sentence, such as prepositions and conjunctions. While some words are just the opposite, they contribute significantly to the semantics, such as nouns and notional verbs. When encoding context, most methods are too simple to inject syntactic features into the word vector, ignoring the fact that words differ in their semantic and syntactic importance. For example, some methods concatenate part of speech tags of words onto their semantic vectors via an embedding layer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>. Other methods combine the word, lexical, and entity class features of the nodes on the shortest entity path in the dependency tree to get the final features, which are then concatenated onto the semantic vector <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. These methods do not distinguish the two roles of a word for sentence semantics and syntax, but rather treat both roles of all words as equally important.</p><p>In this paper, we propose a novel framework for joint entity and relation extraction to address the issues mentioned above. First, our model makes full use of relation triggers, which can indicate a specific type of relation. Without any relation trigger annotations, our model can extract relation triggers in a sentence and provide them as an explanation for model predictions. Second, to enhance the bi-directional interaction between entity recognition and relation extraction tasks, we design a Memory Flow Attention module. It stores the already learned entity category and relation category representations in memory. Then we adopt a memory flow attention mechanism to compute memory-aware sentence encoding, and make the two subtasks mutually boosted by enhancing task-related information of a sentence. The Memory Flow Attention module can easily be extended to multiple language levels, enabling the interaction between the two subtasks at both subword-level and word-level. Finally, we distinguish the syntactic and semantic importance of a word in a sentence and propose a node-wise Graph Weighted Fusion module to dynamically fuse the syntactic and semantic information of words.</p><p>Our main contributions are as follow:</p><p>? Considering the relation triggers, we propose the Trigger Sensor module, which implicitly extracts the relation triggers from a sentence and then aggregates the information of triggers into span-pair representation. Thus, it can improve the model performance and strengthens the model interpretability. ? To model the mutual dependency between entity recognition and relation extraction, we propose the Multi-level Memory Flow Attention module. This module constructs entity memory and relation memory to preserve the learned representations of entity and relation categories. Through the memory flow attention mechanism, it enables the bi-directional interaction between entity recognition and relation extraction tasks at multiple language levels. ? Since the importance of semantic and syntactic roles that words play in a sentence are different, we propose a nodewise Graph Weighted Fusion module to dynamically fuse semantic and syntactic information. ? Experiments show that our model achieves state-of-the-art performance consistently on the SciERC, ACE05, CoNLL04, and ADE datasets, and outperforms several competing baseline models on relation F1 score by 3.2% on SciERC, 4.9% on ACE05, 0.6% on CoNLL04 and 2.3% on ADE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Rule-based Relation Extraction</head><p>Traditional relation extraction methods utilize template-based rules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28]</ref>, which are first formulated by domain experts or automatically generated from a large corpus based on statistical methods. Then, they apply hard matching to extract the corresponding relation facts corresponding to the rules. Later on, some works change the template-based rules to feature-based rules (such as TF-IDF, CBOW) and extract relations by soft matching <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40]</ref>, but still could not avoid mining the rule features from a large corpus using statistical methods. In short, rule-based relation extraction models typically suffer from a number of disadvantages, including tedious efforts on the rule formulation, a lack of extensibility, and low accuracy due to incomplete rule coverage, but they can provide a new idea for neural relation extraction systems. Some recent efforts on neural extraction systems attempt to focus on rules or natural language explanations <ref type="bibr" target="#b34">[35]</ref>. NERO <ref type="bibr" target="#b42">[43]</ref> explicitly exploits labeling rules over unmatched sentences as supervision for training RE models. It consists of a sentence-level relation classifier and a soft rule matcher. The former learns the neural representations of sentences and classifies which relation it talks about. The latter is a learnable module that produces matching scores for unmatched sentences with collected rules. NERO labels sentences according to predefined rules, and makes full use of information from unmatched instances. However, it is still a tedious process to formulate seed rules manually. And the quality of rule-making affects the performance of the entire system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Entity and Relation Extraction</head><p>Previous entity and relation extraction models are pipelined <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>. In these methods, an entity recognition model first recognizes entities of interest, and a relation extraction model then predicts the relation type between the recognized entities. Although pipeline models have the flexibility of integrating different model structures and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. They fall into two main categories: parameter sharing and joint decoding methods.</p><p>Most methods jointly model the two tasks through parameter sharing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42]</ref>. They unite entity recognition and relation extraction modules by sharing input features or internal hidden layer states. Specifically, these methods use the same encoder to provide sentence encoding for both the entity recognition module and the relation extraction module. Some methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> perform entity recognition first and then pair entities of interest for relation classification. While other methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref> are the opposite, they predict possible relations first and then recognize the entities in the sentence. DygIE <ref type="bibr" target="#b26">[27]</ref> constructs a span-graph and uses message propagation methods to enhance interaction between entity recognition and relation extraction. HRL <ref type="bibr" target="#b31">[32]</ref> models the joint extraction problem as a semi-Markov decision process, and uses hierarchical reinforcement learning to extract entities and relations. CASREL <ref type="bibr" target="#b35">[36]</ref> considers the general relation classification as a tagging task. Each relation corresponds to a tagger that recognizes the tail entities based on a head entity and context. CopyMTL <ref type="bibr" target="#b38">[39]</ref> casts the extraction task as a generation task and proposes an encoder-decoder model with a copy mechanism to extract relation tuples with overlapping entities. Although entity recognition and relation extraction modules can adopt different structures in these methods, their independent decoding algorithms result in insufficient interaction between the two modules. Furthermore, subtasks are performed sequentially in these methods, so the interaction between two tasks is uni-directional.</p><p>To enhance the bi-directional interaction between entity recognition and relation extraction tasks, some joint decoding algorithms have been proposed. <ref type="bibr" target="#b36">[37]</ref> proposes to use integer linear planning to enforce constraints on the prediction results of the entity and relation models. <ref type="bibr" target="#b20">[21]</ref> uses conditional random fields for both entity and relation models and obtains the output results of the entity and relation by the Viterbi decoding algorithm. Although the joint decoding-based extraction model strengthens the interaction between two modules, it still requires a trade-off between the richness of features required for different tasks and the accuracy of joint decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRIGGER-SENSE MEMORY FLOW FRAMEWORK 3.1 Framework Overview</head><p>In this section, we will introduce the Trigger-Sense Memory Flow Framework (TriMF) for joint entity and relation extraction, which consists of five main modules: Memory module, Multi-Level Memory Flow Attention module, Syntactic-Semantic Graph Weighted Fusion module, Trigger Sensor module, and Memory-Aware Classifier module.</p><p>The overall architecture of the TriMF is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. We first initialize the Memory, including an Entity Memory M E ? R ?? and a Relation Memory M R ? R ?? , where and denote the number of entity categories and relation categories, ? and ? denote the slot size of entity memory and the relation memory.  Our model performs a four-level sentence encoding (subword, word, span, and span-pair, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>) and two-step classification (entity classification and relation classification). More specifically, a sentence is encoded by BERT <ref type="bibr" target="#b9">[10]</ref> to obtain subword sequence encoding E = R ?? , where denotes the number of subwords in the sentence, and ? denotes the hidden state size of BERT. Based on M R , M E and E , we perform the first Memory Flow Attention at the subword-level. Then we use to aggregate the subword sequence encoding into a word sequence encoding E = R ?? , where denotes the number of words in the sentence, and ? denotes the size of the word vector. Here for , we adopt the max-pooling function. Based on M R , M E and E , we perform the second Memory Flow Attention at the word-level. After that, the word sequence encoding is fed into the Syntactic-Semantic Graph Weighted Fusion module to fuse semantic and syntactic information at the word-level. Then, we combine the word sequence encodings by to obtain the span sequence encodings E = R ?? , where denotes the number of spans in the sentence, and ? denotes the size of the span vector. Here for , we adopt a method of concatenating a span-size embedding on max-pooled word embeddings. We filter out the spans which are classified as the None category by a Memory-Aware Entity Classifier. After pairing the spans of interest, We compute local-context representation and fullcontextual span-pair specific trigger representation g using the Trigger Sensor. We combine the encodings of the head span, tail span, and g to obtain the encoding E ? R ?? , BERT ... , Harvey Oswald, who had just been shot by Jack Ruby, ... where E ( ) denotes the span pair encoding consisting of the ? and ? spans, denotes the number of candidate span pairs, and ? denotes the size of the span pair encoding. Lastly, we input the candidate span-pair representation to the Memory-Aware Relation Classifier and predict the relation type between the two spans. In the next sections, we'll cover five main modules of our model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subword-level Memory Flow Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-level Memory Flow Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic-Semantic Graph Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory-Aware Entity Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory</head><p>Memory holds category representations learned from historical training examples, consist of entity memory and relation memory. Each slot of these two memories indicates an entity category and a relation category respectively. The category representation is held in the corresponding memory slot, which can be used by the Memory Flow Attention module to enhance information related to the tasks in a sentence, or by the Trigger Sensor module to sense triggers.</p><p>In the Memory module, we define two types of processes, Memory Read Process and Memory Write Process, to manipulate the memory. Memory Read Process Given an input E and our memory M, we define two processes to read memory: normal read process and inverse read process. The normal read process takes the input as query, the memory as key and value. First, we calculate the attention weights of the input E on the memory M by bilinear similarity function, and then we weigh the memory by the weights.</p><formula xml:id="formula_0">A (E, M) = softmax EWM (1) Read (E, M) = A (E, M) M<label>(2)</label></formula><p>where W is a learnable parameter for the bilinear attention mechanism. While the inverse read process takes the memory as query, the input as key and value. We first compute 2d-attention weight matrix through bilinear similarity function, and then sum the 2dattention weight matrix on the memory-slot dimension to obtain a 1d-attention weight vector on the input E. The more relevant element in input with the memory has a larger weight. We then multiply the 1d-attention weight vector with E to get a memoryaware sequence encoding:</p><formula xml:id="formula_1">A (E, M) = |M | ?? =1 softmax M WE (3) Read (E, M) = A (E, M) E (4)</formula><p>where W is a learnable parameter for the bilinear attention mechanism and |M| denotes the number of slots in the memory M. Memory Write Process We write entity memory using gradients of entity classification losses and write relation memory using gradients of relation classification losses. If the gradient of the current instance's classification loss is large, it means that the classified instance (span or span-pair) representation is far away from the corresponding memory slot (entity or relation category representation of ground truth) while closer to the memory slots of the other categories, and we need to assign a large weight to this instance when writing it into memory. This makes the representations of the categories stored in memory more accurate. The write process for entity memory and relation memory is described below:</p><formula xml:id="formula_2">M E = M E ? E W L (5) M R = M R ? E ( ) W L (6) = ( = ) 1 ? ( = )<label>(7)</label></formula><formula xml:id="formula_3">= ( = ) 1 ? ( = )<label>(8)</label></formula><p>where L and L denote entity classification loss and relation classification loss, denotes the learning rate, W and W are two weight matrices, ( = ) denotes the probability of span belonging to entity type , ( = ) denotes the probability of span-pair's relation belonging to relation type , and E , E denote candidate span and span-pair encoding, respectively. The above symbols are specifically defined in defined at Sec.3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-level Memory Flow Attention</head><p>We perform a memory flow attention mechanism between the memory and the input sequence to enhance task-relevant information, such as entity surface names and trigger words. Entity memory and relation memory can enhance entity-related and relation-related information in the input instance for the two tasks respectively, thus they can help to strengthen bi-directional interaction between tasks. Memory Flow Attention In order to enhance the task-relevant information in a sentence, we designed the Memory Flow Attention based on the Memory. Given a memory M and a sequence encoding E, We calculate the memory-aware sequence encoding by runing memory inverse read process:</p><formula xml:id="formula_4">MFA (E, M) = Read (E, M)<label>(9)</label></formula><p>A single memory flow can be extended to multiple memory flows. We consider two types in our work: relation memory flow and entity memory flow. So we design a Multi-Memory Flow Attention mechanism, which is calculated as follows:</p><formula xml:id="formula_5">MFA (E, M R , M E ) = mean MFA E, M R , MFA E, M E (10)</formula><p>where M E and M R denote entity and relation memory respectively. we know that languages are hierarchical, and different levels represent semantic information at different levels of granularity. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, we extend the multi-memory flow attention mechanism to multiple levels ( subword-level and word-level ), and design a Multi-Level Multi-Memory Flow Attention mechanism: </p><formula xml:id="formula_6">E = MFA (E , M , M ) (11) E = E (12) E = MFA (E , M , M )<label>(13)</label></formula><p>where E and E denote memory-aware sequence encoding at subword-level and word-level respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Syntactic-Semantic Graph Weighted Fusion</head><p>The semantic information and syntactic structure of a sentence are important for both entity recognition and relation extraction. We consider both by constructing semantic and syntactic graphs from a sentence, with nodes in the graph refer to words in the sentence. We update a node representation based on its neighbor nodes' representations and the graph structure in the two graphs. We note that some words have a significant syntactic role but contribute little to the semantics of a sentence, such as prepositions and conjunctions. While some words are just the opposite, they contribute significantly to the semantics, such as nouns and notional verbs. Therefore, we need to fuse syntactic and semantic graphs based on the relative importance of the syntactic role and semantic role. First, the nodes in the two graphs are initialized as:</p><formula xml:id="formula_7">H (0) = E<label>(14)</label></formula><p>Syntactic Graph We construct a directed syntactic graph from a sentence based on dependency parsing, with the word as a node and the dependency between words as an edge. We then use the R-GCN <ref type="bibr" target="#b30">[31]</ref> to update node representations. The node representations of the syntactic graph H ( ) in ? layer are calculated as:</p><formula xml:id="formula_8">H ( ) = ?? ?R ?? ?N 1 , W ( ) H ( ) + W ( ) 0 H ( )<label>(15)</label></formula><p>where W ( ) and W ( ) 0 denote two learnable weight matrices, and N denotes the set of neighbor indices of node under relation ? R . Semantic Graph We compute the dense adjacency matrix based on semantic similarity and randomly sample from the fully connected graph to construct the semantic graph:</p><formula xml:id="formula_9">= LeakyReLU WH ( ) LeakyReLU WH ( )<label>(16)</label></formula><p>where W denotes a trainable weight matrix. Then we compute a weighted average for aggregation of neighbor nodes N ( ), where the weights come from the normalized adjacency matrices . We update the node representations of semantic graph H ( ) in ? layer, which are calculated as follows:</p><formula xml:id="formula_10">= softmax ( )<label>(17)</label></formula><formula xml:id="formula_11">H ( ) = , WH ( ) + ?? ?N ( ) , WH ( )<label>(18)</label></formula><p>Node-Wise Graph Weighted Fusion We design a graph weighted fusion module to dynamically fuse two graphs according to the relative semantic and syntactic importance of words in a sentence. The [CLS] vector, denote as e , is often used for sentence-level tasks and contains information about the entire sentence. We first calculate the bilinear similarity between e and each node of semantic and syntactic graphs. Then we normalize the similarity vectors across two graphs to obtain two sets of weights, which indicate semantic and syntactic importance respectively. Finally, we fuse all nodes across the graphs based on the weights:</p><formula xml:id="formula_12">w, w = softmax e W H ( ) , e W H ( )<label>(19)</label></formula><formula xml:id="formula_13">H ( +1) = w ? H ( ) + w ? H ( )<label>(20)</label></formula><p>where W is a learnable weight matrix, w and w denote the node importance weights of syntactic and semantic graphs, respectively. Then we map the node representations H ( +1) to the corresponding word representations using mean-pooling:</p><formula xml:id="formula_14">E = mean H ( +1) , E<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Trigger Sensor</head><p>We know that a particular relation usually occurs in conjunction with a particular set of words, which we call relation triggers. They can help explain why humans would extract a relation in the sentence and play an essential role in relation extraction. We present a Trigger Sensor module that senses and enhances the contextual trigger information without any trigger annotations. Relation triggers typically appear in local context between a pair of spans , , and some approaches encode local context directly into the span-pair representation for relation classification. However, these approaches do not consider the case where the triggers are outside the span-pair, resulting in the model ignoring useful information from other contexts. We design both a Local-Context Encoder and a Full-Context Trigger Sensor to compute the local-context representation g and the full-context trigger representation g . Local-Context Encoder We aggregate local-context information between spans of interest using max-pooling. The local-context representation g is calculated as:</p><formula xml:id="formula_15">g = max E , E +1 , ? ? ? , E ?<label>(22)</label></formula><p>where E g k , E g k+1 , ? ? ? , E g h are the encodings of words between the two spans , . Full-Context Trigger Sensor Full-context trigger sensor aims to sense and enhance span-pair specific triggers. Given a pair of spans , , we use head span and tail span as queries respectively and execute normal read process on the relation memory. After obtaining two span-specific memory representations, we perform mean-pooling across them to get the span-pair specific relation representation ( ) :</p><formula xml:id="formula_16">( ) = mean Read E , M R , Read E , M R<label>(23)</label></formula><p>We calculate the similarity between m ( ) and each word representation of a word sequence, and then weigh the word sequence to get the full-context trigger representation g .</p><formula xml:id="formula_17">g = softmax m ( ) (E ) E<label>(24)</label></formula><p>We incorporate the local-context representation g and the full-context trigger representation g into the span-pair encoding E using :</p><formula xml:id="formula_18">E = E , E , g , g<label>(25)</label></formula><p>for we adopt the concatenate function. Trigger Extraction Using the trigger sensor, we can also extract relation triggers and provide a reasonable explanation for model predictions. Based on the similarity of each word representation with the span-pair specific relation representations ( ) , we rank the words. The top-ranked words can be used as relation triggers to explain the model's predictions. We will show the trigger extraction ability of our model in the case study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Memory-Aware Classifier</head><p>Representations of the entity and relation categories are stored in entity memory and relation memory, respectively. Based on the bilinear similarity between instance (span or span-pair) representation and categories representations, we compute the probability of candidate span being an entity :</p><formula xml:id="formula_19">( = ) = exp E W E ?E exp E W M E<label>(26)</label></formula><p>and the probability of candidate span-pair , having a relation :</p><formula xml:id="formula_20">( ) = = sigmoid E ( ) W M R<label>(27)</label></formula><p>where W ? R ? ?? and W ? R ? ?? denote two learnable weight matrices. Finally, we define a joint loss function for entity classification and relation classification:</p><formula xml:id="formula_21">L = L + L</formula><p>where L denotes the cross-entropy loss over entity categories(including the None category), and L denotes the binary cross-entropy loss over relation categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Two-Stage Training</head><p>At the start of training, since the memory is randomly initialized, the Memory Flow Attention module and Trigger Sensor module will introduce noises to the sequence encoding. These noises further corrupt the semantic information of the pre-trained BERT <ref type="bibr" target="#b9">[10]</ref> through the gradient descent. We therefore divide the model training procedure into two stages. In the first stage, we aim to learn more accurate category representations and store them into the corresponding memory slots. We only train Memory-Aware Classifier and Graph Weighted Fusion modules and update the memory through the memory write process. In the second stage, we add Memory Flow Attention and Trigger Sensor modules to the training procedure. Based on the more accurate representations of the categories stored in the memory, we can strengthen the contextual task-related features and relation triggers through memory read process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>We evaluate TriMF described above using the following four datasets:</p><p>? SciERC: The SciERC <ref type="bibr" target="#b25">[26]</ref> includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. The dataset defines 6 types for annotating scientific entities and 7 relation categories. We adopt the same data splits as in <ref type="bibr" target="#b25">[26]</ref>. ? ACE05: ACE05 was built upon ACE04, and is commonly used to benchmark NER and RE methods. ACE05 defines 7 entity categories. For each pair of entities, it defines 6 relation categories. We adopt the same data splits as in <ref type="bibr" target="#b28">[29]</ref>. ? CoNLL04: The CoNLL04 dataset <ref type="bibr" target="#b29">[30]</ref> consists of 1,441 sentences with annotated entities and relations extracted from news articles. It defines 4 entity categories and 5 relation categorie. We adopt the same data splits as in <ref type="bibr" target="#b13">[14]</ref>, which contains 910 training, 243 dev, and 288 test sentences. ? ADE: The Adverse Drug Events (ADE) dataset <ref type="bibr" target="#b14">[15]</ref> consists of 4, 272 sentences and 6, 821 relations extracted from medical reports. These sentences describe the adverse effects arising from drug use. ADE dataset contains two entity categories and a single relation category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>Our model is compared with current advanced joint entity and relation extraction models, divided into three types: general parametersharing based models (Multi-head AT, SPtree, SpERT, SciIE), spangraph based models (DyGIE, DyGIE++), and reading-comprehension based models (multi-turn QA, MRC4ERE).</p><p>Multi-head + AT [4] treats the relation extraction task as a multihead selection problem. Each entity is combined with all other entities to form entity pairs that can be predicted which relations to have. In addition, instead of being a multi-category task where each category is mutually exclusive, the relation classification is treated as multiple bicategorical tasks where each relation is independent, which allows more than one relation to be predicted.</p><p>SPTree <ref type="bibr" target="#b28">[29]</ref> shares parameters of the encoder in joint entity recognition and relation extraction tasks, which strengthens the correlation between the two tasks. SPTree is the first model that adopts a neural network to solve a joint extraction task for entities and relations.</p><p>SpERT <ref type="bibr" target="#b10">[11]</ref> is a simple and effective model for joint entity and relation extraction. It uses BERT <ref type="bibr" target="#b9">[10]</ref> to encode a sentence, and enumerates all spans in the sentence. Then it performs span classification and span-pair classification to extract entities and relations. SciIE <ref type="bibr" target="#b25">[26]</ref> is a framework for extracting entities and relations from the scientific literature. It reduces error propagation between tasks and leverages cross-sentence relations through coreference links by introducing a multi-task setup and a coreference disambiguation task. DyGIE/DYGIE++ <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> dynamically build a span graph, and iteratively refine the span representations by propagating coreference and relation type confidences through the constructed span graph. Also, DyGIE++ takes event extraction into account.</p><p>Multi-turn QA <ref type="bibr" target="#b22">[23]</ref> treats joint entity and relation extraction task as a multiple-round question-and-answer task. Each entity and each relation is depicted using a question-and-answer template, so that these entities and relations can be extracted by answering these templated questions. MRC4ERE++ <ref type="bibr" target="#b40">[41]</ref> introduces a diversity question answering mechanism based on Multi-turn QA. Two answering selection strategies are designed to integrate different answers. Moreover, MRC4ERE++ proposes to predict a subset of potential relations to filter out irrelevant ones to generate questions effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We evaluate these models on both entity recognition and relation extraction tasks. An entity is considered correct if its predicted span and entity label match the ground truth. When evaluating relation extraction task, previous works have used different metrics. For the convenience of comparison, we report multiple evaluation metrics consistent with them. We define a strict evaluation, where a relation is considered correct if its relation type, as well as the two related entities, are both correct, and a boundary evaluation, where entity type correctness is not considered. We reported strict relation f1 on Conll04 and ADE, boundary relation f1 on SciERC, and both on ACE05. Our experiments on these datasets all report a micro-F1 score, except for the ADE dataset, where we report the macro-F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment Settings</head><p>In most experiments, we use BERT <ref type="bibr" target="#b9">[10]</ref> as the encoder, pre-trained on an English corpus. On the SciERC dataset, we replace BERT with SciBERT <ref type="bibr" target="#b5">[6]</ref>. We perform the four-level encoding with a subword encoding size ? = 768, a word encoding size ? = 768, a span encoding size ? = 793, and a span-pair encoding size ? = 2354. We set both entity memory slot size ? and relation memory slot size ? to 768. We just use a single graph neural layer in semantic and syntactic graphs. We initialize entity memory and relation memory using the normal distribution N (0.0, 0.02). We use the Adam Optimizer with a linear warmup-decay learning rate schedule (with a peak learning rate of 5e-5), a dropout before the entity and relation bilinear classifier with a rate of 0.5, a batch size of 8, span width embeddings of 25 dimensions and max span-size of 10. The training is divided into two stages with the first stage of 18 epochs, and the second stage of 12 epochs 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results and Analysis</head><p>Main Results We report the average results over 5 runs on Sci-ERC, ACE05 and CoNLL04 datasets. For ADE, we report metrics averaged across the 10 folds. <ref type="table">Table 1</ref> illustrates the performance of the proposed method as well as baseline models on SciERC, ACE05, CoNLL04 and ADE datasets. Our model consistently outperforms the state-of-the-art models for both entity and relation extraction on all datasets. Specifically, the relation F1 scores of our model advance previous models by +3.2%, +4.9%, +0.6%, +2.3% on SciERC, ACE05, CoNLL04 and ADE respectively. We attribute the improvement to three reasons. First, our model can share learned information between tasks through the Memory module, enhancing task interactions in both directions(from NER to RE, and from RE to NER). Second, the Trigger Sensor module can enhance the relation trigger information, which is essential for relation classification. Lastly, taking a step further from introducing structure information through syntactic graphs, we distinguish the semantic and syntactic importance of words to fuse two-way information through a dynamic Graph Weighted Fusion module. We conduct ablation studies to further investigate the effectiveness of these modules. <ref type="bibr" target="#b0">1</ref> Our code will be available at https://github.com/tricktreat/trimf  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>Effect of Different Modules To prove the effects of each proposed modules, we conduct the ablation study. As shown in <ref type="table" target="#tab_2">Table 2</ref>, all modules contribute to the final performance. Specifically, removing the Trigger Sensor module has the most significant effect, causing the relation F1 score to drop from 52.44% to 51.23% on SciERC, from 62.77% to 61.60% on ACE05. Comparing the effects of Memory-Flow Attention at subword-level and word-level on the two datasets, we find that the improvement of MFA at subword-level is more significant. We thus believe that fine-grained semantic information is more effective for relation extraction. The performance of the Syntactic-Semantic Graph Weighted Fusion module varies widely across datasets, achieving an improvement of 1.09% on ACE05, but only 0.61% on SciERC. This may be related to the different importance of syntactic information for relation extraction on different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Interaction Between Two Subtasks</head><p>There is a mutual dependency between the entity recognition and relation extraction tasks. Our framework models this relationship through the Multi-level Memory Flow Attention module. Depending on the memory that the attention mechanism relies on, it can be divided into Relation-specific MFA and Entity-specific MFA. The Relationspecific MFA module enhances the relation-related information based on the relation memory, allowing the entity recognition task to utilize the information already captured in the relation extraction task, as does Entity-specific MFA. To verify that the Memory Flow Attention module can facilitate the interaction between entity recognition and relation extraction, we perform ablation studies, as shown in <ref type="table">Table 3</ref>. On ACE05 and SciERC, both Entity-specific MFA and Relation-specific MFA bring significant performance improvement. In addition, the Relation-specific MFA improves more compared with Entity-specific MFA. We think the reason may be that our model performs entity recognition first and then relation extraction. This order determines that information from entity recognition has been used by relation extraction, but the information from relation extraction is not fed back to entity recognition.  <ref type="table">Table 3</ref>: Effect of Interaction between NER and RE When using Relation-specific MFA, a bridge for bi-directional information flow is built between the two tasks. Furthermore, when we use both Entity-specific MFA and Relation-specific MFA, the experiment achieves the best performance, indicating that MFA can enhance the bi-directional interaction between entity recognition and relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Different Graph Fusion Methods</head><p>Our proposed graph weighted fusion module employs a node-wise weighted fusion approach based on attention, which enables a flexible fusion of node representations according to words' syntactic importance and semantic importance. To demonstrate the effectiveness of our approach, we compare other node-wise fusion methods, including no-fusion, max-fusion, mean-fusion and sum-fusion, as shown in <ref type="table" target="#tab_5">Table 4</ref>. Comparing the two experiments which only use the semantic graph or syntactic graph, we find that the syntactic graph provides a greater improvement in model performance, probably because the initial encodings of the nodes of the syntactic graph have already contained semantic information. Compared to maxfusion, mean-fusion, and sum-fusion, the node-wise weight-fusion method brings more improvement on relation F1 scores of both SciERC and ACE05, which proves the effectiveness of our method. Effect of Different Stage Divisions for Memory We explored the effect of different two-stage divisions on the relation classification, as shown in <ref type="figure" target="#fig_4">Figure 4</ref> (x-axis is the number of epochs for the first stage and the total number of epochs is 30). We can note that if our model skips the first stage (x=0) or ignores the second stage (x=30), the performance of the model degrades significantly. Specifically, as the proportion of first stage epochs to total epochs increases, our model performs better. But at a certain point, the performance degrades significantly. We believe this is due to a decrease in epochs of the second stage and the memory already written in the first stage is not utilized effectively. Therefore the two-stage training strategy is effective, and a good balance of the two stages can bring out a better model performance.</p><p>Effect of Different Gradients Flow to Memory Our model primarily writes the memory in Memory-Aware Classifier. Furthermore, we can also tune the memory in MFA and Trigger Sensor modules through the backpropagation of gradients. The gradient flows   are divided into three types: Trigger Sensor gradients, Subwordlevel MFA gradients and Word-level MFA gradients, and we investigated the effects of different gradients, as shown in <ref type="table" target="#tab_8">Table 6</ref>. We see that on the ACE05 dataset, when we block any of the gradients flows, the model performance decreases significantly, by 1.35%, 1.54%, and 0.92% on relation F1 score, which indicates that tuning the memory during the second stage is effective. However, On the SciERC dataset, there is no significant drop, and we believe that the model has learned accurate representations of the categories in the first training stage.</p><p>Effect of Relation Filtering Threshold The precision and recall of relation classification are correlated with predefined thresholds. We investigate the impact of the relation filtering threshold on relation F1. <ref type="figure" target="#fig_6">Figure 5</ref> shows the relation F1 score on the SciERC and ACE05 test sets, plotted against the relation filtering threshold. We see that the performance of our model is stable for the choice of relation filtering thresholds. Our model is able to achieve good     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>Trigger Words Extraction With the Trigger Sensor module, our model has the ability to extract the relation triggers. We rank the similarities of each word representation with the span-pair specific relation representation, which have been calculated in the Trigger Sensor. Filtering out the entity surface words and stopwords, the top k words are picked as relation triggers and used to interpret the results of the relation extraction. We show two cases in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Flow Attention Visualization</head><p>We visualize the weights of attention to provide a straightforward picture of how the entity and relation memory flow attention we designed can both enhance the interaction between entity recognition and relation extraction. Also, it can enhance the information about relation triggers in context, to some extent explaining the model's predictions. <ref type="figure">Figure  6</ref> shows two cases of how attention weights on context from a relation memory flow can help the model recognize entities and highlight relation triggers. Each example is split into two visualizations, with the top showing the original attention weights and the bottom showing the attention weights after masking the entities. In the top figure, we can see that the darker words belong to an entity, for example, "Urutigoechea", "Bayonee", "Bonloc" in case 1, "Dallas", "Jack Ruby" in case 2, illustrating that the attention of our relation memory flow attention can highlight relevant entity information. Consistent with <ref type="bibr" target="#b15">[16]</ref>, our attention distribution also illustrates that entity names provide more valid information for relation classification compared to context. To more clearly visualize the attention weights of different contextual words, we mask all entities, formalize the weights of the remaining words, and then visualize them. As shown in the bottom figure, the relation memory flow pays more attention on the words that indicate the type of relation, i.e., relation triggers, such as "in", "southwestern", "west-central" in case 1 can indicate "Located in" relation, and "assassin", "murdering" in case 2 can indicate "Kill" relation. This shows that our relation memory flow is able to highlight relation triggers, helping the model with better performance on relation extraction. Error Cases In addition to visualizing Memory Flow Attention weights on true positives, we also analyze a number of false positives and false negatives. These error cases include relation requiring inference, ambiguous entity recognition and long entity recognition, as shown in <ref type="table" target="#tab_10">Table 7</ref>. In the first case, although our model is able to recognize the four entities about Location, it incorrectly extracts the relation "(Guernsey, Located in, France)" and does not extract the correct one "(Guernsey, Located in, Channel Islands)". This is because the model does not infer the complex location relation between the four entities. Our model is prone to make mistakes when classifying ambiguous entities, and False Positive and False Negative often occur together. For example, in the second row of the <ref type="table" target="#tab_10">Table 7</ref>, the model does not recognize "CBS News" as a Location entity, but recognizes "CBS" which is not labeled in the test set. Furthermore, recognition of long entities is a challenge   for our model due to the fact that long entities are sparse in the dataset. For example, in the third row of the <ref type="table" target="#tab_10">Table 7</ref>, the model fails to recognize the long entity "Organization of the Oppressed on Earth".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and relation extraction. We use the memory to boost the task-related information in a sentence through the Multi-level Memory Flow Attention module. This module can effectively exploit the mutual dependency and enhance the bi-directional interaction between entity recognition and relation extraction tasks. Also, focusing on the relation triggers, we design a Trigger Sensor to sense and enhance triggers based on memory. Our model can extract the relation triggers without any trigger annotations, which can better assist the relation extraction and provide an explanation. Furthermore, we distinguish the semantic and syntactic importance of a word in a sentence and fuse semantic and syntactic graphs dynamically based on the attention mechanism. Experiments on Sci-ERC, ACE05, CoNLL04 and ADE datasets show that our proposed model TriMF achieves state-of-the-art performance.</p><p>In the future, we will improve our work along with two directions. First, we plan to impose constraints on the representations of entity categories and relation categories written in the memory, due to the fact that relations and entities substantively satisfy specific constraints at the ontology level. Second, for improving the model's ability on sensing the trigger, we plan to add weak supervision (e.g. word frequency, entity boundary) to the Trigger Sensor module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Four Levels Encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Trigger-Sense Memory Flow Framework (TriMF) Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Multi-Level Multi-Memory Flow Attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>memory ( xaxis: epoch for the first stage ) Effect of Train Stage Division</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>results on relation classification except for extreme thresholds of 0.0 or 1.0. Therefore, within a reasonable range, our model is not sensitive to choose a threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Effect of Relation Filtering Threshold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Case 2 -Figure 6 :</head><label>26</label><figDesc>with entities masked: Two case studies of relation memory flow attention during inference. The darker cells have higher attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of Different Modules</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>70.33 70.10 52.56 49.59 51.03 Semantic Graph 68.47 69.61 49.04 52.00 50.62 51.30 Syntactic Graph 72.18 70.68 71.42 54.02 48.97 51.37</figDesc><table><row><cell></cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell></row><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell cols="2">SciERC</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">No Graph 69.87 Mean-fusion 69.77 69.02 69.39 53.56 49.39 51.38</cell></row><row><cell>Sum-fusion</cell><cell cols="6">69.45 69.57 69.51 52.94 49.65 51.24</cell></row><row><cell>Max-fusion</cell><cell cols="6">69.12 69.64 69.38 53.01 49.45 51.17</cell></row><row><cell cols="7">Weighted fusion 70.18 70.17 70.17 52.63 52.32 52.44</cell></row><row><cell></cell><cell></cell><cell>ACE05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Graph</cell><cell cols="6">87.24 87.18 87.21 60.11 61.83 60.96</cell></row><row><cell cols="7">Semantic Graph 87.57 87.69 87.63 59.45 62.47 60.92</cell></row><row><cell cols="7">Syntactic Graph 87.47 87.36 87.41 59.29 62.96 61.07</cell></row><row><cell>Mean-fusion</cell><cell cols="6">87.32 87.78 87.55 59.74 62.90 61.28</cell></row><row><cell>Sum-fusion</cell><cell cols="6">87.85 87.47 87.66 60.12 62.26 61.17</cell></row><row><cell>Max-fusion</cell><cell cols="6">87.51 87.62 87.56 60.22 62.25 61.22</cell></row><row><cell cols="7">Weighted fusion 87.67 87.54 87.61 62.19 63.37 62.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of Different Graph Fusion Methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Relation Triggers Urutigoechea and the others were arrested Wednesday in the cities of Bayonee and Bonloc in southwestern France in Poitiers in westcentral France. (Bonloc, Located in, France) southwestern, west-central, cities, of, in Kleber Elias Gia Bustamante, accused by the police of being a member of the "Red Sun" central committee, has been living clandestinely since his escape from the Garcia Moreno Prison, where he was held accused of assassinating the industrialist, Jose Antonio Briz Lopez.</figDesc><table><row><cell>Original Text</cell><cell>Relation Top-5 (Kleber Elias Gia Bustamante, Kill, Jose Antonio Briz Lopez) Prison, assassinating, held, of, accused</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of Trigger Words Extraction</figDesc><table><row><cell></cell><cell>Entity</cell><cell></cell><cell cols="2">Relation</cell></row><row><cell>Method</cell><cell>F1</cell><cell>?</cell><cell>F1</cell><cell>?</cell></row><row><cell></cell><cell>SciERC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TriMF</cell><cell>70.17</cell><cell>-</cell><cell>52.44</cell><cell>-</cell></row><row><cell>w/o Trigger Sensor Grad.</cell><cell cols="4">70.14 -0.03 52.28 -0.16</cell></row><row><cell cols="5">w/o Subword-level MFA Grad. 70.23 +0.08 52.03 -0.41</cell></row><row><cell>w/o Word-level MFA Grad.</cell><cell cols="4">70.12 -0.05 52.14 -0.30</cell></row><row><cell></cell><cell>ACE05</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TriMF</cell><cell>87.61</cell><cell>-</cell><cell>62.77</cell><cell>-</cell></row><row><cell>w/o Trigger Sensor Grad.</cell><cell cols="4">87.55 -0.06 61.42 -1.35</cell></row><row><cell cols="5">w/o Subword-level MFA Grad. 87.43 -0.18 61.23 -1.54</cell></row><row><cell>w/o Word-level MFA Grad.</cell><cell cols="4">87.34 -0.27 61.85 -0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Effect of Gradient Flow to Memory</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The problem is not unusual in[[ Guernsey ] H_Located-in ] H_Located-in , one of [ Britain ] T_Located-in 's [ Channel Islands ] T_Located-in off the coast of [[ France ]] T_Located-in ... and former [[ CBS ] T_Work-for News ] commentator [[ Eric Sevareid ] H_Live-in ] H_Work-for , who was born in [ Velva ] T_Live-in , several miles southeast of [ Minot ]. Text of the statement issued by the [ Organization of the Oppressed on Earth ] claiming [[ U. S. ] T_Live-in ] T_Live-in Marine Lt.[[ William R. Higgins ] H_Live-in ] H_Live-in was hanged.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Typical error examples. Red brackets indicate entities predicted by the model, blue brackets indicate true entities, and the labels in the lower right corner indicate the type of relation between the corresponding entities and the head or tail type (T for the tail entity; H for the head entity)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: Extracting relations from large plain-text collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the fifth ACM conference on Digital</title>
		<imprint>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SRA: Description of the IE2 system used for MUC-7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mila</forename><surname>Ramos-Santacruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Message Understanding Conference</title>
		<meeting><address><addrLine>Fairfax, Virginia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-04-29" />
		</imprint>
	</monogr>
	<note>Proceedings of a Conference Held in</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised bootstrapping of relationship extractors with distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rio J</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06876</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m">SciBERT: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hybrid approach to extract protein-protein interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Chinh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Katrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sloot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="259" to="265" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07755</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RelEx-Relation extraction using dependency parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>K?ffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03186</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th international conference on computational linguistics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Coling</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bootstrapping for text learning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Investigating lstms for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05529</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Triggerner: Learning with entity triggers as explanations for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ho</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09602</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03296</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel use of statistical parsing to extract information from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidi</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Meeting of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ILLINOIS UNIV AT URBANA-CHAMPAIGN DEPT OF COMPUTER SCIENCE</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A hierarchical framework for relation extraction with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiexi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7072" to="7079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel graph scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4461" to="4467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from explanations with neural execution tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Relation-Specific Attention Network for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence 2020. Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4054" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring syntactic features for relation extraction using a convolution tree kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="288" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Asking Effective and Diverse Questions: A Machine Reading Comprehension based Framework for Joint Entity-Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05075</idno>
		<title level="m">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nero: A neural rule grounding framework for label-efficient relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2166" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
