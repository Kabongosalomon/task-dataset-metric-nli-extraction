<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaze360: Physically Unconstrained Gaze Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Recasens</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Research Institute</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@csail.mit.edusimon.stent@tri.global*indicatesequalcontribution</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaze360: Physically Unconstrained Gaze Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In order to better understand humans -their desires, intents and states of mind -one must be able to observe and perceive certain behavioral cues. Eye gaze direction is one such cue: it is a strong form of non-verbal communication, signalling engagement, interest and attention during social interactions <ref type="bibr" target="#b0">[1]</ref>. Detecting and following where another person is looking is a skill developed early on in a child's life -four-month-old infants are known to use eye gaze cuing to help visually process objects, for example <ref type="bibr" target="#b20">[21]</ref>. Just as a parent's gaze can help to guide a child's attention, human gaze fixations have also been found to be useful in helping machines to learn or interact in various contexts <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>. <ref type="bibr">Figure 1</ref>. Overview: we introduce a novel dataset and method for estimating 3D gaze in-the-wild. This figure illustrates our model's output on unseen video gathered from YouTube, demonstrating its robustness to diverse, physically unconstrained scenes.</p><p>In recent years, while methods for related human modeling problems such as 2D body pose and face tracking have achieved impressive success by leveraging the representational power of deep convolutional neural networks along with very large annotated datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>, methods for gaze estimation have not yet reached such levels of performance. This is primarily due to the lack of sufficiently large and diverse annotated training data for the task. Collecting precise and highly varied gaze data with ground truth, particularly outside of the lab, is a challenging task.</p><p>In this work, we introduce an approach to help tackle this task and narrow the perceived performance gap:</p><p>? we first describe a methodology to efficiently collect annotated 3D gaze data in arbitrary environments; ? we use our method to acquire the largest 3D gaze dataset in the literature by subject and variety, capturing video of 238 subjects in indoor and outdoor conditions, and we carefully evaluate the error and characteristics of the dataset; ? we train a variety of 3D gaze estimation models on the dataset before converging on a final model which uniquely takes a multi-frame input (to help resolve single frame ambiguities) and employs a pinball regression loss for error quantile regression to provide an estimate of gaze uncertainty; ? we demonstrate the usefulness of our dataset versus existing datasets by means of a cross-dataset model performance comparison (training on one dataset and testing on another), and introduce a simple method for self-supervised domain adaptation of gaze models; ? finally we demonstrate how our Gaze360 model can be applied to real-world use cases, such as estimating a customer's focus of attention in a supermarket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Gaze datasets. A summary of comparable gaze datasets is shown in <ref type="table" target="#tab_0">Table 1</ref>. While many gaze-related datasets have been published in recent years <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, they are mostly geared towards physically constrained applications such as desktop or smartphone gaze tracking. Typically, these datasets are captured using a static recording setup <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> or a camera integrated in a smartphone <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. The static approach allows for more control and higher accuracy but can lack the diversity in illumination and motion blur useful for more general applications. Smartphone-based solutions overcome these flaws and have the advantage of straightforward scaling via crowd-sourcing to increase the subject variety. However, they lack head pose and gaze variability due to the collocation of the device's camera and screen, as well as the screen's relatively narrow area for projecting targets. To try to capture the nature of human gaze in arbitrary natural scenes, it is important not to overly constrain the subject's pose, allowing for coverage over the full gamut of head and eyeball orientations in relation to the camera. While some existing datasets have relatively small head pose and gaze variation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>, others do provide a wider range <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> but are still restricted to primarily frontal rather than oblique views. While it is true that the eyes become increasingly occluded at larger angles of head yaw, we wish to capture such cases so that our model can be used in less constrained settings.</p><p>In one of the most comprehensive datasets from Zhu and Deng <ref type="bibr" target="#b33">[34]</ref>, the authors increased acquisition speed and viewpont variety by using an array of cameras in different poses. However, the setup was restricted to collecting data in the lab environment. While our approach also uses a multi-camera setup, our goal was to quickly acquire many subjects at once, using a free-moving rather than fixed target that allowed us to capture the full range of gaze directions, as described in <ref type="figure">Fig. 4</ref> and Section 4. Moreover, as our capture setup is mobile, this allowed us to efficiently collect data from a broad demographic in more varied natural lighting environments, including a wider range of scale variation and image blur from subject motion during capture. This more closely approximates the domains of systems such as interactive robots or surveillance/monitoring cameras which might benefit from our gaze tracking model. A recent work which also addresses gaze estimation in natural settings with larger camera-subject distances and less constrained subject motion, is that of <ref type="bibr" target="#b3">[4]</ref>. Their approach to dataset generation was target-free, but required subjects to wear gaze-tracking glasses, used motion capture cameras to recover head pose, and needed a complicated semantic in-painting step to remove the gaze tracking glasses from the target image. In comparison, our approach is relatively simple, allowing us to scale to many more subjects (238 versus 15) and lighting conditions. Geometric gaze models: Geometric models often use corneal reflections of near infra-red light sources <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> or other light sources with known geometry <ref type="bibr" target="#b9">[10]</ref> to fit a model of the eyeball from which gaze can be inferred. Since these methods rely on a physical model, they generalize quite easily to new subjects with little or no training data, but at the cost of higher sensitivity to input noise such as partial occlusions or lighting interference. Since they also rely on a fixed light source, they are not feasible in unconstrained settings such as ours.</p><p>Appearance-based gaze models: Appearance-based methods learn a more direct image-to-gaze mapping, using large datasets of annotated eye or face images. Support vector regression <ref type="bibr" target="#b27">[28]</ref>, random forests <ref type="bibr" target="#b10">[11]</ref> and most recently deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> have been applied in this way. A preprocessing step of eye or face detection is often required <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Our model does not rely on eye or face  <ref type="figure">Figure 2</ref>. Acquisition setup. Our setup allows us to efficiently collect large volumes of diverse, annotated data for 3D gaze estimation. We create a dataset with 238 subjects in a wide range of lighting conditions (both indoor and outdoor) and distances and angles to subjects. detectors, which enables it to achieve higher robustness in unconstrained settings when the required features become partially occluded. Dependency between gaze and head pose can either be handled by training implicitly <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> or modeled explicitly with separate branches <ref type="bibr" target="#b33">[34]</ref>.</p><p>Gaze estimation becomes more difficult under partial occlusion of eyes. Even at 90 ? 135 ? head yaw a significant part of one eyeball is often still visible and informative for gaze estimation (see Supplemental). Existing methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> do not deal with these cases and typically assume that the subject is facing the camera. However, such models do not generalize well to challenging applications such as in robotics or surveillance. Unlike previous approaches, our model is designed to cope with such situations by always providing best effort prediction along with an appropriate confidence measure. We learn to predict uncertainty via quantile regression <ref type="bibr" target="#b14">[15]</ref> learned using a pinball loss. Our model outputs an estimated gaze direction even with fully occluded eyes by relying on visible head features, while at the same time informing about the limited accuracy of its prediction by outputting a correspondingly higher uncertainty value. In addition, unlike previous models, we investigate the use of additional frames to improve gaze estimates through the aggregation of image evidence over time. This increases the chance of capturing relevant features that may only be visible in few frames. We show how using motion significantly helps the system performance over a wide range of view angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset collection method</head><p>There is currently no dataset suitable to learn a model capable of robustly estimating 3D gaze in-the-wild. Previous efforts to record large-scale datasets relied on careful acquisition setups with precisely measured subject and gaze target positioning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. Such setups are nearly impossible to move to different locations, can only record single subjects at a time and require constant verification of the desired gaze from the subject which makes the collection process inflexible and very slow. This is the reason why all existing datasets with 3D gaze labels are recorded in indoor environments and frequently use few subjects. As evidenced by the success of 2D body and face tracking models in the wild <ref type="bibr" target="#b1">[2]</ref>, to improve in-the-wild robustness it is important to collect data with a large number of different subjects, large variation in natural illumination and a wide range of head poses and gaze directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Setup</head><p>To tackle these issues we opted for a setup built around a Ladybug5 360 ? panoramic camera ( <ref type="figure">Fig. 2</ref>) placed on a tripod in the center of the scene, and a large moving rigid target board marked with an AprilTag <ref type="bibr" target="#b24">[25]</ref> and a cross on which subjects were instructed to continuously fixate. This allowed data from multiple subjects to be recorded simultaneously. The Ladybug5 consists of five synchronized and overlapping 5 megapixel camera units each with 120 ? horizontal field of view, plus one additional upwardfacing camera which we do not use. We store each frame as 3382 ? 4096 pixels image after fish-eye lens rectification. The face of a subject standing one meter away from the camera could be fully captured in at least one of the views. The camera is factory-calibrated and we rectified all images after capture to remove barrel distortion. The compactness of the setup, consisting of a single camera unit on a tripod together with a laptop and portable power source, allowed for easy portability and deployment for efficient data collection in many environments.</p><p>Subject positioning. To build the dataset, we use Al-phaPose <ref type="bibr" target="#b2">[3]</ref> to detect the position of head keypoints and feet of subjects in rectified frames from each camera unit independently. For very close subjects whose feet are beyond the camera field of view, we use the average body proportions of standing subjects to estimate their feet position from their hip position. The Ladybug camera provides a 3D ray in a global Ladybug Cartesian coordinate system L = [L x , L y , L z ] for every image pixel. We use it to derive the position of feet and eyes in spherical coordinates. The remaining unknown variable is the distance from Ladybug origin to eyes, d. We exploit a measured camera height above the horizontal ground plane that the camera and all subjects stand on. Although this limits our training data collection to flat surfaces, it is not restrictive at testtime. For further details on the trigonometry, please consult the supplementary materials.</p><p>Target positioning: Our target consists of a white board with a large AprilTag <ref type="bibr" target="#b24">[25]</ref> on one side and a smaller cross beside it on both sides <ref type="figure">(Fig. 2)</ref>. The cross serves as a gaze fixation target for the study subjects while the tag is used for tracking of the board in 3D space. We use the original AprilTag library to detect the marker in each of the camera views and estimate its 3D pose using the known camera calibration parameters and marker size. We then use the pose and known board geometry to find the 3D location of the target cross p t . Gaze direction: We compute the gaze vector in the Ladybug coordinate system as a simple difference g L = p t ? p e . However, such a form would change with rotation of the camera and its coordinate system L. To remedy this, we express the gaze in the observing camera's Cartesian eye</p><formula xml:id="formula_0">coordinate system E = [E x , E y , E z ].</formula><p>E is defined so that the origin is p e , E z has the same direction as g L and E x lies in a plane defined by L x and L y (no roll). We can then convert the gaze vector to the eye coordinate system by:</p><formula xml:id="formula_1">g = E ? g L ||g L || 2 .<label>(1)</label></formula><p>This definition of gaze direction guarantees that g = [0, 0, ?1] when the subject looks directly at the camera, independently of the subject's position, and in general allows to express the gaze orientation from the local appearance of the head without the need for any global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Acquisition procedure</head><p>We acquired an institution review board approval for our dataset collection experiment. Subjects were instructed to stand around a camera at a distance of between 1 ? 3m (average 2.2m) and continuously track the target cross on the side of the marker board visible to them <ref type="figure" target="#fig_0">(Fig. 3</ref>). For safety, subjects were instructed to stay approximately in their starting locations as they would not be able to both track the target and see possible obstacles while moving.</p><p>The marker board was manipulated by one of the investigators who carried it once in a large loop around both the subjects and the camera (2 ? 5m radius) and then in between the camera and subjects <ref type="figure" target="#fig_0">(Fig. 3a)</ref>. While in motion, the target board was simultaneously moved up and down <ref type="figure" target="#fig_0">(Fig. 3c</ref>) to elicit gaze pitch variation. The loop part of the trajectory allowed to cover all possible gaze directions. The inner path was added to sample more extreme gaze pitch variation which can only be achieved from a closer distance due to limitations on the vertical position of the marker in the scene. We ensured that the marker board was always positioned to face the camera with the AprilTag as frontoparallel as possible to reduce pose estimation error <ref type="figure" target="#fig_0">(Fig. 3b)</ref>.</p><p>In order to capture a wide range of relative eyeball and head poses, we alternated between "move" and "freeze" instructions during each capture. While in the "move" state, subjects were allowed to naturally orient their head and body pose to help track the target. When the "freeze" instruction was issued, subjects were only allowed to move their eyes while maintaining a fixed head pose if possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Gaze360 dataset summary</head><p>Our dataset is unique for its combination of 3D gaze annotations, wide range of gaze and head poses, variety of indoor and outdoor capture environments and diversity of subjects. It is only surpassed in number of subjects by the GazeCapture <ref type="bibr" target="#b11">[12]</ref> dataset (1,450 subjects), which is 2D and covers only a narrow gaze range for a limited use case. See <ref type="table" target="#tab_0">Table 1</ref> for a dataset comparison. Notably, our dataset is also the first to provide these qualities for short continuous videos (8 Hz).</p><p>Summary statistics. We collected 238 subjects in 5 indoor (53 subjects) and 2 outdoor (185 subjects) locations over 9 recording sessions. This is an acquisition speed that is unmatched by other on-site techniques and can only be compared to crowd-sourced approaches which, however, cannot compete in terms of experimental control. In total we acquired 129K training, 17K validation and 26K test images with gaze annotation. For privacy reasons we did not survey additional data about our subjects, but a visual inspection shows a wide distribution of subject ages, ethnicities and genders (58 % female, 42 % male). Please refer to <ref type="figure" target="#fig_2">Fig. 5</ref> for examples.</p><p>Data distribution. We plot the angular distribution of the gaze labels covered by our and several other datasets using the Mollweide projection in <ref type="figure">Fig. 4</ref>. This illustrates how our dataset covers the entire horizontal range of 360 ? . While a portion of these gaze orientations correspond to fully occluded eyes (facing away from the camera), our dataset allows for gaze estimation up to the limit of eye visibility. This limit can, in certain cases, correspond to gaze yaws of approximately + ? 140 ? (where the head pose is at 90 ? such that one eye remains visible, and that eye is a further 50 ? rotated). The vertical range is limited by the achievable elevation of the marker. Sampling is less dense in the rear region (around the left and right borders of the map). This can be explained by occlusion of the target board by the subjects.  <ref type="figure">Figure 4</ref>. Dataset statistics. Joint distributions of the gaze yaw and pitch for TabletGaze <ref type="bibr" target="#b9">[10]</ref>, MPIIFaceGaze <ref type="bibr" target="#b30">[31]</ref>, iTracker <ref type="bibr" target="#b11">[12]</ref> and our Gaze360 dataset. The Mollweide projection used to visualize the full unit sphere surface. All intensities are logarithmic. Error characterization. In order to validate the accuracy of our gaze annotations we conducted a control experiment. We followed the standard acquisition procedure with our 360 ? camera and a single participant at a time wearing an additional front-facing test camera mounted above the right eye. We measured the 3D gaze in the test camera using the standard AprilTag based procedure and the known origin coinciding with the camera. Additional AprilTags in the background were used to register both cameras. We measured the mean difference between both gaze labels to be 2.9 ? over three recordings of two subjects. This is well within the error of appearance-based eye tracking at distance, validating our acquisition procedure as a means of collecting an annotated 3D gaze dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Gaze360 model</head><p>Gaze is a naturally continuous signal. Gaze fixations and transitions yield a sequence of gaze directions. To exploit this, we propose a video-based gaze-tracking model  <ref type="figure">Figure 6</ref>. Gaze360 model architecture. The model receives multiple frames of input which are passed through a backbone network. The output for each frame is fed to a bidirectional LSTM to produce the compact representation which is used to make the final prediction of gaze direction and quantile regression. We use a 7-frame input window centered around the target frame.</p><p>using bidirectional Long Short-Term Memory capsules (LSTM) <ref type="bibr" target="#b4">[5]</ref>, which provide a means of modeling sequences where the output for one element is dependent on both past and future inputs. In this paper, we utilize sequences of 7 frames to predict the gaze of the central frame. Note that other sequence lengths including a single central frame alone are also possible. <ref type="figure">Fig. 6</ref> illustrates the architecture of the Gaze360 model. A head crop from each frame is individually processed by a convolutional neural network (backbone), which produces high-level features with dimensionality 256. These features are fed to bidirectional LSTMs with two layers which digest the sequence within forward and backward vectors. Finally, these vectors are concatenated and passed through a fully connected layer to produce two outputs: the gaze prediction and an error quantile estimation.</p><p>The gaze prediction output regresses the angle of the gaze relative to the camera view. In previous work, 3D gaze was predicted as a unit gaze vector <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> or as its spherical coordinates <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. We use spherical coordinates which we believe to be more naturally interpretable in this context. We define the spherical coordinates such that the pole singularities correspond to strictly vertical gaze oriented either up or down, which are very rare directions.</p><p>We use an ImageNet-pretrained ResNet-18 <ref type="bibr" target="#b6">[7]</ref> as the backbone network. All the models were trained in PyTorch using the Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with learning rate 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Error quantile estimation</head><p>To the best of our knowledge, all existing research applying neural networks to the task of gaze estimation do not consider error bounds. Error bounds are useful when estimating gaze in unconstrained environments, because precision is likely to degrade when the eye is viewed from a sideways angle, or when one or more eyes are partially obscured (e.g. by glasses frames). In a classification setting, softmax outputs are often used as a proxy for confidence. However, for regression this is not possible, as the magnitude of the output corresponds directly to the predicted property.</p><p>To model error bounds, we use a pinball loss function <ref type="bibr" target="#b14">[15]</ref> to predict error quantiles. We use one single network to predict both the mean value and the 10% and 90% quantile. The effect of this is that for a given image, we estimate through a single forward pass both the expected gaze direction and a cone of error within which the ground truth should lie 80% of the time. We assume that the distribution is isotropic in our spherical coordinate system. This assumption is not strictly true, especially for large pitch angles due to the space distortion around pole singularities. However, for most of the observed gaze directions <ref type="figure">(Fig. 4)</ref> it is a reasonable approximation to reduce dimensionality and simplify the interpretation of the result.</p><p>The output of our network is f (I) = (?, ?, ?), where (?, ?) is the expected gaze direction in spherical coordinates, for which we already have a corresponding ground truth gaze vector in the eye coordinate system g (see Sec. 3.1) as ? = ? arctan gx gz and ? = arcsin g y . The third parameter, ?, corresponds to the offset from the expected gaze such that ? + ? and ? + ? are the 90% quantiles of their distributions while ? ? ? and ? ? ? are 10% quantiles.</p><p>Finally, we compute the pinball loss of this output. This will naturally force ? and ? to converge to their ground truth values and ? to the quantile threshold. If y = (? gt , ? gt ), the loss L ? for the quantile ? and the angle ? can be written as:</p><formula xml:id="formula_2">q ? = ? gt ? (? ? ?), for ? ? 0.5 ? gt ? (? + ?), otherwise<label>(2)</label></formula><formula xml:id="formula_3">L ? (?, ?, ? gt ) = max(?q ? , ?(1 ? ? )q ? ).<label>(3)</label></formula><p>A similar formulation is used for the angle ?. We average the losses for both angles and quantiles ? = 0.1 and ? = 0.9. Thus, ? is a measure of the difference between the 10% and 90% quantiles and the expected value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Adapting to unseen domains</head><p>Despite the variety in the Gaze360 dataset, some realworld applications may benefit from a closer adaptation of the model to the target domain. For this reason, we introduce a self-supervised method for domain adaptation.</p><p>Our general model is fine-tuned using a mix of the labeled Gaze360 images and unlabeled images from the new domain. Inspired by <ref type="bibr" target="#b23">[24]</ref>, we introduce a discriminator which tries to identify the source domain of the image features as a binary classification task. The features are the output of the backbone network. The discriminator loss L D is added to the original supervised loss L ? for those images where ground truth is available.</p><p>In addition, we added a further loss to exploit the leftright symmetry of the gaze-estimation task as a means of encouraging model output consistency on unlabeled data. We use the model to compute the gaze of the original and horizontally flipped image, and the pinball loss L S to minimize the angular difference between the prediction from the first input and horizontally mirrored prediction from the second input. While this loss by itself can lead to collapse to a gaze prediction along the line of symmetry, our observations in Sec. 6.2 show that this helps when used as a regularizer to improve performance in an unseen target domain. Altogether we minimize L = ? ? L ? + L D + ? ? L S where ? = 60 and ? = 3 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Model evaluation</head><p>In this section, we compare several approaches using the Gaze360 dataset. We compared the following methods: Mean -uses the mean gaze of the training set for all predictions; Deep Head Pose -a deep network based head pose estimator by Ruiz et al. <ref type="bibr" target="#b18">[19]</ref>; Static -the backbone model, ResNet-18, and two final layers to compute the prediction; TRN -a version of Temporal Relation Network <ref type="bibr" target="#b32">[33]</ref> where the features of frames at fixed windows around time t are concatenated before averaging the predictions of the temporal windows; LSTM -refers to the Gaze360 architecture.</p><p>For each of the three architectures introduced above, we report accuracy of different baselines for uncertainty estimation: MSE -uses the mean squared error to regress only the spherical angles of gaze without uncertainty; MSE+Drop -using the MSE model, the uncertainty is estimated by 5 forward passes for each input while randomly dropping neurons in the last layer and computing the variance of the output; Crop augmentation -5 random head crops are sequentially evaluated to estimate uncertainty using the variance of the 5 predictions of the MSE-trained model; and Pinball Loss -gaze direction and error bounds are jointly estimated using the pinball loss.</p><p>The angular errors in <ref type="table" target="#tab_2">Table 2</ref> are provided separately for the entire test set (All 360 ? ) and for samples where the subject is looking within 90 ? (Front 180 ? ) and 20 ? (Front facing) of the camera direction. We also report the Spearman's rank correlation between the error quantile estimate and the actual error, which is a metric for how well the predicted error bounds estimate the actual error.</p><p>The results confirm that eye-free Mean predictions as well as Head pose are insufficient to predict the rich variation of eye movement in our dataset. All of our gaze models outperform these simple baselines. We also observe that, under the same conditions, the error is generally lowest for the model using Pinball loss. The same trend can be seen for the correlation between the predicted uncertainty and actual prediction error. Additionally, only a single forward pass is required for the prediction. Hence, we chose the  Pinball loss as our recommended approach. Switching from a single-frame static model to a temporal model also benefits the gaze prediction accuracy substantially. We conclude that although the performance of TRN and LSTM is similar, we recommend the Pinball LSTM for its slightly better results in our metric and straightforward adaptation to use a different number of input frames. In <ref type="figure" target="#fig_5">Fig. 8</ref> we present the prediction error of the models using Pinball loss as a function of gaze yaw angle. As expected, accuracy falls with increasing gaze yaw angle. Unlike traditional eye trackers, our model smoothly transitions into head pose estimation (between head yaws of 90-150 ? ) to provide a best guess of gaze even for rear views. This is accompanied by a higher associated uncertainty (dashed lines). Although the error for frontal views is generally larger than errors reported on existing highresolution datasets, we next show that this is due to the challenging properties of Gaze360 which allow models trained on it to transfer better to physically unconstrained images.</p><p>In <ref type="figure" target="#fig_4">Fig. 7</ref> we show sample results on our test data. The angular error denoted by the yellow bar intuitively grows as the eyes become smaller due to distance or occluded due to head pose variation. Although the prediction error for awaylooking poses is on average large, the uncertainty measure provides a reasonable prediction of this behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Cross-dataset evaluation</head><p>We evaluate the value of the Gaze360 dataset for gaze estimation in the wild by training the Pinball Static model using multiple pre-existing 3D gaze datasets and measuring cross-dataset test error. The comparison datasets we use are: Columbia [20] -high-resolution close-up faces; MPIIFaceGaze [31] -faces captured by webcams; RT-GENE [4] -low-resolution faces using in-painting to mask out eye-tracking glasses; Gaze360 (Ours) -faces with varying resolution; For those datasets where no official splits were provided <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> we use all available samples for training and do not measure the within-domain error. <ref type="table" target="#tab_3">Table 3</ref> summarizes the results. This task is much more   <ref type="figure">Figure 10</ref>. An example application: we use Gaze360 to passively infer the attention of a customer as they browse products on a shelf, using video (left) from a camera next to the shelf (right). challenging than within-domain tests. The best results are consistently achieved when our dataset is used for training. In addition, we fine-tune our Gaze360-trained model on new domains (Gaze360 + DA) using the self-supervised approach described in Sec. 5.2, which does not utilize the ground truth labels in other datasets. Our domain adaption strategy improves performance further on all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shampoo</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coca-Cola Subject</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Tracking gaze in the wild</head><p>Prediction in unconstrained environments: The variation in appearance of subjects in the Gaze360 dataset allows our model to perform well without further training or fine-tuning on unseen image and video data from uncurated online sources. We demonstrate this visually on numerous examples in Figs. 1 and 9 and in our supplemental video.</p><p>Estimating attention in a supermarket: To illustrate one possible application of Gaze360, we apply it to the task of predicting which objects are being looked at on a su-permarket shelf, which is relevant for product-placement in stores. We recreate a supermarket shelf and ask subjects to look at various objects while self-reporting those objects. We record them with a camera next to the shelf, as shown in <ref type="figure">Fig. 10</ref>. Despite a less than optimal view of the subject , we are able to predict which object is being looked at correctly 51% of the time. Using a smartphone camera embedded directly in the shelf (so that the view of subjects is closer to frontal), the accuracy increases to 68%. The objects along the bottom shelves have highest error rate, as the eyes become almost fully occluded when looking downwards. Finally, we are able to produce a heatmap of customer attention, shown in <ref type="figure">Fig. 10</ref>. While simple, this application demonstrates the flexibility of our system for use in a wide range of real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we introduced a novel approach to efficiently collect annotated gaze data at scale and used it to generate a large and diverse dataset, suitable for deep learning of 3D gaze from images and video. We presented a new temporal appearance-based gaze model using a novel loss function to estimate error quantiles. Finally we demonstrated the value of (i) our dataset via careful crossdataset performance comparison versus three existing 3D gaze datasets, and (ii) our model via application to unconstrained unseen imagery from YouTube videos. It is our hope that by using our dataset and model, researchers across a range of fields will be able to better leverage gaze as a cue to improve vision-based understanding of human behavior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Dataset collection protocol: (a) the top view of the scene and target board trajectory showing full coverage around the subjects; (b) the image of the scene from the camera (stitched for illustration only); (c) the side view of the scene and target board trajectory showing large induced variation in pitch to the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Gaze360 dataset samples: showing the diversity in environment, illumination, age, sex, ethnicity, head pose and gaze direction. Top: full body crops; bottom: closer-up head crops. Yellow arrows show measured ground-truth gaze.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>igure 7 .</head><label>7</label><figDesc>Test set examples: ground truth gaze (yellow) and Gaze360 predictions (red) are shown for unseen test subjects. The bars denote actual (yellow) and predicted (red) errors in degrees. The inset shows a top-down view of the gaze estimates and the predicted error versus ground truth. The bottom row shows sample failure cases where the model was overconfident.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Error measured on Gaze360 dataset using the Pinball models. The full lines show prediction error, the dashed lines show predicted uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Estimating 3D gaze in the wild: further examples of our model's output on unseen video gathered from YouTube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A comparison of popular gaze datasets. The type and range of gaze labels, number of subjects and completeness of image data publicly available. Full stands for full face images, Eyes denotes crops of eye regions and N/A means that the dataset was not available for use. Asterisks indicate datasets containing partially occluded face images.</figDesc><table><row><cell>Dataset</cell><cell>Gaze</cell><cell cols="3">Range # Subj. Image</cell><cell>Outdoor</cell></row><row><cell>TabletGaze [11]</cell><cell>2D</cell><cell>?80 ?</cell><cell>51</cell><cell>Eyes</cell><cell>No</cell></row><row><cell>iTracker [12]</cell><cell>2D</cell><cell>?100 ?</cell><cell>1,450</cell><cell>Full</cell><cell>Partially</cell></row><row><cell>UT MV [23]</cell><cell>3D</cell><cell>?50 ?</cell><cell>50</cell><cell>Eyes</cell><cell>No</cell></row><row><cell>Columbia [20]</cell><cell>3D</cell><cell>60 ?</cell><cell>56</cell><cell>Full*</cell><cell>No</cell></row><row><cell>RT-GENE [4]</cell><cell>3D</cell><cell>75 ?</cell><cell>15</cell><cell>Full*</cell><cell>No</cell></row><row><cell>MPIIFaceGaze [31]</cell><cell>3D</cell><cell>?80 ?</cell><cell>15</cell><cell>Full</cell><cell>No</cell></row><row><cell>EYEDIAP [17]</cell><cell>3D</cell><cell>90 ?</cell><cell>16</cell><cell>Full</cell><cell>No</cell></row><row><cell>Weidenb. [27]</cell><cell>3D</cell><cell>180 ?</cell><cell>20</cell><cell>N/A</cell><cell>No</cell></row><row><cell>Zhu [34]</cell><cell>3D</cell><cell>180 ?</cell><cell>200</cell><cell>N/A</cell><cell>No</cell></row><row><cell>Gaze360 [ours]</cell><cell>3D</cell><cell>360 ?</cell><cell>238</cell><cell>Full</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on Gaze360 dataset. The table below reports the mean angular errors for various models and benchmarks on the Gaze360 test data. The last column shows the correlation between the actual error and the predicted uncertainty.</figDesc><table><row><cell>Model</cell><cell>Uncert. Loss</cell><cell>All 360 ?</cell><cell>Front 180 ?</cell><cell>Front Facing</cell><cell>Uncert. Corr.</cell></row><row><cell>Mean</cell><cell>-</cell><cell>59.0</cell><cell>40.5</cell><cell>19.0</cell><cell>-</cell></row><row><cell>Deep HP</cell><cell>-</cell><cell>49.3</cell><cell>30.7</cell><cell>22.7</cell><cell>-</cell></row><row><cell>MSE Static</cell><cell>No</cell><cell>15.8</cell><cell>13.7</cell><cell>13.4</cell><cell>-</cell></row><row><cell>MSE TRN</cell><cell>No</cell><cell>14.3</cell><cell>11.8</cell><cell>11.8</cell><cell>-</cell></row><row><cell>MSE LSTM</cell><cell>No</cell><cell>14.1</cell><cell>12.1</cell><cell>11.6</cell><cell>-</cell></row><row><cell>MSE+Drop Static</cell><cell>No</cell><cell>15.8</cell><cell>13.7</cell><cell>13.4</cell><cell>0.24</cell></row><row><cell>MSE+Drop TRN</cell><cell>No</cell><cell>14.3</cell><cell>11.8</cell><cell>11.8</cell><cell>0.31</cell></row><row><cell>MSE+Drop LSTM</cell><cell>No</cell><cell>14.1</cell><cell>12.1</cell><cell>11.6</cell><cell>0.31</cell></row><row><cell>Crop Aug. Static</cell><cell>No</cell><cell>16.0</cell><cell>13.2</cell><cell>12.6</cell><cell>0.37</cell></row><row><cell>Crop Aug. TRN</cell><cell>No</cell><cell>14.2</cell><cell>11.5</cell><cell>11.4</cell><cell>0.39</cell></row><row><cell>Crop Aug. LSTM</cell><cell>No</cell><cell>14.1</cell><cell>11.6</cell><cell>11.2</cell><cell>0.37</cell></row><row><cell>PinBall Static</cell><cell>Yes</cell><cell>15.6</cell><cell>13.4</cell><cell>13.2</cell><cell>0.42</cell></row><row><cell>PinBall TRN</cell><cell>Yes</cell><cell>14.1</cell><cell>11.7</cell><cell>11.6</cell><cell>0.46</cell></row><row><cell>Pinball LSTM (i.e., Gaze360)</cell><cell>Yes</cell><cell>13.5</cell><cell>11.4</cell><cell>11.1</cell><cell>0.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Cross-dataset evaluation: we report the mean angular errors for the Static model trained using different datasets.</figDesc><table><row><cell>Train</cell><cell>Test</cell><cell>Columbia</cell><cell>MPII FaceGaze</cell><cell>RT-GENE</cell><cell>Gaze360</cell></row><row><cell>Columbia</cell><cell></cell><cell>-</cell><cell>12.3</cell><cell>32.8</cell><cell>57.9</cell></row><row><cell cols="2">MPIIFaceGaze</cell><cell>12.4</cell><cell>-</cell><cell>26.5</cell><cell>57.8</cell></row><row><cell>RT-GENE</cell><cell></cell><cell>24.2</cell><cell>18.9</cell><cell>-</cell><cell>56.6</cell></row><row><cell>Gaze360</cell><cell></cell><cell>9.0</cell><cell>12.1</cell><cell>23.4</cell><cell>-</cell></row><row><cell cols="2">Gaze360 + DA</cell><cell>8.1</cell><cell>9.9</cell><cell>21.9</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Toyota Research Institute provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Non-verbal communication in human social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Argyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RT-GENE: Real-time eye gaze estimation in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A single camera eye-gaze tracking system with free head motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Hennessey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borna</forename><surname>Noureddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ETRA</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Screenglint: Practical, in-situ gaze estimation on smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajia</forename><surname>Michael Xuelin Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong Va</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tabletgaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Sabharwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchi</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mul-tiPoseNet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Quantile Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Koenker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An eye tracking dataset for point of gaze detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vangelis</forename><surname>Christopher D Mcmurrough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Metsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fillia</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Makedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ETRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Alberto Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRA</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robot reading human gaze: Why eye tracking is better than head tracking for human-robot collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oskar</forename><surname>Palinko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Rea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Sandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Sciutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5048" to="5054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno>abs/1710.00925</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaze locking: Passive eye contact detection for humanobject interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social cognition in the first year</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tricia</forename><surname>Striano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05203</idno>
		<title level="m">Seeing with humans: Gaze-assisted neural image captioning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learningby-synthesis for appearance-based 3d gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuuki</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AprilTag 2: Efficient and robust fiducial detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comprehensive head pose and gaze database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Weidenbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Layher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra-Maria</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IET International Conference on Intelligent Environments</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingmei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">Turkergaze: Crowdsourcing saliency with webcam based eye tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A novel nonintrusive eye gaze estimation using cross-ratio under large head motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung</forename><forename type="middle">Jin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">It&apos;s written all over your face: Full-face appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mpiigaze: Real-world dataset and deep appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="162" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular free-head 3d gaze tracking with deep learning and geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eye gaze tracking under natural head movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
