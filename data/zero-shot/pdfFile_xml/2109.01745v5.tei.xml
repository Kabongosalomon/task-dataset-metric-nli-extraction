<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A realistic approach to generate masked faces applied on two novel masked face recognition data sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-25">25 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><forename type="middle">Mare</forename><surname>Securifai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgian</forename><forename type="middle">Duta</forename><surname>Securifai</surname></persName>
							<email>georgian.duta@securifai.ro</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<email>georgescu_lily@yahoo.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">Sandru</forename><surname>Securifai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
							<email>bogdan.alexe@fmi.unibuc.ro</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<email>popescunmarius@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<email>raducu.ionescu@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">SecurifAI University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">SecurifAI University of Bucharest</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A realistic approach to generate masked faces applied on two novel masked face recognition data sets</title>
					</analytic>
					<monogr>
						<title level="m">35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks</title>
						<imprint>
							<date type="published" when="2021-10-25">25 Oct 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on SparkAR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254 (96.8%) masks for the CelebA data set, releasing the mask images at https://github.com/securifai/masked_faces. We show that our method produces significantly more realistic training examples of masks overlaid on faces by asking volunteers to qualitatively compare it to other methods or data sets designed for the same task. We also demonstrate the usefulness of our method by evaluating state-of-the-art face recognition systems (FaceNet, VGG-face, ArcFace) trained on our enhanced data sets and showing that they outperform equivalent systems trained on original data sets (containing faces without masks) or competing data sets (containing masks generated by related methods), when the test benchmarks contain masked faces.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State-of-the-art systems <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b12">14]</ref> for face recognition, verification or clustering have achieved impressive results in recent years. These systems are trained on data sets whose dimensions have rapidly increased in the last years. Indeed, the number of images of faces of different identities has grown from a few thousands in 2007 (e.g., LFW <ref type="bibr" target="#b6">[8]</ref> contains ? 13, 000 images) to hundreds of thousands in 2015 (e.g., CASIA-WebFace <ref type="bibr" target="#b14">[16]</ref> and CelebA <ref type="bibr" target="#b9">[11]</ref> contain ? 500, 000 images and ? 200, 000 images, respectively) or even millions of images in 2021 (e.g., WebFace260M <ref type="bibr" target="#b17">[19]</ref> and WebFace42M <ref type="bibr" target="#b17">[19]</ref> have ? 260 million and ? 42 million images, respectively). All these data sets have been released in the past 15 years, with much effort being put into collecting and cleaning them. Remarkably, some recognition models perform so well that they can even spot errors in the annotations (see <ref type="bibr" target="#b12">[14]</ref>). However, these models are trained based on the well-known machine learning assumption that training and testing examples follow the same distribution. With the COVID-19 pandemic hitting the entire world in early 2020, we have witnessed a sharp change in the way people across the globe live. People started to wear masks to cover their noses and mouths, keeping them safe from getting infected with the SARS-CoV-2 virus. Hence, the new safety measures generated a change in the distribution of face appearances, which is no longer the same as the one before the pandemic. Thus, the practical use in real scenarios of face recognition systems trained on data lacking masked faces is currently questioned. Intuitively, the discriminative power of such recognition systems, which might reside in analyzing some primary features localized around the mouth, nose or cheeks, could significantly degrade as these regions are now masked. Face masks vary in shape, color, texture or fabrics, and convey almost no information about the covered face.</p><p>The success of modern algorithms for face recognition relies heavily on training complex deep learning models <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b12">14]</ref> on large-scale data sets <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b14">16]</ref>. Unfortunately, current data sets <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18]</ref> with masked faces are a few orders of magnitude smaller than the ones containing regular faces, without masks. To the best of our knowledge, the largest data sets <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b13">15]</ref> containing masked faces contain less than 10, 000 examples.</p><p>We address the lack of data sets containing images with masked faces by proposing a fully automatic method to enhance data sets containing faces without masks. Our method relies on SparkAR Studio [1], a developer program made by Facebook that is typically used to create Instagram face filters. We use SparkAR Studio to create synthetic masks and overlay them on faces in the original images. In our approach, we consider 9 different types of masks of various colors, shapes and textures (see <ref type="figure">Figure 1</ref>). Our approach can be run on any data set containing images of faces without masks. In particular, we have applied our method for generating images with synthesized masks for the CelebA <ref type="bibr" target="#b9">[11]</ref> and CASIA-WebFace <ref type="bibr" target="#b14">[16]</ref> data sets. We release the images with mask overlays at https://github.com/securifai/masked_faces along with a script that superimposes the masks over the original CelebA and CASIA-WebFace images.</p><p>We conduct a human evaluation study for which we designed a protocol to assess how realistic the generated masks look, comparing the output of different methods that synthetically generate masks and overlay them on faces. The results show that our method produces significantly more realistic images with synthesized masked faces when compared to competing methods <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref>. We present an in-depth analysis in Section 4.</p><p>Our enhanced data sets (containing masked faces) can be used in training state-of-the-art face recognition systems in order to capture the new distribution in appearance of masked faces. We show that face recognition systems trained on the enhanced data sets attain superior performance in the matter of analyzing images with masked faces. Moreover, we present empirical evidence indicating that, when it comes to recognizing faces covered by real masks, our synthetically generated masks provide superior results compared with a baseline based on blacking out pixels, as well as the most realistic mask generation competitor <ref type="bibr" target="#b0">[2]</ref>.</p><p>To summarize, the main contributions of our paper are the following:</p><p>? We propose an automatic method for synthesizing masks, applying a series of tools and algorithms (SparkAR Studio, Dlib-ml, ColorMatcher, custom verification scripts) to ensure the realism of the generated masks.</p><p>? We release two enhanced data sets containing a total of ? 640, 000 examples of masked faces.</p><p>Figure 1: Mask database. Our mask database contains 9 masks that vary in color, shape and texture. Best viewed in color.</p><p>? We qualitatively compare the output of our method with the output of several competing methods in terms of how realistic the generated masked faces look.</p><p>? We show that state-of-the-art face recognition systems can perform better in recognizing people with masked faces when trained on our enhanced data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The masked face recognition task is quite new in literature as it emerged as a natural problem to be solved after the COVID-19 pandemic broke out. Publicly available data sets containing real masked faces are rare <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18]</ref> and their sizes are usually by a few orders of magnitude smaller than the large-scale data sets with regular faces. This is due to the complex pipeline of face pre-processing, cleaning the data set, and also carefully selecting images containing real masked faces such that the distribution of collected examples reflects the diversity in race, ethnicity, gender, age, pose, mask types, and so on.</p><p>The work of Wang et al. <ref type="bibr" target="#b13">[15]</ref> was one of the earliest to propose a data set with real masked faces. Their data set, entitled the Real-World Masked Face Recognition Dataset (RMFRD), includes 5, 000 images of 525 subjects wearing masks and 90, 000 images of the same 525 subjects without masks. Anwar et al. <ref type="bibr" target="#b0">[2]</ref> proposed the MFR2 data set, which contains a total of 269 images with 53 identities of celebrities and politicians collected from the Internet. Each identity has 5 images on average, with both masked and unmasked faces. The authors of <ref type="bibr" target="#b3">[5]</ref> employed several data sets for testing, among them introducing the PKU-Masked-Face data set. This data set contains 10, 301 face images of 1, 018 different identities. For each identity, there are 5 images with regular faces and 5 images with masked faces with various orientations, lighting conditions and mask types. The Masked Face Recognition Challenge and Workshop organized in conjunction with ICCV 2021 proposes several benchmarks <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b16">18]</ref> for deep face recognition methods under the existence of facial masks. In the InsightFace track <ref type="bibr" target="#b1">[3]</ref>, the organizers provided a test set with 6, 964 real masked facial images and 13, 928 non-masked facial images of 6, 964 identities. However, in order to avoid data privacy problems, the test set is not publicly available yet. In the WebFace track <ref type="bibr" target="#b16">[18]</ref>, the organizers provided a test benchmark of a total of 60, 926 images with masked and unmasked faces. However, there are only 3, 211 images containing masked faces of 862 identities.</p><p>With algorithms relying heavily on large-scale data sets for training and with the lack of publicly available data sets with real masked faces, researchers have tried to find alternative ways for solving this issue. A simple alternative is to synthetically generate masks and overlay them on faces. This approach holds the promise of producing large-scale synthesized masked face data sets based on automatic algorithms. Wang et al. <ref type="bibr" target="#b13">[15]</ref> proposed a simple algorithm based on the Dlib-ml library <ref type="bibr" target="#b7">[9]</ref> for overlaying synthesized masks on faces. They generated the Simulated Masked Face Recognition Dataset (SMFRD) containing 500, 000 faces of 10, 000 subjects, by applying their algorithm on the LFW <ref type="bibr" target="#b6">[8]</ref> and CASIA-WebFace <ref type="bibr" target="#b14">[16]</ref> data sets. Anwar et al. <ref type="bibr" target="#b0">[2]</ref> proposed the open-source tool entitled MaskTheFace to mask faces in images. MaskTheFace relies on the face landmark detector from the same library, Dlib-ml, in order to estimate the face tilt and identify six key facial features needed for applying the mask. The face tilt is used to select a template mask with a pre-defined orientation, from a database of 7 mask types, 3 orientations and 27 textures. The six key features of the face are needed at aligning the selected mask such that it fits on the face. The work of Huang et al. <ref type="bibr" target="#b5">[7]</ref> introduced Webface-OCC, a simulated occluded face recognition data set with 804, 704 face images of 10, 575 subjects. This data set is built on the CASIA-WebFace data set, combining faces with simulated occlusions with their original unmasked counterparts. The occluded faces are obtained by covering the original faces with a range of occluding objects (e.g., glasses, masks) that vary in texture and color. Ding et al. <ref type="bibr" target="#b3">[5]</ref> applied a similar data augmentation method for training data, automatically generating synthetic masked face images on LFW. They used the Delaunay triangulation algorithm to overlay different masks on the faces based on the automatically detected facial landmarks.</p><p>In Section 4, we compare our approach with competing methods <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref> in terms of how realistic the synthetically generated face masks are, showing that our approach generates significantly more realistic images of masked faces. This is the main differentiating factor from our competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Synthetic generation of masked face data sets</head><p>Our method for synthetic generation of masks relies on SparkAR Studio [1]. SparkAR Studio is a developer program made by Facebook that enables users to create augmented reality effects. Its practical use resides in creating Instagram face filters. In particular, SparkAR can be used to detect a 3D face in an input image and then apply highly realistic shadings and shadow effects on the detected face. Based on these features, we consider SparkAR to be an useful tool in creating realistic synthesized masks and overlaying them on real faces. We employ it to enhance the CASIA-WebFace <ref type="bibr" target="#b14">[16]</ref> and CelebA <ref type="bibr" target="#b9">[11]</ref> data sets with masked faces, as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthetic mask generation</head><p>For a given input image containing a face, our goal is to create a synthesized mask that looks as realistic as possible when overlaid on the face, as illustrated in <ref type="figure">Figure 2</ref>(d). Our mask database consists of 9 masks which vary in color, shape and texture (see <ref type="figure">Figure 1</ref>). <ref type="figure">Figure 2</ref>: Overview of our approach. For the original image (a), we select a random mask and employ SparkAR to transform it (b) based on the 3D face model. We adjust the color of the mask (c) and thus obtain the output of our method. Further, we can overlay the mask (c) over the original image to obtain the masked face image (d). Best viewed in color.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3:</head><p>Examples showcasing key features of our method. Our method relies on SparkAR to correctly predict face positions for various poses of the subjects, even when they are looking in a direction perpendicular to the camera. In such cases, the corresponding parts of the mask are automatically occluded. SparkAR can also recognize facial expressions and modify the 3D mask accordingly, e.g. stretching the mask when appropriate. Color matching the original image helps to blend in the mask over the context image, thus creating a more realistic sample. Best viewed in color.</p><p>We employ the SparkAR software to detect the 3D face in the input image and randomly select a mask out of the existing 9 masks in the database to be mapped over the 3D face. Next, we use SparkAR to transform the selected mask such that it fits well on the 3D face model and then save the processed mask using a green screen background (see <ref type="figure">Figure 2</ref>(b)). This step is necessary to avoid publicly sharing any data from the original CelebA and CASIA-WebFace data sets, which are protected by license agreements that do not permit data redistribution. In order to overlay the mask on the original face, we subtract the green background and compute color matching between the segmented mask and the face. For this step, we use the Color Matcher plugin provided by After Effects. The transformed mask with transparent background and adjusted colors is the output of our method. An output example can be visualized in <ref type="figure">Figure 2</ref>(c). By overlaying the mask on the original image, we obtain the masked face image, as shown in <ref type="figure">Figure 2</ref>(d). To make sure that the masks generated with SparkAR are positioned well, we employ a verification step based on a method that detects facial landmarks, which is implement in Dlib-ml <ref type="bibr" target="#b7">[9]</ref>. Having the facial landmarks, we derive a polygon that should roughly coincide with the region covered by the mask. Then, we compute the Intersection over Union (IoU) between this polygon and the created mask. If the computed IoU has a value lower than a fixed threshold (0.3 for CelebA and 0.5 for CASIA-WebFace), we reprocess the image by employing the Dlib-ml library to apply the mask over the input image.</p><p>Even though SparkAR is very powerful, there are images on which SparkAR cannot recognize the 3D face. In this case, we also fallback to the Dlib-ml library to obtain the synthesized mask.</p><p>CelebA+masks. For CelebA, we consider the keypoints provided with the data set to verify if each mask produced by SparkAR (or Dlib-ml) is correctly placed over the corresponding face. Following the described protocol, we generate the CelebA+masks data set with 196, 254 synthesized masks, which represents 96.8% of the total number of images in the CelebA data set. About 5.4% of these masks are obtained using the Dlib-ml library. We underline that there are some cases when both SparkAR and Dlib-ml fail to produce a mask and this accounts for the remaining 3.2% of all images in the CelebA data set.</p><p>CASIA-WebFace+masks. We use Dlib-ml to generate the face keypoints for the verification step and proceed in the same way (as for CelebA) to remove poor examples of generated masks. We employ our method to generate the CASIA-WebFace+masks data set containing 445, 446 synthesized masks, which represents about 90% of the total number of images in the CASIA-WebFace data set. Roughly 11.5% of the masks are obtained using the Dlib-ml library, which is almost double the percentage of images generated with Dlib-ml for the CelebA data set. We believe that CASIA-WebFace is much more diverse context-wise, presenting a higher difficulty for SparkAR in detecting the 3D face in the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Key features</head><p>Our approach presents a few key features that enable synthetic generation of realistic masks. SparkAR proves to be a powerful tool, as it correctly recognizes the 3D position and rotation of faces in almost 90% of the processed images. This makes our approach robust and flexible compared to other approaches <ref type="bibr" target="#b0">[2]</ref> that are bounded to using a number of predefined directions of the head, by design. From the samples illustrated in <ref type="figure">Figure 3</ref>, we can infer that SparkAR can predict the face position even when the subject is looking in a direction perpendicular to the camera. SparkAR also recognizes facial expressions and modifies the 3D mask accordingly. Therefore, we can expect the mask to cover the subject's face while they are laughing, smiling, yawning, etc. The 3D face mask can also produce occlusions on itself. Thus, when someone turns the head at an angle, a certain part of the mask may get occluded if it is not visible from the camera's viewpoint, creating a realistic image. We place the masks over the context using a smoothed alpha channel. Through this procedure, we eliminate the white jagged edges that appear when applying the mask over the original image. Color matching the images helps to blend in the mask over the context image, thus creating a more realistic sample. Because our mask images have 4 channels (RGBA), we can apply any changes to the color channels and we can use the alpha channel as a segmentation mask when training machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Limitations</head><p>Even though we employ a verification step for our generated masks using Dlib-ml, failure mask generation cases can still be present in our generated data sets, as shown in <ref type="figure" target="#fig_0">Figure 4</ref>. The poor samples can be caused by multiple factors: (i) SparkAR might detect another face than the one of interest (see <ref type="figure" target="#fig_0">Figure 4</ref>(a)); (ii) SparkAR gets an inaccurate 3D face model and the transformed mask covers the face of interest only partially (see <ref type="figure" target="#fig_0">Figure 4</ref>(b)) or not at all (see <ref type="figure" target="#fig_0">Figure 4</ref>(c)); (iii) while the SparkAR software generates masks, we employ a third-party recording software (OBS) to record the screen, which might introduce unwanted artifacts, such as the one illustrated in <ref type="figure" target="#fig_0">Figure 4(d)</ref>. We manually inspected a randomly sampled subset that accounts for about 1% of the generates images from each of the two data sets. Based on the manual annotation of two of the paper co-authors, we estimate that CASIA-WebFace+masks contains about 2.83% failure cases, while CelebA+masks contains about 2.90% failure cases. We underline that this problem is also prevalent in related data sets of masked faces, although not mentioned in the introductory papers. In our case, we tried to  minimize this percentage by employing SparkAR and Dlib-ml, in a cascaded fashion. We would like to add that the faulty images illustrated in <ref type="figure" target="#fig_0">Figure 4</ref> do not represent the actual distribution of fault types. Indeed, most faults (around 90%) are caused by masks not covering the entire face, this being the most acceptable type of error, in our opinion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Quantitative and qualitative evaluation of mask generation methods</head><p>Automatically generating realistic images with synthesized masks overlaid on faces holds the promise to address the lack of large-scale data sets of masked faces.</p><p>We qualitatively compare our approach to the methods presented in <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref> in terms of how realistic the generated images with synthesized masks overlaid on faces are. For each of the three comparisons, we select a random subset of 200 image pairs. Each image pair consists of two images with synthesized masks overlaid on faces. One image is the output of our method, while the other image is the output of one of the three methods <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref> that we compare with. The position of the images displayed in the application interface (left-hand or right-hand side) is randomly picked every time, such that the annotator does not know which method is shown in the left-hand side or in the right-hand side.</p><p>When presented with an image pair, annotators are asked to select the image that looks more realistic. For our method, we consider a fixed set of 200 images containing masked faces for all three comparisons. The set is randomly chosen from CelebA+masks. For Anwar et al. <ref type="bibr" target="#b0">[2]</ref>, we run their open-source software, MaskTheFace, on the randomly chosen subset of 200 images. For the other two competing methods <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b13">15]</ref>, which do not share code to produce masked faces, we randomly select 200 images from their released data sets. More precisely, we take images from the Simulated Masked Face Recognition <ref type="bibr" target="#b13">[15]</ref> and the Webface-OCC <ref type="bibr" target="#b5">[7]</ref> data sets.</p><p>We collect data from 6 impartial annotators and obtain a total of 3, 600 annotations (6 annotators ? 200 image pairs ? 3 comparative studies). <ref type="table" target="#tab_1">Table 1</ref> shows quantitative results of our human evaluation study. For each comparison and annotator, we show the number of votes for each of the two methods involved in the comparison. When compared to <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b13">15]</ref>, the annotators picked our method in more than 97% of the times (on average), which is remarkable. Among our three competitors, Anwar</p><p>Wang et al. <ref type="bibr" target="#b13">[15]</ref> Our favorable (typical) cases Our failure (rare) cases</p><p>Huang et al. <ref type="bibr" target="#b5">[7]</ref> Anwar et al. <ref type="bibr" target="#b0">[2]</ref> Ours <ref type="figure">Figure 5</ref>: Comparing the degree of realism of images with synthetically masked faces. We show six favorable cases and two failure cases for our comparative study. Our images (last row) are compared with the images produced by the competitors <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref> (first three rows). Image pairs are shown column-wise. Examples shown on the first six columns represent the usual cases, when our method is superior to the other methods (see <ref type="table" target="#tab_1">Table 1</ref>). Examples shown on the last two columns represent rare cases, when our method is inferior to one of the competitors. Best viewed in color. et al. <ref type="bibr" target="#b0">[2]</ref> gained the highest number of votes. Still, our method was voted in the majority of cases (87.83%) when it is was compared to that of Anwar et al. <ref type="bibr" target="#b0">[2]</ref>. According to the annotators, we conclude that our method produces significantly more realistic examples of faces with masks.</p><p>In <ref type="figure">Figure 5</ref>, we illustrate a set of typical cases when our method is considered superior in producing more realistic images than the competitors, along with some rare cases when our method is inferior to one of the competitors, especially Anwar et al. <ref type="bibr" target="#b0">[2]</ref>. From the displayed examples, we observe that the method of Wang et al. <ref type="bibr" target="#b13">[15]</ref> does a very poor job at aligning the masks to the corresponding faces, while the method of Huang et al. <ref type="bibr" target="#b5">[7]</ref> fails to properly blend in the mask, leaving clear patterns indicating that the overlaid masks are not realistic. Confirming the results in <ref type="table" target="#tab_1">Table 1</ref>, the masked faces shown in <ref type="figure">Figure 5</ref> indicate that Anwar et al. <ref type="bibr" target="#b0">[2]</ref> is the strongest competitor. Nevertheless, our method is able to more realistically align and blend in the masks. In summary, we underline that the qualitative results shown in <ref type="figure">Figure 5</ref> are in perfect agreement with the quantitative results presented in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental evaluation of face recognition systems</head><p>We evaluate the practical usage of our method by fine-tuning three face recognition systems from the literature, namely FaceNet <ref type="bibr" target="#b12">[14]</ref>, VGG-face <ref type="bibr" target="#b10">[12]</ref> and ArcFace <ref type="bibr" target="#b2">[4]</ref>, on masked face data sets generated by our method. We then test the face recognition models on data sets with faces covered by synthesized or real masks. We conduct our evaluation in the scenario of face verification: given a pair of two faces (i,j), the task is to decide whether faces i and j are of the same identity or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sets.</head><p>As training data, we consider the training splits of the CelebA and CASIA-WebFace data sets, but the images are replaced with our masked faces from CelebA+masks and CASIA-WebFace+masks. Similarly, for testing, we consider the test splits of CelebA and CASIA-WebFace, replacing the original images with the corresponding ones from our data sets. Additionally, we report results on the MFR2 real masked face data set introduced by Anwar et al. <ref type="bibr" target="#b0">[2]</ref>.</p><p>Evaluation protocol. We follow the standard protocol for unrestricted, labeled outside data, as mentioned in <ref type="bibr" target="#b6">[8]</ref>, to evaluate the image pairs. The L 2 distance threshold between two embeddings is calculated on the nine folds of the underlying test set and used on the tenth test fold to report the accuracy, in a 10-fold cross-validation manner.</p><p>Performance metrics. We adopt the notation in <ref type="bibr" target="#b12">[14]</ref> and consider, for a given test set of image pairs, P same to be the set of all face pairs (i,j) of the same identity and P diff to be the set of all face pairs of different identities. The set of pairs correctly classified as same at threshold d is the set of true accepts defined as: TA(d) = {(i, j) ? P same , D(x i , x j ) ? d}, where x i and x j are the neural embeddings of the faces i and j and D is the squared L 2 distance. The set of pairs incorrectly classified as same at threshold d represents the set of false accepts, which is defined as:</p><formula xml:id="formula_1">FA(d) = {(i, j) ? P diff , D(x i , x j ) ? d}.</formula><p>For the threshold d, we select the value that provides the maximum accuracy on the validation folds, considering values in the range [0, 4] with a step of 0.01. We compute the accuracy on the test fold for the optimum threshold.</p><p>Implementation details. Prior to the fine-tuning and evaluation of the face recognition systems, we apply the MTCNN <ref type="bibr" target="#b15">[17]</ref> face detector, choosing the largest bounding box whenever there are multiple face detections in an image. We apply face registration only for ArcFace. The face recognition systems are trained using the original hyperparameters reported in the introductory works <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b12">14]</ref>. For FaceNet <ref type="bibr" target="#b12">[14]</ref>, we select the Inception-ResNet-v1 backbone, while for ArcFace <ref type="bibr" target="#b2">[4]</ref>, we select the ResNet-50 backbone.</p><p>Evaluation on the CelebA+masks data set. After applying our method to enhance the data set with masks, we obtain 158, 256 training faces of 8, 190 identities and 19, 494 test faces of 1, 000 identities. We train the face recognition systems on the train split in two scenarios: on original images when no face masks are provided, and on images modified by our method with synthesized masks. We test the face recognition systems on the test split of images with synthesized masks.</p><p>Evaluation on the CASIA-WebFace+masks data set. After applying our method to enhance the data set with masks, we obtain 264, 168 training faces of 6, 345 identities and 93, 235 test faces of 2, 115 identities. We train the face recognition systems in an analogous manner to the CelebA+masks case. We evaluate the face recognition systems on the test split of images with generated masks.</p><p>Evaluation on the MFR2 data set. We apply the CelebA models on the real images from the MFR2 <ref type="bibr" target="#b0">[2]</ref> data set. On this data set, we consider two additional baselines for each face recognition model, one that is fine-tuned on CelebA+MaskTheFace <ref type="bibr" target="#b0">[2]</ref>, and one that is fine-tuned on our mask segments with black pixels instead of textures.</p><p>Quantitative results. The results of the face recognition systems are presented in <ref type="table">Table 2</ref>. While there is a clear ranking between the evaluated face recognition models, the best one being ArcFace and the least competitive one being VGG-face, we observe that fine-tuning on realistically generated faces is beneficial in each and every case. The improvements brought by our approach are always above 2%. We thus conclude that our approach is useful in increasing the performance of face recognition systems on masked faces.</p><p>To assess the usefulness of our mask generation approach with respect to existing masked face data sets in training state-of-the-art masked face recognition systems, we conduct an additional experiment on MFR2. Accordingly, we use the MaskTheFace open-source tool of Anwar et al. <ref type="bibr" target="#b0">[2]</ref> (which is the second-most realistic according to <ref type="table" target="#tab_1">Table 1</ref>) to generate synthetic masks for the CelebA data set, obtaining a set of 194, 614 masked face images. We train the three face recognition models on the resulting data set (CelebA-train+MaskTheFace) and report the accuracy on the MFR2 test set (containing real faces). The VGG-face trained on CelebA-train+MaskTheFace yields an accuracy of 88.35%, which is nearly 3% lower than the accuracy of the VGG-face trained on our masked faces. We observe a similar pattern when training FaceNet and ArcFace on CelebA-train+MaskTheFace instead of our version of CelebA-train, the only difference being that the performance gap is around 1% for both models. This clearly indicates that having more realistic face masks is useful.</p><p>We also carry out an experiment by training the face recognition models on a version of CelebA-train that uses our mask overlays, but replaces all pixels inside the mask with the black color. We test the resulting models on MFR2, observing that there are significant performance drops (around 3%) for  <ref type="table">Table 2</ref>: Face recognition results of FaceNet, VGG-face and ArcFace on masked faces, when the models are trained on original and masked face images. The accuracy rates are reported for both synthetically generated as well as real masked faces.</p><p>two face recognition models, namely FaceNet and VGG-face. We believe that this observation can be explained by the fact that the actual masks reveal some information about the generic shape of the lower part of the face, which is useful for face recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion.</head><p>A relevant aspect to discuss in the context of masked face recognition is the performance level of models fine-tuned on masked faces, when the evaluation is performed on normal (unmasked) faces. Most likely, the performance on normal faces will not be maintained to the level attained by pre-trained face recognition models. However, we do not consider this as a problem, since we can employ a binary classifier to distinguish between normal and masked faces, and then use the appropriate face recognition system. Our preliminary results based on a MobileNetV2 architecture <ref type="bibr" target="#b11">[13]</ref> show that we can achieve an accuracy of 99.44% for classifying normal faces versus masked faces (synthesized or real). We believe this guarantees the same level of performance, regardless of the occlusion level (mask or no mask).</p><p>In addition to the utility for face recognition systems, we underline that our masked face data sets can be used to train models for other tasks as well, e.g. masked face detection or facial landmark detection of masked faces. This significantly broadens the applicability of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed an automatic method for the synthetic generation of masks using SparkAR Studio for the goal of enhancing data sets with masked faces. We showed that our approach is superior in producing more realistic images than competitors <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref>. For the face verification task, we demonstrated that face recognition systems <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b12">14]</ref> fine-tuned on masked faces perform better than the ones trained on regular faces, when the evaluation is conducted on masked faces. We also showed that the usefulness of masked faces for face recognition systems is proportional to the degree of realism of the synthesized masks.</p><p>In future work, we aim to propose more elaborate ways of taking advantage of our synthetically generated masked face data sets to improve face recognition models. One approach could be to consider knowledge distillation, as in <ref type="bibr" target="#b4">[6]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Failure cases. In less than 3% of the images, the data sets might exhibit the following errors: (a) mask applied on wrong person, (b) mask covering the face only partially, (c) mask not covering the face, (d) compression artifacts. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparative results showing the number of votes for our approach versus the number of votes for each of the three state-of-the-art methods [2, 7, 15]. Each comparative study is based on 200 sample pairs and it was completed by 6 human annotators.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>96% ? 1.13% 84.56% ? 1.40% 91.78% ? 0.56% CelebA-train+masks (ours) 93.58% ? 0.82% 91.51% ? 0.97% 95.43% ? 0.78% Test set: CASIA-WebFace-test+masks (synthetic) CASIA-WebFace-train (no masks) 84.21% ? 1.49% 79.65% ? 1.81% 87.95% ? 1.44% CASIA-WebFace-train+masks (ours) 88.06% ? 1.27% 86.85% ? 1.11% 91.47% ? 0.85% Test set: MFR2 (real) CelebA-train (no masks) 92.20% ? 1.92% 84.29% ? 4.00% 91.39% ? 3.64% CelebA-train+black pixel masks 93.04% ? 2.33% 88.31% ? 3.08% 94.22% ? 2.31% CelebA-train+MaskTheFace [2] 95.16% ? 1.88% 88.35% ? 2.61% 94.33% ? 1.55% CelebA-train+masks (ours) 96.22% ? 1.91% 91.26% ? 2.19% 95.16% ? 2.90%</figDesc><table><row><cell>Train set</cell><cell>Test set: CelebA-test+masks (synthetic) FaceNet VGG-face</cell><cell>ArcFace</cell></row><row><cell>CelebA-train (no masks)</cell><cell>90.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Masked Face Recognition for Secure Authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aqeel</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Raychowdhury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11104</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Masked Face Recognition Challenge: The InsightFace Track Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Workshops (ICCVW)</title>
		<meeting>the International Conference on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1437" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Masked Face Recognition with Latent Part Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2281" to="2289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teacher-Student Training and Triplet Loss for Facial Expression Recognition under Occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><forename type="middle">Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu Tudor</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">When Face Recognition Meets Occlusion: A New Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangli</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech &amp; Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech &amp; Signal Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Faces in Real-Life Images: Detection, Alignment, and Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dlib-ml: A Machine Learning Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SphereFace: Deep Hypersphere Embedding for Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="41" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxi</forename><surname>Wang Nad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjiao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09093</idno>
		<title level="m">Zhibing Huang, and Jinbi Liang. 2020. Masked Face Recognition Dataset and Application</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning Face Representation from Scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07189</idno>
		<title level="m">Dalong Du, and Jie Zhou. 2021. Masked Face Recognition Challenge: The WebFace260M Track Report</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10492" to="10502" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
