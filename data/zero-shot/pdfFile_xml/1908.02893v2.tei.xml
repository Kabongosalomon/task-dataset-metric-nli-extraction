<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EdgeNet: Semantic Scene Completion from a Single RGB-D Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aloisio</forename><surname>Dourado</surname></persName>
							<email>aloisio.dourado.bh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Brasilia Brasilia</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teofilo</forename><surname>Emidio De Campos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Brasilia Brasilia</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansung</forename><surname>Kim</surname></persName>
							<email>h.kim@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
							<email>a.hilton@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EdgeNet: Semantic Scene Completion from a Single RGB-D Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic scene completion is the task of predicting a complete 3D representation of volumetric occupancy with corresponding semantic labels for a scene from a single point of view. In this paper, we present EdgeNet, a new end-toend neural network architecture that fuses information from depth and RGB, explicitly representing RGB edges in 3D space. Previous works on this task used either depth-only or depth with colour by projecting 2D semantic labels generated by a 2D segmentation network into the 3D volume, requiring a two step training process. Our EdgeNet representation encodes colour information in 3D space using edge detection and flipped truncated signed distance, which improves semantic completion scores especially in hard to detect classes. We achieved stateof-the-art scores on both synthetic and real datasets with a simpler and a more computationally efficient training pipeline than competing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The ability of reasoning about scenes in 3D is a natural task for humans, but remains a challenging problem in Computer Vision <ref type="bibr" target="#b5">[1]</ref>. Knowing the complete 3D geometry of a scene and the semantic labels of each 3D voxel has many practical applications, like robotics and autonomous navigation in indoor environments, surveillance, assistive computing and augmented reality.</p><p>Currently available low cost RGB-D sensors generate data form a single viewing position and cannot handle occlusion among objects in the scene. For instance, in the scene depicted on the left part of <ref type="figure">Figure 1</ref>, parts of the wall, floor and furniture are occluded by the bed. There is also self-occlusion: the interior of the bed, its sides and its rear surfaces are hidden by the visible surface.</p><p>Given a partial 3D scene model acquired from a single RGB-D image, the goal of scene completion is to generate a complete 3D volumetric representation where each voxel is labelled as occupied by some object or free space. For occupied voxels, the goal of semantic scene completion is to assign a label that indicates to which class of object it belongs, as illustrated on the right part of <ref type="figure">Figure 1</ref>.</p><p>Before 2018, most of the work on scene reasoning only partially addressees this problem. A number of approaches only infer labels of the visible surfaces <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b7">[3]</ref>, <ref type="bibr" target="#b8">[4]</ref>, while others only consider completing the occluded part of the scene, without semantic labelling <ref type="bibr" target="#b9">[5]</ref>. Another line of work focuses on single objects, without the scene context <ref type="bibr" target="#b10">[6]</ref>.</p><p>The term semantic scene completion was introduced by Song et al. <ref type="bibr" target="#b11">[7]</ref>, who showed that scene completion and semantic labelling are intertwined and training a CNN to jointly deals with both tasks can lead to better results. Their approach only uses depth information, ignoring all information from RGB channels. Colour information is expected to be useful to distinguish objects that approximately share the same plane in the 3D space, and thus, are hard to be distinguished using only depth. Examples of such instances are flat objects attached to the wall, such as posters, paintings and flat TVs. Some types of closed doors and windows are also problematic for depth-only approaches.</p><p>Recent research also explored colour information from on RGB-D images to improve semantic scene completion scores. Some methods project colour information to 3D in a naive way, leading to a problem of data sparsity in the voxelised data that is fed to the 3D CNN <ref type="bibr" target="#b12">[8]</ref>, while others uses RGB information to train a 2D segmentation network and then project generated features to 3D, requiring a complex two step training process <ref type="bibr" target="#b13">[9]</ref>, <ref type="bibr" target="#b14">[10]</ref>.</p><p>Our work focuses on enhancing semantic scene segmentation scores using information from both depth and colour of RGB-D images in an end-to-end manner. In order to address the RGB data sparsity issue, we introduce a new strategy for encoding information extracted from RGB image in 3D space. We also present a new end-to-end 3D CNN architecture to combine and represent the features from colour and depth. Comprehensive experiments are conducted to evaluate the main aspects of the proposed solution. Results show that our fusion approach can enhance results of depth-only solutions and that EdgeNet achieves equivalent performance to current state-of-the-art fusion approach, with a much simpler training protocol.</p><p>To summarise, our main contributions are:</p><p>? EdgeNet, a new end-to-end CNN architecture that fuses depth, RGB edge information to achieve state-of-the-art performance in semantic scene completion with a much simpler approach; ? a new 3D volumetric edge representation using flipped signed-distance functions which improves performance and unifies data agregation for semantic scene completion from RGBD;  <ref type="figure">Fig. 1</ref>: Semantic scene completion. Given an RGB-D image, the goal is to infer a complete 3D occupancy grid with associated semantic labels.</p><p>? a more efficient end-to-end training pipeline for semantic scene completion with relation to previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Scene semantic completion in 3D is a problem that was introduced relatively recently. Previous approaches to 3D SSC rely on Fully Convolutional Neural Network architectures (FCNs, introduced in <ref type="bibr" target="#b15">[11]</ref>) and use SUNCG and NYUDv2 as training sources (these datasets are described in Section IV-A). We classify approaches into three main groups, based on the type of input of the semantic completion CNN: depth maps only, depth maps plus RGB and depth maps plus 2D segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth maps only</head><p>Song et al. <ref type="bibr" target="#b11">[7]</ref> used depth maps from the SUNCG synthetic dataset to train a typical contracting fully convolutional CNN with 3D dilated convolutions, called SSCNet. They showed that jointly training for segmentation and completion leads to better results, as both tasks are inherently intertwined. To deal with data sparsity after projecting depth maps from 2D to 3D, the authors used a variation of Truncated Signed Distance Function (TSDF) that they called Flipped TSDF (F-TSDF). Zhang et al. <ref type="bibr" target="#b16">[12]</ref> used dense conditional random field to enhance SSCNet results. Guo and Tong <ref type="bibr" target="#b17">[13]</ref> applied a sequence of 2D convolutions to the depth maps, used a projection layer to projected the features to 3D and feed the output to a 3D CNN.</p><p>All solutions in this category are end-to-end approaches, in other words, the network is trained as a whole, with no need for extra training stages for specific parts. EdgeNet is an endto-end network as well. RGB edges are aggregated in the same training pipeline of the depth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth maps plus RGB</head><p>Guedes et al. <ref type="bibr" target="#b12">[8]</ref> reported preliminary results obtained by adding colour to an SSCNet-like architecture. In addition to the F-TSDF encoded depth volume, they used three extra projected volumes, corresponding to the channels of the RGB image, with no encoding, resulting in 3 sparse volumetric representation of the partially observed surfaces. The authors reported no significant improvement using the colour information in this sparse manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth maps plus 2D segmentation</head><p>Models in this category use a two step training protocol, where a 2D segmentation CNN is first trained and then it is used to generate input to a 3D semantic scene completion CNN. Current models differ in the way the generated 2D information is fed into the 3D CNN.</p><p>Garbade et al. <ref type="bibr" target="#b13">[9]</ref> used a pre-trained 2D segmentation CNN with a fully connected CRF <ref type="bibr" target="#b18">[14]</ref> to generate a segmentation map, which, after post-processing, was projected to 3D. Liu et al. <ref type="bibr" target="#b14">[10]</ref> used depth maps and RGB information as input to an encoder-decoder 2D segmentation CNN. The encoder branch of the 2D CNN is a ResNet-101 <ref type="bibr" target="#b19">[15]</ref> and the decoder branch contains a series of dense upsampling convolutions. The generated features from the 2D CNN are then reprojected to 3D using camera parameters, before being fed into a 3D CNN. The authors showed results using 2 different strategies to fuse depth and RGB: SNetFusion performs fusion just after the 2D segmentation network, while TNetFusion only performs fusion after the 3D convolutional network. TNetFusion achieves higher performance, with a much higher computational cost. The 2D CNN is also pre-trained offline.</p><p>Using 2D segmentation maps on 3D SSC brings an additional complexity to the training phase which is trains and evaluates the 2D segmentation network prior to the 3D CNN training. In this work, we propose an end-to-end approach to fuse information from depth and colour, where the network can be trained and evaluated as a whole, and still achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR SOLUTION: EDGENET</head><p>Our proposed solution is the first end-to-end approach that successfully uses information from RGB to improve semantic scene completion performance over depth only. It consists in a novel approach to encode information from RGB edges and depth maps and a new 3D CNN architecture to fuse both modalities that we call EdgeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoding edges in 3D</head><p>As discussed earlier, colour information should complement depth maps for 3D semantic scene completion. However, combination of these modalities in a meaningful representation for learning is not trivial. Guedes et al. <ref type="bibr" target="#b12">[8]</ref> naively added 3 channels to each voxel to insert R, G and B colour information into the representation, with no encoding. In this way, the vast majority of voxels have no colour data while only those at the visible surface have a colour value. This explains why they do not improve on the previous approach using depth only. Song et al. <ref type="bibr" target="#b11">[7]</ref> demonstrate that F-TSDF encoding plays an important role in feeding a projected depth map to a 3D CNN and produces better results than TSDF and other encoding techniques. Given a sparse 3D voxel volume, the Truncated Signed Distance Function (TSDF) consists in computing the Euclidean distance of each empty voxel to the nearest occupied voxel. The signal of occluded regions is set to be negative, while visible regions are given positive values. Near the occupied surface, TSDF produces a value that tends to zero on both sides (and its first derivative tends to zero as well). TSDF values are normalised to <ref type="bibr">[-1,1]</ref>. Flipped TSDF (F-TSDF) follows the same principle, but the absolute values of both visible and occluded regions are flipped:</p><formula xml:id="formula_0">F-TSDF = sign(TSDF) ? (1 ? | TSDF |).<label>(1)</label></formula><p>A discontinuity near the occupied surface (from -1 to 1) occurs and the first derivative tends to infinity. F-TSDF encoding of volumetric data can be easily applied to depth maps after 3D projection because each voxel carries binary information: occupied or free. On the other hand, F-TSDF can not straightforwardly be applied to RGB or semantic segmentation maps, because they are not binary.</p><p>To deal with this problem, we introduce a new strategy to fuse colour appearance and depth information for 3D semantic scene completion. Our approach exploits edge detection in the image, which gives a 2D binary representation of the scene that can highlight objects that are hard to detect in depth maps. For instance, a poster on a wall is expected to be invisible in a depth map, especially after down-sampling. On the other hand, RGB edges highlight the presence of that object.</p><p>The main advantage of extracting edges and projecting them to 3D is the possibility to apply F-TSDF on both edges and surface volumes, as they are both binary, providing two meaningful input signals to the 3D CNNs. Another advantage is that due to their simplicity, edges are more transferable, removing the need for the application of a domain adaptation method when learning from synthetic images and applying on real images.</p><p>We apply F-TSDF to 3D edges, similarly to F-TSDF applied to 3D surfaces: for each voxel in the edge volume, we look for the nearest edge to calculate the Euclidean distance. Visible and occluded voxels are related only to edges, not to surfaces. We use the standard Canny edge detector <ref type="bibr" target="#b20">[16]</ref> to perform edge detection. Each edge location is projected to a point in the 3D space using its depth information and the camera calibration matrix. The resulting point cloud is voxelised in the same way as the depth point cloud, resulting in a sparse volume of 240 ? 144 ? 240 voxels. <ref type="figure" target="#fig_0">Figure 2</ref> shows a scene from the SUNCG dataset and its corresponding edges projected to 3D. <ref type="figure" target="#fig_1">Figure 3b</ref> shows in detail a region of the projected edges of 3a after F-TSDF encoding. Note that the greatest gradients occur along the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EdgeNet architecture</head><p>In order to combine depth and edge modalities, we propose a new 3D semantic segmentation CNN architecture that we call EdgeNet. Our proposed solution is a 3D CNN inspired by the U-Net design <ref type="bibr" target="#b21">[17]</ref> which has been successfully used in many 2D semantic segmentation problems, and is presented in <ref type="figure">Figure 4</ref>. We address the degradation problem of deeper networks <ref type="bibr" target="#b22">[18]</ref>, by replacing simple convolutional blocks of U-Net by ResNet modules <ref type="bibr" target="#b19">[15]</ref>. On lower resolutions, the ResNet modules uses dilated convolutions to improve the receptive field. To match the resolution of the output, the input branch reduces the resolution to 1/4 of the input. Next blocks follows encoder-decoder design and the last stage of the decoding branch is responsible for reducing the number of channels to match the desired number of output classes and loss calculations.</p><p>Depth and Edges Fusion Schemes. The encoder-decoder structure of EdgeNet allows us to evaluate three fusion schemes: Early Fusion (EdgeNet-EF), Middle Fusion (EdgeNet-MF) and Late Fusion (EdgeNet-LF). In EdgeNet-EF, just after F-TSDF encoding, both input volumes are concatenated and fed into the main network. In EdgeNet-MF, the input branch is divided into two parts while in EdgeNet-LF, both input and encoding branches are divided. To keep the same memory requirement in all fusion schemes, the total quantity of channels in all scheme is always the same.</p><p>Data balancing and loss function. In volumetric data, occluded and occupied voxels are highly unbalanced, so we use a weighted version of categorical cross entropy as the loss function to train our models. To obtain the weights, for each training batch, we randomly initialize a tensor rand occl of the same shape as the batch with ones and zeroes using the ratio r = (2 occu/ occl), where occl and occu are two tensors obtained from the previously calculated occupancy grid relative to occluded and occupied voxels. The final weight tensor is w = occu + occl rand occl , where denotes the Concat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat.</head><p>Concat. <ref type="figure">Fig. 4</ref>: EdgeNet architecture and fusion schemes (best viewed in colour).</p><p>Hadamard product. Let p be the predicted probabilities of the 12 classes for each voxel and y be the one hot encoded ground truth tensor. The categorical cross entropy loss function is then given by L cce (p, y) = ? (w y log p).</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training pipeline with offline data preparation</head><p>As F-TSDF calculation is computationally intensive, to reduce overall training time, the F-TSDF volumes that feed the models are preprocessed off-line once. The preprocessed dataset is then stored, and may be used as many times as needed, including by different models. Following previous works, we rotate the 3D Scene to align it with gravity and have room orientation based on the Manhattan assumption. We fixed the dimensions of the 3D space to 4.8 m horizontally, 2.88 m vertically and 4.8 m in depth. Voxel grid size is 0.02 m, resulting in a 240 ? 144 ? 240 3D volume. The TSDF truncation value is 0.24 m. Surface and edge projection as well as F-TSDF encoding of all volumes are done in this stage. During preprocessing, we also calculate an occupancy grid where we distinguish occupied voxels inside the room and FOV; non-occupied occluded voxels inside the room and FOV; and all other voxels. This occupancy grid will be further used to balance the dataset during training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section we describe the datasets and the evaluation protocol used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We train and validate our proposed approach on SUNCG <ref type="bibr" target="#b11">[7]</ref> and NYUDv2 <ref type="bibr" target="#b23">[19]</ref> datasets. The SUNCG dataset consists of about 45K synthetic scenes from which more than 130K 3D scenes were rendered with corresponding depth maps and ground truth, divided in train and test datasets. As the original training and test sets did not include RGB images, we extracted the camera poses from the provided ground truth and rendered a new set of depth and RGB images from the SUNCG synthetic scenes. To avoid misalignments, the ground truth volumes were regenerated from the scene meshes.</p><p>NYUDv2 is a widely used dataset of indoor scenes that includes depth and RGB images captured by the Kinect depth sensor, divided in 795 samples for training and 654 for test. Following the majority of works in semantic segmentation we used ground truth obtained by voxelizing the 3D mesh annotations from <ref type="bibr" target="#b25">[20]</ref> and and mapped object categories based on <ref type="bibr" target="#b27">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training protocols</head><p>Our experiments consists in training our models from scratch on SUNCG and NYUDv2, and also fine-tuning models trained from SUNCG to NYUDv2. For experiments in which we trained our models from scratch, we use the technique known as One Cycle Learning <ref type="bibr" target="#b28">[22]</ref>, which is a combination of Curriculum Learning <ref type="bibr" target="#b29">[23]</ref> and Simulated Annealing <ref type="bibr" target="#b30">[24]</ref>. After some preliminary tests, we found 0.01 to be a good base learning rate. We use a maximum of 30 epochs, in order to maintain total training time in a acceptable limit. Following <ref type="bibr" target="#b28">[22]</ref>, we start with the base learning rate and linearly increase the effective learning until 0.1 in the 10th epoch, then linearly decrease the learning rate until reach the start-up level in the 20th epoch. During the annealing phase, we linearly go from 0.01 to 0.0005 in a further 10 epochs. Due to GPU memory size constraints, we use a batches of 3 samples. We also use SGD optimizer with a momentum of 0.9 and decay of 0.0005 in all experiments, as used in most previous works. For SUNCG, each epoch consists of 30,540 scenes randomly selected from the whole training set. For NYUDv2, each epoch comprises the whole training set. For fine tuning, we initialize the network with parameters trained on SUNCG and use the standard training policy with SGD with fixed learning rate of 0.01 and 0.0005 of weight decay.</p><p>Thanks to our lightweight training pipeline with offline F-TSDF preprocessing, our training time is only 4 days on SUNCG and 6 hours on NYUDv2, using a GTX 1080 TI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>For the semantic scene completion task, we report the Intersection over Union (IoU) of each object class on both the observed and occluded voxels. For the scene completion task, all non-empty object classes are considered as one category, and we report Precision, Recall and IoU of the binary predictions on occluded voxels. Voxels outside the view or the room are not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental results</head><p>We compare our results to semantic scene completion approaches that use depth-only <ref type="bibr" target="#b17">[13]</ref>, <ref type="bibr" target="#b11">[7]</ref>, <ref type="bibr" target="#b16">[12]</ref>, depth plus RGB <ref type="bibr" target="#b12">[8]</ref> and depth plus 2D segmentation maps <ref type="bibr" target="#b13">[9]</ref>, <ref type="bibr" target="#b14">[10]</ref>. We also investigate the effects of the main aspects of our proposed solution on SUNCG. Comparative results were extracted from the original papers.</p><p>1) Ablation Studies and results on SUNCG: In <ref type="table" target="#tab_3">Table I</ref>, investigate the effects of the main aspects of our proposed solution. At first, we analyse the effect of our training pipeline. We took SSCNet as a baseline and retrain it, using our lightweight training framework, that allows a batch size of 3 samples in comparison to the 1 sample batch size of original SSCNet. Results of that experiment are shown as SSNet*. We observed a large improvement on SSC scores just using our pipeline.</p><p>After isolating the effect of our training protocol, we investigate the effect of our encoder-decoder architecture, with dilated ResNet modules. To accomplish this, we used EdgeNet-D, that is the Ednet architecture fed only with depth, without edges. Once again we observed a high level of improvement, comparing to SSCNet*. EdgeNet-D also got the best overall scores amongst the depth-only approaches. Next experiment evaluates the effect of adding edges to an existing depthonly architecture. We took SSCNet and fed it with both depth and edges after F-TSDF encoding (SSCNet-E). We observed improvements compared to SSCNet* on overall scores and especially on hard-to-detect classes like TVs and objects.</p><p>Finally, we evaluate the benefits of adding Edges to our architeture in three fusion schemes: EdgeNet-EF, EdgeNet-MF and EdgeNet-LF. Performance gains from EdgeNet-D show, once again, that adding edges is useful. A discussion about fusion schemes is provided on Section V.</p><p>We also compare EdgeNet results to previous approaches. Overall, our proposed solutions achieve the best performance by a large margin. EdgeNet-EF achieves best average scores, while EdgeNet-MF achieves the best score in some classes. EdgeNet-EF surpassed VVNetR120, the best previous approach on average SSC, by 3.3%. As expected, the highest improvements are observed on hard to detect classes, like objects and TVs. Although SUNCG is synthetic, evaluation on this dataset is quite important because of the poor quality of the ground truth in NYU, which impacts negatively accurate models like EdgeNet. <ref type="table" target="#tab_3">Table II</ref> shows the results of EdgeNet on NYUDv2 dataset and compares it with previous approaches. We compare results for models trained only on synthetic data, only on NYUDv2 and on both synthetic and NYUDv2 using fine tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on NYUDv2</head><p>On SUNCG-only and on NYUDv2-only training scenarios, EdgeNet-MF achieved the best overall scores on Scene Completion and Semantic Scene Completion. On SUNCG+NYU training scenario, however, TNetFuse presented the best result. EdgeNet-MF achieved best scores on structural elements and chair. It is worth mentioning that the NYUDv2 dataset has severe ground truth errors and misalignment, so results are not precise, and small differences in results may be questioned (see Section IV-F).</p><p>Despite these problems on NYU ground truth, EdgeNet achieves state-of-the-art level results with a much simpler and more computationally efficient training pipeline. EdgeNet is an end-to-end approach, and its memory consumption allows a batch size of 3 samples in a GTX 1080TI GPU, while TNetFuse requires a complex two step training procedure and uses a batch size of only 1 sample, in the same GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>Qualitative results on NYUDv2 are shown in <ref type="figure">Figure 5</ref>. Models used to generate the inferences were trained on SUNCG and fine tuned on NYUDv2. We compare results of SSCNet* to our three models. It is visually perceptible that EdgeNet presents more accurate results.</p><p>In the first row of images of <ref type="figure">Figure 5</ref>, note the presence of a picture and a window, and observe that the ground truth misses the window. SCCNet* did not detect the picture and the window while EdgeNet-MF detects the window and some parts of the picture. This ground truth mislabelling affects negatively the performance of EdgeNet.</p><p>The second row of <ref type="figure">Figure 5</ref> also depicts some problems related to Ground Truth annotations on NYUDv2 dataset. Note that neither the papers fixed on the wall nor the shelf appear in the Ground Truth. All models captured the shelf, but only EdgetNet inferred the presence of objects fixed on the wall.   When quantitative results are computed, these ground truth annotation flaws unfairly benefit the less precise models and harm more precise models like ours.</p><p>V. DISCUSSION In this section we discuss key aspects and contributions of our proposed approach.</p><p>Has the new training pipeline any influence over results? We compared the results originally achieved by SSCNet to results of the version of it trained with our pipeline (SSCNet*). On SUNCG we observed an improvement of almost 20% on semantic scene completion and more than 10% on scene completion. Besides the improvements on model performance, the more computationally efficient pipeline also contributed to reduce training time from 7 days to 4 days when training on SUNCG and from 30 hours to 6 hours when training on NYUDv2, with a batch of size 3, whereas the original framework only allowed a batch size of 1 sample on a GTX 1080Ti (which has 11GB of memory). Besides reducing training time, larger batch sizes enhance training stability, acting as a regularizer <ref type="bibr" target="#b31">[25]</ref>.</p><p>Is a deeper U-shaped CNN with dilated ResNet modules helpful? We investigated the effects of our architecture with and without aggregating edges. On both scenarios, our proposed architecture outperformed the shallower network, confirming that our network architecture is helpful.</p><p>Is aggregating edges helpful? May Other 3D CNN architectures benefit from aggregating edges? We compared the original SSCNet architecture trained with our pipeline to a modified version of it that aggregates edges encoded with F-TSDF (SSCNet-E). SSCNet-E presented better results on SUNCG, demonstrating that the aggregation of edge information is helpful. We also observed improvements using a deeper depth-only network (EdgeNet-D). This experiments demonstrates that the proposed 3D volumetric representation What is the best fusion strategy? The later the fusion, the higher is the memory requirement, due to the duplication of convolutional branches. Higher memory may imply in smaller batch sizes which may negatively impact learning. Liu et al. <ref type="bibr" target="#b14">[10]</ref> observed better results using late fusion, but they faced the problem of higher memory consumption. Our choice was to fix the memory footprint, reducing the number of channels of duplicated branches without compromising the training time and stability. However, very late fusion schemes may suffer from accuracy degradation due to reduced number of parameters in deeper layers. Taking those aspects into account, we found that a mid-level fusion strategy works and generalizes better for EdgeNet considering both synthetic and real datasets.</p><p>How does EdgeNet compare to other RGB + depth approaches? We have compared EdgeNet with other RGB + depth approachs on SUNCG <ref type="table" target="#tab_3">(Table I)</ref> and NYUDv2 <ref type="table" target="#tab_3">(Table II.</ref> On SUNCG, EdgeNet versions surpassed previous approaches by a large margin. On NYU, EdgeNet got similar results as the solutions from TNetFuse <ref type="bibr" target="#b14">[10]</ref>, with less than 1% difference. It is important to observe NYU ground truth annotations are not precise, which impacts negatively more accurate models. Another aspect that is worth mentioning is that TNetFuse needs a complex and less computationally efficient two-step training protocol, while EdgeNet and the previous depth-only solutions cited in this paper are end-to-end networks, with a much simpler and efficient training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presented a new approach to fuse depth and colour into a CNN for semantic scene completion. We introduced the use of F-TSDF encoded 3D projected edges extracted from RGB images. We also presented a new endto-end network architecture capable of properly aggregating edges and depth, extracting useful information from both sources, without requiring previous 2D semantic segmentation training as is the case of previous approaches that combine depth and colour. Experiments with alternate models, showed that both aggregating edges and the new proposed architecture have positive impact on semantic scene completion, especially for hard to detect objects. Qualitative results show significant improvement for objects such as pictures, which cannot be differentiated by depth only. On SUNCG, we have achieved the best overall result, and on NYU, we have achieved the state-of-the-art results of other approaches that use a more complex training protocol.</p><p>Experiments showed that our proposed approach of aggregating Edges may be applied to other existing solutions, opening room for further improvements.</p><p>We also developed a lightweight training pipeline for the task, which reduced the memory footprint in comparison to other solutions and reduced the training time on SUNCG from 7 to 4 days and on NYUDv2 from 30 to 6 hours, that we intend to make public upon acceptance of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Projection of Edges to 3D: (a) original RGB image, (b) voxelized edges after projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(a) original scene. (b) F-TSDF of edges in 3D. The edge image is a horizontal cut of the scene, taken just above the bed. Only F-TSDF values with absolute value greater than 0.8 are shown (best viewed in colour).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. 89.7 83.8 97.0 94.6 74.3 51.1 43.7 78.2 70.9 49.5 45.2 61.0 51.3 65.2 DCRF [12] 95.4 84.3 57.7 24.5 28.2 63.4 55.3 34.5 19.6 45.8 28.7 48.8 VVNetR-120 [13] 90.8 91.7 84.0 98.4 87.0 61.0 54.8 49.3 83.0 75.5 55.1 43.5 68.8 57.7 66.7 EdgeNet-D 93.1 90.4 84.8 97.2 94.4 78.4 56.1 50.4 80.5 73.8 54.5 49.8 69.5 59.2 69.5 d+s SNetFuse[10] 56.7 91.7 53.9 65.5 60.7 50.3 56.4 26.1 47.3 43.7 30.6 37.2 44.9 30.0 44.8 TNetFuse[10] 53.9 95.2 52.6 60.6 57.3 53.2 52.7 27.4 46.8 53.3 28.6 41.1 44.1 29.0 44.9 d+e SSCNet-E 92.8 89.6 83.8 97.0 94.5 74.6 51.8 43.9 77.0 70.8 49.3 49.2 62.1 52.0 65.7 EdgeNet-EF(Ours) 93.7 90.3 85.1 97.2 94.9 78.6 57.4 49.5 80.5 74.4 55.8 51.9 70.1 62.5 70.3 EdgeNet-MF(Ours) 93.3 90.6 85.1 97.2 95.3 78.2 57.5 51.4 80.7 74.1 54.5 52.6 70.3 60.1 70.2 EdgeNet-LF(Ours) 93.0 89.6 83.9 97.0 94.6 76.4 52.0 44.6 79.8 71.5 48.9 48.3 66.1 55.9 66.8</figDesc><table><row><cell>input prec. rec. d scene completion model SSCNet[7] 76.3 95.2 73.5 96.3 84.9 56.8 28.2 21.3 56.0 52.7 33.7 10.9 44.3 25.4 46.4 semantic scene completion (IoU, in percentages) SSCNet* 92.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Results and ablation studies on SUNCG test set. We took SSCNet as a baseline and show the effect of each one of the main aspects of our proposed approach. Column 'input' indicates the type of input: d = depth only; d+e = depth + edges. SSCNet* is our implementation of the original SSCNet, with our training pipeline. EdgeNet-D has the same architecture of the other versions of EdgeNet, but the edge volume is not fed into the network. EdgeNet-EF achieves the best overall scores and surpassed VVNetR-120 by 3.3% on average IoU for semantic scene completion. IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</figDesc><table><row><cell cols="7">train input d prec. rec. SUNCG scene completion model SSCNet[7] 55.6 91.9 53.2 5.8 81.8 19.6 5.4 12.9 34.4 26 13.6 6.1 9.4 7.4 20.2 semantic scene completion (IoU, in percentages) EdgeNet-EF(Ours) 61.9 80.0 53.6 9.1 92.9 18.3 5.7 15.8 40.4 30.7 9.2 3.3 13.7 11.6 22.8 d+e EdgeNet-MF(Ours) 60.7 80.3 52.8 11.0 92.3 20.5 7.2 16.3 42.8 32.8 10.5 6.0 15.7 11.8 24.3</cell></row><row><cell></cell><cell></cell><cell cols="5">EdgeNet-LF(Ours) 59.9 80.5 52.3 3.2 87.1 19.9 8.6 15.4 43.5 32.3 8.8 4.3 13.7 10.0 22.4</cell></row><row><cell></cell><cell>d</cell><cell>SSCNet[7]</cell><cell cols="4">57.0 94.5 55.1 15.1 94.7 24.4 0.0 12.6 32.1 35.0 13.0 7.8 27.1 10.1 24.7</cell></row><row><cell>NYU</cell><cell>d+e</cell><cell cols="5">EdgeNet-EF(Ours) 78.1 65.1 55.1 21.8 95.0 27.3 8.4 6.8 53.1 38.6 7.5 0.0 30.4 13.3 27.5 EdgeNet-MF(Ours) 76.0 68.3 56.1 17.9 94.0 27.8 2.1 9.5 51.8 44.3 9.4 3.6 32.5 12.7 27.8</cell></row><row><cell></cell><cell></cell><cell cols="5">EdgeNet-LF(Ours) 75.5 67.5 55.4 19.8 94.9 24.4 5.7 7.2 50.3 38.8 10.0 0.0 33.2 12.2 27.0</cell></row><row><cell></cell><cell></cell><cell>SSCNet[7]</cell><cell cols="4">59.3 92.9 56.6 15.1 94.6 24.7 10.8 17.3 53.2 45.9 15.9 13.9 31.1 12.6 30.5</cell></row><row><cell></cell><cell>d</cell><cell>DCRF[12]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.1 92.6 27.1 10.8 18.8 54.3 47.9 17.1 15.1 34.7 13.0 31.8</cell></row><row><cell></cell><cell></cell><cell cols="5">VVNetR-120[13] 69.8 83.1 61.1 19.3 94.8 28.0 12.2 19.6 57.0 50.5 17.6 11.9 35.6 15.3 32.9</cell></row><row><cell>SUNCG + NYU</cell><cell cols="6">d+c Guedes et al. [8] Garbade et al. *[9] 69.5 82.7 60.7 12.9 92.5 25.3 20.1 16.1 56.3 43.4 17.2 10.4 33.0 14.3 31.0 --56.6 -----------30.5 d+s SNetFuse[10] 67.6 85.9 60.7 22.2 91.0 28.6 18.2 19.2 56.2 51.2 16.2 12.2 37.0 17.4 33.6 TNetFuse[10] 67.3 85.8 60.7 17.3 92.1 28.0 16.6 19.3 57.5 53.8 17.7 18.5 38.4 18.9 34.4</cell></row><row><cell></cell><cell></cell><cell cols="5">EdgeNet-EF(Ours) 77.0 70.0 57.9 16.3 95.0 27.9 14.2 17.9 55.4 50.8 16.5 6.8 37.3 15.3 32.1</cell></row><row><cell></cell><cell>d+e</cell><cell cols="5">EdgeNet-MF(Ours) 79.1 66.6 56.7 22.4 95.0 29.7 15.5 20.9 54.1 53.0 15.6 14.9 35.0 14.8 33.7</cell></row><row><cell></cell><cell></cell><cell cols="5">EdgeNet-LF(Ours) 77.6 69.5 57.9 20.6 94.9 29.5 9.8 18.1 56.2 50.5 11.4 5.2 35.9 15.3 31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Semantic scene completion results on NYUDv2 test set. Column input indicates the type of input: d=depth only; d+s=depth and segmentation maps; d+e=depth and edges. Column train indicates dataset used for training the models. SUNCG + NYU means trained on SUNCG and fine tuned on NYUDv2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Qualitative Results. We compare EdgeNet results using SSCNet* as a baseline on SUNCG and NYUDv2. Overall, EdgeNet gives more accurate voxel predictions, especially for hard to detect clases (best viewed in colour). of color edges can improve the performance of other previous depth only approaches.</figDesc><table><row><cell></cell><cell>floor</cell><cell>wall</cell><cell>window</cell><cell>chair</cell><cell>bed</cell><cell>table</cell><cell>sofa</cell><cell>furn.</cell><cell>objects</cell></row><row><cell>(a) RGB image</cell><cell cols="3">(b) Ground Truth</cell><cell>(c) SSCNet*</cell><cell></cell><cell cols="2">(d) EdgeNet-EF</cell><cell cols="2">(e) EdgeNet-MF</cell><cell>(f) EdgeNet-LF</cell></row><row><cell>Fig. 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ResNet module (channels, size, strides, dilation=1)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Maxpooling3D(size, strides)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dilated ResNet module (channels, size, strides,dilation=2)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Conv3DTranspose(channels, size, strides)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Conv3D(channels, size, strides) + Softmax + Categ. Cross Entropy Loss Conv3D(channels, size, strides)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A field model for repairing 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="5676" to="5684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<idno>1802.04735</idno>
		<ptr target="http://arxiv.org/abs/1802.04735" />
	</analytic>
	<monogr>
		<title level="j">CoRR arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two stream 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno>1804.03550</idno>
		<ptr target="http://arxiv.org/abs/1804.03550" />
	</analytic>
	<monogr>
		<title level="j">CoRR arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7310-see-and-think-disentangling-semantic-scene-completion.pdf" />
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<editor>NeurIPS), S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic scene completion with dense CRF from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0925231218310142" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="182" to="195" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View-Volume Network for Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence ((IJCAI))</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence ((IJCAI))<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="726" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1986-11" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="679" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>MICCAI), N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidelberg</forename><surname>Berlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="746" to="760" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting complete 3D models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR arXiv</title>
		<imprint>
			<biblScope unit="volume">1504</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<ptr target="http://arxiv.org/abs/1504.02437" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SceneNet: Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>1511.07041</idno>
		<ptr target="http://arxiv.org/abs/1511.07041" />
	</analytic>
	<monogr>
		<title level="j">CoRR arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A disciplined approach to neural network hyperparameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno>1803.09820</idno>
		<ptr target="http://arxiv.org/abs/1803.09820" />
	</analytic>
	<monogr>
		<title level="j">CoRR arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1553374.1553380</idno>
		<ptr target="http://doi.acm.org/10.1145/1553374.1553380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
