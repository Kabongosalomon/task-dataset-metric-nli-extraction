<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Low-bitwidth Data Free Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukai</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">PengCheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
							<email>bohan.zhuang@monash.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuangrun</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Low-bitwidth Data Free Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data Free Compression ? Low-bitwidth Quantization ? Knowl- edge Matching Generator</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network quantization is an effective way to compress deep models and improve their execution latency and energy efficiency, so that they can be deployed on mobile or embedded devices. Existing quantization methods require original data for calibration or fine-tuning to get better performance. However, in many real-world scenarios, the data may not be available due to confidential or private issues, thereby making existing quantization methods not applicable. Moreover, due to the absence of original data, the recently developed generative adversarial networks (GANs) cannot be applied to generate data. Although the full-precision model may contain rich data information, such information alone is hard to exploit for recovering the original data or generating new meaningful data. In this paper, we investigate a simple-yeteffective method called Generative Low-bitwidth Data Free Quantization (GDFQ) to remove the data dependence burden. Specifically, we propose a knowledge matching generator to produce meaningful fake data by exploiting classification boundary knowledge and distribution information in the pre-trained model. With the help of generated data, we can quantize a model by learning knowledge from the pre-trained model. Extensive experiments on three data sets demonstrate the effectiveness of our method. More critically, our method achieves much higher accuracy on 4-bit quantization than the existing data free quantization method. Code is available at https://github.com/xushoukai/GDFQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) Real data (b) Gaussian input data (c) Fake data (ZeroQ <ref type="bibr" target="#b3">[4]</ref>) (d) Fake data (Ours) <ref type="figure">Fig. 1</ref>: The comparisons of generated fake data. ZeroQ generates fake data by gradient updating; however, since it neglects the inter-class information, it is hard to capture the original data distribution. Meanwhile, the knowledge matching generator produces fake data with label distribution, and it is more likely to produce data distributed near the classifier boundaries.</p><p>However, DNNs often contain a considerable number of parameters, which makes them hard to be deployed on embedded/edge devices due to unbearable computation costs for inference. Network quantization, which aims to reduce the model size by quantizing floating-point values into low precision(e.g., <ref type="bibr">4-bit)</ref>, is an effective way to improve the execution latency and energy efficiency. Existing quantization methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b23">24]</ref> generally require training data for calibration or fine-tuning. Nevertheless, in many real-world applications in medical <ref type="bibr" target="#b46">[47]</ref>, finance <ref type="bibr" target="#b47">[48]</ref> and industrial domains, the training data may not be available due to privacy or confidentiality issues. Consequently, these methods are no longer applicable due to the absence of training data. Therefore, the data-free quantization is of great practical value.</p><p>To address this issue, one possible way is to directly sample random inputs from some distribution and then use these inputs to quantize the model so that the output distributions of the full-precision model and quantized model are as close as possible. Unfortunately, since random inputs contain no semantic information and are far away from the real data distribution, this simple method suffers from huge performance degradation . Alternatively, one can use generative adversarial networks (GANs) to produce data. Due to the absence of training data, GANs cannot be applied in the data-free quantization. In practice, the quantization performance highly depends on the quality of the input data. Therefore, the process of constructing meaningful data to quantize models is very challenging.</p><p>To generate meaningful data, it is important and necessary to exploit data information from a pre-trained model. A recent study <ref type="bibr" target="#b44">[45]</ref> revealed that a welltrained over-parameterized model maintains sufficient information about the entire data set. Unfortunately, what information exists and how to exploit such information are still unknown. In the training, a neural network uses batch normalization <ref type="bibr" target="#b18">[19]</ref> to stabilize the data distribution and learns a classification boundary to divide data into different classes (See <ref type="figure">Fig. 1 (a)</ref>). In this sense, some information about the classification boundary and data distribution is hidden in the pre-trained model. However, these kinds of information are disregarded by existing data-free quantization methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>, which only focus on a single sample or network parameters. For example, ZeroQ <ref type="bibr" target="#b3">[4]</ref> exploits the information of a single sample instead of entire data, causing the constructed distribution to be far away from the real data distribution (See <ref type="figure">Fig. 1 (c)</ref>). To address these issues, how to construct meaningful data by fully exploiting the classification boundary and distribution information from a pre-trained model remains an open question.</p><p>In this paper, we propose a simple-yet-effective method called Generative Low-bitwidth Data Free Quantization to achieve completely data-free quantization. Without original data, we aim to learn a good generator to produce meaningful data by mining the classification boundary knowledge and distribution information in batch normalization statistics (BNS) from the pre-trained full-precision model.</p><p>The main contributions of this paper are summarized as follows:</p><p>-We propose a scheme called Generative Low-bitwidth Data Free Quantization (GDFQ), which performs 4-bitwidth quantization without any real data. To our knowledge, this is the first low-bitwidth data free quantization method. -We propose an effective knowledge matching generator to construct data by mining knowledge from the pre-trained full-precision model. The generated data retain classification boundary knowledge and data distribution. -Extensive experiments on image classification data sets demonstrate the superior performance of our method compared to the existing data-free quantization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Model quantization. Model quantization targets to quantize weights, activations and even gradients to low-precision, to yield highly compact models, where the expensive multiplication operations can be replaced by additions or bit-wise operations. According to the trade-off between accuracy and computational complexity, quantization can be roughly categorized into binary neural networks (BNNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref> and fixed-point quantization <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b51">52]</ref>. Moreover, quantization studies mainly focus on tackling two core bottlenecks, including designing accurate quantizers to fit the categorical distribution to the original continuous distribution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46]</ref>, and approximating gradients of the non-differential quantizer during back-propagation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51]</ref>. In addition, the first practical 4bit post-training quantization approach was introduced in <ref type="bibr" target="#b2">[3]</ref>. To improve the performance of neural network quantization without retraining, outlier channel splitting (OCS) <ref type="bibr" target="#b48">[49]</ref> proposed to move affected outliers toward the center of the distribution. Besides, many mainstream deep learning frameworks support 16-bit half-precision or 8-bit fixed-point quantization, such as TensorFlow <ref type="bibr" target="#b0">[1]</ref>, PyTorch <ref type="bibr" target="#b33">[34]</ref>, PaddleSlim, etc. In particular, these platforms provide both post-training quantization and quantization-aware training. However, all of them require the original data. In short, whether in scientific studies or practical applications, if the original data is unavailable, it is hard for quantization methods to work normally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-free model compression.</head><p>Recently, the researches about data-free had included more model compression methods, such as knowledge distillation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>, low-rank approximation <ref type="bibr" target="#b42">[43]</ref> and model quantization <ref type="bibr" target="#b48">[49]</ref>. A novel framework <ref type="bibr" target="#b7">[8]</ref> exploited generative adversarial networks to perform data-free knowledge distillation. Another work <ref type="bibr" target="#b24">[25]</ref> reconstructed a new data set based solely on the trained model and the activation statistics, and finally distilled the pre-trained "teacher" model into the smaller "student" network. Zero-shot knowledge distillation <ref type="bibr" target="#b30">[31]</ref> synthesized pseudo data by utilizing the prior information about the underlying data distribution. Specifically, the method extracted class similarities from the parameters of the teacher model and modeled the output space of the teacher network as a Dirichlet distribution. KEGNET <ref type="bibr" target="#b42">[43]</ref> proposed a novel data-free low-rank approximation approach assisted by knowledge distillation. This method contained a generator that generated artificial data points to replace missing data and a decoder that aimed to extract the low-dimensional representation of artificial data. In Adversarial Belief Matching <ref type="bibr" target="#b26">[27]</ref>, a generator generated data on which the student mismatched the teacher, and then the student network was trained using these data. Quantization also faces the situation without original data while previous quantization methods generally need original data to improve their performance. However, in some instances, it is difficult to get the original data. Recently, some studies focused on data-free model quantization. DFQ <ref type="bibr" target="#b29">[30]</ref> argued that data-free quantization performance could be improved by equalizing the weight ranges of each channel in the network and correcting biased quantization error. ZeroQ <ref type="bibr" target="#b3">[4]</ref>, a novel zero-shot quantization framework, enabled mixed-precision quantization without any access to the training or validation data. However, these data-free quantization methods work well for 8-bit, but got poor performance in aggressively low bitwidth regimes such as 4-bit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Data-free quantization problem. Quantization usually requires original data for calibration or fine-tuning. In many practical scenarios, the original data may not be available due to private or even confidential issues. In this case, we cannot use any data; thus, the general scheme of the network quantization will lose efficacy and even fail to work completely, resulting in a quantized model with inferior performance. Given a full-precision model M , data-free quantization aims to construct fake data (x, y) and meanwhile quantize a model Q from the model M ? . Specifically, to compensate for the accuracy loss from quantization, training-aware quantization can fine-tune the quantized model by optimizing the following problem: min</p><formula xml:id="formula_0">Q,x Ex ,y [ (Q(x), y)] ,<label>(1)</label></formula><p>wherex is a generated fake sample, y is the corresponding label, and (?, ?) is a loss function, such as cross-entropy loss and mean squared error. Challenges of constructing data. Because of the absence of original data, one possible way is to construct data by exploiting information from a pre-trained full-precision model. Although the full-precision model may contain rich data information, such latent information alone is hard to exploit for recovering the original data. In practice, the performance of quantization highly depends on the quality of constructed data. With the limited information of the pre-trained model, constructing meaningful data is very challenging.</p><p>Recently, one data-free quantization method (ZeroQ) <ref type="bibr" target="#b3">[4]</ref> constructs fake data by using a linear operator with gradient update information. With the help of constructed data, ZeroQ is proposed to improve the quantization performance. However, ZeroQ has insufficient information to improve the performance of quantization with the following two issues. First, it constructs fake data without considering label information, and thus neglects to exploit the classification boundary knowledge from the pre-trained model. Second, it enforces the batch normalization statistics of a single data point instead of the whole data, leading to being far away from the real data distribution. To address these issues, one can construct a generator G to produce fake data by considering label information and using powerful neural networks,</p><formula xml:id="formula_1">x = G(z|y), z ? p(z),<label>(2)</label></formula><p>where z is a random vector drawn from some prior distribution p(z), e.g., Gaussian distribution or uniform distribution. By using the generator, we are able to construct fake data to improve quantization performance. However, what knowledge in the pre-trained model can be exploited and how to learn a good generator remain to be answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Low-bitwidth Data Free Quantization</head><p>In many practical scenarios, training data are unavailable; thus, an existing method <ref type="bibr" target="#b3">[4]</ref> performs data-free quantization by constructing data. However, without exploiting knowledge from a pre-trained model, directly constructing data lacks label information and leads to being far away from the real data distribution. To address this issue, we aim to design a generator to produce fake data. Then, we are able to perform supervised learning to improve the performance of quantization. The overall framework is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Matching Generator</head><p>When training a deep neural network, it captures sufficient data information to make a decision <ref type="bibr" target="#b44">[45]</ref>. In this sense, the pre-trained DNN contains some knowledge information of the training data, e.g., classification boundary information and distribution information. We therefore elaborate how to effectively construct informative data from the pre-trained model.</p><p>Classification boundary information matching. The pre-trained model contains classification boundary information. Unfortunately, such information is difficult to exploit to recover the data near to the classification boundary.</p><p>Recently, generative adversarial networks (GANs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> have achieved considerable success in producing data. Since the real data are unavailable, the discriminator in a GAN cannot work in the data-free quantization task. Without the discriminator, learning a generator to produce meaningful data is difficult.</p><p>In this paper, we propose a generator to produce fake data for the data-free quantization task. For this task, although the original data cannot be observed, we are able to easily confirm the number of categories of original data by the last layer in a pre-trained model. As shown in <ref type="figure">Fig. 1 (a)</ref>, different categories of data should be distributed in different data spaces. To generate fake data, we introduce a noise vector z conditioned on a label y. Here, we sample noise from Gaussian distribution N (0, 1) and uniformly sample a label from {0, 1, ..., n ? 1}. Then, the generator maps a prior input noise vector z and the given label y to the fake datax. Formally, we define the knowledge matching generator as follows:</p><formula xml:id="formula_2">x = G(z|y), z ? N (0, 1).<label>(3)</label></formula><p>To improve the quantization performance, the generator should have the ability to generate data that are effective to fine-tune a quantized model. To this end, given the Gaussian noise z and the corresponding label y, the generated fake data should be classified to the same label y by the full-precision pre-trained model M . Therefore, we introduce the following cross-entropy loss CE(?, ?) to train the generator G:</p><formula xml:id="formula_3">L G CE (G) = E z,y [CE(M (G(z|y)), y)] .<label>(4)</label></formula><p>Distribution information matching. In addition, the pre-trained model contains the distribution information of training data. Such distribution information can be captured by batch normalization <ref type="bibr" target="#b18">[19]</ref>, which is used to control the change of the distribution. While training the full-precision model, the batch normalization statistics (BNS), i.e., the mean and variance, are computed dynamically. For every batch, batch normalization (BN) layers only compute statistics on the current mini-batch inputs and accumulate them with momentum. Finally, the exponential moving average (EMA) batch normalization statistics will be obtained and then they will be used in network validation and test.</p><p>To retain the BNS information, the mean and variance of the generated distribution should be the same as that of the real data distribution. To this end, we learn a generator G with the BNS information (? l and ? l ) encoded in the l-th BN layer of the pre-trained model using L BNS :</p><formula xml:id="formula_4">L BNS (G) = L l=1 ? r l ? ? l 2 2 + ? r l ? ? l 2 2 ,<label>(5)</label></formula><p>where ? r l and ? r l are the mean and variance of the fake datas distribution at the l-th BN layer, respectively, and ? l and ? l are the corresponding mean and variance parameters stored in the l-th BN layer of the pre-trained full-precision model, respectively. In this way, we are able to learn a good generator to keep the distribution information from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fake-data Driven Low-bitwidth Quantization</head><p>With the help of the generator, the data-free quantization problem can be turned to a supervised quantization problem. Thus, we are able to quantize a model using produced meaningful data. However, transferring knowledge from a pretrained model to a quantized model is difficult. To address this, we introduce a fake-data driven quantization method and solve the optimization problem by exploiting knowledge from the pre-trained model.</p><p>Quantization. Network quantization maps full-precision (32-bit) weights and activations to low-precision ones, e.g., 8-bit fixed-point integer. We use a simpleyet-effective quantization method refer to <ref type="bibr" target="#b19">[20]</ref> for both weights and activations. Specifically, given full-precision weights ? and the quantization precision k, we quantize ? to ? q in the symmetric k-bit range:</p><formula xml:id="formula_5">? q = ? ? ? ? ? ?2 k?1 , if ? &lt; ? 2 k?1 2 k?1 ?1, if ? &gt;2 k?1 ?1 ? , otherwise,<label>(6)</label></formula><p>where ? are discrete values mapped by a linear quantization, i.e., ? = ? ? ? ? b , ? is the round function, and ? and b can be computed by ? = 2 k ?1 u?l and b = l ? ? + 2 k?1 . Here, l and u can be set as the minimum and maximum of the floating-point weights ?, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Generative Low-bitwidth Data Free Quantization</head><p>Input: Pre-trained full-precision model M . Output: Generator G, quantized model Q.</p><p>Update G several times as a warm-up process. Quantize model M using Eq. (6), get quantized model Q.</p><p>Fix the batch normalization statistics in all BN layers of quantized model Q.</p><p>for t = 1, . . . , T f ine?tune do Obtain random noise z ? N (0, 1) and label y. Generate fake imagex using Eq. (3). Update generator G by minimizing Loss <ref type="bibr" target="#b8">(9)</ref>. Update quantized model Q by minimizing Loss <ref type="bibr" target="#b9">(10)</ref>. end for Optimization problem. When the real training data are unavailable, quantization may suffer from some limitations. First, direct quantization from a fullprecision model may result in severe performance degradation. To address this issue, we aim to train the quantized model to approximate the full-precision model through the fine-tuning process. To this end, a well fine-tuned quantized model Q and the pre-trained model M should classify the fake data correctly. For this purpose, we use the cross-entropy loss function CE(?, ?) to update Q:</p><formula xml:id="formula_6">L Q CE (Q) = Ex ,y [CE(Q(x), y)] .<label>(7)</label></formula><p>By minimizing Eq. <ref type="formula" target="#formula_6">(7)</ref>, the quantization model can be trained with the generated data to perform multi-class classification. Second, the traditional fine-tuning process with a common classification loss function is insufficient because the data are fake. However, with the help of fake data, we are able to apply knowledge distillation <ref type="bibr" target="#b16">[17]</ref> to improve the quantization performance. Specifically, given the same inputs, the outputs of a quantized model and full-precision model should be close enough to guarantee that the quantized model is able to achieve nearly performance compared with the full-precision model. Therefore, we utilize knowledge distillation to recover the performance of the quantized model by using fake datax to simulate the training data. Then, the quantized model can be fine-tuned using the following Kullback-Leibler loss function KL(?, ?):</p><formula xml:id="formula_7">L KD (Q) = Ex [KL(Q(x), M (x))] .<label>(8)</label></formula><p>By optimizing the loss in (8), the quantization model can learn knowledge from the full-precision model.</p><p>Fine-tuning with fixed BNS. To stabilize the training, we fix the batch normalization statistics (BNS) in the quantized model during fine-tuning. In this way, the BNS in the quantized model are corresponding to the statistics of the real data distribution. With the help of fixed BNS, the quantized model always maintains the real data information to improve quantization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Process</head><p>We propose a low-bitwidth quantization algorithm to alternately optimize the generator G and the quantized model Q. In our alternating training strategy, the generator is able to generate different data with each update. By increasing the diversity of data, the quantized model Q can be trained to improve the performance. In addition, to make the fine-tuning of Q more stable, we firstly train G solely several times as a warm-up process. The overall process is shown in Algorithm 1. In contrast, one can train the generator G with a full-precision model, and then fix the generator to train the quantized model Q until convergence. In this separate training strategy, when the diversity of the generated data is poor, the quantized model has a limitation to improve the quantization performance.</p><p>Training generator G. First, we randomly sample a Gaussian noise vector z ? N (0, 1) with a label y. Then we use G to generate fake data from the distribution and update G. The final generator loss L 1 (G) is formulated as follows:</p><formula xml:id="formula_8">L 1 (G) = L G CE (G) + ?L BNS (G),<label>(9)</label></formula><p>where ? is a trade-off parameter.</p><p>Training quantized model Q. We quantize the model according to Eq. (6). Then, we replace the BNS in the quantized model with the fixed batch normalization statistics (FBNS) as described in Section 4.2. So far, the quantized model has inherited the information contained in BNS and a part of latent knowledge from the parameters of the pre-trained model. In the fine-tuning process, we train the G and Q alternately in every epoch. Based on the warmed up G, we obtain fake samples and use them to optimize the quantized model Q. The final quantized model loss function L 2 (Q) is formulated as follows:</p><formula xml:id="formula_9">L 2 (Q) = L Q CE (Q) + ?L KD (Q),<label>(10)</label></formula><p>where ? is a trade-off parameter. We do not stop updating G because if we have a better G, the fake data will be more similar to real training data and the upper limit of optimizing Q will be improved. Note that we keep the pre-trained full-precision model fixed at all times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Sets and Implementation Details</head><p>We evaluate the proposed method on well-known data sets including CIFAR-10 <ref type="bibr" target="#b22">[23]</ref>, CIFAR-100 <ref type="bibr" target="#b22">[23]</ref>, and ImageNet <ref type="bibr" target="#b10">[11]</ref>. CIFAR-10 consists of 60k images from 10 categories, with 6k images per category. There are 50k images for training and 10k images for testing. CIFAR-100 has 100 classes and each class contains 500 training images and 100 testing images. ImageNet is one of the most challenging and largest benchmark data sets for image classification, which has around 1.2 million real-world images for training and 50k images for validation. Based on the full-precision pre-trained models from pytorchcv 1 , we quantize ResNet-20 <ref type="bibr" target="#b15">[16]</ref> on CIFAR-10/100 and ResNet-18 <ref type="bibr" target="#b15">[16]</ref>, BN-VGG-16 <ref type="bibr" target="#b37">[38]</ref> and Inception v3 <ref type="bibr" target="#b39">[40]</ref> on ImageNet. In all experiments, we quantize all layers including the first and last layers of the network following <ref type="bibr" target="#b3">[4]</ref> and the activation clipping values are per-layer granularity. All implementations are based on PyTorch.</p><p>For CIFAR-10/100, we construct the generator following ACGAN <ref type="bibr" target="#b32">[33]</ref> and the dimension of noise is 100. During training, we optimize the generator and quantized model using Adam <ref type="bibr" target="#b21">[22]</ref> and SGD with Nesterov <ref type="bibr" target="#b31">[32]</ref> respectively, where the momentum term and weight decay in Nesterov are set to 0.9 and 1 ? 10 ?4 . Moreover, the learning rates of quantized models and generators are initialized to 1 ? 10 ?4 and 1 ? 10 ?3 , respectively. Both of them are decayed by 0.1 for every 100 epochs. In addition, we train the generator and quantized model for 400 epochs with 200 iterations per epoch. To obtain a more stable clip range for activation, we calculate the moving average of activation's range in the first four epochs without updating the quantized models and then fix this range for subsequent training. For L 1 (G) and L 2 (Q), we set ? = 0.1 and ? = 1 after a simple grid search. For ImageNet, we replace the generator's standard batch normalization layer with the categorical conditional batch normalization layer for fusing label information following SN-GAN <ref type="bibr" target="#b28">[29]</ref> and set the initial learning rate of the quantized model as 1 ? 10 ?6 . Other training settings are the same as those on CIFAR-10/100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Toy Demonstration for Classification Boundary Matching</head><p>To evaluate that the fake data generated by our G are able to match the classification boundary information, we design a toy experiment. The results are shown in <ref type="figure">Fig. 1</ref>. First, we create a toy binary classification data set by uniform sampling from ?4 to +4, and the label is shown in <ref type="figure">Fig. 1 (a)</ref>. Second, we construct a simple neural network T , which is composed of several linear layers, BN layers, and ReLU layers, and we train it using the toy data. The classification boundaries are shown in each subfigure. To simulate the process of our method, we sample noises from Gaussian distribution and every noise has a random label of 0 or 1 <ref type="figure">(Fig. 1 (b)</ref>). Then, we generate fake data from noises by learning from the pre-trained model T . <ref type="figure">Fig. 1 (c)</ref> and <ref type="figure">Fig. 1 (d)</ref> show the fake data generated by the ZeroQ method and our method, respectively. The data generated by ZeroQ do not capture the real data distribution since it neglects the inter-class information; while our method is able to produce fake data that not only have label information but also match the classification boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison of the Results</head><p>To further evaluate the effectiveness of our method, we include the following methods for study. FP32: the full-precision pre-trained model. FT: we use real <ref type="table">Table 1</ref>: Comparisons on CIFAR-10/100 and ImageNet. We report the average and standard deviation of our method to show that our method is stable. We quantize both the weights and activations of the models to 4-bits and report the top1 accuracy. training data instead of fake data to fine-tune the quantized model by minimizing L 2 . ZeroQ: a data-free post-training quantization method. We obtain the result from the publicly released code of ZeroQ <ref type="bibr" target="#b3">[4]</ref>. We quantize both weights and activations to 4-bit and report the comparison results in <ref type="table">Table 1</ref>. For CIFAR-10, our method achieves much higher accuracy than that of ZeroQ <ref type="bibr" target="#b3">[4]</ref>. When the number of categories increases in CIFAR-100, our method suffers a much smaller degradation in accuracy compared with that of ZeroQ. The main reason is that, our method gains more prior knowledge from the full-precision model. These results demonstrate the effectiveness of our method on simple data sets with 4-bit quantization. For large scale and categories data set, such as ImageNet, existing data-free quantization methods suffer from severe performance degradation. However, our generated images contain abundant category information and similar distribution with real data. As a result, our method recovers the accuracy of quantized models significantly with the help of generated fake data and knowledge distillation on three typical networks. More experiments on different models and methods can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>In this section, we first evaluate the effectiveness of each component in L 1 and L 2 . Second, we explore how fixed BNS affects our method. Then, we compare our method with different quantization methods. Last, we further study the effect of different stopping conditions. All the ablation experiments are conducted on the CIFAR-10/100 data sets.</p><p>Effect of different losses. To verify the effectiveness of different components in our method, we conduct a series of ablation experiments on CIFAR-100 with ResNet-20. <ref type="table">Table 2</ref> reports the top-1 accuracy of quantized models with different components of L 1 . In this ablation experiment, we fine-tune quantized models with complete L 2 . Since we do not use both CE loss and BNS loss, we have no way to optimize G, which means we use the fake data generated from the initial <ref type="table">Table 2</ref>: Effect of different loss functions of generator G. We quantize both the weights and activations of the models to 4-bits and report the top1 accuracy on CIFAR-100.  G to fine-tune the quantized model. In this case, the distribution of the fake data is far away from that of original data because the generator receives no guidance from the full-precision model. Therefore, the quantized model suffers from a large performance degradation. To utilize the knowledge in the full-precision model, we use CE loss to optimize G and achieve a better quantized model. In this case, the generator produces fake data that can be classified with high confidence by the full-precision model. Last, we combine CE loss and BNS loss with a coefficient and achieve the best result. The BNS loss encourages the generator to generate fake data that match the statistics encoded in full-precision model's BN layers so that these fake data have a much similar distribution with real data. In summary, both CE loss and BNS loss contribute to better performance of the quantized model. We further conduct ablation experiments to analyze the effectiveness of each component in L 2 . <ref type="table" target="#tab_2">Table 3</ref> reports the top-1 accuracy of quantized models with different components of L 2 . In this experiment, we optimize the generator with complete L 1 . When only introducing KD loss, the quantized model receives knowledge from the full-precision model's prediction and achieves 62.98% on top-1 accuracy. To use the additional label information, we combine BNS loss with CE loss. The resultant model achieves a 0.93% improvement on top-1 accuracy.</p><p>Effect of the fixed BNS. To verify the effectiveness of fixing batch normalization, we conduct ablation studies with ResNet-20 on CIFAR-10/100. The results are shown in <ref type="table">Table 4</ref>. When we fix batch normalization statistics during fine-tuning, we narrow the statistics gap between the quantized model and the full-precision model. As a result, we achieve a much higher top-1 accuracy than that with standard batch normalization. <ref type="table">Table 4</ref>: Ablation experiments on the fixed BNS (FBNS). We keep the weights and activations of the models to be 4-bits and report the top1 accuracy on CIFAR-10/100. We use "w/o FBNS" to represent that we use fake data to finetune the quantized models without fixed BNS. Similarly, we use "w/ FBNS" to represent the fine-tuning process with fixed BNS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Further Experiments</head><p>Comparisons with different post-training quantization methods. We compare different post-training quantization methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref> on CIFAR-10/100 and ImageNet and show the results in <ref type="table" target="#tab_4">Table 5</ref>. In this experiment, we use images from real data sets as calibration sets for quantized models with different post-training quantization methods. Specifically, we compare our quantization method with MSE(mean squared error), ACIQ, and KL(Kullback-Leibler), which are popular methods to decide the clip values of weight and activation. Our method shows much better performance than that of the other methods, which means it is more suitable in low-bitwidth quantization. Furthermore, the experimental results show that as the data set gets larger, the accuracy decreases.</p><p>With the decrease of precision, all the quantization methods behave more poorly. Specifically, when the precision drops to 4-bit, the accuracy declines sharply.</p><p>Effect of two training strategies. We investigate the effect of two kinds of training strategies. 1) Training generator and quantized model in two steps. We first train the generator by minimizing the loss (9) until convergence. Then, we train the quantized model by minimizing the loss (10). 2) Training the generator  and quantized model alternately in each iteration following Algorithm 1. From the results in <ref type="table" target="#tab_5">Table 6</ref>, alternating training performs significantly better than separate training. Therefore, we use alternating training in other experiments.</p><p>Effect of different thresholds in stopping conditions. In this experiment, we stop the training of the generator if the classification accuracy of the fullprecision model on fake data is larger than a threshold ?. <ref type="table" target="#tab_6">Table 7</ref> reports the results of different thresholds ? in stopping condition. When increasing the threshold, the generator is trained with quantized models for more epochs, and we get a better fine-tuning result. We achieve the best performance when we do not stop optimizing the generator. These results demonstrate that optimizing the generator and quantized model simultaneously increases the diversity of data, which is helpful for fine-tune quantized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a Generative Low-bitwidth Data Free Quantization scheme to eliminate the data dependence of quantization methods. First, we have constructed a knowledge matching generator to produce fake data for the fine-tuning process. The generator is able to learn the classification boundary knowledge and distribution information from the pre-trained full-precision model. Next, we have quantized the full-precision model and fine-tuned the quantized model using the fake data. Extensive experiments on various image classification data sets have demonstrated the effectiveness of our data-free method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Low-bitwidth Data Free Quantization (Supplementary Materials)</head><p>A More Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experiments on More Networks</head><p>We conduct more experiments on different networks and report the results in <ref type="table" target="#tab_7">Table 8</ref>. From the table, our method achieves much better performance than ZeroQ <ref type="bibr" target="#b3">[4]</ref> in terms of accuracy at different precisions. More importantly, our method shows larger performance gain over ZeroQ at lower bit-width (e.g., W4A4) for all models. These results demonstrate the effectiveness of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparisons with DFQ</head><p>We implement the algorithm in DFQ <ref type="bibr" target="#b29">[30]</ref> and report the results in <ref type="table" target="#tab_8">Table 9</ref>. Our method achieves much higher performance than DFQ, especially in low bit-width. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of the proposed method. Given Gaussian noise and the label as input, the generator creates fake data and feeds them into both the fullprecision model and the quantized model. The fixed full-precision model provides knowledge for updating the generator. The quantized model learns latent knowledge from the generator and the full-precision model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of different loss functions of Q. We keep the weights and activations of the models to 4-bits and report the top1 accuracy on CIFAR-100.</figDesc><table><row><cell>Model</cell><cell>CE Loss</cell><cell>KD Loss</cell><cell>Acc. (%)</cell></row><row><cell>ResNet-20 (4-bit)</cell><cell>?</cell><cell>?</cell><cell>55.55 62.98 63.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different post-training quantization methods. We use real data sets as calibration sets and report the accuracy of the quantized model without fine-tuning.</figDesc><table><row><cell>Data Set</cell><cell>Model</cell><cell>Method</cell><cell>W8A8</cell><cell>W6A6</cell><cell>W5A5</cell><cell>W4A4</cell></row><row><cell></cell><cell></cell><cell>MSE [39]</cell><cell>93.86</cell><cell>93.10</cell><cell>91.38</cell><cell>81.59</cell></row><row><cell>CIFAR-10</cell><cell>ResNet-20 (94.03)</cell><cell>ACIQ [2] KL [9]</cell><cell>93.69 93.72</cell><cell>92.22 92.32</cell><cell>86.66 90.71</cell><cell>61.87 80.05</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>93.92</cell><cell>93.38</cell><cell>92.39</cell><cell>85.20</cell></row><row><cell></cell><cell></cell><cell>MSE [39]</cell><cell>70.11</cell><cell>66.87</cell><cell>60.49</cell><cell>27.11</cell></row><row><cell>CIFAR-100</cell><cell>ResNet-20 (70.33)</cell><cell>ACIQ [2] KL [9]</cell><cell>69.29 70.15</cell><cell>63.21 67.65</cell><cell>48.21 57.55</cell><cell>8.72 15.83</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>70.29</cell><cell>68.63</cell><cell>64.03</cell><cell>43.12</cell></row><row><cell></cell><cell></cell><cell>MSE [39]</cell><cell>71.01</cell><cell>66.96</cell><cell>54.23</cell><cell>15.08</cell></row><row><cell>ImageNet</cell><cell>ResNet-18 (71.47)</cell><cell>ACIQ [2] KL [9]</cell><cell>68.78 70.69</cell><cell>61.15 61.34</cell><cell>46.25 56.13</cell><cell>7.19 16.27</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>71.43</cell><cell>70.43</cell><cell>64.68</cell><cell>33.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of separate training and alternating training of G and Q.</figDesc><table><row><cell>Training Strategy</cell><cell>Acc. (%)</cell></row><row><cell>Separate Training</cell><cell>61.81</cell></row><row><cell>Alternating Training</cell><cell>63.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effect of different thresholds in the stopping condition of G.</figDesc><table><row><cell>Threshold ? (%)</cell><cell>90.00</cell><cell>95.00</cell><cell>99.00</cell><cell>99.50</cell><cell>w/o stopping condition</cell></row><row><cell>Acc. (%)</cell><cell>57.20</cell><cell>57.56</cell><cell>59.09</cell><cell>59.67</cell><cell>63.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Experiments on more networks on ImageNet. We quantize ResNet-50, MobileNetV2, and ShuffleNet to 6-bit and 4-bit, and report the Top-1 accuracy. Ours 77.72 76.59 55.65 73.03 70.98 51.30 65.07 60.12 21.78</figDesc><table><row><cell>ResNet-50</cell><cell></cell><cell cols="2">MobileNet V2</cell><cell>ShuffleNet</cell><cell></cell></row><row><cell cols="6">FP32 W6A6 W4A4 FP32 W6A6 W4A4 FP32 W6A6 W4A4</cell></row><row><cell>ZeroQ 77.72 72.86</cell><cell>0.12</cell><cell>73.03 69.62</cell><cell>3.31</cell><cell>65.07 46.25</cell><cell>0.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparisons with the DFQ method. We quantize MobileNetV2 and ResNet-18 to 8-bit, 6-bit and 4-bit, and report the Top-1 accuracy on ImageNet. Ours 73.03 72.80 70.98 51.30 71.74 70.68 70.13 60.52</figDesc><table><row><cell cols="2">MobileNet V2</cell><cell></cell><cell cols="2">ResNet-18</cell><cell></cell></row><row><cell cols="6">FP32 W8A8 W6A6 W4A4 FP32 W8A8 W6A6 W4A4</cell></row><row><cell>DFQ 73.03 66.06</cell><cell>52.82</cell><cell>0.11</cell><cell>71.74 70.13</cell><cell>14.67</cell><cell>0.10</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pypi.org/project/pytorchcv/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the Key-Area Research and Development Program of Guangdong Province 2018B010107001, Program for Guangdong Introducing Innovative and Entrepreneurial Teams 2017ZT07X183, Fundamental Research Funds for the Central Universities D2191240.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Aciq: Analytical clipping for integer quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Post training 4-bit quantization of convolutional networks for rapid-deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zeroq: A novel zero shot quantization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning with low precision by half-wave gaussian quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial learning with local coordinate coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-marginal wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<title level="m">Pact: Parameterized clipping activation for quantized neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Repren</title>
		<meeting>Int. Conf. Learn. Repren</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nat: Neural architecture transformer for accurate and compact architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integerarithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to quantize deep networks by optimizing quantization intervals with task loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Repren</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting>Int. Conf. Learn. Repren</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08444</idno>
		<title level="m">Defensive quantization: When efficiency meets robustness</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07535</idno>
		<title level="m">Data-free knowledge distillation for deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relaxed quantization for discretized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Repren</title>
		<meeting>Int. Conf. Learn. Repren</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Zero-shot knowledge transfer via adversarial belief matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09768</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?ernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association (ISCA)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data-free quantization through weight equalization and bias correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Baalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge distillation in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o (1/k?2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USSR Academy of Sciences</title>
		<meeting>the USSR Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<title level="m">Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration. PyTorch: Tensors and dynamic neural networks in Python with strong GPU acceleration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association (ISCA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Repren</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting>Int. Conf. Learn. Repren</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06488</idno>
		<title level="m">Resiliency of deep neural networks under quantization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantization networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge extraction with no observable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Repren</title>
		<meeting>Int. Conf. Learn. Repren</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From whole slide imaging to microscopy: Deep microscopy adaptation network for histopathology cancer image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cost-sensitive portfolio selection via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving neural network quantization without retraining using outlier channel splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dotzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training quantized neural networks with a full-precision auxiliary module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards effective low-bitwidth convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Structured binary neural networks for accurate image classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
