<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantal</forename><surname>Amrhein</surname></persName>
							<email>amrhein@cl.uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Moghe</surname></persName>
							<email>nikita.moghe@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liane</forename><surname>Guillou</surname></persName>
							<email>lguillou@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As machine translation (MT) metrics improve their correlation with human judgement every year, it is crucial to understand the limitations of such metrics at the segment level. Specifically, it is important to investigate metric behaviour when facing accuracy errors in MT because these can have dangerous consequences in certain contexts (e.g., legal, medical). We curate ACES 1 , a Translation Accuracy ChallengE Set, consisting of 68 phenomena ranging from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. We use ACES to evaluate a wide range of MT metrics including the submissions to the WMT 2022 metrics shared task and perform several analyses leading to general recommendations for metric developers. We recommend: a) combining metrics with different strengths, b) developing metrics that give more weight to the source and less to surfacelevel overlap with the reference and c) explicitly modelling additional language-specific information beyond what is available via multilingual embeddings.</p><p>2. The extent to which reference-based metrics rely on surface-level overlap with the reference.</p><p>3. Whether using multilingual embeddings results in better metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Challenge sets have already been created for measuring the success of systems or metrics on a particular phenomenon of interest for a range of NLP tasks, including but not limited to: Sentiment Analysis 2 <ref type="bibr" target="#b28">(Li et al., 2017;</ref><ref type="bibr" target="#b32">Mahler et al., 2017;</ref><ref type="bibr" target="#b59">Stali?nait? and Bonfil, 2017)</ref>, Natural Language Inference <ref type="bibr" target="#b34">(McCoy and Linzen, 2019;</ref><ref type="bibr" target="#b52">Rocchietti et al., 2021)</ref>, Question Answering <ref type="bibr" target="#b47">(Ravichander et al., 2021)</ref>, Machine Reading Comprehension * Equal contribution by all authors. <ref type="bibr">1</ref> Our dataset is available at https://huggingface.co/ datasets/nikitam/ACES and the corresponding evaluation scripts at https://github.com/EdinburghNLP/ACES 2 Submitted to the EMNLP 2017 "Build It Break It" shared task on sentiment analysis <ref type="bibr" target="#b20">(Khashabi et al., 2018)</ref>, Machine Translation (MT) <ref type="bibr" target="#b21">(King and Falkedal, 1990;</ref><ref type="bibr" target="#b18">Isabelle et al., 2017)</ref>, and the more specific task of pronoun translation in MT <ref type="bibr" target="#b13">(Guillou and Hardmeier, 2016)</ref>. They are useful to compare the performance of different systems, or to identify performance improvement/degradation between a modified system and a previous iteration.</p><p>In this work, we describe the University of Zurich -University of Edinburgh submission to the Challenge Sets subtask of the Conference on Machine Translation (WMT) 2022 Metrics shared task. Our Translation Accuracy ChallengE Set (ACES) consists of 36,476 examples covering 146 language pairs and representing challenges from 68 phenomena (see Appendix A.4 for the distribution of examples across language pairs and Appendix A.5 for the distribution of language pairs across phenomena). We focus on translation accuracy errors and base the phenomena covered in our challenge set on the Multidimensional Quality Metrics (MQM) ontology <ref type="bibr" target="#b31">(Lommel et al., 2014)</ref>. We include phenomena ranging from simple perturbations involving the omission/addition of characters or tokens, to more complex examples involving mistranslation e.g. ambiguity and hallucinations in translation, untranslated elements of a sentence, discourse-level phenomena, and real-world knowledge. We evaluate the metrics submitted to the WMT 2022 metrics shared task and a range of baseline metrics on ACES. Additionally, we perform an extensive analysis, which aims to reveal:</p><p>1. The extent to which reference-based and reference-free metrics take into account the source sentence context. Based on our analysis, we recommend that metric developers consider: a) combining metrics with different strengths, e.g. in the form of ensemble models, b) paying more attention to the source and avoiding reliance on surface-overlap with the reference, and c) explicitly modelling additional language-specific information beyond what is available via multilingual embeddings. We also propose that ACES be used as a benchmark for developing evaluation metrics for MT to monitor which error categories can be identified better, and also whether there are any categories for which metric performance degrades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>With the advent of neural networks and especially Transformer-based architectures <ref type="bibr" target="#b64">(Vaswani et al., 2017)</ref>, machine translation outputs have become more and more fluent <ref type="bibr" target="#b2">(Bentivogli et al., 2016;</ref><ref type="bibr" target="#b61">Toral and S?nchez-Cartagena, 2017;</ref><ref type="bibr" target="#b3">Castilho et al., 2017)</ref>. Fluency errors are also judged less severely than accuracy errors by human evaluators <ref type="bibr">(Freitag et al., 2021a)</ref> which reflects the fact that accuracy errors can have dangerous consequences in certain contexts, for example in the medical and legal domains <ref type="bibr" target="#b65">(Vieira et al., 2021)</ref>.</p><p>For these reasons, we decided to build a challenge set focused on accuracy errors. Specifically, we use the hierarchy of errors under the class Accuracy from the MQM ontology to design these challenge sets. We extend this ontology by two er-ror classes (translations defying real-world knowledge and translations in the wrong language) and specify several more specific subclasses such as discourse-level errors or ordering mismatches. A full overview of all error classes can be seen in <ref type="figure">Figure</ref> 1. Our challenge set consists of synthetically generated adversarial examples, examples from repurposed contrastive MT test sets (both marked in red), and manually annotated examples (marked in blue). To create the challenge sets, we use test sets from tasks such as adversarial paraphrase detection, Natural Language Inference, and contrastive MT test sets created independently of the WMT shared tasks to avoid overlap with the data that is used to train neural evaluation metrics.</p><p>Another aspect we focus on is including a broad range of language pairs in ACES. Whenever possible we create examples for all language pairs covered in a source dataset when we use automatic approaches. For phenomena where we create examples manually, we also aim to cover at least two language pairs per phenomenon, but are of course limited to the languages spoken by the authors.</p><p>Finally, we aim to offer a collection of challenge sets covering both easy and hard phenomena. While it may be of interest to the community to continuously test on harder examples to check where machine translation evaluation metrics still break, we believe that easy challenge sets are just as important to ensure that metrics do not suddenly become worse at identifying error types that were previously considered "solved". Therefore, we take a holistic view when creating ACES and do not filter out individual examples or exclude challenge sets based on baseline metric performance or other factors.</p><p>We first discuss previous efforts to create challenge sets (Section 3), before giving a broad overview of the datasets used to construct ACES (Section 4) and discussing the individual challenge sets in more detail (Section 5). We then introduce the metrics that participated in the shared task (Section 6), present an overview of their performance on ACES (Section 7) and detailed analyses (Section 8) that lead to a set of recommendations for future metric development (Section 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Challenge sets are used to study a particular phenomenon of interest rather than the general distribution of phenomena in standard test sets <ref type="bibr" target="#b45">(Popovi? and Castilho, 2019)</ref>. The earliest introduction of challenge sets was by <ref type="bibr" target="#b21">King and Falkedal (1990)</ref> who probed acceptability of machine translations for different domains. Challenge sets have been prevalent in different fields within NLP such as parsing <ref type="bibr">(Rimell et al., 2009), NLI (McCoy and</ref><ref type="bibr" target="#b34">Linzen, 2019;</ref><ref type="bibr" target="#b52">Rocchietti et al., 2021)</ref>, question answering <ref type="bibr" target="#b47">(Ravichander et al., 2021)</ref>, reading comprehension <ref type="bibr" target="#b20">(Khashabi et al., 2018)</ref> and sentiment analysis <ref type="bibr" target="#b28">(Li et al., 2017;</ref><ref type="bibr" target="#b32">Mahler et al., 2017;</ref><ref type="bibr" target="#b59">Stali?nait? and Bonfil, 2017)</ref>, to name a few. These challenge sets provide insights on whether state-of-the-art models are robust to domain shifts, and whether they have some understanding of linguistic phenomena like negation/commonsense or they simply rely on shallow heuristics. Another line of work under "adversarial datasets" also focuses on creating examples by perturbing the standard test set to fool the model <ref type="bibr" target="#b57">(Smith (2012)</ref>; <ref type="bibr" target="#b19">Jia and Liang (2017)</ref>, inter-alia).</p><p>Challenge sets for evaluating MT systems have focused on the translation models' ability to generate the correct translation given a phenomenon of interest. These include word sense ambiguity <ref type="bibr" target="#b62">(Vamvas and Sennrich, 2021)</ref>, gender bias <ref type="bibr" target="#b53">(Rudinger et al., 2017;</ref><ref type="bibr" target="#b71">Zhao et al., 2018;</ref><ref type="bibr" target="#b60">Stanovsky et al., 2019)</ref>, structural divergence <ref type="bibr" target="#b18">(Isabelle et al., 2017)</ref> and discourse level phenomena <ref type="bibr" target="#b13">(Guillou and Hardmeier, 2016;</ref><ref type="bibr" target="#b8">Emelin and Sennrich, 2021)</ref>.</p><p>While such challenge sets focus on evaluating specific machine translation models, it is necessary to identify whether the existing machine translation evaluation metrics also perform well under these and related phenomena. Developing challenge sets for machine translation metric evaluation has gained considerable interest because recently, neural MT evaluation metrics have shown improved correlation with human judgements <ref type="bibr">(Freitag et al., 2021b;</ref><ref type="bibr" target="#b22">Kocmi et al., 2021)</ref>. However, their weaknesses remain relatively unknown and only a small number of works (e.g. <ref type="bibr" target="#b15">Hanna and Bojar (2021)</ref> and <ref type="bibr" target="#b0">Amrhein and Sennrich (2022)</ref>) have proposed systematic analyses to uncover them.</p><p>Previous challenge sets for metric evaluation focused on negation and sentiment polarity <ref type="bibr" target="#b58">(Specia et al., 2020)</ref> and synthetic perturbations such as antonym replacement, word omission, number swapping, punctuation removal, etc. <ref type="bibr">(Freitag et al., 2021b)</ref>. <ref type="bibr" target="#b1">Avramidis et al. (2018)</ref> developed a manually constructed test suite of linguistically motivated perturbations for identifying weaknesses in reference-free evaluation. However, these challenge sets for metrics are only focused on highresource language pairs such as English?German and English?Chinese. In this work, we repurpose existing machine translation challenge sets to evaluate machine translation evaluation metrics. We introduce several synthetically generated and manually created challenge sets that broadly focus on translation accuracy errors for 146 language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>The majority of the examples in our challenge set were based on data extracted from three main datasets: FLORES-101, PAWS-X, and XNLI (with additional translations from XTREME).</p><p>The FLORES-101 evaluation benchmark <ref type="bibr" target="#b12">(Goyal et al., 2022)</ref> consists of 3,001 sentences extracted from English Wikipedia and translated into 101 languages by professional translators. <ref type="bibr">FLORES-200 (NLLB Team et al., 2022)</ref> expands the set of languages in FLORES-101. Originally intended for multilingual and low-resource MT evaluation, these datasets have a particular focus on low-resource languages.</p><p>PAWS-X <ref type="bibr" target="#b68">(Yang et al., 2019)</ref>, a cross-lingual dataset for paraphrase identification, consists of pairs of sentences that are labelled as true or adversarial paraphrases. It comprises the Wikipedia portion of the PAWS corpus  translated from English into six languages: French, Spanish, German, Chinese, Japanese, and Korean.</p><p>The development and test sets (23,659 sentences total) were manually translated by professional translators, and the training set was translated using NMT systems via Google Cloud Translation 3 .</p><p>XNLI <ref type="bibr" target="#b5">(Conneau et al., 2018</ref>) is a multilingual Natural Language Inference (NLI) dataset consisting of 7,500 premise-hypothesis pairs with their corresponding inference label. The English examples were generated by crowd source workers before being manually translated into 14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu. In addition, we use the automatic translations from XTREME <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> of the XNLI test set examples from these 14 languages into English.</p><p>For the mistranslation phenomena Gender in Occupation Names and Word Sense Disambiguation, we leveraged the WinoMT and MuCoW datasets.</p><p>WinoMT <ref type="bibr" target="#b60">(Stanovsky et al., 2019)</ref>, a challenge set developed for analysing gender bias in MT, contains 3,888 English examples extracted from the Winogender <ref type="bibr" target="#b53">(Rudinger et al., 2017)</ref> and WinoBias <ref type="bibr" target="#b71">(Zhao et al., 2018)</ref> coreference test sets. WinoMT sentences cast participants into non-stereotypical gender roles and the dataset has an equal balance of male and female genders, and of stereotypical and non-stereotypical gender-role assignments (e.g., a female nurse vs. a female doctor). Mu-CoW <ref type="bibr" target="#b46">(Raganato et al., 2019)</ref> is a multilingual contrastive, word sense disambiguation test suite for machine translation. The dataset covers 16 language pairs with more than 200,000 contrastive sentence pairs. It was automatically constructed from word-aligned parallel corpora and BabelNet's <ref type="bibr" target="#b39">(Navigli and Ponzetto, 2012)</ref> wide-coverage multilingual sense inventory.</p><p>For the discourse-level phenomena, we relied on annotated resources developed specifically to support work on those phenomena in an MT setting. The WMT 2018 English-German pronoun translation evaluation test suite <ref type="bibr" target="#b14">(Guillou et al., 2018)</ref> contains 200 examples of the ambiguous English pronouns it and they extracted from the TED talks portion of ParCorFull . The example sentences were translated into German by the 16 English-German systems submitted to WMT 2018, and the (German) pronoun translations were manually judged by human annotators as "good/bad". Wino-X (Emelin 3 https://cloud.google.com/translate and Sennrich, 2021) is a parallel dataset of German, French, and Russian Winograd schemas, aligned with their English counterparts. It was developed for commonsense reasoning and coreference resolution and used for this purpose to generate examples for Commonsense Co-Reference Disambiguation. The Europarl ConcoDisco corpus <ref type="bibr" target="#b26">(Laali and Kosseim, 2017)</ref> comprises the English-French parallel texts from Europarl <ref type="bibr" target="#b24">(Koehn, 2005)</ref> over which automatic methods were used to perform PDTB-style discourse connective annotation. Discourse connectives are labelled with their sense type and are aligned between the two languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Challenge Sets</head><p>Creating a contrastive challenge set for evaluating a machine translation evaluation metric requires a source sentence, a reference translation, and two translation hypotheses: one which contains an error or phenomenon of interest (the "incorrect" translation) and one which is a correct translation in that respect (the "good" translation). One possible way to create such challenge sets is to start with two alternative references (or two identical copies of the same reference) and insert errors into one of them to form an incorrect translation while the uncorrupted version can be used as the good translation. This limits the full evaluation scope to translation hypotheses that only contain a single error. To create a more realistic setup, we also create many challenge sets where the good translation is not free of errors, but it is a better translation than the incorrect translation. For automatically created challenge sets, we put measures in place to ensure that the incorrect translation is indeed a worse translation than the good translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Addition and Omission</head><p>We create a challenge set for addition and omission errors which are defined in the MQM ontology as "target content that includes content not present in the source" and "errors where content is missing from the translation that is present in the source", respectively. We focus on the level of constituents and use an implementation by <ref type="bibr" target="#b63">Vamvas and Sennrich (2022)</ref> to create synthetic examples of addition and omission errors.</p><p>To generate examples, we use the concatenated dev and devtest sets from the FLORES-101 evaluation benchmark. We focus on the 46 languages for which there exists a stanza parser 4 and create datasets for all languages paired with English plus ten additional language pairs that we selected randomly. The script by <ref type="bibr" target="#b63">Vamvas and Sennrich (2022)</ref> randomly drops constituents from the source sentence and then generates two translations, one of the full source and one of the partial source without the constituent. Here is an example of two resulting translations: Only partial translations that can be constructed by deleting spans from the full translation are considered. For translation, we use the M2M100 5 model with 1.2B parameters <ref type="bibr" target="#b9">(Fan et al., 2021)</ref>.</p><p>We create omission examples by taking the original source and reference and using the translation of the full source as a good translation and the translation of the partial source as an incorrect translation. For addition errors, we test if the deleted span also occurs in the reference. If it doesn't, we discard the example, if it does, we delete that span from the reference and pair this partial reference with the partial source. Then, the good translation is the translation of the partial source and the incorrect translation is the translation of the full source. For language pairs with a BLEU score of less than 13 between the good translation and the reference, we manually check the examples to ensure the challenge set features appropriate examples of additions and omissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mistranslation -Ambiguous Translation</head><p>This error type is defined in the MQM ontology as a case where "an unambiguous source text is translated ambiguously". For this error type, we create challenge sets where MT metrics are presented with an unambiguous source and an ambiguous reference. The metrics then need to choose between two disambiguated translation hypotheses where only one meaning matches the source sentence. Therefore, these challenge sets test whether metrics consider the source when the reference is not expressive enough to identify the better translation. Since many reference-based metrics, by design, do not include the source to compute evaluation scores, we believe that this presents a challenging test set.</p><p>Our method for creating examples is inspired by <ref type="bibr" target="#b62">Vamvas and Sennrich (2021)</ref> who score a translation against two versions of the source sentence, one with an added correct disambiguation cue and one with a wrong disambiguation cue to determine whether a translation model produced the correct translation or not. Instead of adding the disambiguation cues to the source, we use an unambiguous source and add disambiguation cues to an ambiguous reference to create two contrasting translation hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Ambiguity -Occupation Names Gender</head><p>First, we create a challenge set based on WinoMT, where the challenge is to choose either a translation with a "female" or "male" disambiguation cue based on the source sentence: The manager fired the female baker. :</p><p>The manager fired the male baker.</p><p>We take all English sentences from the WinoMT dataset where either a pro-stereotypical or an antistereotypical occupation name occurs. The original sentences in WinoMT contain additional context from which the gender in the English sentence can be inferred. For example, the sentence above exists in the dataset once as "The manager fired the baker because she was too rebellious." from which it is clear that the baker is female, and once as "The manager fired the baker because he was upset." from which it is clear that the manager is male. To make the English sentences ambiguous, we remove the explanatory subordinate clauses using a sequence of regular expressions, so that the sentence becomes "The manager fired the baker." where the gender of the manager and the baker are ambiguous.</p><p>We then add the disambiguation cues ("female" or "male") to the ambiguous English sentences and translate them into German, French and Italian which are all languages that mark gender morphologically on most nouns that refer to a person. For translation, we use Google Translate 6 because we find that this system produces gendered occupation names that are largely faithful to the disambiguation cues. Finally, we remove explicit translations of "female" and "male" from the German, French or Italian output that would help the disambiguation beyond morphological cues. We predict the gender of the occupation names using the scripts provided by <ref type="bibr" target="#b60">Stanovsky et al. (2019)</ref> and only keep translation pairs where both the translation of the maledisambiguated source is predicted to be male and the translation of the female-disambiguated source is predicted to be female. We then use either the German, French or Italian translation as the source sentence, the disambiguated English sentences as the translation candidates, and the ambiguous English sentence as the reference, as shown in the example above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Ambiguity -Word Sense Disambiguation</head><p>Second, we create a challenge set based on Mu-CoW, where the challenge is to choose a translation with a sense-matching disambiguation cue based on the unambiguous source sentence: SRC (de): Was heisst "Br?he"? REF (en): What does "stock" mean? :</p><p>What does "vegetable stock" mean? :</p><p>What does "penny stock" mean?</p><p>We start with disambiguation cues that were automatically extracted by <ref type="bibr" target="#b62">Vamvas and Sennrich (2021)</ref> via masked language modelling. Initial screening of the data shows that some disambiguation cues are not sense-specific enough. Therefore, we decide to manually check all disambiguation cues and ensure they are sense-specific and if necessary, replace them with other cues. We generate three pairs of contrasting disambiguation cues per example and use the question "What does X mean?" as a pattern to create the challenge set examples. We decided against using sentences where ambiguous words occur naturally since it may be possible to infer the correct sense from the context of the English sentence rather than by looking at the unambiguous source word. We annotate each example as to whether the correct sense is the more frequent or less frequent sense using frequency counts provided by <ref type="bibr" target="#b62">Vamvas and Sennrich (2021)</ref>. Following this methodology, we create challenge sets for German into English and Russian into English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Ambiguity -Discourse Connectives</head><p>Third, we create a challenge set where the challenge is to identify a translation with the correct discourse connective based on the unambiguous source sentence:</p><formula xml:id="formula_0">SRC (fr):</formula><p>Aucun test de qualit? de l'air n'ait ?t? r?alis? dans ce b?timent depuis notre ?lection. <ref type="bibr">REF (en)</ref>: No air quality test has been done on this particular building since we were elected. :</p><p>No air quality test has been done on this particular building from the time we were elected. :</p><p>No air quality test has been done on this particular building because we were elected.</p><p>The English discourse connective "since" can have either causal or temporal meaning, which is expressed explicitly in both French and German. Exploiting this fact, we use the ambiguous "since" in the reference and create two contrastive translations one with "because" for causal meaning and one with "from the time" for temporal meaning. The correct translation is determined by looking at the French or German source sentence where this information is marked explicitly. We use the discourse connective annotations in the Europarl ConcoDisco corpus for this challenge set. We use an automatic-guided search based on the French discourse connective "depuis" (which has temporal meaning) to identify candidate translation pairs. We then manually construct valid contrasting examples for causal and temporal "since" based on the English reference. This results in a challenge set for French-English but we also create a German-English version of the challenge set, where we translate the French source sentences into German and manually correct them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mistranslation -Hallucinations</head><p>In this category, we group together several subcategories of mistranslation errors that happen at the word level and could occur due to hallucination by an MT model. Such errors are wrong units, wrong dates or times, wrong numbers or named entities, as well as hallucinations at the subword level that result in nonsensical words. We also present a challenge set of annotated hallucinations in real MT outputs. These challenge sets test whether the machine translation evaluation metrics can reliably identify hallucinations when presented with a correct alternative translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Hallucination -Date-Time Errors</head><p>We create a challenge set for the category of "datetime errors". To do this, we collect month names and their abbreviations for several language pairs. We then form a good translation by swapping a month's name with its abbreviation. The corresponding incorrect translation is generated by swapping the month name with another month name: SRC (pt): Os manifestantes esperam coletar uma peti??o de 1,2 milh?o de assinaturas para apresentar ao Congresso Nacional em novembro. REF (en): Protesters hope to collect a petition of 1.2 million signatures to present to the National Congress in November. :</p><p>The protesters expect to collect a petition of 1.2 million signatures to be submitted to the National Congress in Nov. :</p><p>The protesters expect to collect a petition of 1.2 million signatures to be submitted to the National Congress in August.</p><p>To create this dataset, we use the automatic translations of the FLORES-101 dataset from Section 5.1. We choose all pairs with target languages for which we know the abbreviations for months 7 which results in 70 language pairs. As a measure of control, we check that the identified month names in the translation also occur in the reference. If they do not, we exclude the example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Hallucination -Numbers and Named Entities</head><p>We create a challenge set for numbers and named entities where the challenge is to identify translations with incorrect numbers or named entities. Following the analysis by <ref type="bibr" target="#b0">Amrhein and Sennrich (2022)</ref>, we perform character-level edits (adding, removing or substituting digits in numbers or characters in named entities) as well as word-level edits (substituting whole numbers or named entities). In the 2021 WMT metrics shared task, number differences were not a big issue for most neural metrics <ref type="bibr">(Freitag et al., 2021b)</ref>. However, we believe that simply changing a number in an alternative translation and using this as an incorrect translation as done by <ref type="bibr">Freitag et al. (2021b)</ref> is an overly simplistic setup and does not cover the whole translation hypothesis space.</p><p>To address this shortcoming, we propose a threelevel evaluation (see examples below). The first, easiest level follows <ref type="bibr">Freitag et al. (2021b)</ref> and applies a change to an alternative translation to form an incorrect translation. The second level uses an alternative translation that is lexically very similar to the reference as the good translation and applies a change to the reference to form an incorrect translation. The third, and hardest level, uses an alternative translation that is lexically very different from the reference as the good translation and applies a change to the reference to form an incorrect translation. In this way, our challenge set tests whether number and named entity differences can still be detected as the surface similarity between the two translation candidates decreases and the surface similarity between the incorrect translation and the reference increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRC (es):</head><p>Sin embargo, Michael Jackson, Prince y Madonna fueron influencias para el ?lbum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REF (en):</head><p>Michael Jackson, Prince and Madonna were, however, influences on the album.</p><p>Level-1 : However, Michael Jackson, Prince, and Madonna were influences on the album. Level-1 :</p><p>However, Michael Jackson, Prince, and Garza were influences on the album.</p><p>Level-2 : However, Michael Jackson, Prince, and Madonna were influences on the album. Level-2 :</p><p>Michael Jackson, Prince and Garza were, however, influences on the album.</p><p>Level-3 : The record was influenced by Madonna, Prince, and Michael Jackson though. Level-3 :</p><p>Michael Jackson, Prince and Garza were, however, influences on the album.</p><p>We use cross-lingual paraphrases from the PAWS-X dataset as a pool of alternative translations to create this challenge set. For levels 2 and 3, we measure surface-level similarity with Levenshtein distance 8 at the character-level and use spacy 9 (Honnibal et al., 2020) for identifying named entities of type "person". To substitute whole named entities, we make use of the names 10 Python library. We only consider language pairs for which we can use a spacy NER model on the target side, which results in 42 language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Hallucination -Unit Conversion</head><p>We create a challenge set for unit conversions where the challenge is to identify the correct unit conversion: <ref type="bibr">SRC (de)</ref>: Auf einem 100 Fu? langen Teilabschnitt l?uft Wasser ?ber den Damm. REF (en): Water is spilling over the levee in a section 100 feet wide. :</p><p>On a 30.5 metres long section, water flows over the dam. :</p><p>On a 100 metres long section, water flows over the dam.</p><p>We take all source sentences, reference sentences and translations of the FLORES-101 sets from Section 5.1. We only use the 45 language pairs into English since the Python packages we use for unit conversion only work for English. We first use the Python package quantulum3 11 to extract unit mentions from text. We only consider sentences where we identify the same unit mentions in the translation as in the reference and we remove self-disambiguating unit mentions, like "645 miles (1040 km)" from the reference and translation. Then, we use the Python package pint 12 to convert unit mentions in the translation into different units. The permitted conversions are listed in Appendix A.2.</p><p>The sentence with the converted amount and new unit is considered to be the good translation. Based on this sentence, we construct two incorrect versions, one where the amount matches the reference but the unit is still converted (see example above) and one where the amount is the converted amount but the unit is copied from the reference. We pair each incorrect translation with the good translation and add both examples to the challenge set individually. We are aware that this challenge set lies beyond the ability of current MT systems and evaluation metrics, however, we believe challenge sets such as these incentivise future work on such capabilities which would reduce the workload in post-editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Hallucination -Nonsense Words</head><p>We also consider more natural hallucinations at the subword level. Because recent MT systems are trained with subwords <ref type="bibr" target="#b55">(Sennrich et al., 2016)</ref>, an MT model may choose a wrong subword at a specific time step such that the resulting token is not a known word in the target language. With this challenge set, we are interested in how well neural MT evaluation metrics that incorporate subword-level tokenisation can identify such "nonsense" words.</p><p>To create this challenge set, we consider tokens which are broken down into at least two subwords and then randomly swap those subwords with other subwords to create nonsense words. In the example below, "mass" is broken down as "mas" and "##s" using subwords and the new word is created by swapping "mas" with "in" while retaining "##s", creating "ins" as the nonsense word. We use the paraphrases from the PAWS-X dataset as good translations and randomly swap one subword in the reference to generate an incorrect translation. This perturbation is language-agnostic. We use the multilingual BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> tokeniser to replace the subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRC (de): Die Massenproduktion von elektron-</head><p>ischen und digitalen Filmen war bis zum Aufkommen der pornographischen Videotechnik direkt mit der Mainstream-Filmindustrie verbunden. REF (en): The mass production of electronic and digital films was directly linked to the mainstream film industry until the emergence of pornographic video technology. :</p><p>Until the advent of pornographic video technology , the mass production of electronic and digital films was tied directly to the mainstream film industry. :</p><p>The ins production of electronic and digital films was directly linked to the mainstream film industry until the emergence of pornographic video technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Hallucination -Real Data Hallucinations</head><p>The previously discussed hallucination challenge sets were all created automatically. In addition to these challenge sets, we also create one with real data hallucinations. For this dataset, we manually check the translations of the FLORES-101 dev and devtest sets for four language pairs: de?en, en?de, fr?de and en?mr. We consider both cases where a more frequent, completely wrong word occurs and cases where the MT model started with the correct subword but then produced random subwords as hallucinations. Translations with a hallucination are used as incorrect translations. We manually replace the hallucination part with its correct translation to form the good translation. If possible, we create one good translation by copying the corresponding token(s) from the reference and one with a synonymous token that does not match the reference: SRC (de): Es wird angenommen, dass dieser voll gefiederte warmbl?tige Raubvogel aufrecht auf zwei Beinen lief und Krallen wie der Velociraptor hatte. REF (en): This fully feathered, warm blooded bird of prey was believed to have walked upright on two legs with claws like the Velociraptor. (copy): It is believed that this fully feathered warm-blooded predator ran upright on two legs and had claws like the Velociraptor. (syn.):</p><p>It is believed that this fully feathered warm-blooded predator ran upright on two legs and had talons like the Velociraptor. :</p><p>It is believed that this fully feathered warm-blooded predator ran upright on two legs and had crumbs like the Velociraptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Mistranslation -Lexical Overlap</head><p>Language models trained with the masked language modelling objective are successful on downstream tasks because they model higher-order word co-occurrence statistics instead of syntactic structures <ref type="bibr" target="#b56">(Sinha et al., 2021)</ref>. Although this has been shown for a monolingual English model, we expect that multilingual pre-trained models, as well as MT metrics finetuned on such models, exhibit such behaviour. Similarly, existing surface-level metrics rely on n-gram matching between the hypothesis and the reference. Thus, we are interested in whether MT evaluation metrics can reliably identify the incorrect translation if it shares a high degree of lexical overlap with the reference: He served as a guest speaker for <ref type="bibr">ICM in 1924</ref><ref type="bibr">ICM in , 1932</ref><ref type="bibr">ICM in and 1936</ref> in Toronto, Oslo and Zurich. :</p><p>He was an invited spokesman for the ICM in <ref type="bibr">Toronto in 1924</ref><ref type="bibr">, in Zurich in 1932</ref><ref type="bibr">and in Oslo in 1936</ref> In this example, Oslo and Zurich are swapped in the "incorrect translation" making the sentence factually incorrect. To create such examples, we use the PAWS-X dataset for which adversarial paraphrase examples were constructed by changing the word order and/or the syntactic structure while maintaining a high degree of lexical overlap. We only consider examples in the development set that are adversarial paraphrases.</p><p>We automatically translate the first example in a pair (fr?en, en?fr, en?ja) and then manually correct the translations for en, fr, and ja to obtain 100 "good translations" per language. We use the corresponding first paraphrase as the "reference" and the second (adversarial) paraphrase as the "incorrect translation". We then pair these examples with the first paraphrase in the remaining six languages in PAWS-X to obtain the "source". Following this methodology we create examples for each target language (xx?en, xx?fr, xx?ja).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Mistranslation -Linguistic Modality</head><p>Modal auxiliary verbs signal the function of the main verb that they govern. For example, they may be used to denote possibility ("could"), permission ("may"), the giving of advice ("should"), or necessity ("must"). We are interested in whether MT evaluation metrics can identify when modal auxiliary verbs are incorrectly translated: With the introduction of this regulation, this freedom could end. :</p><p>With the introduction of this regulation, this freedom will end.</p><p>We focus on the English modal auxiliary verbs: "must" (necessity), and "may", "might", "could" (possibility). We begin by identifying parallel sentences where there is a modal verb in the German source sentence and one from our list (above) in the English reference. We then translate the source sentence using Google Translate to obtain the "good" translation and manually replace the modal verb with an alternative with the same meaning where necessary (e.g. "have to" denotes necessity as does "must"; also "might", "may" and "could" are considered equivalent). For the incorrect translation, we manually substitute the modal verb that conveys a different meaning or epistemic strength e.g. in the example above "might" (possibility) is replaced with "will", which denotes (near) certainty. Instances of "may" with deontic meaning (e.g. expressing permission) are excluded from the set, leaving only those with an epistemic meaning (ex-pressing probability or prediction). We also construct examples in which the modal verb is omitted from the incorrect translation.</p><p>We employ two strategies to create examples: one in which the modal auxiliary is substituted, and another where it is deleted. We use a combination of the FLORES-200 and PAWS-X datasets as the basis of the challenge sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Mistranslation -Overly Literal Translations</head><p>MQM defines this error type as translations that are overly literal, for example literal translations of figurative language. Here, we look specifically at idioms and at real-data errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Overly Literal -Idioms</head><p>Idioms tend to be translated overly literally <ref type="bibr" target="#b6">(Dankers et al., 2022)</ref> and it is interesting to see if such translations are also preferred by neural machine translation evaluation metrics, which likely have not seen many idioms during finetuning: SRC (de): Er hat versucht, mir die Spielregeln zu erkl?ren, aber ich verstand nur Bahnhof. REF (en): He tried to explain the rules of the game to me, but I did not understand them. :</p><p>He tried to explain the rules of the game to me, but it was all Greek to me. :</p><p>He tried to explain the rules of the game to me, but I only understood train station.</p><p>We create this challenge set based on the PIE 13 parallel corpus of English idiomatic expressions and literal paraphrases <ref type="bibr" target="#b72">(Zhou et al., 2021)</ref>. We manually translate 102 parallel sentences into German for which we find a matching idiom that is not a word-by-word translation of the original English idiom. Further, we create an overly-literal translation of the English and German idioms. We use either the German or English original idiom as the source sentence. Then, we either use the correct idiom in the other language as the reference and the literal paraphrase as the good translation, or vice versa. The incorrect translation is always the overly-literal translation of the source idiom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Overly-Literal -Real Data Errors</head><p>We are also interested in overly-literal translations occurring in real data: For this challenge set, we manually check MT translations of the FLORES-101 datasets. If we find an overly-literal translation, we manually correct it to form the good translation. We create one good translation where we copy the part of the reference that corresponds to the overly-literal part and, if possible, another good translation where we use a synonym of the reference token. This challenge set contains examples for four language pairs: de?en, en?de, fr?de and en?mr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Mistranslation -Sentence-Level Meaning Error</head><p>We also consider a special case of sentence-level semantic error that arises due to the nature of the task of Natural Language Inference (NLI). The task of NLI requires identifying where the given hypothesis is an entailment, contradiction, or neutral, with respect to a given premise. As a result, the premise and hypothesis have substantial overlap but they vary in meaning. We are interested in whether MT evaluation metrics can pick up on such sentencelevel meaning changes:</p><formula xml:id="formula_1">SRC (el): ? ??????????? ??????? ?????? ???? ????????????. REF (en): Real noise appeals to the old. (premise) :</formula><p>The real noise attracts the elderly. :</p><p>Real noise appeals to the young and appalls the old. (hypothesis)</p><p>We use the XNLI dataset to create such examples. We consider examples where there is at least 0.5 chrF score between the English premise and hypothesis and where the labels are either contradiction or neutral. Examples with an entailment label are excluded as some examples in the dataset are paraphrases of each other and there would be no sentence-level meaning change. We discuss ef-fects of entailment in Section 5.12.1. We use either the premise or the hypothesis as the reference and an automatic translation as the "good translation". The corresponding premise or hypothesis from the remaining 14 languages is used as the source. The "incorrect translation" is either the premise if the reference is the hypothesis, or vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Mistranslation -Ordering Mismatch</head><p>We also investigate the effects of changing word order in a way that changes meaning: Fill your home with a delicious coffee in the morning and some relaxing chamomile tea in the evening. :</p><p>Fill your home with a delicious chamomile tea in the morning and some relaxing coffee in the evening.</p><p>This challenge set is created manually by changing translations from the FLORES-101 dataset and covers de?en, en?de and fr?de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Mistranslation -Discourse-level Errors</head><p>We introduce a new subclass of mistranslation errors that specifically cover discourse-level phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.1">Discourse-level Errors -Pronouns</head><p>First, we are interested in how MT evaluation metrics handle various discourse-level phenomena related to pronouns. To create these challenge sets, we use the English-German pronoun translation evaluation test suite from the WMT 2018 shared task as the basis for our examples.</p><p>We extract all translations (by the English-German WMT 2018 systems) that were marked as "correct" by the human annotators, for the following six categories derived from the manually annotated pronoun function and attribute labels: pleonastic it, anaphoric subject and non-subject position it, anaphoric they, singular they, and group it/they. In the case of anaphoric pronouns, we select only the inter-sentential examples (i.e. where the sentence contains both the pronoun and its antecedent). We use the MT translations as the "good" translations and automatically generate "incorrect" translations using one of the following strategies:</p><p>omission -the translated pronoun is deleted from the MT output, substitution -the "correct" pronoun is replaced with an "incorrect" form.</p><p>For anaphoric pronouns, when translated from English into a language with grammatical gender, such as German, the pronoun translation must a) agree in number and gender with the translation of its antecedent, and b) have the correct grammatical case. We propose "incorrect" translations as those for which this agreement does not hold: Conversely, for pleonastic uses of "it" no agreement is required, instead, the correct translation in German requires a simple mapping: "it" ? "es". An 'incorrect" translation of pleonastic 'it' in German could be "er" (masc. sg.) or "sie" (fem. sg., or pl.). We create, for each "correct" translation a set of possible "incorrect" values and automatically select one at random to replace the "correct" pronoun. For example, in the pleonastic case:</p><formula xml:id="formula_2">SRC (en): It is raining REF (de): Es regnet :</formula><p>Es regnet (subs.):</p><p>Er regnet (omit):</p><p>? regnet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.2">Discourse-level Errors -Discourse Connectives</head><p>The English discourse connective "while" is ambiguous -it may be used with either a Comparison.Contrast or Temporal.Synchrony sense -as are two of its possible translations into French: "tandis que" and "alors que". We leverage a corpus of parallel English/French sentences with discourse connectives marked and annotated for sense, and select examples with ambiguity in the French source sentence. We construct the good translation by replacing instances of "while" temporal with "as" or "as long as" and instances of "while" comparison as "whereas" (ensuring grammaticality is preserved). For the incorrect translation, we replace the discourse connective with one with the alternative sense of "while" e.g. we use "whereas" (comparison) where a temporal sense is required:</p><formula xml:id="formula_3">SRC (fr):</formula><p>Dans l'UE-10, elles ont progress? de 8% tandis que la dette pour l'UE-2 a augment? de 152%. REF (en): In EU-10 they grew by 8% while the debt for the EU-2 increased by 152%. :</p><p>In the EU-10, they increased by 8% when the debt for the EU-2 increased by 152%. :</p><p>In the EU-10, they increased by 8% whereas the debt for the EU-2 increased by 152%.</p><p>We extract our examples from the Europarl Con-coDisco dataset. We automatically selected the sentence pairs that contain an instance of "while" in English and either "alors que" or "tandis que" in French. Our dataset contains examples for both the Comparison.Contrast sense and the Temporal.Synchrony sense.</p><p>This challenge set complements the discourse connectives set in section 5.2.3, in which the English discourse connective "since" is ambiguous, but the corresponding connectives in French and German are not. Note that while in the previous challenge set the correct translation can be identified by looking at the source, here metrics can only rely on context to identify the correct discourse connective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.3">Discourse-level Errors -Commonsense</head><p>Co-Reference Disambiguation</p><p>One of the greater challenges within computational coreference resolution is referring to the correct antecedent by using commonsense/real-world knowledge. <ref type="bibr" target="#b8">Emelin and Sennrich (2021)</ref> construct a benchmark to test whether multilingual language models and neural machine translation models can perform such commonsense coreference resolutions. We are interested in whether such commonsense coreference resolutions pose a challenge for MT evaluation metrics: SRC (en): It took longer to clean the fish tank than the dog cage because it was dirtier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REF (de):</head><p>Das Reinigen des Aquariums dauerte l?nger als das des Hundek?figs, da es schmutziger war. :</p><p>Das Reinigen des Aquariums dauerte l?nger als das des Hundek?figs, da das Aquarium schmutziger war. :</p><p>Die Reinigung des Aquariums dauerte l?nger als die des Hundek?figs, da er schmutziger war.</p><p>The English sentences in the Wino-X challenge set were sampled from the Winograd schema. All contain the pronoun it and were manually translated into two contrastive translations for de, fr, and ru. Based on this data, we create our challenge sets covering two types of examples: For the first, the good translation contains the pronoun referring to the correct antecedent, while the incorrect translation contains the pronoun referring to the incorrect antecedent. For the second, the correct translation translates the instance of it into the correct disambiguating filler, while the second translation contains the pronoun referring to the incorrect antecedent (see example above).</p><p>The sentences for en?de were common across both the challenge sets developed by <ref type="bibr" target="#b8">Emelin and Sennrich (2021)</ref>. Hence, the corresponding correct translations from the two challenge sets were used as the "good" translation for our evaluation setup. For en?ru and en?fr, the source containing the ambiguous pronoun was machine translated and then verified by human annotators to form the "good" translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Untranslated</head><p>MQM defines this error type as "errors occurring when a text segment that was intended for translation is left untranslated in the target content". In ACES, we consider both word-level and sentencelevel untranslated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.1">Untranslated -Word-Level</head><p>For word-level untranslated content, we manually annotate translations of the FLORES-101 dev and devtest sets: SRC (fr): ? l'origine, l'?mission mettait en sc?ne des com?diens de doublage amateurs, originaires de l'est du Texas. REF <ref type="formula">(</ref> We do not only count complete copies as untranslated content but also content that clearly comes from the source language but was only adapted to look more like the target language (as in the example above). If we encounter an untranslated span, we use this translation as the incorrect translation and create a good translation by copying the correct span from the reference and, if possible, a second good translation where we use a synonym for the correct reference span. We manually annotate such untranslated errors for en?de, fr?de, de?en, en?mr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.2">Untranslated -Full Sentences</head><p>In the case of underperforming machine translation models, sometimes the generated output contains a majority of the tokens from the source language to the extent of copying the entire source sentence. <ref type="bibr">14</ref> We create a challenge set by simply copying the entire source sentence as the incorrect translation. We used a combination of examples from the FLORES-200, XNLI, and PAWS-X datasets to create these examples.</p><p>We expect that this challenge set is likely to break embedding-based, reference-free evaluation because the representation of the source and the incorrect translation will be the same, thus leading to a higher score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Do Not Translate Errors</head><p>This category of errors is defined in MQM as content in the source that should be copied to the output in the source language, but was mistakenly translated into the target language. Common examples of this error type are company names or slogans. Here, we manually create a challenge set based on the PAWS-X data which contains many song titles that should not be translated: SRC (en): Dance was one of the inspirations for the exodus -song "The Toxic Waltz", from their 1989 album "Fabulous Disaster".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REF (de): Dance war eine der Inspirationen f?r das</head><p>Exodus-Lied "The Toxic Waltz" von ihrem 1989er Album "Fabulous Disaster". :</p><p>Der Tanz war eine der Inspirationen f?r den Exodus-Song "The Toxic Waltz", von ihrem 1989er Album "Fabulous Disaster". :</p><p>Der Tanz war eine der Inspirationen f?r den Exodus-Song "Der Toxische Walzer", von ihrem 1989er Album "Fabulous Disaster".</p><p>To construct the challenge set, we use one paraphrase as the good translation and manually translate an English sequence of tokens (e.g. a song title) into German to form the incorrect translation. 14 Through observations of Swahili ? English translation; unpublished work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">Overtranslation and Undertranslation</head><p>Hallucinations from a translation model can often produce a term which is either more generic than the source word or more specific. Within the MQM ontology, the former is referred to as undertranslation while the latter is referred to as overtranslation. For example, "car" may be substituted with "vehicle" (undertranslation) or "BMW" (overtranslation). To automate the generation of such errors, we use Wordnet <ref type="bibr" target="#b35">(Miller, 1994)</ref>. In our setup a randomly selected noun from the reference translation is replaced by its corresponding hypernym or hyponym to simulate undertranslation or overtranslation errors, respectively: Bob and Ted were brothers, and Ted is John's son. :</p><p>Bob and Ted were brothers. Ted is John 's male offspring.</p><p>During the implementation, we only replaced the first sense listed in Wordnet for the corresponding noun, which may not be appropriate in the given translation. We constructed this challenge set for hypernyms and hyponyms using the PAWS-X dataset, only considering the language pairs where the target language is English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12">Real-world Knowledge</head><p>We manually constructed examples each for en?de and de?en for the first four phenomena described in this section. We used German-English examples from XNLI, plus English translations from XTREME as the basis for our examples. Typically, we select a single sentence, either the premise or hypothesis from XNLI, and manipulate the MT translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12.1">Real-world Knowledge -Textual Entailment</head><p>We test whether the metrics can recognise textual entailment -that is, whether a metric can recognise that the meaning of the source/reference is entailed by the "good" translation. We construct examples for which the good translation entails the meaning of the original sentence (and its reference). For example, we use the entailment was murdered ? died (i.e. if a person is murdered then they must have died) to construct the good translation in the example above. We construct the incorrect translation by replacing the entailed predicate (died) with a related but non-entailed predicate (here was attacked) -a person may have been murdered without being attacked, i.e. by being poisoned for example. When constructing our examples we focus solely on leveraging directional entailments. We specifically exclude paraphrases as these are bidirectional.</p><p>In cases where an antonymous predicate is available, we use that predicate in the incorrect translation. For example, if "lost" is in the source/reference, we use "won" in the incorrect translation (lost ? won). <ref type="bibr">SRC (de)</ref>: Ein Mann wurde ermordet. REF (en): A man was murdered. :</p><p>A man died.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(omit):</head><p>A man was attacked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12.2">Real-world Knowledge -Hypernyms and Hyponyms</head><p>We consider a translation that contains a hypernym of a word to be better than one that contains a hyponym. For example, whilst translating "Hund" ("dog") with the broader term "animal" results in some loss of information, this is preferable over hallucinating information by using a more specific term such as "labrador" (i.e. an instance of the hyponym class "dog"): We used Wordnet and WordRel.com 15 (an online dictionary of words' relations) to identify hypernyms and hyponyms of nouns within the reference sentences, and used these as substitutions in the MT output: hypernyms are used in the "good" translations and hyponyms in the "incorrect" translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12.3">Real-world Knowledge -Hypernyms and Distractors</head><p>Similar to the hypernym vs. hyponym examples, we construct examples in which the good translation contains a hypernym (here "pet") of the word 15 https://wordrel.com/ in the reference (here "dog"). We form the incorrect translation by replacing the original word in the source/reference with a different member from the same class (here "cat"; both cats and dogs belong to the class of pets ... the cat belonged to my sister.</p><p>As before, we used Wordnet and WordRel.com to identify hypernyms of nouns present in the reference translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12.4">Real-world Knowledge -Antonyms</head><p>Similar to the generation of over-and undertranslations, we also constructed "incorrect" translations by replacing words with their corresponding antonyms from Wordnet. We construct challenge sets for both nouns and verbs.</p><p>For nouns, we automatically constructed "incorrect" translations by replacing nouns in the reference with their antonyms. The "good" translation is not amended. This method may result in noisy replacement of nouns with their respective antonyms.</p><p>In the case of verbs, we manually constructed a more challenging set of examples intended to be used to assess whether the metrics are able to distinguish between translations that contain a synonym versus an antonym of a given word. We replaced verbs in the reference with a synonym to produce the good translation, and with their antonym to produce the incorrect translation: For the verbs challenge set, we consider a translation that contains a synonym of a word in the reference to be a "good" translation, and one that contains an antonym of that word to be "incorrect". As in the example above the use of synonyms preserves the meaning of the original sentence, and the antonyms introduce a polar opposite meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12.5">Real-world Knowledge -Commonsense</head><p>We are also interested in whether evaluation metrics prefer translations that adhere to common sense.</p><p>To test this, we remove explanatory subordinate clauses from the sources and references in the dataset described in Section 5.8.3. This guarantees that when choosing between the good and incorrect translation, the metric cannot infer the correct answer from looking at the source or the reference: SRC (en): Die Luft im Haus war k?hler als in der Wohnung. REF (de): The air in the house was cooler than in the apartment. :</p><p>The air in the house was cooler than in the apartment because the apartment had a broken air conditioner. :</p><p>The air in the house was cooler than in the apartment because the house had a broken air conditioner.</p><p>We remove the explanatory subordinate clauses using a sequence of regular expressions. We then pair the shortened source and reference sentences with the full translation that follows commonsense as the good translation and the full translation with the other noun as the incorrect translation.</p><p>Since we present several challenge sets in Section 5.2 where the good translation can only be identified by looking at the source sentence, we also create a version of this challenge set where the explanatory subordinate clause is only removed from the reference but not from the source. By comparing this setup with the results from the setup described above, we achieve another way of quantifying how much a metric considers the source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.13">Wrong Language</head><p>Most of the representations obtained from large multilingual language models do not explicitly use the language identifier (id) as an input while encoding a sentence. Here, we are interested in checking whether sentences which have similar meanings are closer together in the representation space of neural MT evaluation metrics, irrespective of their language. We create a challenge set for embeddingbased metrics where the incorrect translation is in a similar language (same typology/same script) to the reference (e.g. a Catalan translation may be used as the incorrect translation if the target language is Spanish). Note that this is also a common error with multilingual machine translation models. We constructed these examples using the FLORES-200 dataset where the "good" translation was the automatic translation and the "incorrect" translation was the reference from a language similar to the target language: SRC (en): Cell comes from the Latin word cella which means small room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REF (es):</head><p>El t?rmino c?lula deriva de la palabra latina cella, que quiere decir ?cuarto peque?o?. (es):</p><p>La c?lula viene de la palabra latina cella que significa habitaci?n peque?a. (ca):</p><p>C?l?lula ve de la paraula llatina cella, que vol dir habitaci? petita.</p><p>We construct two categories within this challenge set: one where the target language is a higherresource language and the incorrect language is a lower-resource language and vice-versa. The languages we consider are (src-tgt-sim): en-hi-mr, en-es-ca, en-cs-pl, fr-mr-hi, en-pl-cs, and en-ca-es.</p><p>Note that if we were to compare references for different languages and not an automatic translation vs. a reference, this challenge set should be considered unsolvable for reference-free metrics if there is no way to specify the desired target language. But in this case, we expect reference-free metrics to prefer the reference that we use as the "incorrect translation" since there may be translation errors in the automatically translated "good translation".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.14">Fluency</head><p>Although the focus of ACES is on accuracy errors, we also include a small set of fluency errors for the punctuation category. Future work might consider expanding this set to include other categories of fluency errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.14.1">Punctuation</head><p>We assess the effect of deleting and substituting punctuation characters. We employ four strategies: 1) deleting all punctuation, 2) deleting only quotation marks (i.e. removing indications of quoted speech), 3) deleting only commas (i.e. removing clause boundary markers), 4) replacing exclamation points with question marks (i.e. statement ? question).</p><p>In strategies 1 and, especially, 3 and 4, some of the examples may also contain accuracy-related errors. For example, the meaning of the sentence could be changed in the incorrect translation if we remove a comma, e.g. in the (in)famous example "Let's eat, Grandma!" vs. "Let's eat Grandma!". We use the TED Talks from the WMT 2018 English-German pronoun translation evalua-tion test suite and apply all deletions and substitutions automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Methodology</head><p>We shall now briefly describe the metrics that participated in the challenge set shared task. The organisers of the shared task also provided scores by a number of baseline metrics, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline Metrics</head><p>BLEU <ref type="bibr" target="#b41">(Papineni et al., 2002)</ref> compares the token-level n-grams of the hypothesis with the reference translation and then computes a precision score weighted by a brevity penalty.</p><p>spBLEU <ref type="bibr" target="#b12">(Goyal et al., 2022)</ref> is BLEU computed over text tokenised with a single language-agnostic SentencePiece subword model. The spBLEU baselines, F101SPBLEU and F200SPBLEU, are named according to whether the SentencePiece tokeniser <ref type="bibr" target="#b25">(Kudo and Richardson, 2018)</ref> was trained using data from the FLORES-101 or FLORES-200 languages.</p><p>chrF <ref type="bibr" target="#b44">(Popovi?, 2017)</ref> evaluates translation outputs based on a character n-gram F-score by computing overlaps between the hypothesis and the reference.</p><p>BERTScore <ref type="bibr" target="#b69">(Zhang et al., 2020)</ref> uses contextual embeddings from pre-trained language models to compute the similarity between the tokens in the reference and the generated translation using cosine similarity. The similarity matrix is used to compute precision, recall, and F1-scores.</p><p>BLEURT20 <ref type="bibr" target="#b54">(Sellam et al., 2020</ref>) is a BERT-based <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> regression model, which is first trained on scores of automatic metrics/similarity of pairs of reference sentences and their corrupted counterparts. It is then fine-tuned on the WMT human evaluation data to produce a score for a hypothesis given a reference translation.</p><p>COMET-20 <ref type="bibr" target="#b49">(Rei et al., 2020)</ref> uses a cross-lingual encoder (XLM-R <ref type="bibr" target="#b4">(Conneau et al., 2020)</ref>) and pooling operations to obtain sentence-level representations of the source, hypothesis, and reference. These sentence embeddings are combined and then passed through a feedforward network to produce a score. COMET is trained on human evaluation scores of machine translation systems submitted to WMT until 2020.</p><p>COMET-QE was trained similarly to COMET-20 but as this is a reference-free metric, only the source and the hypothesis are combined to produce a final score.</p><p>YiSi-1 <ref type="bibr" target="#b30">(Lo, 2019)</ref> measures the semantic similarity between the hypothesis and the reference by using cosine similarity scores of multilingual representations at the lexical level. It optionally uses a semantic role labeller to obtain structural similarity. Finally, a weighted f-score based on structural and lexical similarity is used for scoring the hypothesis against the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Metrics Submitted to WMT 2022</head><p>We list the descriptions provided by the authors of the respective metrics and refer the reader to the relevant system description papers for further details.</p><p>COMET-22 <ref type="bibr" target="#b48">(Rei et al., 2022)</ref> is an ensemble between a vanilla COMET model trained with Direct Assessment (DA) scores and a Multitask model that is trained on regression (MQM regression) and sequence tagging (OK/BAD word identification from MQM span annotations). These models are ensembled together using a hyperparameter search that weights different features extracted from these two evaluation models and combines them into a single score. The vanilla COMET model is trained with DA's ranging 2017 to 2020 while the Multitask model is trained using DA's ranging from 2017 to 2020 plus MQM annotations from 2020 (except for en-ru that uses TedTalk annotations from 2021).</p><p>Metric-X is a massive multi-task metric, which fine tunes large language model checkpoints such as mT5 on a variety of human feedback data such as Direct Assessment, MQM, QE, NLI and Summarization Eval. Scaling up the metric is the key to unlocking quality and makes the model work in difficult settings such as evaluating without a reference, evaluating short queries, distinguishing high quality outputs, and evaluating on other generation tasks such as summarisation. The four metrics are referred to according to the mT5 model variant used (xl or xxl) and the fine-tuning data: METRICX_*_DA_2019 only used 2015-19 Di-rect Assessment data for fine-tuning, whereas MET-RICX_*_MQM_2020 used a mixture of Direct Assessment 2015-19 and MQM 2020 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COMET-22</head><p>and MS-COMET-QE-22 <ref type="bibr">(Kocmi et al., 2022)</ref> are built on top of the COMET <ref type="bibr" target="#b49">(Rei et al., 2020)</ref> architecture. They are trained on a several times larger set of human judgements covering 113 languages and covering 15 domains. Furthermore, the authors propose filtering of human judgements with potentially low quality. MS-COMET-22 receives the source, the MT hypothesis and the human reference as input, while MS-COMET-QE calculates scores in a quality estimation fashion with access only to the source segment and the MT hypothesis.</p><p>UniTE <ref type="bibr" target="#b66">(Wan et al., 2022)</ref>, Unified Translation Evaluation, is a metric approach where the modelbased metrics can possess the ability of evaluating translation outputs following all three evaluation scenarios, i.e. source-only, reference-only, and source-reference-combined. These are referred to in this paper as UNITE-SRC, UNITE-REF, and UNITE respectively. <ref type="bibr" target="#b48">(Rei et al., 2022)</ref> ensembles two QE models similarly to COMET-22. The first model follows the classic Predictor-Estimator QE architecture where MT and source are encoded together. This model is trained on DAs ranging 2017 to 2019 and then fine-tuned on DAs from MLQE-PE (the official DA from the QE shared task). The second model is the same multitask model used in the COMET-22 submission but without access to a reference translation. This means that this model is a multitask model trained on regression and sequence tagging. Both models are ensembled together using a hyperparameter search that weights different features extracted from these two QE models and combines them into a single score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMET-Kiwi</head><p>Huawei submitted several metrics to the shared task . Cross-QE is a submission based on the COMET-QE architecture. HWTSC-Teacher-Sim is a reference-free metric constructed by fine-tuning the multilingual Sentence BERT model: paraphrase-multilingual-mpnet-base-v2 <ref type="bibr" target="#b50">(Reimers and Gurevych, 2019)</ref>. HWTSC-TLM is a reference-free metric which only uses a target-side language model and only uses the system translations as input. KG-BERTScore is a reference-free machine translation evaluation metric, which incorporates a multilingual knowledge graph into BERTScore by linearly combining the results of BERTScore and bilingual named entity matching.</p><p>MATESE metrics <ref type="bibr" target="#b42">(Perrella et al., 2022)</ref> leverage Transformer-based multilingual encoders to identify error spans in translations, and classify their severity between MINOR and MAJOR. The quality score returned for a translation is computed following the MQM error weighting introduced in <ref type="bibr">Freitag et al. (2021a)</ref>. MATESE is reference-based, while MATESE-QE is its reference-free version, with the source sentence used in place of the reference.</p><p>MEE <ref type="bibr" target="#b36">(Mukherjee et al., 2020)</ref> is an automatic evaluation metric that leverages the similarity between embeddings of words in candidate and reference sentences to assess translation quality, focusing mainly on adequacy. Unigrams are matched based on their surface forms, root forms and meanings which aims to capture lexical, morphological and semantic equivalence. Semantic evaluation is achieved by using pretrained fasttext embeddings provided by Facebook to calculate the word similarity score between the candidate and reference words. MEE computes an evaluation score using three modules namely exact match, root match and synonym match. In each module, fmean-score is calculated using the harmonic mean of precision and recall by assigning more weightage to recall. The final translation score is obtained by taking average of fmean-scores from individual modules.</p><p>MEE2 and MEE4 <ref type="bibr" target="#b38">(Mukherjee and Shrivastava, 2022b)</ref> are improved versions of MEE, focusing on computing contextual and syntactic equivalences along with lexical, morphological and semantic similarity. The intent is to capture fluency and context of the MT outputs along with their adequacy. Fluency is captured using syntactic similarity and context is captured using sentence similarity leveraging sentence embeddings. The final sentence translation score is the weighted combination of three similarity scores: a) Syntactic Similarity achieved by modified BLEU score; b) Lexical, Morphological and Semantic Similarity: measured by explicit unigram matching similar to MEE score; c) Contextual Similarity: Sentence similarity scores are calculated by leveraging sentence embeddings of Language-Agnostic BERT models.</p><p>REUSE <ref type="bibr" target="#b37">(Mukherjee and Shrivastava, 2022a</ref>) is a REference-free UnSupervised quality Estimation Metric. This is a bilingual untrained metric. It estimates the translation quality at chunklevel and sentence-level. Source and target sentence chunks are retrieved by using a multi-lingual chunker. Chunk-level similarity is computed by leveraging BERT contextual word embeddings and sentence similarity scores are calculated by leveraging sentence embeddings of Language-Agnostic BERT models. The final quality estimation score is obtained by mean pooling the chunk-level and sentence-level similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation of Metrics</head><p>For all phenomena in ACES where we generated more than 1,000 examples, we randomly subsample 1,000 examples according to the per language pair distribution to include in the final challenge set to keep the evaluation of new metrics tractable.</p><p>We follow the evaluation of the challenge sets from the 2021 edition of the WMT metrics shared task <ref type="bibr">(Freitag et al., 2021b)</ref> and report performance with Kendall's tau-like correlation. This metric measures the number of times a metric scores the good translation above the incorrect translation (concordant) and equal to or lower than the incorrect translation (discordant):</p><formula xml:id="formula_4">? = concordant ? discordant concordant + discordant</formula><p>Ties are considered as discordant. Note that a higher ? indicates a better performance and that the values can range between -1 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Phenomena-level Results</head><p>We start by providing a broad overview of metric performance on the different categories of phenomena. We compute Kendall's tau-like correlation scores (Section 6) for the 24 metrics which a) provide segment-level scores and b) provide scores for all language pairs and directions in ACES. We first compute the correlation scores for all of the individual phenomena and then take the average score over all phenomena in each of the nine toplevel accuracy categories in ACES plus the fluency category punctuation (see <ref type="table">Table 1</ref>). The performance of the metrics varies greatly and there is no clear winner in terms of performance across all of the categories. There is also a high degree of variation in terms of metric performance when each category is considered in isolation. Whilst each of the categories proves challenging for at least one metric, some categories are more challenging than others. For example, looking at the average scores in the last row of <ref type="table">Table 1</ref>, and without taking outliers into account, we might conclude that addition, undertranslation, real-world knowledge, and wrong language (all with average Kendall tau-like correlation of &lt; 0.3) present more of a challenge than the other categories. On the other hand, for omission and do not translate (with an average Kendall tau-like correlation of &gt; 0.7) metric performance is generally rather high.</p><p>We also observe variation in terms of the performance of metrics belonging to the baseline, reference-based, and reference-free groups. For example, the baseline metrics appear to struggle more on the overtranslation and undertranslation categories than the metrics belonging to the other groups. Reference-based metrics also appear to perform better overall on the untranslated category than the reference-free metrics. This makes sense as a comparison with the reference is likely to highlight tokens that ought to have been translated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">ACES Score</head><p>To analyse general, high-level, performance trends of the metrics on the ACES challenge set, we define a weighted combination of the top-level categories to derive a single score. We call this score the "ACES -Score":</p><formula xml:id="formula_5">ACES = sum ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>5 * ?addition 5 * ?omission 5 * ?mistranslation 1 * ?untranslated 1 * ?do not translate 5 * ?overtranslation 5 * ?undertranslation 1 * ?real-world knowledge 1 * ?wrong language 0.1 * ?punctuation  <ref type="table">Table 1</ref>: Average Kendall's tau-like correlation results for the nine top level categories in the ACES ontology, plus the additional fluency category: punctuation. The horizontal lines delimit baseline metrics (top), participating reference-based metrics (middle) and participating reference-free metrics (bottom). The best result for each category is denoted by bold text with a green highlight. Note that Average is an average over averages. The last column shows the ACES-Score, a weighted sum of the correlations. The ACES-Score ranges from -29.1 (all phenomena have a correlation of -1) to 29.1 (all phenomena have a correlation of +1).</p><formula xml:id="formula_6">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>The weights correspond to the values under the MQM framework that <ref type="bibr">Freitag et al. (2021a)</ref> recommend for major (weight=5), minor (weight=1) and fluency/punctuation errors (weight=0.1). We determined that untranslated, do not translate and wrong language errors should be counted as minor errors because they can be identified automatically with language detection tools and should also be easy to spot in post-editing. We also include realworld knowledge under minor errors since we do not expect that current MT evaluation metrics have any notion of real-world knowledge and we do not want to punish them too severely if they do not perform well on this challenge set.</p><p>We caution that our weighting for the ACES-Score is not ideal, as some phenomena within a broad category might be more difficult than others. Still, we believe that an ACES-Score will be helpful to quickly identify changes in performance of a metric (e.g. following modifications), prior to conducting in-depth analyses at the category and sub-category levels. The ACES-Score ranges from -29.1 (all phenomena have a correlation of -1) to 29.1 (all phenomena have a correlation of +1).</p><p>The ACES-Score results can be seen in the last column of <ref type="table">Table 1</ref>. Using the ACES-Score, we can see at a glance that the majority of the metrics submitted to the WMT 2022 shared task outperform the baseline metrics. Interestingly, many referencefree metrics also perform on par with referencebased metrics. The best performing metric is a reference-free metric, namely KG-BERTSCORE, closely followed by the reference-based metric METRICX_XL_DA_2019. Perhaps unsurprisingly, the worst performing metric is BLEU. However, we caution against making strong claims about which metrics perform best or worst on the challenge set based on this score alone. Instead, we recommend that ACES be used to highlight general trends as to what the outstanding issues are for MT evaluation metrics. More fine-grained analyses are reported in the following sections.</p><p>More generally, work on analysing system performance on ACES prompts the question: What is the definition of a good metric? One might consider that a good metric exhibits a strong correlation with human judgements on whether a translation is good/bad and assigns sufficiently different scores to a good vs. an incorrect translation. The latter criterion would provide evidence of the ability of the metric to discriminate reliably between good and incorrect translations, but it may be difficult to establish what this difference should be, especially without knowing to what degree the translations are good/bad without human judgements and because the scales of different metrics are not comparable. We leave an analysis of metrics' confidence on different error types for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Mistranslation Results</head><p>Next, we drill down to the fine-grained categories of the largest category: mistranslation. We present metric performance on its sub-level categories in <ref type="table" target="#tab_9">Table 2</ref>. Again, we find that performance on the different sub-categories is variable, with no clear winner among the metrics. The results suggest that hallucination phenomena are generally more challenging than discourse-level phenomena. Performance on the hallucination sub-category is poor overall, although it appears to be particularly challenging for the baseline metrics. We present additional, more fine-grained, performance analyses for individual phenomena in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Language-level Results</head><p>Another possible way to evaluate the metrics' performance is not to look at the phenomena but rather at the results on different language pairs. Since ACES covers 146 language pairs and for some of these language pairs we only have very few examples, we decide to split this analysis into four main categories:</p><p>? trained: language pairs for which this year's WMT metrics shared task provided training material (en-de, en-ru and zh-en). This category also allows us to analyse the metrics that only cover these specific language pairs and not the full set of language pairs in ACES.</p><p>? en-x: language pairs where the source language is English.</p><p>? x-en: language pairs where the target language is English.</p><p>? x-y: all remaining language pairs, where neither the source language nor the target language are English.  we can compare metrics on each of the language pair groups individually. First, it can again be observed that most submitted metrics outperform the baseline metrics (first group). This shows that the field is advancing and MT evaluation metrics have improved since last year (i.e. 2021). Interestingly, the six metrics that only scored the trained language pairs (last group in the table) do not outperform the other metrics on the "trained" category. Note, however, that the MEE* metrics and REUSE are unsupervised metrics and that the MATESE metrics only used MQM training data. Therefore, we cannot comment on creating metrics that are specific to a language pair would result in better metrics. In any case, our findings in Section 8.3.1 suggest that generalisation to unseen language pairs is generally quite good for the multilingual metrics which might be a more desirable property than increased performance on specific language pairs. MATESE -0.281 n/a n/a n/a MEE -0.078 n/a n/a n/a MEE2 0.340 n/a n/a n/a MEE4 0.391 n/a n/a n/a REUSE 0.430 n/a n/a n/a MATESE-QE -0.313 n/a n/a n/a <ref type="table" target="#tab_8">Table 3</ref>: Average Kendall's tau-like correlation results grouped by language pairs: trained language pairs (ende, en-ru, zh-en), from English (en-x), into English (x-en) and language pairs not involving English (xy). The horizontal lines delimit baseline metrics (top), all language pairs participating reference-based metrics (second), all language pairs participating reference-free metrics (third) and trained language pairs only metrics (bottom). The best result for each category is denoted by bold text with a green highlight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis</head><p>Aside from high-level evaluations of which metrics perform best, we are mostly interested in metricspanning weaknesses that we can identify using ACES. This section shows an analysis of three general questions that we aim to answer using ACES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">How sensitive are metrics to the source?</head><p>We designed our challenge sets for the type of ambiguous translation in a way that the correct translation candidate given an ambiguous reference can only be identified through the source sentence.</p><p>Here, we present a targeted evaluation intended to provide some insights into how important the  source is for different metrics. We exclude all metrics that do not take the source as input, all metrics that do not cover all language pairs, and the smaller versions of METRIC-X (metricx_xl_DA_2019 and metricx_xl_MQM_2020) from this analysis. This leaves us with seven reference-based metrics and seven reference-free metrics. <ref type="table" target="#tab_11">Table 4</ref> shows the detailed results of each metric on the considered phenomena.</p><p>The most important finding is that the referencefree metrics generally perform much better on these challenge sets than the reference-based metrics. This indicates that reference-based metrics rely too much on the reference. Interestingly, most of the metrics that seem to ignore the source do not randomly guess the correct translation (which is a valid alternative choice when the correct meaning is not identified via the source) but rather they strongly prefer one phenomenon over the other. For example, several metrics show a gender bias either towards female occupation names (female correlations are high, male low) or male occupation names (vice versa). Likewise, most metrics prefer translations with frequent senses for the word-sense disambiguation challenge sets, although the difference between frequent and infrequent is not as pronounced as for gender.</p><p>Only metrics that look at the source and exhibit fewer such preferences can perform well on average on this collection of challenge sets. COMET-  22 performs best out of the reference-based metrics and COMET-KIWI performs best of all referencefree metrics. It is noteworthy that there is still a considerable gap between these two models, suggesting that reference-based models should pay more attention to the source when a reference is ambiguous to reach the performance of referencefree metrics. This finding is also supported by our real-world knowledge commonsense challenge set. If we compare the scores on the examples where the subordinate clauses are missing from both the source and the reference to the ones where they are only missing from the reference, we can directly see the effect of disambiguation through the source. The corresponding correlation gains are shown in <ref type="table" target="#tab_13">Table 5</ref>. All reference-based model correlation scores improve less than most reference-free correlations when access to the subordinate clause is given through the source. This highlights again that reference-based metrics do not give enough weight to the source sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">How much do metrics rely on surface-overlap with the reference?</head><p>Another question we are interested in is whether neural reference-based metrics still rely on surfacelevel overlap with the reference. For this analysis, we use the dataset we created for hallucinated named entities and numbers. We take the average correlation for all reference-based metrics 16 and the average correlation of all reference-free metrics that cover all languages and plot the decrease in correlation with increasing surface-level similarity of the incorrect translation to the reference. The result can be seen in <ref type="figure" target="#fig_7">Figure 2</ref>.</p><p>We can see that on average reference-based metrics have a much steeper decrease in correlation than the reference-free metrics as the two translation candidates become more and more lexically diverse and the surface overlap between the incorrect translation and the reference increases. This indicates a possible weakness of reference-based metrics: If one translation is lexically similar to the reference but contains a grave error while others are correct but share less surface-level overlap with  the reference, the incorrect translation may still be preferred.</p><p>We also show that this is the case for the challenge set where we use an adversarial paraphrase from PAWS-X that shares a high degree of lexical overlap with the reference but does not have the same meaning as an incorrect translation. On average, the reference-based metrics only reach a correlation of 0.05 ? 0.12 on this challenge set, whereas the reference-free metrics reach a correlation of 0.23 ? 0.15. This shows that reference-based metrics are less robust when the incorrect translation has high lexical overlap with the reference.</p><p>Finally, we can also see a clear effect of surfacelevel overlap with the source on three real error challenge sets where we have different versions of the good translation: some where the error was corrected with the corresponding correct token from the reference and some where the error was corrected with a synonym for the correct token from the reference. As seen in <ref type="table" target="#tab_15">Table 6</ref>, the referencebased metrics show a much larger difference in correlation between the challenge sets with referencecopied good translations and the challenge sets with the synonymous good translations, than the reference-free metrics. For example, for the hallucination test set, reference-free metrics have very similar average performance when the good translation contains the same word as the reference vs. when it contains a synonym (? of +0.04). On the other hand, the reference-based metrics lose on average -0.22 in correlation when the good translation contains the synonym rather than the same word as the reference. Based on all of these results, we conclude that even though state-of-the-art reference-based MT evaluation metrics are not only reliant on surface-level overlap anymore, such overlap still considerably influences their predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Do multilingual embeddings help design better metrics?</head><p>As the community moves towards building metrics that use multilingual encoders, we investigate if some (un)desirable properties of multilingual embeddings are propagated in these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Zero-shot Performance</head><p>Similar to <ref type="bibr" target="#b22">Kocmi et al. (2021)</ref>, we investigate whether there is a difference in the performance of metrics on our challenge sets when evaluated on non-WMT language pairs i.e. language pairs unseen during the training of the metrics. For this analysis, we include only those metrics for which the training data consisted of some combination of WMT human evaluation data. As different metrics used data from different years, we consider an intersection of languages across these years as WMT language pairs. For a fair comparison, we consider   We draw similar conclusions to <ref type="bibr" target="#b22">Kocmi et al. (2021)</ref>, namely that trained metrics are not overfitted to the WMT language pairs. We observe that the median difference of ? between WMT and non-WMT language pairs is 0.056, indicating a good generalisation to unseen languages. We still note that performance on the phenomena is variable when we compare the results on WMT language pairs versus non-WMT language pairs. In the case of real-world knowledge commonsense, performance is slightly better on the non-WMT language pairs 17 , while the opposite is (generally) true for the antonym replacement and, especially, the nonsense phenomena for certain metrics. Further analysis is required to better understand metric behaviour on zero-shot language pairs, especially considering that some of the analysed non-WMT language pairs have a target language that is also the target language in at least one of the WMT language pairs (e.g. English).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">Language Dependent Representations</head><p>Multilingual models often learn cross-lingual representations by abstracting away from languagespecific information <ref type="bibr" target="#b67">(Wu and Dredze, 2019)</ref>. We are interested in whether the representations are still language-dependent in neural MT evaluation metrics which are trained on such models. For this analysis, we look at the sentence-level untranslated text challenge set (see <ref type="figure" target="#fig_8">Figure 3</ref>) and wrong language phenomena (see <ref type="table">Table 1</ref>). We only consider metrics that provided scores for examples in all language pairs. <ref type="figure" target="#fig_8">Figure 3</ref> shows the correlations for all referencebased and reference-free metrics. Unsurprisingly, some reference-free metrics struggle considerably on this challenge set and almost always prefer the copied source to the real translation. The representations of the source and the incorrect translation are identical, leading to a higher surface and embedding similarity, and thus a higher score. We do, however, find some exceptions to this trend -COMET-KIWI and MS-COMET-QE-22 both have a high correlation on sentence-level untranslated text. This suggests that these metrics could have learnt language-dependent representations.</p><p>Most reference-based metrics have good to almost perfect correlation and can identify the copied source quite easily. As reference-based metrics tend to ignore the source (see Section 8.2), the scores are based on the similarity between the reference and the MT output. In this challenge set, the similarity between the good-translation and the reference is likely to be higher than the incorrecttranslation and the reference. The former MT output is in the same language as the reference and will have more surface level overlap. We believe the reference here acts as grounding.</p><p>However, this grounding property of the reference is only robust when the source and reference languages are dissimilar, as is the case with language pairs in the sentence-level untranslated text challenge set. We find that reference-based metrics struggle on wrong language phenomena (see <ref type="table">Table  1</ref>) where the setup is similar, but now the incorrect translation and the reference are from similar languages (e.g. one is in Hindi and the other is in Marathi). Naturally, there will be surface level overlap between the reference and both the goodtranslation and the incorrect-translation. For example, both Marathi and Hindi use named entities with identical surface form, and so these will appear in the reference and also in both the good-translation and the incorrect-translation. Thus, the semantic content drives the similarity scores between the MT outputs and the references. It is possible that the human translation in the similar language (labelled as the incorrect-translation) has a closer representation to the human reference because in the MT output (labelled as the good-translation) some semantic information may be lost. We leave further investigation of this for future work.</p><p>While multilingual embeddings help in effective zero-shot transfer to new languages, some properties of the multilingual representation space may need to be altered to suit the task of machine translation evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Recommendations</head><p>Based on the metrics results on ACES and our analysis, we derived the following list of recommendations for future MT evaluation metric development:</p><p>No metric to rule them all: Both the evaluation on phenomena and on language pair categories in Section 7 showed that there is no single bestperforming metric. This divergence is likely to become even larger if we evaluate metrics on different domains. For future work on MT evaluation, it may be worthwhile thinking about how different metrics can be combined to make robust decisions as to which is the best translation. This year's submissions to the metrics shared task already suggest that work in that direction is ongoing as some groups submitted metrics that combined ensembles of models or multiple components (COMET-22, COMET-KIWI, KG-BERTSCORE, MEE*, REUSE).</p><p>The source matters: Our analysis in Section 8.1 highlighted that many reference-based metrics that take the source as input do not consider it enough. Cases where the correct translation can only be identified through the source are currently better handled by reference-free metrics. This is a serious shortcoming of reference-based metrics and should be addressed in future research, also considering that many reference-based metrics do not even take the source as input.</p><p>Surface-overlap still prevails: In Section 8.2, we showed that despite moving beyond only surface-level comparison to the reference, most reference-based metric scores are still considerably influenced by surface-level overlap. We expect future metrics to use more lexically diverse references in their training regime to mitigate this issue.</p><p>Multilingual embeddings are not perfect: Some properties of multilingual representations, especially, being language-agnostic, can result in undesirable effects on MT evaluation (Section 8.3). It could be helpful for future metrics to incorporate strategies to explicitly model additional languagespecific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We presented ACES, a translation accuracy challenge set based on the MQM ontology. ACES consists of 36,476 examples covering 146 language pairs and representing challenges from 68 phenomena. We used ACES to evaluate the baseline and submitted metrics from the WMT 2022 metrics shared task. Our overview of metric performance at the phenomena and language levels in Section 7 reveals that there is no single best-performing metric. The more fine-grained analyses in Section 8 highlight that 1) many reference-based metrics that take the source as input do not consider it enough, 2) most reference-based metric scores are still considerably influenced by surface overlap with the reference, and 3) the use of multilingual embeddings can have undesirable effects on MT evaluation.</p><p>We recommend that these shortcomings of existing metrics be addressed in future research, and that metric developers should consider a) combining metrics with different strengths, e.g. in the form of ensemble models, b) developing metrics that give more weight to the source and less to surface-level overlap with the reference, and c) incorporating strategies to explicitly model additional language-specific information (rather than simply relying on multilingual embeddings).</p><p>We have made ACES publicly available and hope that it will provide a useful benchmark for MT evaluation metric developers in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The ACES challenge set exhibits a number of biases. Firstly, there is greater coverage in terms of phenomena and number of examples for the en-de and en-fr language pairs. This is in part due to the manual effort required to construct examples for some phenomena, in particular those belonging to the discourse-level and real-world knowledge categories. Further, our choice of language pairs is also limited to the ones available in XLM-R. Secondly, ACES contains more examples for those phenomena for which examples could be generated automatically, compared to those that required manual construction/filtering. Thirdly, some of the automatically generated examples require external libraries which are only available for a few languages (e.g. Multilingual Wordnet). Fourthly, the focus of the challenge set is on accuracy errors. We leave the development of challenge sets for fluency errors to future work.</p><p>As a result of using existing datasets as the basis for many of the examples, errors present in these datasets may be propagated through into ACES. Whilst we acknowledge that this is undesirable, in our methods for constructing the incorrect translation we aim to ensure that the quality of the incorrect translation is always worse than the corre-sponding good translation.</p><p>The results and analyses presented in the paper exclude those metrics submitted to the WMT 2022 metrics shared task that provide only system-level outputs. We focus on metrics that provide segmentlevel outputs as this enables us to provide a broad overview of metric performance on different phenomenon categories and to conduct fine-grained analyses of performance on individual phenomena. For some of the fine-grained analyses, we apply additional constraints based on the language pairs covered by the metrics, or whether the metrics take the source as input, to address specific questions of interest. As a result of applying some of these additional constraints, our investigations tend to focus more on high and medium-resource languages than on low-resource languages. We hope to address this shortcoming in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>Some examples within the challenge set exhibit biases, however this is necessary in order to expose the limitations of existing metrics. Wherever external help was required in verifying translations, the annotators were compensated at a rate of ?15/hour. Our challenge set is based on publicly available datasets and will be released for future use.  <ref type="table">Table 9</ref> contains the Kendall tau-like correlation scores for neural metrics on WMT language pairs (a subset of those seen during training) and non-WMT language pairs (unseen), for three phenomena: antonym replacement, real-world knowledge commonsense, and nonsense. The table contains the complete set of scores, and complements Table 7, which reports only the difference between the non-WMT and WMT correlation scores. See Section 8.3.1 on zero-shot performance. We shall now list the language pairs across the different phenomena:</p><p>Antonym Replacement WMT:</p><p>de-en non-WMT: ko-en, es-en</p><p>Real-world Knowledge -Commonsense WMT: de-en, ru-en, en-ru, en-de non-WMT:</p><p>ru-de, fr-ru, ru-fr, de-ru Nonsense WMT: de-en non-WMT:</p><p>fr-ja, ko-ja, en-ko, ko-en Note that the subset of examples used in this analysis only consists of mid/high resource language pairs; investigation into the performance on lowresource languages is left for future work. <ref type="table">Table 10</ref> contains the total number of examples per language pair in the challenge set. As can be seen in the table, the distribution of examples is variable across language pairs. The dominant language pairs are: en-de, de-en, and fr-en. <ref type="table">Table 11</ref> contains the list of language pairs per phenomena in the challenge set. As can be seen in the table, the distribution of language pairs is variable across phenomena. Addition and omission have the highest variety of language pairs. en-de is the most frequent language pair across all phenomena.  <ref type="table">Table 9</ref>: Zero-shot performance of neural metrics on three phenomena to measure the ability of metrics to generalise to new language pairs. WMT language pairs consist of a subset of languages seen during training of the metrics, while non-WMT language pairs are unseen. Results show that the metrics are able to generalise to unseen languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Distribution of Examples Across Language Pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Distribution of Language Pairs Across Phenomena</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of the error categories on which our collection of challenge sets is based. Red means challenge sets are created automatically, blue means challenge sets are created manually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>SRC (de): Der Manager feuerte die B?ckerin. REF (en): The manager fired the baker. :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SRC (de): Mit der Einf?hrung dieser Regelung k?nnte diese Freiheit enden. REF (en): With this arrangement in place, this freedom might end. :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>SRC (de): Bob und Ted waren Br?der. Ted ist der Sohn von John. REF (en): Bob and Ted were brothers. Ted is John's son. :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>SRC (de):Ich hasste jedes St?ck der Schule! REF (en):I hated every bit of school! (synonym): I loathed every bit of school! (antonym):I loved every bit of school!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Decrease in correlation for reference-based and reference-free metrics on the named entity and number hallucination challenge sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Correlation of reference-based metrics (blue) and reference-free metrics (orange) on the sentencelevel untranslated test challenge set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In 1924 he was an invited spokesman for the ICM inToronto, in Oslo in 1932 and  in 1936 in Zurich.  :   </figDesc><table><row><cell>SRC (fr):</cell><cell>En 1924, il a ?t? porte-parole invit? de</cell></row><row><cell></cell><cell>l'ICM ? Toronto, ? Oslo en 1932 et ?</cell></row><row><cell></cell><cell>Zurich en 1936.</cell></row><row><cell>REF (en):</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>13 https://github.com/zhjjn/MWE_PIE SRC (de): Today, the only insects that cannot fold back their wings are dragon flies and mayflies. REF (en): Heute sind Libellen und Eintagsfliegen die einzigen Insekten, die ihre Fl?gel nicht zur?ckklappen k?nnen. (copy) : Heute sind die einzigen Insekten, die ihre</figDesc><table><row><cell></cell><cell>Fl?gel nicht zur?ckbrechen k?nnen, Li-</cell></row><row><cell></cell><cell>bellen und Mayflies.</cell></row><row><cell>(syn.):</cell><cell>Heute sind die einzigen Insekten, die</cell></row><row><cell></cell><cell>ihre Fl?gel nicht zur?ckbrechen k?nnen,</cell></row><row><cell></cell><cell>Wasserjungfern und Mayflies.</cell></row><row><cell>:</cell><cell>Heute sind die einzigen Insekten, die</cell></row><row><cell></cell><cell>ihre Fl?gel nicht zur?ckbrechen k?nnen,</cell></row><row><cell></cell><cell>Drachenfliegen und Mayflies.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SRC (en): I have a shopping bag; it is red. REF (de): Ich habe eine Einkaufst?te; sie ist rot.</figDesc><table><row><cell>:</cell><cell>Ich habe einen Einkaufsbeutel; er ist rot.</cell></row><row><cell>(subs.):</cell><cell>Ich habe einen Einkaufsbeutel; sie ist rot.</cell></row><row><cell>(omit):</cell><cell>Ich habe einen Einkaufsbeutel; ? ist rot.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>SRC (de): ..., dass der Hund meiner Schwester geh?rt. REF (en): ... the dog belonged to my sister. (hypernym): ... the pet belonged to my sister. (hyponym):... the labrador belonged to my sister.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc>shows the results for all metrics. It is important to note that the results for different language pair categories cannot be directly compared because the examples and covered phenomena categories are not necessarily the same. However, disco. halluci. other</figDesc><table><row><cell>Examples</cell><cell>3698</cell><cell>10270</cell><cell>10489</cell></row><row><cell>BLEU</cell><cell cols="3">-0.048 -0.420 -0.251</cell></row><row><cell>f101spBLEU</cell><cell cols="3">0.105 -0.206 -0.153</cell></row><row><cell>f200spBLEU</cell><cell cols="3">0.094 -0.191 -0.149</cell></row><row><cell>chrF</cell><cell cols="2">0.405 -0.137</cell><cell>0.161</cell></row><row><cell>BERTScore</cell><cell cols="2">0.567 -0.058</cell><cell>0.362</cell></row><row><cell>BLEURT-20</cell><cell>0.695</cell><cell>0.142</cell><cell>0.402</cell></row><row><cell>COMET-20</cell><cell>0.641</cell><cell>0.016</cell><cell>0.399</cell></row><row><cell>COMET-QE</cell><cell>0.666</cell><cell>0.303</cell><cell>0.208</cell></row><row><cell>YiSi-1</cell><cell>0.609</cell><cell>0.019</cell><cell>0.368</cell></row><row><cell>COMET-22</cell><cell>0.682</cell><cell>0.461</cell><cell>0.542</cell></row><row><cell>metricx_xl_DA_2019</cell><cell>0.701</cell><cell>0.493</cell><cell>0.458</cell></row><row><cell>metricx_xl_MQM_2020</cell><cell>0.573</cell><cell>0.677</cell><cell>0.394</cell></row><row><cell>metricx_xxl_DA_2019</cell><cell>0.768</cell><cell>0.541</cell><cell>0.463</cell></row><row><cell cols="2">metricx_xxl_MQM_2020 0.716</cell><cell>0.713</cell><cell>0.392</cell></row><row><cell>MS-COMET-22</cell><cell>0.645</cell><cell>0.148</cell><cell>0.360</cell></row><row><cell>UniTE</cell><cell>0.746</cell><cell>0.322</cell><cell>0.424</cell></row><row><cell>UniTE-ref</cell><cell>0.776</cell><cell>0.396</cell><cell>0.437</cell></row><row><cell>COMETKiwi</cell><cell>0.733</cell><cell>0.493</cell><cell>0.637</cell></row><row><cell>Cross-QE</cell><cell>0.639</cell><cell>0.395</cell><cell>0.563</cell></row><row><cell>HWTSC-Teacher-Sim</cell><cell>0.594</cell><cell>0.296</cell><cell>0.330</cell></row><row><cell>HWTSC-TLM</cell><cell>0.756</cell><cell>0.306</cell><cell>0.151</cell></row><row><cell>KG-BERTScore</cell><cell>0.593</cell><cell>0.387</cell><cell>0.472</cell></row><row><cell>MS-COMET-QE-22</cell><cell>0.626</cell><cell>0.243</cell><cell>0.416</cell></row><row><cell>UniTE-src</cell><cell>0.172</cell><cell>0.463</cell><cell>0.551</cell></row><row><cell>Average</cell><cell>0.586</cell><cell>0.242</cell><cell>0.331</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Average Kendall's tau-like correlation re-</cell></row><row><cell>sults for the sub-level categories in mistranslation:</cell></row><row><cell>discourse-level, hallucination, and other errors. The</cell></row><row><cell>horizontal lines delimit baseline metrics (top), partici-</cell></row><row><cell>pating reference-based metrics (middle) and participat-</cell></row><row><cell>ing reference-free metrics (bottom). The best result for</cell></row><row><cell>each category is denoted by bold text with a green high-</cell></row><row><cell>light. Note that Average is an average over averages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Results on the challenge sets where the good translation can only be identified through the source sentence. Upper block: reference-based metrics, lower block: reference-free metrics. Best results for each phenomenon and each group of models is marked in bold and green and the average over all can be seen in the last column.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Results on the real-world knowledge commonsense challenge set with reference-based metrics in the upper block and reference-free metrics in the lower block. The numbers are computed as the difference between the correlation with the subordinate clause in the source and the correlation without the subordinate clause in the source. Largest gains are bolded.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 6 :</head><label>6</label><figDesc>Average correlation difference and standard deviation between the challenge sets with referencecopied good translations and the challenge sets with the synonymous good translations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>Correlation difference between the performance of WMT and non-WMT language pairs reported for trained metrics across a subset of examples. ?= ? WMT -? non WMT . WMT language pairs consist of a subset of languages seen during training of the metrics, while non-WMT language pairs are unseen. Results show that the metrics are able to generalise to unseen languages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8 :</head><label>8</label><figDesc>ISO 2-Letter language codes of the languages included in the challenge setA.2 Permitted Unit ConversionsWe allow the following unit conversions for the challenge set that covers such errors:? miles per hour ? kilometres per hour ? kilometres per hour ? miles per hour ? kilometres per second ? miles per second ? miles per second ? kilometres per second</figDesc><table><row><cell>Distance:</cell></row><row><cell>? miles ? metres ? kilometres ? miles ? kilometres ? metres ? metres ? feet ? metres ? yards ? feet ? metres ? feet ? yards ? centimetres ? inches ? centimetres ? millimetres ? inches ? centimetres</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://stanfordnlp.github.io/stanza/ available_models.html 5 https://huggingface.co/facebook/m2m100_1.2B</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://translate.google.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://web.library.yale.edu/cataloging/ months</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://github.com/nielstron/quantulum3 12 https://github.com/hgrecco/pint</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Excluding surface-level baseline metrics: BLEU, SP-BLEU and CHRF.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">We also observe better performance on non-WMT language pairs for the similar language high phenomenon.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the organisers of the WMT 2022 Metrics task for setting up this shared task and for their feedback throughout the process, and the shared task participants for scoring our challenge sets with their systems. We are grateful to Stephanie Droop, Octave Mariotti, and Kenya Murakami for helping us with the annotations. We thank the StatMT group at Edinburgh, especially Alexandra Birch, Barry Haddow, and Ulrich Germann, and the attendees at the MT Marathon 2022 for their valuable feedback. We thank Janis Goldzycher, Mark Steedman, Rico Sennrich, and the anonymous reviewers for their insightful comments and suggestions. This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh (Moghe), by the Swiss National Science Foundation (project MUTAMUR; no. 176727) (Amrhein) and by the ERC H2020 Advanced Fellowship GA 742137 SEMANTAX (Guillou). We also thank Huawei for their support (Moghe).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>hallucination-number-level-1 hallucination-number-level-2 hallucination-number-level-3 en-de, ja-de, en-ko, de-zh, ja-en, es-de, fr-en, es-ko, ko-ja, es-ja, deja, zh-es, fr-zh, fr-ja, es-en, fr-ko, zh-en, ko-de, ko-es, de-ko, ko-en, fr-es, ja-es, ja-ko, zh-fr, en-es, de-en, ja-fr, ko-zh, en-fr, de-fr, ko-fr, es-fr, zh-ko, fr-de, ja-zh, de-es, es-zh, en-ja, zh-de, en-zh, zh-ja hyponym-replacement hypernym-replacement fr-en, ko-en, ja-en, es-en, zh-en, de-en lexical-overlap fr-en, en-fr, de-fr, ko-en, es-ja, ja-en, ko-fr, es-fr, ko-ja, de-ja, zh-en, ja-fr, zh-fr, en-ja, es-en, fr-ja, de-en, zh-ja xnli-addition-contradiction xnli-addition-neutral xnli-omission-contradiction xnli-omission-neutral fr-en, vi-en, sw-en, tr-en, zh-en, ru-en, bg-en, el-en, th-en, es-en, hi-en, de-en, ar-en, ur-en hallucination-unit-conversion-amount-matches-ref hallucination-unit-conversion-unit-matches-ref <ref type="bibr">et-en, wo-en, da-en, no-en, uk-en, ta-en, fi-en, pl-en, ja-en, hy-en, ur-en, hr-en, fr-en, lt-en, tr-en, he-en, bg-en, ro-en, sv-en, ru-en, es-en, nl-en, zh-en, hu-en, be-en, lv-en, ko-en, ga-en, sk-en, af-en, sl-en, sr-en, ca-en, de-en, mr-en, id-en, vi-en, gl-en, pt-en, fa-en, hi-en, el-en, ar-en, it-en, cs-en hallucination-date-time en-de, et-en, ca-es, en-et, hr-lv, da-en, no-en, uk-en, fi-en, en-da, taen, pl-en, ja-en, en-hr, hy-en, ur-en, fr-en, hr-en, lt-en, sr-pt, en-sv, tr-en, en-no, en-sl, he-en, pl-sk, ru-en, ro-en, sv-en, en-lt, es-en, ennl, nl-en, bg-en, he-sv, zh-en, hu-en, be-en, lv-hr, lv-en, bg-lt, en-ro, sk-pl, ko-en, ga-en, sk-en, af-en, sl-en, en-hu, sr-en, en-es, ca-en, ensk, de-en, mr-en, id-en, vi-en, gl</ref>-en, en-fr, de-fr, pt-en, fr-de, en-pt, fa-en, hi-en, el-en, ar-en, it-en, en-pl, cs-en commonsense-only-ref-ambiguous commonsense-src-and-ref-ambiguous en-de, fr-en, ru-fr, en-fr, de-fr, ru-de, fr-de, ru-en, en-ru, fr-ru, de-ru, de-en copy-source ar-fr, ru-es, ur-en, fr-en, tr-en, zh-de, bg-en, ru-en, es-en, zh-en, swen, ja-ko, th-en, de-en, pl-mr, vi-en, hi-en, el-en, ar-en addition omission en-ca, en-el, en-et, en-ta, pl-en, hr-en, he-en, pl-sk, en-ar, ru-en, enfi, zh-en, hu-en, be-en, lv-hr, en-he, ko-en, en-fa, sl-en, ca-en, en-gl, en-tr, en-sk, de-en, en-sr, fa-af, fa-en, ar-en, cs-en, en-de, en-hy, arhi, no-en, uk-en, fi-en, en-be, sr-pt, en-ru, sv-en, nl-en, sk-pl, en-hi, en-hu, mr-en, hi-ar, id-en, gl-en, en-fr, en-lv, fr-de, ca-es, en-uk, addition omission en-ur, en-hr, ur-en, en-no, en-sl, ro-en, en-vi, en-lt, es-en, en-nl, hesv, en-it, en-ro, af-fa, en-id, lt-bg, en-af, af-en, es-ca, vi-en, sv-he, de-fr, pt-en, en-pl, et-en, hr-lv, wo-en, da-en, en-ko, en-da, ja-en, hyen, pt-sr, hy-vi, fr-en, en-cs, lt-en, en-sv, tr-en, bg-en, lv-en, bg-lt, sr-en, en-es, en-bg, en-pt, hi-en, el-en, it-en </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying weaknesses in machine translation metrics through minimum bayes risk decoding: A case study for COMET</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantal</forename><surname>Amrhein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation of quality estimation for machine translation based on a linguistically motivated test suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleftherios</forename><surname>Avramidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Macketanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arle</forename><surname>Lommel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</title>
		<meeting>the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="243" to="248" />
		</imprint>
	</monogr>
	<note>Association for Machine Translation in the Americas</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joss</forename><surname>Moorkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Gaspari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tinsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<idno type="DOI">10.1515/pralin-2017-0013</idno>
		<title level="m">Is neural machine translation the new state of the art? The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can transformer be too compositional? analysing idiom processing in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verna</forename><surname>Dankers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3608" to="3626" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wino-X: Multilingual Winograd schemas for commonsense reasoning and coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Emelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8517" to="8532" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond english-centric multilingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">107</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00437</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1460" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alon Lavie, and Ond?ej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expertbased human evaluations on TED and news domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="733" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjana</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00474</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="522" to="538" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PROTEST: A test suite for evaluating pronouns in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liane</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>Portoro?</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A pronoun test suite evaluation of the English-German MT systems at WMT 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liane</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lapshinova-Koltunski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharid</forename><surname>Lo?iciga</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6435</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="570" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A fine-grained analysis of BERTScore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">spaCy: Industrial-strength Natural Language Processing in Python</title>
		<idno type="DOI">10.5281/zenodo.1212303</idno>
		<editor>Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A challenge set approach to evaluating machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2486" to="2496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using test suites in evaluation of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Falkedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Papers presented to the 13th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">To ship or not to ship: An extensive evaluation of automatic metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="478" to="494" />
		</imprint>
	</monogr>
	<note>Hitokazu Matsushita, and Arul Menezes</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MS-COMET: More and Better Human Judgements Improve Metric Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitokazu</forename><surname>Matsushita</surname></persName>
		</author>
		<idno>ermann. 2022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</title>
		<meeting>the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit X: Papers</title>
		<meeting>Machine Translation Summit X: Papers<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving discourse relation projection to build discourse annotated corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Laali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Kosseim</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-049-6_054</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="407" to="416" />
		</imprint>
	</monogr>
	<note>INCOMA Ltd</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ParCorFull: a parallel corpus annotated with full coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lapshinova-Koltunski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Krielke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BIBI system description: Building with CNNs and breaking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5404</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</title>
		<meeting>the First Workshop on Building Linguistically Generalizable NLP Systems<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Partial Could Be Better Than Whole: HW-TSC 2022 Submission for the Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanglin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</title>
		<meeting>the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">YiSi -a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5358</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="507" to="513" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics. Tradum?tica: tecnologies de la traducci?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arle</forename><surname>Lommel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoscha</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="DOI">10.5565/rev/tradumatica.77</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="455" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Breaking NLP: Using morphosyntax, semantics, pragmatics and world knowledge to fool sentiment analysis systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Shain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symon</forename><surname>Stevens-Guille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Building Linguistically Generalizable NLP</title>
		<meeting>the First Workshop on Building Linguistically Generalizable NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
	</analytic>
	<monogr>
		<title level="j">Systems</title>
		<imprint>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-entailed subsequences as a challenge for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Society for Computation in Linguistics (SCiL)</title>
		<meeting>the Society for Computation in Linguistics (SCiL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="358" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey</title>
		<imprint>
			<date type="published" when="1994-03-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mee : An automatic metric for evaluation using embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Ala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti Misra</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1109/DSAA49011.2020.00042</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">REUSE: REference-free UnSupervised quality Estimation Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</title>
		<meeting>the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised Embedding-based Metric for MT Evaluation with Improved Human Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</title>
		<meeting>the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2012.07.001</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Nllb Team</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>?elebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Kalbassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skyler</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bapi</forename><surname>Youngblood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">Mejia</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prangthip</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hansanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2207.04672</idno>
	</analytic>
	<monogr>
		<title level="j">Semarley Jarrett</title>
		<imprint/>
	</monogr>
	<note>Necip Fazil Ayan. and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Perrella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Proietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Scir?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niccol?</forename><surname>Campolungo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Machine Translation Evaluation as a Sequence Tagging Problem</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</title>
		<meeting>the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">chrF++: words helping character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4770</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Challenge test sets for MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Castilho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit XVII: Tutorial Abstracts</title>
		<meeting>Machine Translation Summit XVII: Tutorial Abstracts<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>European Association for Machine Translation</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The MuCoW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5354</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="470" to="480" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1)</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NoiseQA: Challenge set evaluation for user-centric question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Ryskina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2976" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duarte</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrysoula</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Zerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taisiya</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Glushkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><forename type="middle">F T</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation</title>
		<meeting>the Seventh Conference on Machine Translation<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">COMET: A neural framework for MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2685" to="2702" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unbounded dependency recovery for parser evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="813" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fancy: A diagnostic data-set for nli models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Rocchietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavia</forename><surname>Achena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Marziano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Salaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Italian Conference on Computational Linguistics</title>
		<meeting>the Eighth Italian Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>CLiC-it</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Social bias in elicited natural language inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-1609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to evaluate translation beyond English: BLEURT submissions to the WMT metrics 2020 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="921" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Masked language modeling and the distributional hypothesis: Order word matters pre-training for little</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2888" to="2913" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Adversarial evaluation for models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1207.0245</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Findings of the WMT 2020 shared task on machine translation robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="76" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Breaking sentiment analysis of movie reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieva</forename><surname>Stali?nait?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bonfil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</title>
		<meeting>the First Workshop on Building Linguistically Generalizable NLP Systems<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="61" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Evaluating gender bias in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1164</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1679" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A multifaceted evaluation of neural versus phrasebased machine translation for 9 language directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S?nchez-Cartagena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1063" to="1073" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Contrastive conditioning for assessing disambiguation in MT: A case study of distilled bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Vamvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.803</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10246" to="10265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">As little as possible, as much as necessary: Detecting over-and undertranslations with contrastive conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Vamvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Understanding the societal impacts of machine translation: a critical review of the literature on medical and legal use cases. Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas Nunes</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Minako</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol O&amp;apos;</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sullivan</surname></persName>
		</author>
		<idno type="DOI">10.1080/1369118X.2020.1776370</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Communication &amp; Society</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1515" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Alibaba-Translate China&apos;s Submission for WMT2022 Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</title>
		<meeting>the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3687" to="3692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">PAWS: Paraphrase adversaries from word scrambling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">PIE: A parallel idiomatic expression corpus for idiomatic sentence generation and paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.mwe-1.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021)</title>
		<meeting>the 17th Workshop on Multiword Expressions (MWE 2021)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="33" to="48" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
