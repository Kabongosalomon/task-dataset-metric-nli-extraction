<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongbin</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Fan</surname></persName>
							<email>zfan@stu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Engineering</orgName>
								<orgName type="institution">Shantou University</orgName>
								<address>
									<postCode>515000</postCode>
									<settlement>Shantou</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengquan</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<postCode>LS2 9JT</postCode>
									<settlement>Leeds</settlement>
									<country>UK, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Wang</surname></persName>
							<email>wangxm@cug.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuda</forename><surname>Yu</surname></persName>
							<email>yuqiuda@163.com</email>
						</author>
						<title level="a" type="main">CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Springer Nature 2021 L A T E X template</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depth estimation</term>
					<term>Semantic segmentation</term>
					<term>Attention mechanism</term>
					<term>Task interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimation and semantic segmentation are two fundamental goals of scene understanding. Due to the advantages of task interaction, many works study the joint task learning algorithm. However, most existing methods fail to fully leverage the semantic labels, ignoring the provided context structures and only using them to supervise the prediction of segmentation split, which limit the performance of both tasks. In this paper, we propose a network injected with contextual information (CI-Net) to solve the problem. Specifically, we introduce self-attention block in the encoder to generate attention map. With supervision from the ideal attention map created by semantic label, the network is embedded with contextual information so that it could 1 arXiv:2107.13800v2 [cs.CV] 1 Sep 2021 Springer Nature 2021 L A T E X template 2 CI-Net: a Joint Task learning Network using Contextual Information understand scene better and utilize correlated features to make accurate prediction. Besides, a feature sharing module is constructed to make the task-specific features deeply fused and a consistency loss is devised to make the features mutually guided. We evaluate the proposed CI-Net on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental results validate that our proposed CI-Net could effectively improve the accuracy of semantic segmentation and depth estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene understanding is an important yet challenging task in computer vision, which has significant contribution to visual simultaneous localization and mapping(vSLAM) system <ref type="bibr" target="#b0">[1]</ref>, robot navigation <ref type="bibr" target="#b1">[2]</ref>, autonomous driving <ref type="bibr" target="#b2">[3]</ref> and other applications. Two fundamental goals of scene understanding are monocular depth estimation <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> and semantic segmentation <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, which have been extensively researched by utilizing deep learning. Recently, some works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> noticed the interactions between these two tasks and utilized the common characteristics to improve each other, which achieved great performance. However, most of them used a deep structure as encoder such as ResNet-101 <ref type="bibr" target="#b11">[12]</ref>, ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, SE-ResNet <ref type="bibr" target="#b14">[15]</ref>, introducing a large number of downsampling and stride operations, which had negative influence <ref type="bibr" target="#b15">[16]</ref> towards depth estimation and semantic segmentation where fine-grained information is crucial. Despite there are also some works adopting strategies like skip-connection <ref type="bibr" target="#b14">[15]</ref>, up-projection <ref type="bibr" target="#b12">[13]</ref> and multi-scale training loss <ref type="bibr" target="#b13">[14]</ref> to mitigate this problem, these schemes have great demands on computation and memory.</p><p>Another shortcoming is that current works about joint learning didn't fully exploit the contextual information of the semantic labels. As far as we know, most of them simply utilized the labels to supervise the predictions of semantic and depth splits, making limited contribution to scene understanding of the network. In <ref type="bibr" target="#b16">[17]</ref>, Chen et al. pointed out that how to obtain the correlation of inter-object and intra-object is crucial for depth estimation. Yu et al. <ref type="bibr" target="#b17">[18]</ref> also argued such context make feature representation more robust for semantic segmentation. Therefore, could we excavate the information of labels more deeply to assist the network modeling such correlation? Moreover, most approaches achieve task interaction through adding pixel-wise features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>, simply sharing encoder commonly <ref type="bibr" target="#b19">[20]</ref> or sharing parameters of convolutional layers <ref type="bibr" target="#b12">[13]</ref>. Although these methods leveraged the correlation between tasks, their ways of fusing features were rough. For example, in <ref type="bibr" target="#b14">[15]</ref>, they fused the features via direct weighted addition and then added them to task-specific features. This simple structure may make the network difficult to learn more useful representations of the shared features.</p><p>To overcome the mentioned problems, this paper presents a network injected with contextual information (CI-Net). We adopt the dilated residual structure where the dilation operation replaces a part of downsampling layers, guaranteeing large receptive fields and avoiding introducing unnecessary parameters. To fully leverage the provided context of semantic labels, we plug a scene understanding module (SUM) with contextual supervision, which captures the similarity of pixels belong to the same classes and difference of those pertaining to different classes. Specifically, we introduce self-attention block <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> to generate attention map and exploit the semantic labels to create the ideal attention map indicating whether pair of pixels belongs to the same classes or not. The attention training loss injects the contextual prior into the network, which makes the structure know to use correlated features for more accurate prediction. To make these two tasks deeply interacted, we present two approaches. The first one is that we design a feature sharing module (FSM). Instead of simply adding the task-specific features, we concatenate and put them through a series of downsampling and upsampling operations for more useful representations being obtained. We also devise a consistency loss between the depth and semantic features, forcing them to maintain the intrinsic consistency of first-order relationship.</p><p>To summarize, the contributions of this paper are in three aspects:</p><p>-We propose a dilated network embedded a scene understanding module with contextual supervision to inject contextual prior about the correlation of inter-class and intra-class, predicting both the depth and semantic segmentation maps. -We construct a feature sharing module to deeply fuse the task-specific features and put forward a consistency loss to keep the respective features consistent in the relationship with adjacent features. -Extensive experiments are performed to demonstrate the effectiveness of our methods. And the proposed model achieves competitive results against other approaches of depth estimation and semantic segmentation on NYU-Depth-v2 and SUN-RGBD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Monocular Depth Estimation</head><p>The first research about predicting monocular depth is Saxena's work <ref type="bibr" target="#b21">[22]</ref>, which introduced Markov Random Field (MRF) to exploit the geometic priors. Later on, with the appearance of convolutional neural network (CNN), Eigen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a model intergrating the global and local information of a single image. Since then, a large number of studies using deep learning methods have emerged. Liu et al. <ref type="bibr" target="#b5">[6]</ref> combined Convolutional Neural Fields (CRF) with fully convolutional networks and put foward a superpixel pooling method making the inference faster. Laina et al. <ref type="bibr" target="#b22">[23]</ref> proposed a fully convolutional residual architecture and four up-sampling models to restore the resolution of depth map. To reduce the information loss induced by excessive pooling, Fu et al. <ref type="bibr" target="#b15">[16]</ref> employed the atrous spatial pyramid pooling (ASPP) and presented an ordinal loss to model the depth prediction as an ordinal regression problem. Inspired by <ref type="bibr" target="#b15">[16]</ref>, Chen et al. <ref type="bibr" target="#b16">[17]</ref> proposed soft ordinal inference to exploit the predicted probabilities of the whole depth intervals and replaced ASPP with self-attention module to capture the global context. Recently, Yin et al. <ref type="bibr" target="#b23">[24]</ref> projected the depth map to obtain the 3D point cloud, exploiting the loss between virtual normal and ground truth to train the model, which significantly improved the accuracy. There were also some works <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> who employed the geometric constraints of the consective image sequence to complete unsupervised depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RGB-D Semantic Segmentation</head><p>The outstanding work proposed by Long et al. <ref type="bibr" target="#b9">[10]</ref>, Fully Convolutional Network (FCN) achieved great improvement of semantic segmentation. Since then, many scholars <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> researched on this scene understanding task using single RGB images. After the RGB-D dataset was released, some approaches <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> discovered that fusing depth images could significantly improve the segmentation results. Recently, Hu et al. <ref type="bibr" target="#b33">[34]</ref> proposed Attention Complementary Network which fused weighted depth and semantic features in the encoder. The fusion implementation enabled ACNet to exploit more high-quality features. Hung et al. <ref type="bibr" target="#b34">[35]</ref> designed LDFNet, which is also a fusion-based network. Its novelty of incorporating luminance, depth and color information made great success in semantic segmentation. To reduce the inference time, Chen et al. <ref type="bibr" target="#b35">[36]</ref> proposed Spatial information guided Convolution (S-Conv) which extracted geometric information as convolutional weights and infers the sampling offset according to the 3D spatial information. Different from these algorithms who aimed to improve semantic segmentation with the facilitation of RGB-D images, we design a joint-task learning network to boost both depth estimation and semantic segmentation with only RGB images as input through the deep interaction of each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Joint Semantic Segmentation and Depth Estimation</head><p>Due to the common character of pixel-level prediction among different tasks, some works paid attention to studying joint learning. In <ref type="bibr" target="#b36">[37]</ref>, Wang et al. used a network to jointly predict a depth map and a semantic probability map. The network, however, is limited by only the last layer being changed for prediction. Jiao et al. <ref type="bibr" target="#b18">[19]</ref> proposed a network with a backbone encoder and two subnetworks as decoders for respective prediction, increasing both the accuracy of depth estimation and semantic segmentation dramatically. Later, PAD-Net was proposed by Xu et al. <ref type="bibr" target="#b37">[38]</ref>, which used four intermediate auxiliary tasks, providing abundant information for prediction. In the recent research, the SOSD-Net <ref type="bibr" target="#b38">[39]</ref> made full use of the geometric relations between depth estimation and semantic segmentation to predict. Although these works achieved outstanding performance, they didn't exploit the feature that semantic labels could help network capture prior contextual knowledge of the scene to improve the accuracy of prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Attention Mechanism</head><p>Xu et al. <ref type="bibr" target="#b39">[40]</ref> was the pioneer who innovated attention mechanism into deep learning. Since then, a profusion of attention methods were designed mainly including channel attention <ref type="bibr" target="#b40">[41]</ref>, spatial attention <ref type="bibr" target="#b41">[42]</ref> and self-attention <ref type="bibr" target="#b7">[8]</ref>. Inspired by them, some works that incorporated attention mechanism into depth prediction have emerged. Chen et al. <ref type="bibr" target="#b42">[43]</ref> interpolated channel attention block into the encoder and spatial block into the decoder to avoid losing structural information. Xu et al. <ref type="bibr" target="#b4">[5]</ref> presented a fused CRF model guided by multi-scale attention. Choi et al. <ref type="bibr" target="#b14">[15]</ref> designed an affinity propagation unit to make the depth features guided by semantic attention map.</p><p>In this work, we present a model jointly learning semantic and depth representations. We introduce a shared attention block for these two tasks with contextual supervision so that the network can understand the scene better for prediction. Moreover, we design feature sharing modules to combine the semantic and depth features, making use of the task-wise information. Notice that the similarity and discrepancy of the two kinds of features should keep consistent , we also construct a novel consistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section illustrates our proposed method for joint semantic segmentation and depth estimation from single RGB image. The first three subsections introduce the architecture of our proposed CI-Net and its sub-modules. The last subsection outlines the training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The proposed Contextual Information Network (CI-Net) uses the encoderdecoder scheme as shown in <ref type="figure">Figure 1</ref>. For the encoder, we choose the ResNet <ref type="bibr" target="#b43">[44]</ref> for its identity mapping tackling the vanishing gradient problem in deeper network. Another benefit of ResNet is its large receptive field <ref type="bibr" target="#b22">[23]</ref>, which is a crucial factor for depth estimation and semantic segmentation. However, instead of deploying ResNet as encoder directly like <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, we also adopt dilation strategy <ref type="bibr" target="#b44">[45]</ref> to mitigate the negative effect of overdownsampling in ResNet which may hinder the predictions of fine-grained depth and semantic maps. With the last two 2-dilated and 4-dilated residual blocks, the original resolution is lowered to 1/8 instead of 1/32, reducing the detailed information loss.</p><p>In the decoder, a scene understanding module (SUM) with supervised attention block is designed to fully exploit the semantic labels to obtain context prior of inter-class and intra-class, which benefits the model to understand scene better for later prediction. The network then breaks apart into two branches for estimating depth and segmenting semantics. During this stage, we present a feature sharing module (FSM) to share feature representations so that these two splits could fully exploit different levels of features. And a consistency loss is formulated to keep the task-specific features consistent. More details about these methods are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Understanding Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Sharing Module</head><p>Feature Sharing Module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Sharing Module</head><p>Consistency Loss 2 4</p><p>n Dilated Opration with rate n <ref type="figure">Fig. 1</ref> The overview of our CI-Net for joint depth estimation and semantic segmentation. We adopt dilated operation in the backbone to mitigate the harm of over-downsampling. At the end of the encoder, the SUM is designed to aggregate the contextual information.</p><p>Then the network breaks into depth and segmentation split. To deeply fuse the task-specific features, the FSM is proposed. Finally, a consistency loss is formulated to make the depth and segmentation features mutually guided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scene Understanding Module</head><p>Our motivation mainly comes from two folds: i) Pixels of same objects tend to have continuous or similar depth values while the depths of different objects have large discrepancies. ii) Under the background joint task learning of semantic segmentation and depth estimation, semantic labels contain information of each class so that it's easy to know whether pixels belong to the same classes or not. Thus our goal is to find an effective way to make the network have prior knowledge of the categorical relationship. To achieve that, we utilize the semantic labels for supervising the network with an attention loss to capture the correlation of the pixels belonging to same classes and the distinction of different classes. With the prior knowledge of the scene, the profitable information for prediction could be searched in a limited, related space instead of the whole region. Then the depth split, on the one hand, will be prevented from capturing the unrelated features. For example, the region of sky should not be used to predict the depth of ground and this behavior is hindered by the context prior for the gap between these two objects. On the other hand, the semantic one also benefits since it makes better judgments from the information of inter-class and intra-class. We encapsulate this process of obtaining contextual information in the scene understanding module, which is illustrated in the following content. The architecture of scene understanding module is presented in <ref type="figure" target="#fig_0">Figure 2</ref>. Similar to non-local block <ref type="bibr" target="#b7">[8]</ref>, it first uses 1 ? 1 convolutions to transform the input features X ? R N ?C f into query,key and value results represented by Q ? R N ?Cq , K ? R N ?C k and V ? R N ?Cv respectively, where N = H ? W is the resolution. Then the predicted attention map? can be obtained with</p><formula xml:id="formula_0">A = Sigmoid(Q T K),<label>(1)</label></formula><p>where Sigmoid( ? ) is the sigmoid function, which ensures the attention values are in range [0, 1]. After that, the value results are multiplied with the attention map to capture the correlation with each pixel. By finally being added a skip connection to avoid the problem of vanishing gradient, the output Y ? R N ?C f can be defined as</p><formula xml:id="formula_1">Y =?V + X.<label>(2)</label></formula><p>To capture the context prior of pixels, we adopt the method of <ref type="bibr" target="#b17">[18]</ref> to generate the ideal attention map. As it can be seen in <ref type="figure">Figure 3</ref>, given the ground truth , we can know the label of each pixel. To transform it into the relation between different pixels, the ground truth is first down-sampled into the size of H ? W and then flattened into a vector m of size 1 ? N . After the one-hot encoding implementation, new binary columns which indicate the presence of value from the ground truth are created, leading to a H ? W ? C matrix M, where C represents the number of total categories. The matrix M is then reshaped into size of N ? C and finally the ideal attention map A is constructed with</p><formula xml:id="formula_2">A = MM T .<label>(3)</label></formula><p>It's clear that in the ideal attention map, pixels of same classes are labeled as 1, and 0 otherwise, which aggregates the contextual information of intra-class and inter-class. And we employ the binary cross entropy loss as the attention loss:</p><formula xml:id="formula_3">L att = ? i,j (A i,j log? i,j + ((1 ? A i,j )log(1 ?? i,j )),<label>(4)</label></formula><p>where A i,j denotes the pixel at the location (i, j) of the predicted attention map.</p><p>It is worth noting that we utilize semantic labels instead of depth to inject context prior. One reason is that it is difficult to find a feasible and suitable representation of depth context. Although there exist some works proposing to use Kullback-Leibler divergence <ref type="bibr" target="#b16">[17]</ref> or planar structures <ref type="bibr" target="#b45">[46]</ref>, their methods are limited to only depth estimation task. But for joint task learning the correlation of different objects is harmful to semantic segmentation. The process of generating the ideal attention map. First we implement one-hot encoding for the semantic label and then flatten it into a HW ? 1 ? C matrix M where C denotes the number of categories. After the operation of outer product, the ideal attention map A is constructed with size of HW ? HW . It can be noticed that the pixels of same classes in semantic label have the same value 1 in the attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature sharing Module</head><p>In the decoder, the network splits into depth and semantic branches. We design a feature sharing module (FSM) aiming to make two branches share the features with each other so that they can take full advantage of semantic and depth information. The structure of FSM is presented in <ref type="figure" target="#fig_2">Figure 4</ref>(b). The depth features fd t and segmentation features fs t are first concatenated, then fed into an architecture resembling encoder-decoder. We utilize C( ? ) to represent the series of convolutions in the aforementioned process. It can be noticed that we use depthwise separable convolution to reduce computational cost. Eventually the commonly shared features are allocated adaptively into  <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. our module can capture extensive interacted context, while the comparative one only captures local interaction.</p><formula xml:id="formula_4">two branches via 1 ? 1 convolutions C 1?1 fd ( ? ), C 1?1 fs ( ? )</formula><p>. Followed by residual connections, the features d t+1 and s t+1 can be obtained by:</p><formula xml:id="formula_5">fd t+1 = fd t + C 1?1</formula><p>fd (C(concat(fd t , fs t ))), fs t+1 = fs t + C 1?1 fs (C(concat(fd t , fs t ))).</p><p>We also compare our structure with lateral sharing unit (LSU) proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>, which is shown in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. It can be seen that the task-shared features fs t+1 and fd t+1 are obtained with summation of three features, which can be formulated by:</p><formula xml:id="formula_7">fd t+1 = fd t + C 1?1 fd1 (fd t ) + C 1?1 fs2 (fs t ), fs t+1 = fs t + C 1?1 fd2 (fd t ) + C 1?1 fs1 (fs t ).<label>(6)</label></formula><p>Although their method, to some extend, realizes the interaction of different features, providing information for later predictions, we argue that the elementwise summation can only obtain local information, making limited use of the fused features. For example, the depth features located at (i, j) can only sum with the corresponding semantic features. Contrast to LSU, our method implements sampling towards the interacted features, which encapsulates large area of features and augments the presentation ability. Therefore, each task-specific feature acquires more useful information. We insert FSM before each stage of upsampling, benefiting depth split and segmentation split to exploit multi-level fused information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Loss Function</head><p>Besides the previously mentioned attention loss, our loss function includes three other parts: consistency loss, depth loss and segmentation loss. Consistency Loss: Inspired by <ref type="bibr" target="#b11">[12]</ref>, we design a consistency loss to make semantic and depth branches guide each other mutually. Specifically, the features that are distinct or similar in semantic feature map should keep the same nature as in depth representations. For example, the semantic features of the sky and tree are extremely different since they belong to different classes, while the corresponding two depth features are also discrepant because the distances of them have large gap. Therefore, we employ this character to supervise the consistency loss of task-specific features, the form of which is defined as</p><formula xml:id="formula_8">L con = l i,j ?(s i,j , s i,j (l))|D(fd i,j , l) ? D(fs i,j , l)|, D(fd i,j , l) = exp[? 1 2 (fd i,j ?fd i,j (l)) T ? ?1 fd (fd i,j ?fd i,j (l))],<label>(7)</label></formula><p>where s ij denotes the semantic label of ground truth. fd i,j (l) is the depth feature which has an offset of l along x or y direction and fs i,j (l) is the semantic feature. We use the exponential form of Mahalanobis Distance to measure the discrepancies between features where the covariance matrix ? fd is set as a diagonal matrix ? 2 I C . Here ? is a learned parameter from each feature map. Considering that the depth features at the inner edges vary widely while the semantic representations are similar, we weight L con by the function ?( ? ) which returns 1 when the corresponding labels are different and 0 otherwise. Since it is not realistic to take account of all the feature relationships, we select l from the set {1, 2} so that each feature would be compared for 8 times, which has adequately good performance in our experiments.</p><p>Depth Loss: The depth loss is composed of three items L berhu , L pair and L norm .The L berhu represents the BerHu Loss providing a good balance of the L1 norm and L2 norm, which is effective in the occasion errors following a heavy-tailed distribution <ref type="bibr" target="#b22">[23]</ref>. The L berhu is defined by</p><formula xml:id="formula_9">L berhu = i,j |d i,j ?d i,j | if|d i,j ?d i,j | ? c, (di,j ?di,j ) 2 +c 2 2c if|d i,j ?d i,j | &gt; c ,<label>(8)</label></formula><p>where d i,j andd i,j are respectively the true and estimated depth values. c is a threshold and we set it to be c = 1 5 max k (|d k ?d k |), that is 0.2 times of the max error in a batch.</p><p>We also introduce the loss term L pair to ensure the smoothness in the homogeneous regions and the relative distance of different areas. The formulation of L pair is</p><formula xml:id="formula_10">L pair = p,q??,p =q |(d p ? d q ) ? (d p ?d q )|,<label>(9)</label></formula><p>in which ? = {(i 1 , j 1 ), (i 2 , j 2 ), . . . , (i n , j n )} denotes the set of pixel indices which are selected randomly. We argue that L pair not only keeps the advantages of the gradient loss <ref type="bibr" target="#b42">[43]</ref> which penalizes the adjacent pixels of smooth areas and discontinued borders, but also provides similarity of pixels that are far apart, guaranteeing relative distances of different objects. Another loss term is L norm , which is employed to emphasize small-size structures and high-frequency details:</p><formula xml:id="formula_11">L norm = i,j 1 ? n i,j ?? i,j |n i,j | ? |? i,j | ,<label>(10)</label></formula><p>where n i,j is the surface normal calculated by n i,j = (?? x d i,j , ?? y d i,j , 1) T , in which ? x and ? y represents the gradient values along the x-axis and yaxis separately. The depth loss is then calculated by the weighted summation of these three terms:</p><formula xml:id="formula_12">L depth = L berhu + ?L pair + ?L norm ,<label>(11)</label></formula><p>where ?,? are weights to balance the depth loss terms. Segmentation Loss: To ensure the accuracy of semantic segmentation and avoid unfavorable depth estimation along object boundaries, we introduce the weighted cross-entropy loss as segmentation loss L seg :</p><formula xml:id="formula_13">L seg = ? i,j c ? c ?(s i,j , c)log(p(s i,j , c)),<label>(12)</label></formula><p>where ? c = N total ?Nc N total weights the segmentation loss to mitigate the category imbalance problem. p(s i,j , c) is the predicted probability value of class c. Then our total loss is L = L att + ?L depth + ?L con + ?L seg ,</p><p>where ?,?,? denotes the weight coefficients for each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the training datasets and evaluation metrics.Then we illustrate the implementation details of training our model. Next, we investigate the effectiveness of our proposed network and compare it with other methods. Ablation study is also performed to show the benefits of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metrics</head><p>Dataset: We use NYU-Depth-v2 <ref type="bibr" target="#b46">[47]</ref> and SUN-RGBD <ref type="bibr" target="#b47">[48]</ref> datasets to evaluate our presented model. NYU-Depth-v2 includes around 120K RGB-D images of 464 indoor scenes where only 1449 images from them have semantic labels. We follow the methods adopted in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> to use the standard 795 training pairs and 654 testing pairs. SUN-RGBD is another dataset obtained from the indoor scenes containing about 10K images(5285 images for training and 5050 images for testing). Since the dataset has both semantic labels and depth labels, the whole training set is utilized to train our model and the test set to evaluate our semantic predictions. Both of these two datasets are employed for comparing with the other methods. Metrics: Like the previous works <ref type="bibr" target="#b3">[4]</ref>, we assess our predicted depth maps using the following metrics:</p><p>Accuracy with threshold (? p ): % of d i,j s.t. max(d i,j di,j , di,j di,j ) = ? p &lt; 1.25 p (p = 1, 2, 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMSE (rms):</head><formula xml:id="formula_15">1 N i,j (d i,j ?d i,j ) 2 RMSE in log space (rms log): 1 N i,j (ln d i,j ? lnd i,j ) 2 Mean absolute relative error (abs rel): 1 N i,j |di,j ?di,j | di,j</formula><p>Mean relative square error (sq rel): 1</p><formula xml:id="formula_16">N i,j (di,j ?di,j ) 2 d 2 i,j</formula><p>The signal N represents the number of valid pixels.</p><p>To evaluate the predictions of semantic segmentation, we refer to the recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">49]</ref> and introduce pixel accuracy (pAcc) and mean intersection over union (mIoU) as metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement the model using the open source machine learning framework Pytorch on a single Nvidia GTX1080Ti GPU. As for the encoder of CI-Net, we choose the ResNet-50 and ResNet-101 as the candidates and both of them are pretrained on the ImageNet classification task <ref type="bibr" target="#b49">[50]</ref>. The learning rates of the pretrained layers are set to be 10 times smaller than the other layers. To avoid the overfitting problem, we adopt the data augmentation strategies, including random rotation, random scaling, random crop, random horizontal flip and random color jitter. The Optimization Algorithm we used is stochastic gradient descent (SGD) <ref type="bibr" target="#b50">[51]</ref> where we set the momentum as 0.9 and the weight decay as 5e ?4 . To guarantee computational efficiency and fully optimizing the network, we choose the number of set ? as 500 to compute the pair loss L pair . The weight coefficients (?, ?, ?, ?, ?) are set to <ref type="bibr">(1,</ref>  <ref type="figure" target="#fig_3">5, 0.3, 1, 5)</ref> respectively. The training process is divided into three stages. At first, we replace the SUM module with the ground truth attention maps and the model is trained using only L depth and L seg (epochs and learning rates are (300, 6e ?4 ) for NYU-Depth-v2 and (60, 6e ?4 ) for SUN-RGBD). During the second stage, the SUM is added to the model and L att participates in the training process (epochs and learning rates are (200, 2e ?4 ) for NYU-Depth-v2 and (40, 3e ?4 ) for SUN-RGBD). In the last stage, we employ all the loss costs to train the entire model (we use the polynomical decay strategy with decayed power of 0.9, epochs and initial learning rates are (200, 2e ?4 ) for NYU-Depth-v2 and (40, 3e ?4 ) for SUN-RGBD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this subsection, we conduct numerous ablative experiments to analyze the effectiveness of our settings for the model. The experiments are evaluated on NYU-Depth-v2 dataset. To make an ablation study, we set a baseline network comprised of one encoder followed by two separate decoders each of which corresponds to a single task and consists of three blocks. On the basis of baseline, we also train a network with FSM to evaluate the effectiveness of SUM and FSM. Both networks and our proposed model are equipped with dilated ResNet-101 as encoder. We first analyze the contribution of scene understanding module. The ablative visual results are shown in (d) and (e) of <ref type="figure" target="#fig_3">Figure 5</ref>. It can be observed that without context prior, both the depth estimation and segmentation results of <ref type="table">Table 1</ref> Ablation study of our method on the NYU-Depth-v2 dataset. The baseline model is our model removed FSM and SUM. Both depth estimation and semantic segmentation results are presented. baseline with FSM suffer noticeable errors especially in the white dashed line boxes. We also observe that SUM can significantly reduce the adverse effect of uneven illumination. For example, in the second scene there is a lamp shedding intense light which impairs the baseline prediction of the surrounding region. In this case, SUM provides the understanding of scene, which helps accurately predict depth and semantic information. In addition, we visualize the attention maps with and without the supervision of attention loss respectively. <ref type="figure" target="#fig_5">Figure 6</ref> shows that with the guidance of attention loss, the model does capture the correlated contextual areas and adapts to different scenes well. The attention map with supervision could be regarded as a structural extractor since it extracts intact object shapes, revealing the layout of scene. Whereas for the attention maps are not properly guided, the resulting arbitrary concerned region can be harmful. Next, it is obvious that the feature sharing strategy improves the performance of both semantic segmentation and depth estimation. For the example in the forth scene of <ref type="figure" target="#fig_3">Figure 5</ref>, the baseline fails to predict the wall behind fridge while FSM helps to perceive the existence of these two objects and make boundaries in the depth map clearer. These improvements are facilitated by deeply fusing the task specific features, providing more robust information for prediction. To see the improvement of SUM and FSM, we also perform quantitative comparisons, which can be seen in <ref type="table">Table 1</ref>. It can be obviously found that both the introductions of SUM and FSM improve the performance significantly, which verifies the effectiveness of these two blocks. For the case with scene understanding module, i.e., baseline with SUM, the original baseline network could obtain a prominent gain on both tasks, especially in rms and mIoU (7.9% reduced and 10.7% increased respectively). This result agrees well with the qualitative data and indicates that improving the understanding of scene is effective. Moreover, we also exhibits that when the extra supervision of consistency loss is added, the accuracies are slightly enhanced.</p><formula xml:id="formula_17">(a) (b) (c) (d) (e) (a) (b) (c) (d) (e) (a) (b) (c) (d) (e) (a) (b) (c) (d) (e)</formula><formula xml:id="formula_18">(a) (c) (d) (b) (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons with other methods</head><p>In this subsection, we first analyze the differences between FSM and LSU quantitatively and qualitatively. Then we compare the experimental results of our model with other algorithms according to different tasks.</p><p>Comparisons of LSU and FSM: We test the effectiveness of FSM and LSU <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> whose results can be seen in <ref type="table" target="#tab_1">Table 3</ref>. It is noticed that although the LSU does improve the performance, the improvements are not as obvious as those by adding FSM, which proves the effectiveness of sampling operations. To make a deep analysis of the difference, we visualize the allocated features F LSU d , F LSU s , F F SM d , F F SM s (summed long the channel dimension and normalized to 0 ? 1 range) of LSU and FSM, which are respectively formulated as:</p><formula xml:id="formula_19">(a) (b) (c) (d) (e)</formula><formula xml:id="formula_20">F LSU d = C 1?1 fd1 (fd t ) + C 1?1 fs2 (fs t ) F LSU s = C 1?1 fd2 (fd t ) + C 1?1 fs1 (fs t ) F F SM d = C 1?1</formula><p>fd (C(concat(fd t , fs t ))) F F SM s = C 1?1 fs (C(concat(fd t , fs t )))</p><p>In <ref type="figure" target="#fig_6">Figure 7</ref>, we can easily observe that the features learned by LSU almost pay attention to the whole region, which is not reasonable in depth estimation and semantic segmentation since if all the features of different objects are emphasized, the classification and depth estimation of objects would be confused for using highlighted features from others. In contrast, with larger receptive fields and deeper structure, our proposed module could learn more useful information such as objects, which are important in both tasks. In addition, it could be observed that FSM learns clear black boundaries between different objects, when the features are used for lateral convolution, such contours of zero values would inhibit the convoluted operation from using features of other objects, avoiding generating confused features. Depth Estimation: Comparisons of depth values are made among our presented CI-Net and other algorithms on NYU-Depth-v2 dataset where the number of categories are 40. The results are summarized in <ref type="table" target="#tab_0">Table 2</ref>. The values are used directly from the original papers. As it can be seen in the <ref type="table" target="#tab_0">Table 2</ref>, our model with ResNet-101 achieves best results in most metrics compared to the  model using the same scale data. The rms of CI-Net is even better than those methods fed with larger training set. It's also worth noting that although the methods in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b56">57]</ref> perform well in part of the evaluating indicators, they all feed numerous images (120k) to boost the performance. In addition, <ref type="figure" target="#fig_7">Figure 8</ref> displays some visual examples. It can be seen that though the predictions of the method by Laina <ref type="bibr" target="#b22">[23]</ref> are smoothed as a whole, they lose some details, bringing in the blurred object boundaries especially in desk, washing machine and sofa. Besides, the precision of depth maps is weak, the depths of some regions deviate the ground truth severely. Although <ref type="bibr" target="#b4">[5]</ref> has great values in evaluation metrics as seen in <ref type="table" target="#tab_0">Table 2</ref>, the contours in predicted depth maps of their models are not sharp which make the depth maps not clear enough. Compared to them, our results match the structures of scenes and have sharper object boundaries benefiting from prior inter-class and intra-class information. Semantic Segmentation: Apart from depth estimation, we also show the semantic segmentation results in <ref type="table" target="#tab_2">Table 4</ref> and <ref type="table" target="#tab_3">Table 5</ref> for NYU-Depth-v2 and SUN-RGBD datasets respectively. The data types of RGB and RGBD in tables mean using RGB images or both RGB and depth images as input. As reported in <ref type="table" target="#tab_2">Table 4</ref>, our model with ResNet-101 performs best among all the listed methods in both pixel-acc and mIoU even though our method only uses RGB images, which demonstrate that the context prior can also benefit the semantic segmentation and the deep feature interaction does help this task leverage more useful information. As for the results of SUN-RGBD dataset in <ref type="table" target="#tab_3">Table 5</ref>, we can notice that our method still performs competitively. Although RefineNet-101 achieves best performance in mIoU metric, our method is on a par with it in pixel-acc metric. Some selected visual maps of both datasets are also shown in <ref type="figure" target="#fig_8">Figure 9</ref>, it can be seen that our predictions are in high quality and even give the correct labels of the invalid regions in ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, a network for joint task learning was proposed. By employing the scene understanding module, the presented network was able to capture the contextual information of inter-class and intra-class, which is crucial for the network to understand which useful context can be exploited to make predictions. To fuse the task-specific features deeply, we designed a feature sharing module which enlarged the receptive fields and augmented the presentation ability through upsampling and downsampling operations. In addition, a consistency loss was proposed to make the task-specific features mutually guided, which kept consistent relationships with the respective adjacent features. Ablation study demonstrated the improvements of our proposed modules and the comparative results with other methods on NYU-Depth-v2 and SUN RGB-D datasets the effectiveness of our method. In the future, we plan to explore the contextual information in the attention map and incorporate other pixel-level tasks such as surface normal prediction, edge detection into this work. Also, we are interested in combining the depth prediction with semantic SLAM to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>? Funding: Not Available ? Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use): Not Available ? Availability of data and materials: Not Available ? Code availability: Not Available</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>The structure of SUM. The generated attention map captures context prior of inter-class and intra-class so that the network understand the scene better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FO</head><label></label><figDesc>Flattening and one-hot encoding T Transpose Outer product Fig. 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>(a) The architecture of LSU; (b) The architecture of our proposed FSM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Ablative visual comparisons. (a) input image; (b) ground truth; (c) results of baseline; (d) results of baseline with FSM; (e) results of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Comparisons of the learned attention map. (a) input images; (b) ideal attention maps; (c) and (d) represent the attention maps produced by our model without and with supervision of the Latt respectively. For each scene we show two different attention maps pertain to the locations where the red plus signs mark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Visual results of allocated features(summed along the channel dimension and normalized to 0 ? 1 range). (a) input images; (b) features added to semantic branch learned by LSU; (c) features added to depth branch learned by LSU; (d) features added to semantic branch learned by FSM (e) features added to depth branch learned by FSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>Visual comparison with some approaches on NYU-depth-v2 dataset. (a) input images; (b) ground truth; (c) predictions of<ref type="bibr" target="#b22">[23]</ref>; (d) predictions of<ref type="bibr" target="#b4">[5]</ref> (e) predictions of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9</head><label>9</label><figDesc>Visualized semantic segmentation maps on SUN-RGBD dataset. (a) input images; (b) ground truth; (c) semantic segmentation predictions of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Comparisons with the depth estimation methods on NYU-Depth-V2 dataset.</figDesc><table><row><cell>Methods</cell><cell>Data</cell><cell cols="3">higher is better</cell><cell></cell><cell cols="2">lower is better</cell><cell></cell></row><row><cell></cell><cell></cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell><cell>rms</cell><cell cols="3">rms log abs rel sq rel</cell></row><row><cell>Wang et al.[37]</cell><cell>795</cell><cell cols="3">0.605 0.890 0.970</cell><cell>0.745</cell><cell>0.262</cell><cell>0.220</cell><cell>0.210</cell></row><row><cell>DCNF[6]</cell><cell>795</cell><cell cols="3">0.614 0.883 0.971</cell><cell>0.824</cell><cell>/</cell><cell>0.230</cell><cell>/</cell></row><row><cell>HCRF[52]</cell><cell>795</cell><cell cols="3">0.621 0.886 0.968</cell><cell>0.821</cell><cell>/</cell><cell>0.232</cell><cell>/</cell></row><row><cell>Roy et al.[53]</cell><cell>795</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>0.744</cell><cell>/</cell><cell>0.187</cell><cell>/</cell></row><row><cell>PAD-Net [38]</cell><cell>795</cell><cell cols="3">0.817 0.954 0.987</cell><cell>0.582</cell><cell>/</cell><cell>0.120</cell><cell>/</cell></row><row><cell>FCRN [23]</cell><cell>12k</cell><cell cols="3">0.811 0.953 0.988</cell><cell>0.573</cell><cell>0.195</cell><cell>0.127</cell><cell>/</cell></row><row><cell>Li et al. [54]</cell><cell>12k</cell><cell cols="3">0.820 0.960 0.989</cell><cell>0.545</cell><cell>/</cell><cell>0.139</cell><cell>/</cell></row><row><cell>GeoNet [55]</cell><cell>16k</cell><cell cols="3">0.834 0.960 0.990</cell><cell>0.569</cell><cell>/</cell><cell>0.128</cell><cell>/</cell></row><row><cell>Eigen et al.[4]</cell><cell>120k</cell><cell cols="3">0.611 0.887 0.971</cell><cell>0.907</cell><cell>0.285</cell><cell>0.158</cell><cell>0.121</cell></row><row><cell>Eigen et al.[56]</cell><cell>120k</cell><cell cols="3">0.769 0.950 0.988</cell><cell>0.641</cell><cell>0.214</cell><cell>0.158</cell><cell>0.121</cell></row><row><cell>CRFs[5]</cell><cell>95k</cell><cell cols="3">0.811 0.954 0.987</cell><cell>0.586</cell><cell>/</cell><cell>0.121</cell><cell>/</cell></row><row><cell>DORN [16]</cell><cell>120k</cell><cell cols="3">0.828 0.965 0.992</cell><cell>0.509</cell><cell>/</cell><cell>0.115</cell><cell>/</cell></row><row><cell>Hu et al. [57]</cell><cell>120k</cell><cell cols="3">0.866 0.975 0.993</cell><cell>0.530</cell><cell>/</cell><cell>0.115</cell><cell>/</cell></row><row><cell>CI-Net</cell><cell>795</cell><cell cols="3">0.812 0.957 0.990</cell><cell>0.504</cell><cell>0.181</cell><cell>0.129</cell><cell>0.112</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Comparisons between FSM and LSU</figDesc><table><row><cell></cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell><cell>rms abs rel pAcc mIoU</cell></row><row><cell>baseline</cell><cell cols="4">0.769 0.942 0.982 0.563 0.159 67.1 37.2</cell></row><row><cell cols="5">baseline + LSU 0.778 0.948 0.986 0.547 0.152 68.3 38.5</cell></row><row><cell cols="5">baseline + FSM 0.785 0.950 0.988 0.538 0.152 69.5 39.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Comparisons with the semantic segmentation methods on NYU-Depth-v2 dataset</figDesc><table><row><cell>Methods</cell><cell>Data</cell><cell cols="2">pAcc mIoU</cell></row><row><cell>FCN [10]</cell><cell>RGB</cell><cell>60.0</cell><cell>29.2</cell></row><row><cell>Context [11]</cell><cell>RGB</cell><cell>70.0</cell><cell>40.6</cell></row><row><cell>Eigen et al. [56]</cell><cell>RGB</cell><cell>65.6</cell><cell>34.1</cell></row><row><cell>B-SegNet [30]</cell><cell>RGB</cell><cell>68.0</cell><cell>32.4</cell></row><row><cell>RefineNet-LW-101 [58]</cell><cell>RGB</cell><cell>/</cell><cell>43.6</cell></row><row><cell>TD2-PSP50 [59]</cell><cell>RGB</cell><cell>/</cell><cell>43.5</cell></row><row><cell>Deng et al. [49]</cell><cell>RGBD</cell><cell>63.8</cell><cell>31.5</cell></row><row><cell>3DGNN [31]</cell><cell>RGBD</cell><cell>/</cell><cell>42.0</cell></row><row><cell>D-CNN [8]</cell><cell>RGBD</cell><cell>/</cell><cell>41.0</cell></row><row><cell>CI-Net</cell><cell>RGB</cell><cell>72.7</cell><cell>42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Comparisons with the semantic segmentation methods on SUN-RGBD dataset</figDesc><table><row><cell>Methods</cell><cell>Data</cell><cell cols="2">pAcc mIoU</cell></row><row><cell>Context [11]</cell><cell>RGB</cell><cell>78.4</cell><cell>42.3</cell></row><row><cell>B-SegNet [30]</cell><cell>RGB</cell><cell>71.2</cell><cell>30.7</cell></row><row><cell>RefineNet-101 [60]</cell><cell>RGB</cell><cell>80.4</cell><cell>45.7</cell></row><row><cell>AdaptNet++ [61]</cell><cell>RGB</cell><cell>/</cell><cell>38.4</cell></row><row><cell>SSMA [61]</cell><cell>RGBD</cell><cell>80.2</cell><cell>43.9</cell></row><row><cell>CMoDE [62]</cell><cell>RGBD</cell><cell>79.8</cell><cell>41.9</cell></row><row><cell>LFC [63]</cell><cell>RGBD</cell><cell>79.4</cell><cell>41.8</cell></row><row><cell>FuseNet [32]</cell><cell>RGBD</cell><cell>76.3</cell><cell>37.3</cell></row><row><cell>D-CNN [64]</cell><cell>RGBD</cell><cell>/</cell><cell>42.0</cell></row><row><cell>CI-Net</cell><cell>RGB</cell><cell>80.7</cell><cell>44.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new approach to simultaneous localization and map building with implicit model learning using neuro evolutionary optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-010-0257-9</idno>
		<ptr target="https://doi.org/10.1007/s10489-010-0257-9" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="269" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recent advances in evolutionary and bio-inspired adaptive robotics: Exploiting embodied dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garvie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Domcsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nowotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Philippides</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-021-02275-9</idno>
		<ptr target="https://doi.org/10.1007/s10489-021-02275-9" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6467" to="6496" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning and control algorithms of direct perception for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-020-01827-9</idno>
		<ptr target="https://doi.org/10.1007/s10489-020-01827-9" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="237" to="247" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2283</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00412</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00412" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299152</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7299152" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcsvt.2017.2740321</idno>
		<ptr target="https://doi.org/10.1109/tcsvt.2017.2740321" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3174" to="3182" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00813</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00813" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MMNet: Multi-modal multi-stage network for RGB-t image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-021-02687-7</idno>
		<ptr target="https://doi.org/10.1007/s10489-021-02687-7" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298965</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298965" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.348</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.348" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semanticallyguided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12319</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00423</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2019.00423" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01249-6_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01249-615" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="238" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Safenet: Self-supervised monocular depth estimation with semantic-aware feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02893</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00214</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00214" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-based context aggregation network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13042-020-01251-y</idno>
		<ptr target="https://doi.org/10.1007/s13042-020-01251-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1583" to="1596" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01243</idno>
		<ptr target="https://doi.org/10.1109/cvpr42600.2020.01243" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01267-0_4</idno>
		<idno>1007/978-3-030-01267-0 4</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="55" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfsupervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58565-5_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58565-535" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2074</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-2074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1109/3dv.2016.32</idno>
		<ptr target="https://doi.org/10.1109/3dv.2016.32" />
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00578</idno>
		<ptr target="https://doi.org/10.1109/iccv.2019.00578" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00212</idno>
		<idno>2018.00212</idno>
		<ptr target="https://doi.org/10.1109/cvpr" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LEGO: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00031</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00031" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.699</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.699" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58539-6_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58539-611" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OCNet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-021-01465-9</idno>
		<ptr target="https://doi.org/10.1007/s11263-021-01465-9" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2375" to="2398" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<title level="m">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.556</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.556" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FuseNet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54181-5_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-54181-514" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2016</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">STD2p: RGBD semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.757</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.757" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ACNET: Attention based network to exploit complementary features for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2019.8803025</idno>
		<ptr target="https://doi.org/10.1109/icip.2019.8803025" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incorporating luminance, depth and color information by a fusion-based network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2019.8803360</idno>
		<ptr target="https://doi.org/10.1109/icip.2019.8803360" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial information guided convolution for real-time RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2021.3049332</idno>
		<ptr target="https://doi.org/10.1109/tip.2021.3049332" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2313" to="2324" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298897</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298897" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PAD-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00077</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00077" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SOSD-net: Joint semantic object segmentation and depth estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.01.126</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2021.01.126" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00745</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00745" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving monocular depth estimation by leveraging structural awareness and complementary datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58568-6_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58568-66" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="90" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.75</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.75" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil?</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58574-7_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58574-735" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33715-4_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-33715-454" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SUN RGB-d: A RGB-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298655</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298655" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic segmentation of RGBD images with mutex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.202</idno>
		<ptr target="https://doi.org/10.1109/iccv.2015.202" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning 3-d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-7908-2604-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-7908-2604-316" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298715</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298715" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.594</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.594" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated CNNs and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.05.029</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2018.05.029" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GeoNet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00037</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00037" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.304</idno>
		<ptr target="https://doi.org/10.1109/iccv.2015.304" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv.2019.00116</idno>
		<ptr target="https://doi.org/10.1109/wacv.2019.00116" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Light-weight refinenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03272</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00884</idno>
		<ptr target="https://doi.org/10.1109/cvpr42600.2020.00884" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/cvpr.2017.549</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.549" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01188-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-019-01188-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1239" to="1285" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-50115-4_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-50115-441" />
	</analytic>
	<monogr>
		<title level="m">Springer Proceedings in Advanced Robotics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">AdapNet: Adaptive semantic segmentation in adverse environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra.2017.7989540</idno>
		<ptr target="https://doi.org/10.1109/icra.2017.7989540" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01252-6_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01252-69" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="144" to="161" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
