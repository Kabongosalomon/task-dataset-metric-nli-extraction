<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Entity Matching with Pre-Trained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>XXX-XXX</publisher>
				<availability status="unknown"><p>Copyright XXX-XXX</p>
				</availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
							<email>yuliang@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
							<email>jinfeng@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
							<email>wangchiew@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Megagon Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Entity Matching with Pre-Trained Language Models</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Deep Entity Matching with Pre-Trained Language Models. PVLDB</title>
						<imprint>
							<publisher>XXX-XXX</publisher>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.14778/3421424.3421431</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Ditto, a novel entity matching system based on pretrained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-ofthe-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn "harder" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%. PVLDB Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Entity Matching (EM) refers to the problem of determining whether two data entries refer to the same real-world entity. Consider the two datasets about products in <ref type="figure">Figure 1</ref>. The goal is to determine the set of pairs of data entries, one entry from each table so that each pair of entries refer to the same product.</p><p>If the datasets are large, it can be expensive to determine the pairs of matching entries. For this reason, EM is typically accompanied by a pre-processing step, called blocking, to prune pairs of entries that are unlikely matches to reduce the number of candidate pairs This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097. doi: <ref type="bibr">10.14778/3421424.3421431</ref> to consider. As we will illustrate, correctly matching the candidate pairs requires substantial language understanding and domainspecific knowledge. Hence, entity matching remains a challenging task even for the most advanced EM solutions.</p><p>We present Ditto, a novel EM solution based on pre-trained Transformer-based language models (or pre-trained language models in short). We cast EM as a sequence-pair classification problem to leverage such models, which have been shown to generate highly contextualized embeddings that capture better language understanding compared to traditional word embeddings. Ditto further improves its matching capability through three optimizations: <ref type="bibr" target="#b0">(1)</ref> It allows domain knowledge to be added by highlighting important pieces of the input that may be useful for matching decisions. (2) It summarizes long strings so that only the most essential information is retained and used for EM. (3) It augments training data with (difficult) examples, which challenges Ditto to learn "harder" and also reduces the amount of training data required. <ref type="figure" target="#fig_0">Figure 2</ref> depicts Ditto in the overall architecture of a complete EM workflow.</p><p>There are 9 candidate pairs of entries to consider for matching in total in <ref type="figure">Figure 1</ref>. The blocking heuristic that matching entries must have one word in common in the title will reduce the number of pairs to only 3: the first entry on the left with the first entry on the right and so on. Perhaps more surprisingly, even though the 3 pairs are highly similar and look like matches, only the first and last pair of entries are true matches. Our system, Ditto, is able to discern the nuances in the 3 pairs to make the correct conclusion for every pair while some state-of-the-art systems are unable to do so.</p><p>The example illustrates the power of language understanding given by Ditto's pre-trained language model. It understands that instant immersion spanish deluxe 2.0 is the same as instant immers spanish dlux 2 in the context of software products even though they are syntactically different. Furthermore, one can explicitly emphasize that certain parts of a value are more useful for deciding matching decisions. For books, the domain knowledge that the grade level or edition is important for matching books can be made explicit to Ditto, simply by placing tags around the grade/edition values. Hence, for the second candidate pair, even though the titles are highly similar (i.e., they overlap in many words), Ditto is able to focus on the grade/edition information when making the matching decision. The third candidate pair shows the power of language understanding for the opposite situation. Even though the entries look dissimilar Ditto is able to attend to the right parts of a value (i.e., the manf./modelno under different attributes) and also understand the semantics of the model number to make the right decision.</p><p>Contributions In summary, the following are our contributions:</p><p>? We present Ditto, a novel EM solution based on pre-trained language models (LMs) such as BERT. We fine-tune and cast EM   In addition to the training data, the user of Ditto can specify (1) a method for injecting domain knowledge (DK), (2) a summarization module for keeping the essential information, and (3) a data augmentation (DA) operator to strengthen the training set.</p><p>as a sequence-pair classification problem to leverage such models with a simple architecture. To our knowledge, Ditto is one of the first EM solutions that leverage pre-trained Transformer-based LMs 1 to provide deeper language understanding for EM. ? We also developed three optimization techniques to further improve Ditto's matching capability through injecting domain knowledge, summarizing long strings, and augmenting training data with (difficult) examples. The first two techniques help Ditto focus on the right information for making matching decisions. The last technique, data augmentation, is adapted from <ref type="bibr" target="#b30">[31]</ref> for EM to help Ditto learn "harder" to understand the data invariance properties that may exist but are beyond the provided labeled examples and also, reduce the amount of training data required. ? We evaluated the effectiveness of Ditto on three benchmark datasets: the Entity Resolution benchmark <ref type="bibr" target="#b25">[26]</ref>, the Magellan dataset <ref type="bibr" target="#b24">[25]</ref>, and the WDC product matching dataset <ref type="bibr" target="#b38">[39]</ref> of various sizes and domains. Our experimental results show that Ditto consistently outperforms the previous SOTA EM solutions in all datasets and by up to 31% in F1 scores. Furthermore, Ditto consistently performs better on dirty data and is more label efficient: it achieves the same or higher previous SOTA accuracy using less than half the labeled data. ? We applied Ditto to a real-world large-scale matching task on two company datasets, containing 789K and 412K entries respectively. To deploy an end-to-to EM pipeline efficiently, we developed an advanced blocking technique to help reduce the number of pairs to consider for Ditto. Ditto obtains high accuracy, 96.5% F1 on a holdout dataset. The blocking phase also helped speed up the end-to-end EM deployment significantly, by up to 3.8 times, compared to naive blocking techniques. ? Finally, we open-source Ditto at https://github.com/megagonlabs/ ditto. <ref type="bibr" target="#b0">1</ref> There is a concurrent work <ref type="bibr" target="#b5">[6]</ref> which applies a similar idea.</p><p>Outline Section 2 overviews Ditto and pre-trained LMs. Section 3 describes how we optimize Ditto with domain knowledge, summarization, and data augmentation. Our experimental results are described in Section 4 and the case study is presented in Section 5. We discuss related work in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND ARCHITECTURE</head><p>We present the main concepts behind EM and provide some background on pre-trained LMs before we describe how we fine-tune the LMs on EM datasets to train EM models. We also present a simple method for reducing EM to a sequence-pair classification problem so that pre-trained LMs can be used for solving the EM problem.</p><p>Notations Ditto's EM pipeline takes as input two collections D and D ? of data entries (e.g., rows of relational tables, XML documents, JSON files, text paragraphs) and outputs a set M ? D ? D ? of pairs where each pair (e, e ? ) ? M is thought to represent the same real-world entity (e.g., person, company, laptop, etc.). A data entry e is a set of key-value pairs e = {(attr i , val i )} 1?i ?k where attr i is the attribute name and val i is the attribute's value represented as text. Note that our definition of data entries is general enough to capture both structured and semi-structured data such as JSON files.</p><p>As described earlier, an end-to-end EM system consists of a blocker and a matcher. The goal of the blocking phase is to quickly identify a small subset of D ? D ? of candidate pairs of high recall (i.e., a high proportion of actual matching pairs are that subset). The goal of a matcher (i.e., Ditto) is to accurately predict, given a pair of entries, whether they refer to the same real-world entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-trained language models</head><p>Unlike prior learning-based EM solutions that rely on word embeddings and customized RNN architectures to train the matching model (See Section 6 for a detailed summary), Ditto trains the matching models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type="bibr" target="#b12">[13]</ref> and GPT-2 <ref type="bibr" target="#b40">[41]</ref> have demonstrated good performance on a wide range of NLP tasks. They are typically deep neural networks with multiple Transformer layers <ref type="bibr" target="#b50">[51]</ref>, typically 12 or 24 layers, pre-trained on large text corpora such as Wikipedia articles in an unsupervised manner. During pretraining, the model is self-trained to perform auxiliary tasks such as missing token and next-sentence prediction. Studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref> have shown that the shallow layers capture lexical meaning while the deeper layers capture syntactic and semantic meanings of the input sequence after pre-training.</p><p>A specific strength of pre-trained LMs is that it learns the semantics of words better than conventional word embedding techniques such as word2vec, GloVe, or FastText. This is largely because the Transformer architecture calculates token embeddings from all the tokens in the input sequence and thus, the embeddings it generates are highly-contextualized and captures the semantic and contextual understanding of the words. Consequently, such embeddings can capture polysemy, i.e., discern that the same word may have different meanings in different phrases. For example, the word Sharp has different meanings in "Sharp resolution" versus "Sharp TV". Pre-trained LMs will embed "Sharp" differently depending on the context while traditional word embedding techniques such as FastText always produce the same vector independent of the context. Such models can also understand the opposite, i.e., that different words may have the same meaning. For example, the words immersion and immers (respectively, (deluxe, dlux) and (2.0, 2)) are likely the same given their respective contexts. Thus, such language understanding capability of pre-trained LMs can improve the EM performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-tuning pre-trained language models</head><p>A pre-trained LM can be fine-tuned with task-specific training data so that it becomes better at performing that task. Here, we fine-tune a pre-trained LM for the EM task with a labeled training dataset consisting of positive and negative pairs of matching and non-matching entries as follows: (1) Add task-specific layers after the final layer of the LM. For EM, we add a simple fully connected layer and a softmax output layer for binary classification. (2) Initialize the modified network with parameters from the pretrained LM. (3) Train the modified network on the training set until it converges.</p><p>The result is a model fine-tuned for the EM task. See Appendix A for the model architecture. In Ditto, we fine-tune the popular 12-layer BERT model <ref type="bibr" target="#b12">[13]</ref>, RoBERTa <ref type="bibr" target="#b28">[29]</ref>, and a 6-layer smaller but faster variant DistilBERT <ref type="bibr" target="#b44">[45]</ref>. However, our proposed techniques are independent of the choice of pre-trained LMs and Ditto can potentially perform even better with larger pre-trained LMs. The pair of data entries is serialized (see next section) as input to the LM and the output is a match or no-match decision. Ditto's architecture is much simpler when compared to many state-of-the-art EM solutions today <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>. Even though the bulk of the "work" is simply off-loaded to pre-trained LMs, we show that this simple scheme works surprisingly well in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Serializing the data entries for Ditto</head><p>Since LMs take token sequences (i.e., text) as input, a key challenge is to convert the candidate pairs into token sequences so that they can be meaningfully ingested by Ditto.</p><p>Ditto serializes data entries as follows: for each data entry e = {(attr i , val i )} 1?i ?k , we let where [SEP] is the special token separating the two sequences and [CLS] is the special token necessary for BERT to encode the sequence pair into a 768-dimensional vector which will be fed into the fully connected layers for classification.</p><p>Other serialization schemes There are different ways to serialize data entries so that LMs can treat the input as a sequence classification problem. For example, one can also omit the special tokens "[COL]" and/or "[VAL]", or exclude attribute names attr i during serialization. We found that including the special tokens to retain the structure of the input does not hurt the performance in general and excluding the attribute names tend to help only when the attribute names do not contain useful information (e.g., names such as attr1, attr2, ...) or when the entries contain only one column. A more rigorous study on this matter is left for future work.</p><p>Heterogeneous schemas As shown, the serialization method of Ditto does not require data entries to adhere to the same schema. It also does not require that the attributes of data entries to be matched prior to executing the matcher, which is a sharp contrast to other EM systems such as DeepER <ref type="bibr" target="#b13">[14]</ref> or DeepMatcher 2 <ref type="bibr" target="#b33">[34]</ref>. Furthermore, Ditto can also ingest and match hierarchically structured data entries by serializing nested attribute-value pairs with special start and end tokens (much like Lisp or XML-style parentheses structure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPTIMIZATIONS IN DITTO</head><p>As we will describe in Section 4, the basic version of Ditto, which leverages only the pre-trained LM, is already outperforming the SOTA on average. Here, we describe three further optimization techniques that will facilitate and challenge Ditto to learn "harder", and consequently make better matching decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Leveraging Domain Knowledge</head><p>Our first optimization allows domain knowledge to be injected into Ditto through pre-processing the input sequences (i.e., serialized data entries) to emphasize what pieces of information are potentially important. This follows the intuition that when human workers make a matching/non-matching decision on two data entries, they typically look for spans of text that contain key information before making the final decision. Even though we can also train deep learning EM solutions to learn such knowledge, we will require a significant amount of training data to do so. As we will describe, this pre-processing step on the input sequences is lightweight and yet can yield significant improvements. Our experiment results show that with less than 5% of additional training time, we can improve the model's performance by up to 8%.</p><p>There are two main types of domain knowledge that we can provide Ditto.</p><p>Span Typing The type of a span of tokens is one kind of domain knowledge that can be provided to Ditto. Product id, street number, publisher are examples of span types. Span types help Ditto avoid mismatches. With span types, for example, Ditto is likelier to avoid matching a street number with a year or a product id. <ref type="table" target="#tab_1">Table 1</ref> summarizes the main span types that human workers would focus on when matching three types of entities in our benchmark datasets. The developer specifies a recognizer to type spans of tokens from attribute values. The recognizer takes a text string v as input and returns a list recognizer(v) = {(s i , t i , type i )} i ?1 of start/end positions of the span in v and the corresponding type of the span. Ditto's current implementation leverages an open-source Named-Entity Recognition (NER) model <ref type="bibr" target="#b47">[48]</ref> to identify known types such as persons, dates, or organizations and use regular expressions to identify specific types such as product IDs, last 4 digits of phone numbers, etc.</p><p>After the types are recognized, the original text v is replaced by a new text where special tokens are inserted to reflect the types of the spans. For example, a phone number "(866) 246-6453" may be replaced with "( 866 ) 246 -[LAST] 6453 [/LAST]" where [LAST]/[/LAST] indicates the start/end of the last 4 digits and additional spaces are also added because of tokenization. In our implementation, when we are sure that the span type has only one token or the NER model is inaccurate in determining the end position, we drop the end indicator and keep only the start indicator token.</p><p>Intuitively, these newly added special tokens are additional signals to the self-attention mechanism that already exists in pretrained LMs, such as BERT. If two spans have the same type, then Ditto picks up the signal that they are likelier to be the same and hence, they are aligned together for matching. In the above example,</p><formula xml:id="formula_0">"..246-[LAST] 6453 [/LAST] .. [SEP] .. [LAST] 0000 [/LAST].."</formula><p>when the model sees two encoded sequences with the [LAST] special tokens, it is likely to take the hint to align "6453" with "0000" without relying on other patterns elsewhere in the sequence that may be harder to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Span Normalization</head><p>The second kind of domain knowledge that can be passed to Ditto rewrites syntactically different but equivalent spans into the same string. This way, they will have identical embeddings and it becomes easier for Ditto to detect that the two spans are identical. For example, we can enforce that "VLDB journal" and "VLDBJ" are the same by writing them as VLDBJ. Similarly, we can enforce the general knowledge that "5 %" vs. "5.00 %" are equal by writing them as "5.0%".</p><p>The developer specifies a set of rewriting rules to rewrite spans. The specification consists of a function that first identifies the spans of interest before it replaces them with the rewritten spans. Ditto contains a number of rewriting rules for numbers, including rules that round all floating point numbers to 2 decimal places and dropping all commas from integers (e.g., "2,020" ? "2020"). For abbreviations, we allow the developers to specify a dictionary of synonym pairs to normalize all synonym spans to be the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Summarizing long entries</head><p>When the value is an extremely long string, it becomes harder for the LM to understand what to pay attention to when matching. In addition, one limiting factor of Transformer-based pre-trained LMs is that there is a limit on the sequence length of the input. For example, the input to BERT can have at most 512 sub-word tokens. It is thus important to summarize the serialized entries down to the maximum allowed length while retaining the key information. A common practice is to truncate the sequences so that they fit within the maximum length. However, the truncation strategy does not work well for EM in general because the important information for matching is usually not at the beginning of the sequences.</p><p>There are many ways to perform summarization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. In Ditto's current implementation, we use a TF-IDF-based summarization technique that retains non-stopword tokens with the high TF-IDF scores. We ignore the start and end tags generated by span typing in this process and use the list of stop words from scikit-learn library <ref type="bibr" target="#b36">[37]</ref>. By doing so, Ditto feeds only the most informative tokens to the LM. We found that this technique works well in practice. Our experiment results show that it improves the F1 score of Ditto on a text-heavy dataset from 41% to over 93% and we plan to add more summarization techniques to Ditto's library in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Augmenting training data</head><p>We describe how we apply data augmentation to augment the training data for entity matching.</p><p>Data augmentation (DA) is a commonly used technique in computer vision for generating additional training data from existing examples by simple transformation such as cropping, flipping, rotation, padding, etc. The DA operators not only add more training data, but the augmented data also allows to model to learn to make predictions invariant of these transformations.</p><p>Similarly, DA can add training data that will help EM models learn "harder". Although labeled examples for EM are arguably not hard to obtain, invariance properties are very important to help make the solution more robust to dirty data, such as missing values (NULLs), values that are placed under the wrong attributes or missing some tokens.</p><p>Next, we introduce a set of DA operators for EM that will help train more robust models.</p><p>Augmentation operators for EM The proposed DA operators are summarized in <ref type="table" target="#tab_1">Table 2</ref>. If s is a serialized pair of data entries with a match or no-match label l, then an augmented example is a pair (s ? , l), where s ? is obtained by applying an operator o on s and s ? has the same label l as before.</p><p>The operators are divided into 3 categories. The first category consists of span-level operators, such as span_del and span_shuffle. These two operators are used in NLP tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b56">57]</ref> and shown to be effective for text classification. For span_del, we randomly delete from s a span of tokens of length at most 4 without special tokens (e.g., [SEP], [COL], [VAL]). For span_shuffle, we sample a span of length at most 4 and randomly shuffle the order of its tokens. The model may learn to predict "no-match" based on the phone number alone, which is insufficient in general. On the other hand, by corrupting parts of the input sequence (e.g., dropping phone numbers), DA forces the model to learn beyond that, by leveraging the remaining signals, such as the company name, to predict "nomatch". The second category of operators is attribute-level operators: attr_del and attr_shuffle. The operator attr_del randomly deletes an attribute (both name and value) and attr_shuffle randomly shuffles the order of the attributes of both data entries. The motivation for attr_del is similar to span_del and span_shuffle but it gets rid of an attribute entirely. The attr_shuffle operator allows the model to learn the property that the matching decision should be independent of the ordering of attributes in the sequence.</p><p>The last operator, entry_swap, swaps the order of the pair (e, e ? ) with probability 1/2. This teaches the model to make symmetric decisions (i.e., F (e, e ? ) = F (e ? , e)) and helps double the size of the training set if both input tables are from the same data source.</p><p>MixDA: interpolating the augmented data Unlike DA operators for images which almost always preserve the image labels, the operators for EM can distort the input sequence so much that the label becomes incorrect. For example, the attr_del operator may drop the company name entirely and the remaining attributes may contain no useful signals to distinguish the two entries.</p><p>To address this issue, Ditto applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type="bibr" target="#b30">[31]</ref> illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Instead of using the augmented example directly, MixDA computes a convex interpolation of the original example with the augmented examples. Hence, the interpolated example is somewhere in between, i.e., it is a "partial" augmentation of the original example and this interpolated example is expected to be less distorted than the augmented one.</p><p>The idea of interpolating two examples is originally proposed for computer vision tasks <ref type="bibr" target="#b62">[63]</ref>. For EM or text data, since we cannot directly interpolate sequences, MixDA interpolates their representations by the language model instead. In practice, augmentation with MixDA slows the training time because the LM is called twice. However, the prediction time is not affected since the DA operators are only applied to training data. Formally, given an operator o (e.g., span deletion) and an original example s, to apply o on s with MixDA (as <ref type="figure" target="#fig_2">Figure 3</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We present the experiment results on benchmark datasets for EM: the ER Benchmark datasets <ref type="bibr" target="#b25">[26]</ref>, the Magellan datasets <ref type="bibr" target="#b24">[25]</ref> and the WDC product data corpus <ref type="bibr" target="#b38">[39]</ref>. Ditto achieves new SOTA results on all these datasets and outperforms the previous best results by up to 31% in F1 score. The results show that Ditto is more robust to dirty data and performs well when the training set is small. Ditto is also more label-efficient as it achieves the previous SOTA results using only 1/2 or less of the training data across multiple subsets of the WDC corpus. Our ablation analysis shows that (1) using pretrained LMs contributes to over 50% of Ditto's performance gain and (2) all 3 optimizations, domain knowledge (DK), summarization (SU) and data augmentation (DA), are effective. For example, SU improves the performance on a text-heavy dataset by 52%, DK leads to 1.2% average improvement on the ER-Magellan datasets and DA improves on the WDC datasets by 2.53% on average. In addition, we show in Appendix B that although Ditto leverages deeper neural nets, its training and prediction time is comparable to the SOTA EM systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark datasets</head><p>We experimented with all the 13 publicly available datasets used for evaluating DeepMatcher <ref type="bibr" target="#b33">[34]</ref>. These datasets are from the ER Benchmark datasets <ref type="bibr" target="#b25">[26]</ref> and the Magellan data repository <ref type="bibr" target="#b11">[12]</ref>. We summarize the datasets in <ref type="table" target="#tab_1">Table 3</ref> and refer to them as ER-Magellan. These datasets are for training and evaluating matching models for various domains including products, publications, and businesses. Each dataset consists of candidate pairs from two structured tables of entity records of the same schema. The pairs are sampled from the results of blocking and manually labeled. The positive rate (i.e., the ratio of matched pairs) ranges from 9.4% (Walmart-Amazon) to 25% (Company). The number of attributes ranges from 1 to 8.</p><p>Among the datasets, the Abt-Buy and Company datasets are text-heavy meaning that at least one attributes contain long text. Also, following <ref type="bibr" target="#b33">[34]</ref>, we use the dirty version of the DBLP-ACM, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets Domains</head><p>Amazon-Google, Walmart-Amazon * software / electronics Abt-Buy ? , Beer product DBLP-ACM*, DBLP-Scholar*, iTunes-Amazon* citation / music Company ? , Fodors-Zagats company / restaurant DBLP-Scholar, iTunes-Amazon, and Walmart-Amazon datasets to measure the robustness of the models against noise. These datasets are generated from the clean version by randomly emptying attributes and appending their values to another randomly selected attribute.</p><p>Each dataset is split into the training, validation, and test sets using the ratio of 3:1:1. The same split of the datasets is also used in the evaluation of other EM solutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. We list the size of each dataset in <ref type="table" target="#tab_1">Table 5</ref>.</p><p>The WDC product data corpus <ref type="bibr" target="#b38">[39]</ref> contains 26 million product offers and descriptions collected from e-commerce websites <ref type="bibr" target="#b55">[56]</ref>. The goal is to find product offer pairs that refer to the same product.</p><p>To evaluate the accuracy of product matchers, the dataset provides 4,400 manually created golden labels of offer pairs from 4 categories: computers, cameras, watches, and shoes. Each category has a fixed number of 300 positive and 800 negative pairs. For training, the dataset provides for each category pairs that share the same product ID such as GTINs or MPNs mined from the product's webpage. The negative examples are created by selecting pairs that have high textual similarity but different IDs. These labels are further reduced to different sizes to test the models' label efficiency. We summarize the different subsets in <ref type="table" target="#tab_1">Table 4</ref>. We refer to these subsets as the WDC datasets. Each entry in this dataset has 4 attributes: title, description, brand, and specTable. Following the setting in <ref type="bibr" target="#b38">[39]</ref> for DeepMatcher, we allow Ditto to use any subsets of attributes to determine the best combination. We found in our experiments that Ditto achieves the best performance when it uses only the title attribute. We provide further justification of this choice in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and experimental setup</head><p>We implemented Ditto in PyTorch <ref type="bibr" target="#b35">[36]</ref> and the Transformers library <ref type="bibr" target="#b57">[58]</ref>. We currently support 4 pre-trained models: Distil-BERT <ref type="bibr" target="#b44">[45]</ref>, BERT <ref type="bibr" target="#b12">[13]</ref>, RoBERTa <ref type="bibr" target="#b28">[29]</ref>, and XLNet <ref type="bibr" target="#b60">[61]</ref>. We use the base uncased variant of each model in all our experiments. We further apply the half-precision floating-point (fp16) optimization to accelerate the training and prediction speed. In all the experiments, we fix the max sequence length to be 256 and the learning rate to be 3e-5 with a linearly decreasing learning rate schedule. The batch size is 32 if MixDA is used and 64 otherwise. The training process runs a fixed number of epochs (10, 15, or 40 depending on the dataset size) and returns the checkpoint with the highest F1 score on the validation set. We conducted all experiments on a p3.8xlarge AWS EC2 machine with 4 V100 GPUs (1 GPU per run).</p><p>Compared methods. We compare Ditto with the SOTA EM solution DeepMatcher. We also consider other baseline methods including Magellan <ref type="bibr" target="#b24">[25]</ref>, DeepER <ref type="bibr" target="#b13">[14]</ref>, and follow-up works of DeepMatcher <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. We also compare with variants of Ditto without the data augmentation (DA) and/or domain knowledge (DK) optimization to evaluate the effectiveness of each component. We summarize these methods below. We report the average F1 of 5 repeated runs in all the settings.</p><p>? DeepMatcher: DeepMatcher <ref type="bibr" target="#b33">[34]</ref> is the SOTA matching solution. Compared to Ditto, DeepMatcher customizes the RNN architecture to aggregate the attribute values, then compares/aligns the aggregated representations of the attributes. DeepMatcher leverages FastText <ref type="bibr" target="#b4">[5]</ref> to train the word embeddings. When reporting DeepMatcher's F1 scores, we use the numbers in <ref type="bibr" target="#b33">[34]</ref> for the ER-Magellan datasets and numbers in <ref type="bibr" target="#b38">[39]</ref> for the WDC datasets. We also reproduced those results using the open-sourced implementation. ? DeepMatcher+: Follow-up work <ref type="bibr" target="#b22">[23]</ref> slightly outperforms Deep-Matcher in the DBLP-ACM dataset and <ref type="bibr" target="#b16">[17]</ref> achieves better F1 in the Walmart-Amazon and Amazon-Google datasets. According to <ref type="bibr" target="#b33">[34]</ref>, the Magellan system ( <ref type="bibr" target="#b24">[25]</ref>, based on classical ML models) outperforms DeepMatcher in the Beer and iTunes-Amazon datasets. We also implemented and ran DeepER <ref type="bibr" target="#b13">[14]</ref>, which is another RNN-based EM solution. We denote by DeepMatcher+ (or simply DM+) the best F1 scores among DeepMatcher and these works aforementioned. We summarize in Appendix C the implementation details and performance of each method. ? Ditto: This is the full version of our system with all 3 optimizations, domain knowledge (DK), TF-IDF summarization (SU), and data augmentation (DA) turned on. See the details below. ? Ditto(DA): This version only turns on the DA (with MixDA) and SU but does not have the DK optimization. We apply one of the span-level or attribute-level DA operators listed in <ref type="table" target="#tab_1">Table 2</ref> with the entry_swap operator. We compare the different combinations and report the best one. Following <ref type="bibr" target="#b30">[31]</ref>, we apply MixDA with the interpolation parameter ? sampled from a Beta distribution Beta(0.8, 0.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Ditto(DK): With only the DK and SU optimizations on, this</head><p>version of Ditto is expected to have lower F1 scores but train much faster. We apply the span-typing to datasets of each domain according to <ref type="table" target="#tab_1">Table 1</ref> and apply the span-normalization on the number spans. ? Baseline: This base form of Ditto corresponds simply to finetuning a pre-trained LM on the EM task. We did not apply any optimizations on the baseline. For each ER-Magellan dataset, we tune the LM for the baseline and found that RoBERTa generally achieves the best performance. Thus, we use RoBERTa in the other 3 Ditto variants (Ditto, Ditto(DA), and Ditto(DK)) by default across all datasets. The Company dataset is the only exception, where we found that the BERT model performs the best. For the WDC benchmark, since the training sets are large, we use DistilBERT across all settings for faster training. There is a concurrent work <ref type="bibr" target="#b5">[6]</ref>, which also applies pre-trained LM to the entity matching problem. The proposed method is similar to the baseline method above, but due to the difference in the evaluation methods ( <ref type="bibr" target="#b5">[6]</ref> reports the best epoch on the test set, instead of the validation set), the reported results in <ref type="bibr" target="#b5">[6]</ref> is not directly comparable. We summarize in Appendix E the difference between Ditto and <ref type="bibr" target="#b5">[6]</ref> and explain why the reported results are different. <ref type="table" target="#tab_1">Table 5</ref> shows the results of the ER-Magellan datasets. Overall, Ditto (with optimizations) achieves significantly higher F1 scores than the SOTA results (DM+). Ditto without optimizations (i.e., the baseline) achieves comparable results with DM+. Ditto outperforms DM+ in all 13 cases and by up to 31% (Dirty, Walmart-Amazon) while the baseline outperforms DM+ in 12/13 cases except for the Company dataset with long text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main results</head><p>In addition, we found that Ditto is better at datasets with small training sets. Particularly, the average improvement on the 7 smallest datasets is 15.6% vs. 1.48% on average on the rest of datasets. Ditto is also more robust against data noise than DM+. In the 4 dirty datasets, the performance degradation of Ditto is only 0.57 on average while the performance of DM+ degrades by 8.21. These two properties make Ditto more attractive in practical EM settings.</p><p>Moreover, in Appendix D, we show an evaluation of Ditto's label efficiency on 5 of the ER-Magellan medium-size datasets. In 4/5 cases, when trained on less than 20% of the original training data, Ditto is able to achieve close or even better performance than DM+ when the full training sets are in use.</p><p>Ditto also achieves promising results on the WDC datasets ( <ref type="table" target="#tab_1">Table 6</ref>). Ditto achieves the highest F1 score of 94.08 when using all the 215k training data, outperforming the previous best result by <ref type="bibr">3.92</ref>. Similar to what we found in the ER-Magellan datasets, the improvements are higher on settings with fewer training examples (to the right of <ref type="table" target="#tab_1">Table 6</ref>). The results also show that Ditto is more label efficient than DeepMatcher. For example, when using only 1/2 of the data (Large), Ditto already outperforms DeepMatcher with all the training data (xLarge) by 2.89 in All. When using only 1/8 of the data (Medium), the performance is within 1% close to DeepMatcher's F1 when 1/2 of the data (Large) is in use. The only exception is the shoes category. This may be caused by the large gap of the positive label ratios between the training set and the test set (9.76% vs. 27.27% according to <ref type="table" target="#tab_1">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Next, we analyze the effectiveness of each component (i.e., LM, SU, DK, and DA) by comparing Ditto with its variants without these optimizations. The results are shown in <ref type="table" target="#tab_1">Table 5</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>The use of a pre-trained LM contributes to a large portion of the performance gain. In the ER-Magellan datasets (excluding Company), the average improvement of the baseline compared to Deep-Matcher+ is 7.75, which accounts for 78.5% of the improvement of   We expect DK to be more effective if we use an NER model trained on the product domain. DA is effective on both datasets and more significantly on the WDC datasets. The average F1 score of the full Ditto improves upon Ditto(DK) (without DA) by 1.39 and 2.53 respectively in the two datasets. In the WDC datasets, we found that the span_del operator always performs the best while the best operators are diverse in the ER-Magellan datasets. We list the best operator for each dataset in <ref type="table" target="#tab_1">Table 7</ref>. We note that there is a large space of tuning these operators (e.g., the MixDA interpolation parameter, maximal span length, etc.) and new operators to further improve the performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY: EMPLOYER MATCHING</head><p>We present a case of applying Ditto to a real-world EM task. An online recruiting platform would like to join its internal employer records with newly collected public records to enable downstream aggregation tasks. Given two tables A and B (internal and public) of employer records, the goal is to find, for each record in table B, a record in table A that represents the same employer. Both tables have 6 attributes: name, addr, city, state, zipcode, and phone. Our goal is to find matches with both high precision and recall.</p><p>Basic blocking. Our first challenge is size of the datasets. <ref type="table" target="#tab_1">Table  8</ref> shows that both tables are of nontrivial sizes even after deduplication. The first blocking method we designed is to only match companies with the same zipcode. However, since 60% of records in <ref type="table" target="#tab_1">Table A</ref> do not have the zipcode attribute and some large employers have multiple sites, we use a second blocking method that returns for each record in <ref type="table" target="#tab_1">Table B</ref> the top-20 most similar records in A ranked by the TF-IDF cosine similarity of name and addr attributes. We use the union of these two methods as our blocker, which produces 10 million candidate pairs.</p><p>Data labeling. We labeled 10,000 pairs sampled from the results of each blocking method (20,000 labels in total). We sampled pairs of high similarity with higher probability to increase the difficulty of the dataset to train more robust models. The positive rate of all the labeled pairs is 39%. We split the labeled pairs into training, validation, and test sets by the ratio of 3:1:1.</p><p>Applying Ditto. The user of Ditto does not need to extensively tune the hyperparameters but only needs to specify the domain knowledge and choose a data augmentation operator. We observe that the street number and the phone number are both useful signals for matching. Thus, we implemented a simple recognizer that tags the first number string in the addr attribute and the last 4 digits of the phone attribute. Since we would like the trained model to be robust against the large number of missing values, we choose the attr_del operator for data augmentation. We plot the model's performance in <ref type="figure" target="#fig_6">Figure 5</ref>. Ditto achieves the highest F1 score of 96.53 when using all the training data. Ditto outperforms DeepMatcher (DM) in F1 and trains faster (even when using MixDA) than DeepMatcher across different training set sizes.  Advanced blocking. Optionally, before applying the trained model to all the candidate pairs, we can use the labeled data to improve the basic blocking method. We leverage Sentence-BERT <ref type="bibr" target="#b42">[43]</ref>, a variant of the BERT model that trains sentence embeddings for sentence similarity search. The trained model generates a high-dimensional (e.g., 768 for BERT) vector for each record. Although this model has a relatively low F1 (only 92%) thus cannot replace Ditto, we can use it with vector similarity search to quickly find record pairs that are likely to match. We can greatly reduce the matching time by only testing those pairs of high cosine similarity. We list the running time for each module in <ref type="table" target="#tab_1">Table 9</ref>. With this technique, the overall EM process is accelerated by 3.8x (1.69 hours vs. 6.49 hours with/without advanced blocking). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK AND DISCUSSION</head><p>EM solutions have tackled the blocking problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref> and the matching problem with rules <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>, crowdsourcing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52]</ref>, or machine learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46]</ref>. Recently, EM solutions used deep learning and achieved promising results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b63">64]</ref>. DeepER <ref type="bibr" target="#b13">[14]</ref> trains EM models based on the LSTM <ref type="bibr" target="#b20">[21]</ref> neural network architecture with word embeddings such as GloVe <ref type="bibr" target="#b37">[38]</ref>. DeepER also proposed a blocking technique to represent each entry by the LSTM's output. Our advanced blocking technique based on Sentence-BERT <ref type="bibr" target="#b42">[43]</ref>, described in Section 5, is inspired by this. Auto-EM <ref type="bibr" target="#b63">[64]</ref> improves deep learningbased EM models by pre-training the EM model on an auxiliary task of entity type detection. Ditto also leverages transfer learning by fine-tuning pre-trained LMs, which are more powerful in language understanding. We did not compare Ditto with Auto-EM in experiments because the entity types required by Auto-EM are not available in our benchmarks. However, we expect that pre-training Ditto with EM-specific data/tasks can improve the performance of Ditto further and is part of our future work. DeepMatcher introduced a design space for applying deep learning to EM. Following their template architecture, one can think of Ditto as replacing both the attribute embedding and similarity representation components in the architecture with a single pre-trained LM such as BERT, thus providing a much simpler overall architecture.</p><p>All systems, Auto-EM, DeepER, DeepMatcher, and Ditto formulate matching as a binary classification problem. The first three take a pair of data entries of the same arity as input and aligns the attributes before passing them to the system for matching. On the other hand, Ditto serializes both data entries as one input with structural tags intact. This way, data entries of different schemas can be uniformly ingested, including hierarchically formatted data such as those in JSON. Our serialization scheme is not only applicable to Ditto, but also to other systems such as DeepMatcher. In fact, we serialized data entries to DeepMatcher under one attribute using our scheme and observed that DeepMatcher improved by as much as 5.2% on some datasets.</p><p>A concurrent work <ref type="bibr" target="#b5">[6]</ref> also applies pre-trained LMs to the entity matching problem and achieves good performance. While the proposed method in <ref type="bibr" target="#b5">[6]</ref> is similar to the baseline version of Ditto, Ditto can be further optimized using domain knowledge, data augmentation, and summarization. We also present a comprehensive experiment analysis on more EM benchmarks using a more standard evaluation method. We provide a detailed comparison between Ditto and <ref type="bibr" target="#b5">[6]</ref> in Appendix E.</p><p>External knowledge is known to be effective in improving neural network models in NLP tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b59">60]</ref>. To incorporate domain knowledge, Ditto modularizes the way domain knowledge is incorporated by allowing users to specify and customize rules for preprocessing input entries. Data augmentation (DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59]</ref>. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type="bibr" target="#b30">[31]</ref>, a recently proposed DA strategy based on convex interpolation. To our knowledge, this is the first time data augmentation has been applied to EM.</p><p>Active learning is a recent trend in EM to train high-quality matching models with limited labeling resources <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>. Under the active learning framework, the developer interactively labels a small set of examples to improve the model while the updated model is used to sample new examples for the next labeling step. Although active learning's goal of improving label efficiency aligns with data augmentation in Ditto, they are different solutions, which can be used together; active learning requires human interaction in each iteration, whereas data augmentation does not. According to <ref type="bibr" target="#b29">[30]</ref>, one needs to adjust the model size and/or the training process such that the response time becomes acceptable for user interaction in active learning. Thus, we consider applying it to Ditto is not straightforward because of the relatively long fine-tuning time of the Ditto. We leave this aspect to future development of Ditto.</p><p>Discussion. Like other deep learning-based EM solutions, Ditto requires a non-trivial amount of labeled training examples (e.g., the case study requires 6k examples to achieve 95% F1) and Ditto's DA and DK optimizations help reduce the labeling requirement to some extent. Currently, the LMs that we have tested in Ditto are pre-trained on general English text corpora and thus might not capture well EM tasks with a lot of numeric data and/or specific domains such as the scientific domain. For domain-specific tasks, a potential solution is to leverage specialized LMs such as SciB-ERT <ref type="bibr" target="#b2">[3]</ref> or BioBERT <ref type="bibr" target="#b26">[27]</ref> trained on scientific and biology corpus respectively. For numeric data, a good candidate solution would be a hybrid neural network similar to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b61">62]</ref> that combines the numeric features with the textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present Ditto, an EM system based on fine-tuning pre-trained Transformer-based language models. Ditto uses a simple architecture to leverage pre-trained LMs and is further optimized by injecting domain knowledge, text summarization, and data augmentation. Our results show that it outperforms existing EM solutions on all three benchmark datasets with significantly less training data. Ditto's good performance can be attributed to the improved language understanding capability mainly through pre-trained LMs, the more accurate text alignment guided by the injected knowledge, and the data invariance properties learned from the augmented data. We plan to further explore our design choices for injecting domain knowledge, text summarization, and data augmentation. In addition, we plan to extend Ditto to other data integration tasks beyond EM, such as entity type detection and schema matching with the ultimate goal of building a BERT-like model for tables. <ref type="figure">Figure 6</ref> shows the model architecture of Ditto's language models such as BERT <ref type="bibr" target="#b12">[13]</ref>, DistilBERT <ref type="bibr" target="#b44">[45]</ref>, and RoBERTa <ref type="bibr" target="#b28">[29]</ref>. Ditto serializes the two input entries entries as one sequence and feeds it to the model as input. The model consists of (1) token embeddings and Transformer layers <ref type="bibr" target="#b57">[58]</ref> from a pre-trained language model (e.g., BERT) and (2) task-specific layers (linear followed by softmax). Conceptually, the [CLS] token "summarizes" all the contextual information needed for matching as a contextualized embedding vector E ? [CLS] which the task-specific layers take as input for classification.  <ref type="figure">Figure 6</ref>: Ditto's model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ARCHITECTURE OF THE PRE-TRAINED LANGUAGE MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TRAINING TIME AND PREDICTION TIME EXPERIMENTS</head><p>We plot the training time required by DeepMatcher and Ditto in <ref type="figure">Figure 7</ref>.  <ref type="table" target="#tab_1">Table 11</ref> shows Ditto's average prediction time per entry pair in each benchmark. The results show that DeepMatcher and Ditto have comparable prediction time. Also, the DK optimization only adds a small overhead to the prediction time (less than 2%). The prediction time between the two benchmarks are different because of the difference in their sequence length distributions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C BREAKDOWN OF THE DM+ RESULTS AND EXPERIMENTS</head><p>In this section, we provide a detailed summary of how we obtain the DeepMatcher+ (DM+) baseline results. Recall from Section 4.2 that DM+ is obtained by taking the best performance (highest F1 scores) of multiple baseline methods including DeepER <ref type="bibr" target="#b13">[14]</ref>, Magellan <ref type="bibr" target="#b24">[25]</ref>, DeepMatcher <ref type="bibr" target="#b33">[34]</ref>, and DeepMatcher's follow-up work <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b22">[23]</ref>.</p><p>We summarize these baseline results in <ref type="table" target="#tab_1">Table 10</ref> on the ER-Magellan benchmarks and explain each method next.</p><p>DeepER: The original paper <ref type="bibr" target="#b13">[14]</ref> proposes a DL-based framework for EM. Similar to DeepMatcher, DeepER first aggregates both data entries into their vector representations and uses a feedforward neural network to perform the binary classification based on the similarity of the two vectors. Each vector representation is obtained either by a simple averaging over the GloVe <ref type="bibr" target="#b37">[38]</ref> embeddings per attribute or a RNN module over the serialized data entry. DeepER computes the similarity as the cosine similarity of the two vectors. Although <ref type="bibr" target="#b13">[14]</ref> reported results on the Walmart-Amazon, Amazon-Google, DBLP-ACM, DBLP-Scholar, and the Fodors-Zagat datasets, the numbers are not directly comparable to the presented results of Ditto because their evaluation and data preparation methods are different (e.g., they used k-fold cross-validation while we use the train/valid/test splits according to <ref type="bibr" target="#b33">[34]</ref>). In our experiments, we implemented DeepER with LSTM as the RNN module and GloVe for the tokens embeddings as described in <ref type="bibr" target="#b13">[14]</ref> and with the same hyperparameters (a learning rate of 0.01 and the Adam optimizer <ref type="bibr" target="#b23">[24]</ref>). We then evaluate DeepER in our evaluation settings. For each dataset, we report the best results obtained by the simple aggregation and the RNN-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepMatcher (DM):</head><p>We have summarized DM in Section 4.2. In addition to simply taking the numbers from the original paper <ref type="bibr" target="#b33">[34]</ref>, we also ran their open-source version (DM (reproduced)) with the default settings (the Hybrid model with a batch size of 32 and 15 epochs). The reproduced results are in general lower than the original reported numbers in <ref type="bibr" target="#b33">[34]</ref> (the 3rd column) because we did not try the other model variants and hyperparameters as in the original experiments. The code failed in the Fodors-Zagat and the Company datasets because of out-of-memory errors.</p><p>In addition, one key difference between DM and Ditto is that Ditto serializes the data entries while DM does not. One might wonder if DM can obtain better results by simply replacing its input with the serialized entries produced by Ditto. We found that the results do not significantly improved overall, but it is up to 5.2% in the Abt-Buy dataset.</p><p>Others: We obtained the results for Magellan by taking the reported results from <ref type="bibr" target="#b33">[34]</ref> and the two follow-up works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> of DeepMatcher (denoted as ACL '19 and IJCAI '19 in <ref type="table" target="#tab_1">Table 10</ref>). We did not repeat the experiments since they have the same evaluation settings as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D LABEL EFFICIENCY EXPERIMENTS ON THE ER-MAGELLAN BENCHMARK</head><p>We also evaluate the label efficiency of Ditto on the ER-Magellan benchmark. We conducted the experiments on 5 representative datasets (Amazon-Google, DBLP-ACM, DBLP-Scholar, Walmart-Amazon, and Abt-Buy) of size ?10k to ?30k. For each dataset, we vary the training set size from 500 to 2,000 and uniformly sample from the original training set. We then follow the same setting as in Section 4 to evaluate the 4 variants of Ditto: baseline, Ditto(DA), Ditto(DK), and Ditto. We summarize the results in <ref type="figure">Figure 8</ref>. We also plot the result of DM+ trained on the full datasets (denoted as DM+ (full)) as a reference. As shown in <ref type="figure">Figure 8</ref>, Ditto is able to reach similar or better performance to DM+ on 3 of the datasets (Amazon-Google, DBLP-ACM, and Walmart-Amazon) with 2,000 train examples (so ? 20%). With only 500 examples, Ditto is able to outperform DM+ trained on the full data in the Abt-Buy dataset. These results confirm that Ditto is more label efficient than existing EM solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E THE DIFFERENCE BETWEEN DITTO AND A CONCURRENT WORK</head><p>There is a concurrent work <ref type="bibr" target="#b5">[6]</ref> which also applies pre-trained LMs to entity matching and obtained good results. The method proposed in <ref type="bibr" target="#b5">[6]</ref> is essentially identical to the baseline version of Ditto which only serializes the data entries into text sequences and fine-tunes the LM on the binary sequence-pair classification task. On top of that, Ditto also applies 3 optimizations of injecting domain knowledge, data augmentation, and summarization to further improve the model's performance. We also evaluate Ditto more comprehensively as we tested Ditto on all the 13 ER-Magellan datasets, the WDC product benchmark, and a company matching dataset while <ref type="bibr" target="#b5">[6]</ref> experimented in 5/13 of the ER-Magellan datasets. On these 5 evaluated datasets, one might notice that the reported F1 scores in <ref type="bibr" target="#b5">[6]</ref> are slightly higher compared to the baseline's F1 scores shown in <ref type="table" target="#tab_1">Table 5</ref>. The reason is that according to <ref type="bibr" target="#b5">[6]</ref>, for each run on each dataset, the F1 score is computed as the model's best F1 scores on the test set among all the training epochs, while we report the test F1 score of the epoch with the best F1 on the validation set. Our evaluation method is more standard since it prevents overfitting the test set (See Chapter 4.6.5 of <ref type="bibr" target="#b32">[33]</ref>) and is also used by DeepMatcher and Magellan <ref type="bibr" target="#b33">[34]</ref>. It is not difficult to see that over the same set of model snapshots, the F1 score computed by the <ref type="bibr" target="#b5">[6]</ref>'s evaluation method would be greater or equal to the F1 score computed using our method, which explains the differences in the reported values between us and <ref type="bibr" target="#b5">[6]</ref>. <ref type="table" target="#tab_1">Table 12</ref> summarizes the detailed comparison of the baseline Ditto, the proposed method in <ref type="bibr" target="#b5">[6]</ref>, and the full Ditto. Recall that we construct the baseline by taking the best performing pre-trained model among DistilBERT <ref type="bibr" target="#b44">[45]</ref>, BERT <ref type="bibr" target="#b12">[13]</ref>, XLNet <ref type="bibr" target="#b60">[61]</ref>, and RoBERTa <ref type="bibr" target="#b28">[29]</ref> following <ref type="bibr" target="#b5">[6]</ref>. Although the baseline Ditto does not outperform <ref type="bibr" target="#b5">[6]</ref> because of the different evaluation method, the optimized Ditto is able to outperform <ref type="bibr" target="#b5">[6]</ref> in 4/5 of the evaluated datasets. <ref type="table" target="#tab_1">Table 12</ref>: The F1 scores of the baseline method with different pretrained LMs. The first 4 columns are performance of the baseline Ditto using the 4 different LMs. We highlight the LM of the best performance on each dataset, which form the baseline column in <ref type="table" target="#tab_1">Table 5</ref>. We turned on the summarization (SU) optimization for the Company dataset to get F1 scores closer to the full Ditto.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F EXPERIMENTS ON DIFFERENT WDC PRODUCT ATTRIBUTES</head><p>Following the settings in <ref type="bibr" target="#b38">[39]</ref> for the evaluated models, we evaluate Ditto on 4 different subsets of the product attributes as input so that Ditto and DeepMatcher are evaluated under the same setting. We list the 4 attributes in <ref type="table" target="#tab_1">Table 13</ref>. Note that except for title, the attributes can be missing the the data entries. For example, the SpecTable attribute only appears in 7% of the entries in the full training set. We summarize the results in <ref type="table" target="#tab_1">Table 14</ref>. Among all the tested combinations (the same as the ones tested for DeepMatcher in <ref type="bibr" target="#b38">[39]</ref>), the combination consisting of only the title attribute works significantly better than the others. The difference ranges from 3.2% (computer, xlarge) to over 30% (watches, small). According to this result, we only report Ditto's results on the title attribute while allowing DeepMatcher to access all the 4 attributes to ensure its best performance.</p><p>The performance of Ditto drops when more attributes are added is because of the sequence length. For example, for the combination title+description, we found that the average sequence length grows from 75.5 (title only) to 342.7 which is beyond our default max length of 256 tokens. As a results, some useful information from the title attributes is removed by the summarization operator. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An EM system architecture with Ditto as the matcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>illustrates), (1) Randomly sample ? from a Beta distribution ? ? Beta(?, ?) with a hyper-parameter ? ? [0, 1] (e.g., 0.8 in our experiments); (2) Denote by LM(s) the LM representation of a sequence s. Let LM(s ?? ) = ? ? LM(s) + (1 ? ?) ? LM(augment(s, o)). Namely, LM(s ?? ) is the convex interpolation between the LM outputs of s and the augmented s ? = augment(s, o); (3) Train the model by feeding LM(s ?? ) to the rest of the network and back-propagate. Back-propagation updates both the LM and linear layer's parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Data augmentation with MixDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>F1 scores on the WDC datasets of different versions of Ditto. DM: DeepMatcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>mance. The suffix (S)/(D) and (Both) denote the clean/dirty version of the dataset or both of them. All operators are applied with the entry_swap operator. Operator Datasets span_shuffle DBLP-ACM (Both), DBLP-Google (Both), Abt-Buy span_del Walmart-Amazon(D), Company, all of WDC attr_del Beer, iTunes-Amazon(S), Walmart-Amazon(S) attr_shuffle Fodors-Zagats, iTunes-Amazon(D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>F1 and training time for the employer matching models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Entity Matching: determine the matching entries from two datasets.</figDesc><table><row><cell></cell><cell>title instant immersion spanish deluxe 2.0 adventure workshop 4th-6th grade 7th edition sharp printing calculator</cell><cell cols="2">manf./modelno topics entertainment encore software sharp el1192bl</cell><cell>price 49.99 19.99 37.63</cell><cell>? ? ?</cell><cell>title instant immers spanish dlux 2 encore inc adventure workshop 4th-6th grade 8th edition new-sharp shr-el1192bl two-color printing calculator 12-digit lcd black red</cell><cell>manf./modelno price NULL 36.11 NULL 17.1 NULL 56.0</cell></row><row><cell>Table A:</cell><cell cols="2">Figure 1: Blocker</cell><cell>Matcher</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table B</head><label>B</label><figDesc></figDesc><table><row><cell>Candidate Pairs</cell><cell></cell><cell></cell><cell cols="2">Ditto</cell><cell>Matched Pairs</cell></row><row><cell></cell><cell>Serialize</cell><cell></cell><cell></cell><cell></cell></row><row><cell>:</cell><cell>Inject DK</cell><cell>?</cell><cell>Train</cell><cell></cell></row><row><cell>Sample &amp; Label</cell><cell>Summarize</cell><cell>?</cell><cell>Augment</cell><cell>?</cell></row><row><cell cols="2">Train Advanced Blocking</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>serialize(e) ::= [COL] attr 1 [VAL] val 1 . . . [COL] attr k [VAL] val k , where [COL] and [VAL] are special tokens for indicating the start of attribute names and values respectively. For example,the first entry of the second table is serialized as: To serialize a candidate pair (e, e ? ), we let serialize(e, e ? ) ::= [CLS] serialize(e) [SEP] serialize(e ? ) [SEP],</figDesc><table><row><cell>[COL] title [VAL] instant immers spanish dlux 2 [COL] manf./modelno</cell></row><row><cell>[VAL] NULL [COL] price [VAL] 36.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Main span types for matching entities in our benchmark datasets.</figDesc><table><row><cell>Entity Type</cell><cell>Types of Important Spans</cell></row><row><cell cols="2">Publications, Movies, Music Persons (e.g., Authors), Year, Publisher</cell></row><row><cell>Organizations, Employers</cell><cell>Last 4-digit of phone, Street number</cell></row><row><cell>Products</cell><cell>Product ID, Brand, Configurations (num.)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Data augmentation operators in Ditto. The operators are 3 different levels: span-level, attribute-level, and entry-level. All samplings are done uniformly at random. of the two data entries e and e ? These two operators are motivated by the observation that making a match/no-match decision can sometimes be "too easy" when the candidate pair of data entries contain multiple spans of text supporting the decision. For example, suppose our negative examples for matching company data in the existing training data is similar to what is shown below. [CLS] . . . [VAL] Google LLC . . . [VAL] (866) 246-6453 [SEP] . . .</figDesc><table><row><cell>Operator</cell><cell>Explanation</cell></row><row><cell>span_del</cell><cell>Delete a randomly sampled span of tokens</cell></row><row><cell cols="2">span_shuffle Randomly sample a span and shuffle the tokens' order</cell></row><row><cell>attr_del</cell><cell>Delete a randomly chosen attribute and its value</cell></row><row><cell>attr_shuffle</cell><cell>Randomly shuffle the orders of all attributes</cell></row><row><cell>entry_swap</cell><cell>Swap the order</cell></row></table><note>[VAL] Alphabet inc . . . [VAL] (650) 253-0000 [SEP]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The 13 datasets divided into 4 categories of domains. The datasets marked with ? are text-heavy (Textual). Each dataset with * has an additional dirty version to test the models' robustness against noisy data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Different subsets of the WDC product data corpus. Each subset (except Test) is split into a training set and a validation set with a ratio of 4:1 according to the dataset provider<ref type="bibr" target="#b38">[39]</ref>. The last column shows the positive rate (%POS) of each category in the xLarge set. The positive rate on the test set is 27.27% for all the categories.</figDesc><table><row><cell cols="6">Categories Test Small Medium Large xLarge %POS</cell></row><row><cell cols="2">Computers 1,100 2,834</cell><cell>8,094</cell><cell>33,359</cell><cell cols="2">68,461 14.15%</cell></row><row><cell>Cameras</cell><cell>1,100 1,886</cell><cell>5,255</cell><cell>20,036</cell><cell cols="2">42,277 16.98%</cell></row><row><cell>Watches</cell><cell>1,100 2,255</cell><cell>6,413</cell><cell>27,027</cell><cell cols="2">61,569 15.05%</cell></row><row><cell>Shoes</cell><cell>1,100 2,063</cell><cell>5,805</cell><cell>22,989</cell><cell>42,429</cell><cell>9.76%</cell></row><row><cell>All</cell><cell>4,400 9,038</cell><cell>25,567</cell><cell cols="3">103,411 214,736 14.10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>F1 scores on the ER-Magellan EM datasets. The numbers of DeepMatcher+ (DM+) are the highest available found in<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> or re-produced by us.</figDesc><table><row><cell>Datasets</cell><cell>DM+</cell><cell>Ditto</cell><cell>Ditto (DA)</cell><cell>Ditto (DK)</cell><cell cols="2">Baseline Size</cell></row><row><cell>Structured</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Amazon-Google 70.7 75.58 (+4.88) 75.08</cell><cell>74.67</cell><cell cols="2">74.10 11,460</cell></row><row><cell>Beer</cell><cell cols="3">78.8 94.37 (+15.57) 87.21</cell><cell>90.46</cell><cell>84.59</cell><cell>450</cell></row><row><cell>DBLP-ACM</cell><cell cols="3">98.45 98.99 (+0.54) 99.17</cell><cell>99.10</cell><cell cols="2">98.96 12,363</cell></row><row><cell>DBLP-Google</cell><cell>94.7</cell><cell>95.6 (+0.9)</cell><cell>95.73</cell><cell>95.80</cell><cell cols="2">95.84 28,707</cell></row><row><cell>Fodors-Zagats</cell><cell cols="4">100 100.00 (+0.0) 100.00 100.00</cell><cell>98.14</cell><cell>946</cell></row><row><cell cols="4">iTunes-Amazon 91.2 97.06 (+5.86) 97.40</cell><cell>97.80</cell><cell>92.28</cell><cell>539</cell></row><row><cell cols="4">Walmart-Amazon 73.6 86.76 (+13.16) 85.50</cell><cell>83.73</cell><cell cols="2">85.81 10,242</cell></row><row><cell>Dirty</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DBLP-ACM</cell><cell cols="3">98.1 99.03 (+0.93) 98.94</cell><cell>99.08</cell><cell cols="2">98.92 12,363</cell></row><row><cell>DBLP-Google</cell><cell cols="3">93.8 95.75 (+1.95) 95.47</cell><cell>95.57</cell><cell cols="2">95.44 28,707</cell></row><row><cell cols="4">iTunes-Amazon 79.4 95.65 (+16.25) 95.29</cell><cell>94.48</cell><cell>92.92</cell><cell>539</cell></row><row><cell cols="4">Walmart-Amazon 53.8 85.69 (+31.89) 85.49</cell><cell>80.67</cell><cell cols="2">82.56 10,242</cell></row><row><cell>Textual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Abt-Buy</cell><cell cols="3">62.8 89.33 (+26.53) 89.79</cell><cell>81.69</cell><cell>88.85</cell><cell>9,575</cell></row><row><cell>Company</cell><cell cols="3">92.7 93.85 (+1.15) 93.69</cell><cell>93.15</cell><cell cols="2">41.00 112,632</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>F1 scores on the WDC product matching datasets. The numbers for DeepMatcher (DM) are taken from<ref type="bibr" target="#b38">[39]</ref>. DistilBERT) are comparable on the Structured datasets, the baseline performs much better on all the Dirty datasets and the Abt-Buy dataset. This confirms our intuition that the language understanding capability is a key advantage of Ditto over existing EM solutions. The Company dataset is a special case because the length of the company articles (3,123 words on average) is much greater than the max sequence length of 256. The SU optimization increases the F1 score of this dataset from 41% to over 93%. In the WDC datasets, across the 20 settings, LM contributes to 3.41 F1 improvement on average, which explains 55.3% of improvement of the full Ditto (6.16).The DK optimization is more effective on the ER-Magellan datasets. Compared to the baseline, the improvement of Ditto(DK) is 1.08 on average and is up to 5.88 on the Beer dataset while the improvement is only 0.22 on average on the WDC datasets. We inspected the span-typing output and found that only 66.2% of entry pairs have spans of the same type. This is caused by the current NER module not extracting product-related spans with the correct types.</figDesc><table><row><cell>Size</cell><cell>xLarge (1/1) Large (1/2) Medium (1/8) Small (1/20)</cell></row><row><cell>Methods</cell><cell>DM Ditto DM Ditto DM Ditto DM Ditto</cell></row><row><cell>Computers</cell><cell>90.80 95.45 89.55 91.70 77.82 88.62 70.55 80.76 +4.65 +2.15 +10.80 +10.21</cell></row><row><cell>Cameras</cell><cell>89.21 93.78 87.19 91.23 76.53 88.09 68.59 80.89 +4.57 +4.04 +11.56 +12.30</cell></row><row><cell>Watches</cell><cell>93.45 96.53 91.28 95.69 79.31 91.12 66.32 85.12 +3.08 +4.41 +11.81 +18.80</cell></row><row><cell>Shoes</cell><cell>92.61 90.11 90.39 88.07 79.48 82.66 73.86 75.89 -2.50 -2.32 +3.18 +2.03</cell></row><row><cell>All</cell><cell>90.16 94.08 89.24 93.05 79.94 88.61 76.34 84.36 +3.92 +3.81 +8.67 +8.02</cell></row><row><cell cols="2">the full Ditto (9.87). While DeepMatcher+ and the baseline Ditto</cell></row><row><cell cols="2">(essentially fine-tuning</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Datasets that each DA operator achieves the best perfor-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Sizes of the two employer datasets to be matched.</figDesc><table><row><cell></cell><cell>TableA</cell><cell></cell><cell>TableB</cell><cell>#Candidates</cell></row><row><cell cols="5">original deduplicated original deduplicated Basic blocking</cell></row><row><cell>Size 789,409</cell><cell>788,094</cell><cell>412,418</cell><cell>62,511</cell><cell>10,652,249</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Running time for blocking and matching with Ditto. Advanced blocking consists of two steps: computing the representation of each record with Sentence-BERT<ref type="bibr" target="#b42">[43]</ref> (Encoding) and similarity search by blocked matrix multiplication<ref type="bibr" target="#b0">[1]</ref> (Search). With advanced blocking, we only match each record with the top-10 most similar records according to the model.</figDesc><table><row><cell>Basic</cell><cell cols="2">Encoding Search</cell><cell cols="2">Matching</cell></row><row><cell>Blocking</cell><cell>(GPU)</cell><cell cols="2">(CPU) (top-10)</cell><cell>(ALL)</cell></row><row><cell>Time (s) 537.26</cell><cell cols="4">2,229.26 1,981.97 1,339.36 22,823.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Baseline results from different sources. Training time vs. dataset size for the ER-Megallan datasets (left) and the WDC datasets (right). Each point corresponds to the training time needed for a dataset using different methods. Ditto(DK) and Baseline do not use MixDA thus is faster than the full Ditto. The DK optimization only adds a small overhead (5%) to the training time. DeepMatcher (DM) ran out of memory on the Company dataset so the data point is not reported.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DeepER (reproduced)</cell><cell cols="2">DM (reproduced)</cell><cell>DM (reported in [34])</cell><cell>DM (using Ditto's input)</cell><cell>Magellan (reported in [34])</cell><cell>ACL '19 [23] IJCAI '19 [17]</cell></row><row><cell></cell><cell></cell><cell cols="2">Structured</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Amazon-Google</cell><cell>56.08</cell><cell></cell><cell>67.53</cell><cell>69.3</cell><cell>65.78</cell><cell>49.1</cell><cell>-</cell><cell>70.7</cell></row><row><cell></cell><cell></cell><cell>Beer</cell><cell></cell><cell>50</cell><cell></cell><cell>69.23</cell><cell>72.7</cell><cell>-</cell><cell>78.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">DBLP-ACM</cell><cell>97.63</cell><cell></cell><cell>98.42</cell><cell>98.4</cell><cell>98.86</cell><cell>98.4</cell><cell>98.45</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">DBLP-Scholar</cell><cell>90.82</cell><cell></cell><cell>94.32</cell><cell>94.7</cell><cell>94.56</cell><cell>92.3</cell><cell>92.94</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Fodors-Zagats</cell><cell>97.67</cell><cell></cell><cell>-</cell><cell>100</cell><cell>-</cell><cell>100</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">iTunes-Amazon</cell><cell>72.46</cell><cell></cell><cell>86.79</cell><cell>88</cell><cell>88</cell><cell>91.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Walmart-Amazon</cell><cell>50.62</cell><cell></cell><cell>63.33</cell><cell>66.9</cell><cell>61.67</cell><cell>71.9</cell><cell>-</cell><cell>73.6</cell></row><row><cell></cell><cell></cell><cell>Dirty</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DBLP-ACM</cell><cell>89.62</cell><cell></cell><cell>97.53</cell><cell>98.1</cell><cell>96.03</cell><cell>91.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">DBLP-Scholar</cell><cell>86.07</cell><cell></cell><cell>92.8</cell><cell>93.8</cell><cell>93.75</cell><cell>82.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">iTunes-Amazon</cell><cell>67.80</cell><cell></cell><cell>73.08</cell><cell>79.4</cell><cell>70.83</cell><cell>46.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Walmart-Amazon</cell><cell>36.44</cell><cell></cell><cell>47.81</cell><cell>53.8</cell><cell>48.45</cell><cell>37.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Textual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Abt-Buy</cell><cell>42.99</cell><cell></cell><cell>66.05</cell><cell>62.8</cell><cell>67.99</cell><cell>43.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Company</cell><cell>62.17</cell><cell></cell><cell>-</cell><cell>92.7</cell><cell>90.70</cell><cell>79.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Training time (s)</cell><cell>10 2 10 3 10 4</cell><cell>DM</cell><cell>Ditto</cell><cell>10 2 10 3 10 4 Ditto(DA)</cell><cell></cell><cell>Ditto(DK)</cell><cell>Baseline</cell></row><row><cell></cell><cell></cell><cell cols="3">1k 3k 10k 30k 100k Training set size</cell><cell>2k</cell><cell cols="2">10k Training set size 50k 200k</cell></row><row><cell cols="2">Figure 7:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>The average prediction time (ms) per data entry pair of</figDesc><table><row><cell>Ditto.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Ditto-DistilBERT Ditto-RoBERTa DM w. DK w/o. DK w. DK w/o. DK</cell></row><row><cell cols="2">ER-Magellan 8.01</cell><cell>7.87</cell><cell>6.82</cell><cell>6.78</cell><cell>6.62</cell></row><row><cell>WDC</cell><cell>1.82</cell><cell>1.80</cell><cell>2.11</cell><cell>2.11</cell><cell>2.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>F1 scores on 5 ER-Magellan datasets using different variants of Ditto. We also plot the score of DeepMatcher+ on the full datasets (denoted as DM+(full)) as reference. Recall that full = {11460, 12363, 28707, 10242, 9575} for the 5 datasets respectively.</figDesc><table><row><cell>F1 score</cell><cell>0.5k 1k 1.5k 2k full 50 60 70 80 Amazon-Google</cell><cell>0.5k 1k 1.5k 2k full 96 97 98 99 DBLP-ACM</cell><cell>0.5k 1k 1.5k 2k full Train Size 90 92 94 96 DBLP-Scholar</cell><cell>0.5k 1k 1.5k 2k full 50 60 70 80 90 Walmart-Amazon</cell><cell>0.5k 1k 1.5k 2k full 60 70 80 90 Abt-Buy</cell><cell>DM+ (full) Ditto Ditto (DA) Ditto (DK) Baseline</cell></row><row><cell cols="2">Figure 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>The 4 attributes of the WDC benchmarks used in trainingDitto and DM according to<ref type="bibr" target="#b38">[39]</ref>.</figDesc><table><row><cell cols="2">Attributes Examples</cell><cell>%Available</cell></row><row><cell>Title</cell><cell>Corsair Vengeance Red LED 16GB 2x 8GB</cell><cell>100%</cell></row><row><cell></cell><cell>DDR4 PC4 21300 2666Mhz dual-channel Kit -</cell><cell></cell></row><row><cell></cell><cell>CMU16GX4M2A2666C16R Novatech</cell><cell></cell></row><row><cell cols="2">Description DDR4 2666MHz C116, 1.2V, XMP 2.0 red-led, Life-</cell><cell>54%</cell></row><row><cell></cell><cell>time Warranty</cell><cell></cell></row><row><cell>Brand</cell><cell>AMD</cell><cell>19%</cell></row><row><cell cols="2">SpecTable Memory Type DDR4 (PC4-21300) Capacity 16GB</cell><cell>7%</cell></row><row><cell></cell><cell>(2 x 8GB) Tested Speed 2666MHz Tested Latency</cell><cell></cell></row><row><cell></cell><cell>16-18-18-35 Tested Voltage 1.20V Registered / Un-</cell><cell></cell></row><row><cell></cell><cell>buffered Unbuffered Error Checking Non-ECC</cell><cell></cell></row><row><cell></cell><cell>Memory Features -red-led XMP 2.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>F1 scores of Ditto on the WDC datasets with different subsets of the product attributes title title_description title_description_brand title_description_brand_specTable small medium large xlarge small medium large xlarge small medium large xlarge small medium large xlarge all 84.36 88.61 93.05 94.08 69.51 75.91 81.56 87.62 68.34 75.43 84.80 85.19 67.08 75.55 83.08 84.44 cameras 80.89 88.09 91.23 93.78 61.64 73.41 79.51 83.61 59.97 73.16 78.60 82.61 55.04 68.81 76.53 80.09 computers 80.76 88.62 91.70 95.45 66.56 75.60 87.39 92.26 65.15 73.55 86.05 90.36 60.82 66.90 84.25 88.45 shoes 75.89 82.66 88.07 90.10 59.57 69.25 76.33 76.27 57.43 71.57 77.07 77.39 56.57 71.02 76.58 75.63 watches 85.12 91.12 95.69 96.53 58.16 70.14 81.03 84.55 59.66 73.06 81.92 84.46 52.49 68.67 79.58 82.48</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In DeepMatcher, the requirement that both entries have the same schema can be removed by treating the values in all columns as one value under one attribute.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To Index or Not to Index: Optimizing Exact Maximum Inner Product Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Abuzaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE &apos;19. IEEE</title>
		<meeting>ICDE &apos;19. IEEE</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1250" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A comparison of fast blocking methods for record</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linkage Rohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m">SciBERT: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive duplicate detection using learnable string similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD &apos;03</title>
		<meeting>KDD &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entity matching with transformer architectures-a step forward in data integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursin</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Stockinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural natural language inference models enhanced with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL &apos;18</title>
		<meeting>ACL &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2406" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of indexing techniques for scalable record linkage and deduplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter Christen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1537" to="1555" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What Does BERT Look at? An Analysis of BERT&apos;s Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BlackBoxNLP &apos;19</title>
		<meeting>BlackBoxNLP &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to match and cluster large high-dimensional data sets for data integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD &apos;02</title>
		<meeting>KDD &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="475" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal hashing schemes for entity matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhor</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW &apos;13</title>
		<meeting>WWW &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="295" to="306" />
		</imprint>
	</monogr>
	<note>Anish Das Sarma, and Tamas Sarlos</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjib</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Chaitanya Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradap</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Govind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Paulsen</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/anhaidgroup/projects/data" />
		<title level="m">The Magellan Data Repository</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT &apos;19</title>
		<meeting>NAACL-HLT &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of tuples for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ebraheem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saravanan</forename><surname>Thirumuruganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mourad</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1454" to="1467" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NADEEF/ER: generic and interactive entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ihab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mourad</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge-Arnulfo</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Quian?-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD &apos;14</title>
		<meeting>SIGMOD &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1071" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A clusteringbased framework to control block sizes for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Christen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD &apos;15</title>
		<meeting>KDD &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end multi-perspective matching for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI &apos;19</title>
		<meeting>IJCAI &apos;19</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4961" to="4967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Corleone: Hands-off crowdsourcing for entity matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjib</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narasimhan</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><surname>Rampalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD &apos;14</title>
		<meeting>SIGMOD &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning-Based Methods with Human-in-the-Loop for Entity Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sairam</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2969" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe? Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02349</idno>
		<title level="m">TAPAS: Weakly Supervised Table Parsing via Pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Human-powered Sorts and Joins. PVLDB</title>
		<editor>Adam Marcus Eugene Wu David Karger and Samuel Madden Robert Miller</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-resource Deep Entity Resolution with Transfer and Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sairam</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL &apos;19</title>
		<meeting>ACL &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5851" to="5861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradap</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjib</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Ardalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">R</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemah</forename><surname>Panahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shishir</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rohit Deep, and Vijay Raghavendra. 2016. Magellan: Toward Building Entity Matching Management Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1197" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Evaluation of entity resolution approaches on real-world match problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>K?pcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Thor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="484" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00584</idno>
		<title level="m">Deep Entity Matching with Pre-Trained Language Models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prithviraj Sen, and Mohamed Sarwat. 2020. A Comprehensive Benchmark Framework for Active Learning Methods in Entity Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Venkata Vamsikrishna Meduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="1133" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Snippext: Semi-supervised Opinion Mining with Augmented Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW &apos;20</title>
		<meeting>WWW &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP &apos;04</title>
		<meeting>EMNLP &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>McGraw Hill</publisher>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="870" to="877" />
			<pubPlace>Burr Ridge, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning for entity matching: A design space exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidharth</forename><surname>Mudgal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngchoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD &apos;18</title>
		<meeting>SIGMOD &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
	<note>Rohit Deep, Esteban Arcaute, and Vijay Raghavendra</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Skoutas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06167</idno>
		<title level="m">Emmanouil Thanos, and Themis Palpanas. 2019. Blocking and Filtering Techniques for Entity Resolution: A Survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS &apos;19</title>
		<meeting>NeurIPS &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP &apos;14</title>
		<meeting>EMNLP &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The WDC training dataset and gold standard for large-scale product matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Primpeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proc. WWW &apos;19</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="381" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Active learning for large-scale entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-IJCNLP &apos;19</title>
		<meeting>EMNLP-IJCNLP &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP &apos;15</title>
		<meeting>EMNLP &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMC 2 &apos;19</title>
		<meeting>EMC 2 &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interactive deduplication using active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD &apos;02</title>
		<meeting>KDD &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Synthesizing entity matching rules by examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Venkata Vamsikrishna Meduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge-Arnulfo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armando</forename><surname>Quian?-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="189" to="202" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spacy</surname></persName>
		</author>
		<ptr target="https://spacy.io/api/entityrecognizer" />
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">BERT Rediscovers the Classical NLP Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL &apos;19</title>
		<meeting>ACL &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS &apos;17</title>
		<meeting>NIPS &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CrowdER: crowdsourcing entity resolution. PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1483" to="1494" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Entity matching: How similar is similar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Xu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="622" to="633" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic-aware blocking for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhi</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="166" to="180" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD &apos;19. 950???958</title>
		<meeting>KDD &apos;19. 950???958</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">WDC Product Data Corpus</title>
		<ptr target="http://webdatacommons.org/largescaleproductcorpus/v2" />
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-IJCNLP &apos;19</title>
		<meeting>EMNLP-IJCNLP &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Leveraging Knowledge Bases in LSTMs for Improving Machine Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL &apos;17</title>
		<meeting>ACL &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1436" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS &apos;19</title>
		<meeting>NeurIPS &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08314</idno>
		<title level="m">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR &apos;18</title>
		<meeting>ICLR &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Auto-EM: End-to-end Fuzzy Entity-Matching using Pre-trained Deep Models and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeye</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW &apos;19</title>
		<meeting>WWW &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2413" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
