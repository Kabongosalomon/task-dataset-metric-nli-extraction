<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DocFormer: End-to-End Transformer for Document Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikar</forename><surname>Appalaraju</surname></persName>
							<email>srikara@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavan</forename><surname>Jasani</surname></persName>
							<email>bjasani@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urala</forename><surname>Bhargava</surname></persName>
							<email>bharkota@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Kota</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
							<email>manmatha@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">DocFormer: End-to-End Transformer for Document Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present DocFormer -a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of Visual Document Understanding (VDU) aims at understanding digital documents either born as PDF's or as images. VDU focuses on varied document related tasks like entity grouping, sequence labeling, document classification. While modern OCR engines <ref type="bibr" target="#b33">[34]</ref> have become good at predicting text from documents, VDU often requires understanding both the structure and layout of documents. The use of text or even text and spatial features alone is not sufficient for this purpose. For the best results, one needs to exploit the text, spatial features and the image. One way to exploit all these features is using transformer models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">53]</ref>. Transformers have recently been used for VDU <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. These models differ in how the unsupervised pre-training is done, the way self-attention is modified for the VDU domain or how they fuse modalities (text and/or image and spatial). There have been text only <ref type="bibr" target="#b14">[15]</ref>, text plus spatial features only <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">56]</ref> approaches for VDU. However, the holy-grail is to fuse all three modalities (text, <ref type="figure">Figure 1</ref>: Snippet of a Document: Various VDU tasks on this document may include labeling each text token into fixed classes or grouping tokens into a semantic class and finding relationships between tokens e.g. ("DATE PREPARED" Key and "1/29/74" Value) or classifying the document into different categories. Note a document could have "other" text e.g. "C-5" which the model should ignore or classify as "other" depending on the task. visual and spatial features). This is desirable since there is some information in text that visual features miss out (language semantics), and there is some information in visual features that text misses out (text font and visual layout for example).</p><p>Multi-modal training in general is difficult since one has to map a piece of text to an arbitrary span of visual content. For example in <ref type="figure">Figure 1</ref>, "ITEM 1" needs to be mapped to the visual region. Said a different way, text describes semantic high-level concept(s) e.g. the word "person" whereas visual features map to the pixels (of a person) in the image. It is not easy to enforce feature correlation across modalities from text ? ? ? image. We term this issue as cross-modality feature correlation and reference it later to show how DocFormer presents an approach to address this.</p><p>DocFormer follows the now common, pre-training and fine-tuning strategy. DocFormer incorporates a novel multimodal self-attention with shared spatial embeddings in an encoder only transformer architecture. In addition, we pro-pose three pre-training tasks of which two are novel unsupervised multi-modal tasks: learning-to-reconstruct and multi-modal masked language modeling task. Details are provided in Section 3. To the best of our knowledge, this is the first approach for doing VDU which does not use bulky pre-trained object-detection networks for visual feature extraction. DocFormer instead uses plain ResNet50 <ref type="bibr" target="#b21">[22]</ref> features along with shared spatial (between text and image) embeddings which not only saves memory but also makes it easy for DocFormer to correlate text, visual features via spatial features. DocFormer is trained end-to-end with the visual branch trained from scratch. We now highlight the contributions of our paper:</p><p>? A novel multi-modal attention layer capable of fusing text, vision and spatial features in a document.</p><p>? Three unsupervised pre-training tasks which encourage multi-modal feature collaboration. Two of these are novel unsupervised multi-modal tasks: learningto-reconstruct task and a multi-modal masked language modeling task.</p><p>? DocFormer is end-to-end trainable and it does not rely on a pre-trained object detection network for visual features simplifying its architecture. On four varied downstream VDU tasks, DocFormer achieves state of the art results. On some tasks it out-performs large variants of other transformer almost 4x its size (in the number of parameters). In addition, DocFormer does not use custom OCR unlike some of the recent papers <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Document understanding methods in the literature have used various combinations of image, spatial and text features in order to understand and extract information from structurally rich documents such as forms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b12">13]</ref>, tables <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b24">25]</ref>, receipts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> and invoices <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b38">39]</ref>. Finding the optimal way to combine these multi-modal features is an active area of research.</p><p>Grid based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref> were proposed for invoice images where text pixels are encoded using character or word vector representations and classified into field types such as Invoice Number, Date, Vendor Name and Address etc. using a convolutional neural network.</p><p>BERT <ref type="bibr" target="#b14">[15]</ref> is a transformer-encoder <ref type="bibr" target="#b52">[53]</ref> based neural network that has been shown to work well on language understanding tasks. LayoutLM <ref type="bibr" target="#b55">[56]</ref> modified the BERT architecture by adding 2D spatial coordinate embeddings along with 1D position and text token embeddings. They also added visual features for each word token, obtained using a Faster-RCNN and its bounding box coordinates.   <ref type="bibr" target="#b47">[48]</ref>, LayoutLMv2 <ref type="bibr" target="#b56">[57]</ref>, VisualBERT <ref type="bibr" target="#b32">[33]</ref>, MMBT <ref type="bibr" target="#b30">[31]</ref>, UNITER <ref type="bibr" target="#b8">[9]</ref> Type B) Two-stream Multi-Modal: CLIP <ref type="bibr" target="#b41">[42]</ref>, VilBERT <ref type="bibr" target="#b37">[38]</ref>, Type C) Single-stream Multi-Modal, Type D) Ours: Discrete Multi-modal. e.g. DocFormer . Note: in each transformer layer, each input modality is self-attended separately. Best viewed in color.</p><p>LayoutLM was pre-trained on 11 million unlabeled pages and was then finetuned on several document understanding tasks -form processing, classification and receipt processing. This idea of pre-training on large datasets and then finetuning on several related downstream tasks is also seen in general vision and language understanding work <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> etc. <ref type="figure" target="#fig_0">Figure 2</ref> shows a comparison of multimodal transformer encoder architectures.</p><p>Recently, LayoutLMv2 <ref type="bibr" target="#b56">[57]</ref> improved over LayoutLM by changing the way visual features are input to the model -treating them as separate tokens as opposed to adding visual features to the corresponding text tokens. Further, additional pre-training tasks were explored to make use of unlabeled document data.</p><p>BROS <ref type="bibr" target="#b26">[27]</ref> also uses a BERT based encoder, with a graph-based classifier based on SPADE <ref type="bibr" target="#b28">[29]</ref>, which is used to predict entity relations between text tokens in a document. They also use 2D spatial embeddings added along with text tokens and evaluate their network on forms, receipts document images. Multi-modal transformer encoderdecoder architectures based on T5 <ref type="bibr" target="#b42">[43]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Conceptual Overview: We first present a conceptual overview of architectures used in Transformer Encoder Multi-Modal training, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. (a) Joint Multi-Modal: VL-BERT <ref type="bibr" target="#b47">[48]</ref>, LayoutLMv2 <ref type="bibr" target="#b56">[57]</ref>, Visu-alBERT <ref type="bibr" target="#b32">[33]</ref>, MMBT <ref type="bibr" target="#b30">[31]</ref>: In this type of architecture, vision and text are concatenated into one long sequence which makes transformers self-attention hard due to the cross-modality feature correlation referenced in the introduction. (b) Two-Stream Multi-Modal CLIP <ref type="bibr" target="#b41">[42]</ref>, Vil-BERT <ref type="bibr" target="#b37">[38]</ref>: It is a plus that each modality is a separate branch which allows one to use an arbitrary model for each branch. However, text and image interact only at the end which is not ideal. It might be better to do early fusion. (c) Single-stream Multi-Modal: treats vision features also as tokens (just like language) and adds them with other features. Combining visual features with language tokens this way (simple addition) is unnatural as vision and language features are different types of data. (d) Discrete Multi-Modal: In this paper, DocFormer unties visual, text and spatial features. i.e. spatial and visual features are passed as residual connections to each transformer layer. We do this because spatial and visual dependencies might differ across layers. In each transformer layer, visual and language features separately undergo self-attention with shared spatial features. In order to pre-train DocFormer we use a subset of 5 million pages from the IIT-CDIP document collection <ref type="bibr" target="#b31">[32]</ref> for pre-training. In order to do multi-modal VDU, we first extract OCR, which gives us text and corresponding word-level bounding boxes for each document. We next describe the model-architecture, followed by the pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>DocFormer is an encoder-only transformer architecture. It also has a CNN backbone for visual feature extraction. All components are trained end-to-end. DocFormer enforces deep multi-modal interaction in transformer layers using novel multi-modal self-attention. We describe how three modality features (visual, language and spatial) are prepared before feeding them into transformer layers.</p><p>Visual features: Let v ? R 3?h?w be the image of a document, which we feed through a ResNet50 convolutional neural network f cnn (?, v). We extract lower-resolution visual embedding at layer 4 i.e. v l4 ? R c?h l ?w l . Typical values at this stage are c = 2048 and h l = h 32 , w l = w</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32</head><p>(c = number of channels and h l and w l are the height and width of the features). The transformer encoder expects a flattened sequence as input of d dimension. So we first apply a 1 ? 1 convolution to reduce the channels c to d. We then flatten the ResNet features to (d, h l ? w l ) and use a linear transformation layer to further convert it to (d, N ) where d = 768, N = 512. Therefore, we represent the visual embedding as V = linear(conv 1?1 (f cnn (?, v))).</p><p>Language features: Let t be the text extracted via OCR from a document image. In order to generate language embeddings, we first tokenize text t using a word-piece tokenizer <ref type="bibr" target="#b54">[55]</ref> to get t tok , this is then fed through a trainable embedding layer W t . t tok looks like [CLS], t tok1 , t tok2 , . . . , t tokn where n = 511. If the number of tokens in a page is &gt; 511, we ignore the rest. For a document with fewer than 511 tokens, we pad the sequence with a special [P AD] token and we ignore the [P AD] tokens during self-attention computation. We ensure that the text embedding, T = W t (t tok ), is of the same shape as the visual embedding V . Following prior art <ref type="bibr" target="#b56">[57]</ref>, we initialize W t with LayoutLMv1 <ref type="bibr" target="#b55">[56]</ref> pre-trained weights.</p><p>Spatial Features:</p><p>For each word k in the text, we also get bounding box coordinates b k = (x 1 , y 1 , x 2 , y 2 , x 3 , y 3 , x 4 , y 4 ). 2D spatial coordinates b k provide additional context to the model about the location of a word in relation to the entire document. This helps the model make better sense of the content. For each word, we encode the top-left and bottom-right coordinates using separate layers W x and W y for x and y-coordinates respectively. We also encode more spatial features: bounding box height h, width w, the Euclidean distance from each corner of a bounding box to the corresponding corner in the bounding box to its right and the distance between centroids of the bounding boxes, e.g.</p><formula xml:id="formula_0">A rel = {A k+1 num ?A k num }; A ? (x, y); num ? (1, 2, 3, 4, c)</formula><p>, where c is the center of the bounding box. Since transformer layers are permutation-invariant, we also use absolute 1D positional encodings P abs . We create separate spatial embeddings for visual V s and language T s features since spatial dependency could be modality specific. </p><formula xml:id="formula_1">V s = W x v (x 1 , x 3 , w, A x rel )+ W y v (y 1 , y 3 , h, A y rel ) + P abs v (1) T s = W x t (x 1 , x 3 , w, A x rel )+ W y t (y 1 , y 3 , h, A y rel ) + P abs t (2)</formula><p>Multi-Modal Self-Attention Layer: We now describe in detail our novel multi-modal self-attention layer. Consider a transformer encoder f enc (?, V , V s , T , T s ), where ? are trainable parameters of the transformer, V , V s , T and T s are visual, visual-spatial, language and language-spatial features respectively, and are obtained as described previously. Transformer f enc outputs a multi-modal feature representation M of the same shape d = 768, N = 512 as each of the input features.</p><p>Self-attention, i.e., scaled dot-product attention as introduced in <ref type="bibr" target="#b52">[53]</ref>, for a single head is defined as querying a dictionary with key-value pairs. i.e. in a transformer layer l and i th input token in a feature length of L.</p><formula xml:id="formula_2">M l i = L j=1 exp (? ij ) n j =1 exp (? ij ) x l j W V,l<label>(3)</label></formula><p>where ? ij is defined as self-attention which is computed as (attention in layer l between tokens x i and x j ).</p><formula xml:id="formula_3">? ij = 1 ? d x l i W Q,l x l j W K,l T<label>(4)</label></formula><p>Here, d is the dimension of the hidden representation, W Q,l , W K,l ? R d?d K , and W V ? R d?d V are learned parameter matrices which are not shared among layers or attention heads. Without loss of generality, we remove the dependency on layer l and get a simplified view of Eq. 4 as:</p><formula xml:id="formula_4">? ij = x i W Q ? x j W K T<label>(5)</label></formula><p>We modify this attention formulation for the multimodal VDU task. DocFormer tries to infuse the following inductive bias into self-attention formulation: for most VDU tasks, local features are more important than global ones. We modify Eq. 5, to add relative features. Specifically, the attention distribution for visual features is:</p><formula xml:id="formula_5">? v ij = (x v i W Q v )(x v j W K v ) T</formula><p>key-query attn.</p><formula xml:id="formula_6">+ (x v i W Q v a ij ) query 1D relative attn. + (x v j W K v a ij )</formula><p>key 1D relative attn.</p><formula xml:id="formula_7">+ (V s W Q s )(V s W K s ) visual spatial attn.<label>(6)</label></formula><p>Here, x v denotes visual features, W K v , W Q v denote learnable matrices for key, query visual embeddings respectively. W K s , W Q s denote learnable matrices for key, query spatial embeddings respectively. a ij is 1D relative position embedding between tokens i, j i.e. a ij = W rel j?i where W rel learns how token i attends to j. We clip the relative attention so DocFormer gives more importance to local features. We get a similar equation for language attention ? t ij :</p><formula xml:id="formula_8">? t ij = (x i W Q t )(x j W K t ) T + (x i W Q t a ij ) + (x j W K t a ij ) + (T s W Q s )(T s W K s ) (7)</formula><p>Here, x is the output of the previous encoder layer, or word embedding layer if l = 1. An important aspect of Eq. 6 and Eq. 7 is that we share spatial weights in each layer. i.e. the spatial attention weights (W Q s , W K s ) are shared across vision and language. This helps the model correlate features across modalities.</p><p>Using the visual self-attention computed using Eq. 6 in Eq. 3, gets us spatially aware, self-attended visual feature? V l . Similarly using Eq. 7 in Eq. 3, gets us language feature? T l . The multi-modal feature output is given by M l =V l + T l . It should be noted that for layers l &gt; 1, features x in Eq. 7 are multi-modal because we combine visual and language features at the output of layer l ? 1. The final M 12 is consumed by downstream linear layers.</p><p>Why do multi-modal attention this way? We untie the visual and spatial information and pass them to each layer of transformer. We posit that making visual and spatial information accessible across layers acts as an information residual connection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b53">54]</ref> and is beneficial for generating superior multi-modal feature representation hence better addressing the issue of cross-modality feature correlation. This is verified in our experiments (Section 4), where we show that DocFormer obtains state-of-the-art performance even when compared to models having four times the number of the parameters in some cases. Further, sharing spatial weights across modalities in each layer gives DocFormer an opportunity to learn cross-modal spatial interactions while also reducing the number of parameters. In Sec. 4, we show that DocFormer is the smallest amongst its class of models, yet it is able to show superior performance. Code in supple.</p><p>Run-time Complexity: The run-time complexity of DocFormer is of the same order as that of the original selfattention model <ref type="bibr" target="#b52">[53]</ref> (for details see supplemental material)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training</head><p>The ability to design new and effective unsupervised pretraining strategies is still an open problem. Our pre-training process involves passing the document image, its extracted OCR text, and its corresponding spatial features. All pretraining tasks were designed such that the network needs the collaboration of both visual and language features, thereby truly learning a superior representation than training with only one of the modalities. See <ref type="figure" target="#fig_2">Figure 3</ref> for a high-level overview of the pre-training tasks.</p><p>Multi-Modal Masked Language Modeling (MM-MLM): This is a modification of the original masked language modeling (MLM) pre-text task introduced in BERT <ref type="bibr" target="#b14">[15]</ref>, and may be thought of as a text de-noising task i.e. for a text sequence t, a corrupted sequence is generated t. The transformer encoder predictst and is trained with an objective to reconstruct entire sequence. In our case, we use a multi-modal feature embedding M for reconstruction of the text sequence. In prior art <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b55">56]</ref>, for a masked text token, the corresponding visual region was also masked to prevent "cheating". Instead, we intentionally do not mask visual regions corresponding to [M ASK] text. This is to encourage visual features to supplement text features and thus minimize the text reconstruction loss. The masking percentage is the same as originally proposed <ref type="bibr" target="#b14">[15]</ref>. Cross-entropy loss is used for this task (L M M ?M LM ).</p><p>Learn To Reconstruct (LTR): In this novel pre-text task, we do the image version of the MM-MLM task, i.e. we do an image reconstruction task. The multi-modal feature predicted by DocFormer is passed through a shallow decoder to reconstruct the image (the same dimension as the input image). In this case this task is similar to an auto-encoder image reconstruction but with multi-modal features. The intuition is that in the presence of both image and text features, the image reconstruction would need the collaboration of both modalities. We employ a smooth-L1 loss between the reconstructed image and original input image (L LT R ).</p><p>Text Describes Image (TDI): In this task, we try to teach the network if a given piece of text describes a document image. For this, we pool the multi-modal features using a linear layer to predict a binary answer. This task differs from the above two tasks in that this task infuses the global pooled features into the network (as opposed to MM-MLM and LTR focusing purely on local features). In a batch, 80% of the time the correct text and image are paired, for the remaining 20% the wrong image is paired with the text. A binary cross-entropy loss (L T DI ) is used for this task. Since the 20% negative pair scenario interferes with the LTR task (for a text ? ? ? image pair mismatch the pair reconstruction loss would be high), the LTR loss is ignored for cases where there is a mismatch.</p><p>The final pre-training loss L pt = ?L M M ?M LM + ?L LT R + ?L T DI . In practice ? = 5, ? = 1 and ? = 5. DocFormer is pre-trained for 5 epochs, then we remove all three task heads. We add one linear projection head and fine-tune all components of the model for all downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>For all experiments, we fine-tune on the training set and report numbers on the corresponding test/validation dataset. No dataset specific hyper-parameter tuning was done. We treat this as a plus and our reported numbers could be higher if dataset specific fine-tuning was done. For all downstream tasks, we use the official provided annotations unless otherwise stated. A common theme amongst these datasets is the relatively small amount of training data (most with &lt;1000 samples). We posit that pre-training is much more helpful in such scenarios and helps measure the generalization capability of DocFormer .</p><p>Notations: <ref type="table" target="#tab_5">Tables 1, 2</ref>  phasize the importance of warm-up steps and learning rate scale. We found that these settings have a non-trivial impact on pre-training result as well as downstream task performance. We used Pytorch <ref type="bibr" target="#b39">[40]</ref> and the Huggingface library <ref type="bibr" target="#b51">[52]</ref>. Models: We employ the commonly used terminology for transformer encoder models -base with 12 transformer layers (768 hidden state and 12 attention heads) and large with 24 transformer layers (1024 hidden state and 16 attention heads). We show that DocFormer -base gets SOTA for three of the 4 tasks beating even large models and for the 4th task is close to a large model. In addition to the multi-modal DocFormer , we also present a text and spatial DocFormer by pre-training DocFormer multi-modally but fine-tuning with only text and spatial features. We do this to show the flexibility of our model and show that during pretraining visual features were infused into DocFormer leading it to do better than pure text and spatial models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sequence Labeling Task</head><p>FUNSD <ref type="bibr" target="#b17">[18]</ref> dataset is a form understanding task. It contains 199 noisy documents (149 train, 50 test) which are scanned and annotated. We focus on the semantic entity-labeling task (i.e., group tokens which belong to the same class). We measure entity-level performance using F1 score shown in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: FUNSD comparison: DocFormer does better than models its size and compares well with even larger models <ref type="figure">Figure 4</ref>: Amount of Pre-training matters:</p><p>x-axis is the number of pretraining samples needed. y-axis is the F1-score on FUNSD task. DocFormer -base gets 83.34 after pre-training on only 5M pages and outperforms current SOTA LayoutLMv2-base's 82.76 which was pretrained on more than 2x more data.</p><p>creasing number of pre-training samples. As seen in <ref type="figure">Figure</ref> 4, our base model achieves state-of-the-art performance of 83.34 F1-score in-spite of being pre-trained with only 5M documents. Previous SOTA needed more than 2x pretraining documents (11M) to achieve (82.76). Also Doc-Former converges faster. DocFormer performance without images: Please note DocFormer -base T+S model which was pre-trained with (I+T+S) but was fine-tuned on FUNSD without Images gives F1 of 80.54 which is +1.88% higher than a Lay-outLMv1 (78.66%) which was purely pre-trained and finetuned on T+S. We hypothesize that DocFormer was infused with visual features during pre-training and is better than text-only pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Document Classification Task</head><p>For this task we use pooled features to predict a classification label for a document. The RVL-CDIP <ref type="bibr" target="#b19">[20]</ref> dataset consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. Overall there are 320,000 training images, 40,000 validation images, and 40,000 test images. We report performance on test and eval metric is the overall classification accuracy. In line with prior art <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b25">26]</ref> text and layout information is extracted using Textract OCR.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Entity Extraction Task</head><p>We report performance on two different entity extraction datasets:</p><p>CORD Dataset <ref type="bibr" target="#b46">[47]</ref>: consists of receipts. It defines 30 fields under 4 categories. The task is to label each word to the right field. The evaluation metric is entity-level F1. We use the provided OCR annotations and bounding boxes for fine-tuning <ref type="table" target="#tab_7">(Table 3)</ref>. DocFormer -base achieves 96.33% F1 on this dataset besting all prior *-base and virtually all *-large variants tying with TILT-large <ref type="bibr" target="#b40">[41]</ref> which has higher number of parameters. DocFormer -large achieves 96.99% besting all other *-large variants achieving SOTA.</p><p>Kleister-NDA <ref type="bibr" target="#b16">[17]</ref>: dataset consists of legal NDA documents. The task with Kleister-NDA data is to extract the values of four fixed labels. The approach needs to learn to ignore unrelated text. This dataset is challenging since it  has some "decoy" text, for which no label should be given. Also, there might be more than one value given for a given label and all values need to be extracted. In line with priorart we measure F1-score on validation data (since ground truth is not provided for test data). Also we extract OCR and apply heuristics to create train/validation ground-truth on OCR <ref type="table" target="#tab_9">(Table 4</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">More Experiments</head><p>We conduct further analysis on the behavior of Doc-Former pertaining to pre-training tasks, network structure and spatial embedding weight sharing. Shared or Independent Spatial embeddings? One of the benefits of our proposed DocFormer multi-modal selfattention architecture <ref type="figure" target="#fig_0">(Fig. 2</ref> and Eq. 6,7) is that sharing spatial embeddings across vision and language makes it easier for the model to learn feature-correlation across modalities. We see ablation on this aspect in <ref type="table" target="#tab_11">Table 5</ref>.  Do our pre-training tasks help? Pretraining is essential for low-to-medium data regimes (FUNSD and CORD). but even for downstream tasks with a lot of training samples (RVL-CDIP) it helps to improve performance and convergence (  Does a deeper projection head help? So far we used a single linear layer for downstream evaluation as is common practice <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> to compare against prior art. Recent publications <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref> in self-supervision show that a deeper projection head with ReLU activation acts as a one-way filter to enrich the representation space. We adapt this practice and see if a deeper projection head (fc ReLU Layer-Norm fc) can improve downstream performance. <ref type="table" target="#tab_15">Table  7</ref> shows that in the low-to-medium data regime adding a more powerful projection head is harmful and could lead to over-fitting. For the medium-to-large downstream task data regime, adding a deeper projection head is beneficial.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Since it takes a long time to pre-train on the entire 5M pages and to minimize environmental impact <ref type="bibr" target="#b23">[24]</ref>, we do all ablation experiments in <ref type="table">Table 8</ref> and 9 by pre-training with only 1M documents for 5 epochs. In both <ref type="table">Table 8</ref> and 9, we show performance in addition to the previous row in the table. Impact due to adding that component is shown in brackets. We can see in <ref type="table">Table 8</ref> that each of our pre-training tasks have something to contribute to the downstream task (a) Ground Truth (b) Text + Spatial model <ref type="bibr" target="#b55">[56]</ref> (c) DocFormer multi-modal performance. The contribution also seem to vary depending on the nature of the downstream task.  <ref type="table">Table 8</ref>: Ablation on pre-training tasks: We show the impact of various pre-training tasks on two downstream tasks. MLM: masked language modeling <ref type="bibr" target="#b14">[15]</ref>. MM-MLM: multi-modal MLM described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DocFormer architecture ablation:</head><p>In this ablation we look at the impact of various architectural components of DocFormer . Depending on the down-stream task the impact of the proposed multi-modal self-attention varies from 3.89% to 1.08%. This shows that the proposed architecture has indeed learned to fuse multiple modalities. <ref type="bibr">Model</ref>   Qualitative Analysis: We share some qualitative examples of the predictions from DocFormer . <ref type="figure" target="#fig_3">Figure 5</ref> shows some sequence labeling predictions on the FUNSD dataset.</p><p>(more examples are in the supplemental).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present DocFormer , a multi-modal endto-end trainable transformer based model for various Visual Document Understanding tasks. We presented the novel multi-modal attention and two novel vision-plus-language pre-training tasks that allows DocFormer to learn effectively without labeled supervision. We have shown experimentally that DocFormer indeed learns generalized features through its unsupervised pre-training by matching or surpassing existing state-of-the-art results on 4 datasets that cover a variety of document types. We emphasize that Doc-Former showed superior performance against strong baselines in-spite of being one of the smallest model (in terms of # of parameters) in its class.</p><p>In the future, we plan to improve DocFormer's generalizability in multi-lingual settings as well as for more document types such as info-graphics, maps, and web-pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplemental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>We present all the hyper-parameters in <ref type="table" target="#tab_20">Table 10</ref> used for pre-training and fine-tuning DocFormer . We fine-tune on downstream tasks on the same number of epochs as prior art <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b25">26]</ref>: FUNSD <ref type="bibr" target="#b17">[18]</ref>, Kleister-NDA <ref type="bibr" target="#b16">[17]</ref> datasets were fine-tuned for 100 epochs. CORD <ref type="bibr" target="#b46">[47]</ref> for 200 epochs. RVL-CDIP <ref type="bibr" target="#b19">[20]</ref> for 30 epochs. For Key, Query 1-D relative local attention we choose a span of 8 i.e. for a particular multi-modal feature, DocFormer gives more attention 8 tokens to its left and right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-Parameter</head><p>Pre  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Run-time Complexity Analysis</head><p>Since we propose a variant of the self-attention <ref type="bibr" target="#b52">[53]</ref> operation, we compute the train and inference run-time analysis in big-o notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Type</head><p>Run-time Complexity Seq. Complexity Please note that the full run-time complexity for Doc-Former has been abridged as the self-attention is the most significant operation (keeping in line with big-O notation). In addition, the presence of 2 is to signify the unique MMSA operation proposed in this paper, where multimodal feature from each layer is added with image and spatial features (see Section 3.1). We see that DocFormer's multi-modal self-attention (Section 3.1) is an efficient way to do multi-modal learning.</p><formula xml:id="formula_9">Convolution O k ? n ? d 2 O(1) Recurrent O n ? d 2 O(n) Self-Attention O n 2 ? d O(1) Self-Attention (relative) O (r ? n ? d) O(1) DocFormer MMSA 2 ? [O n 2 ? d ] O(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Pseudo-code</head><p>We present a rough pseudo-code for our novel multimodal self-attention (MMSA) as described in section 3.1. We believe the pseudo-code would aid an independent researcher to better replicate our proposed novelty. Please note omitting dropout and layer norm at the end for brevity. <ref type="bibr" target="#b0">1</ref> ##Multi Modal Self Attention </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">DocFormer Architecture for Downstream Tasks</head><p>DocFormer is pre-trained as mentioned in section 3.2. After training it for 5 epochs, we remove the pre-training multi-task heads and use DocFormer (including the visual branch) as a backbone. We simply add a trainable linearhead which predicts the appropriate number of classes which is dataset specific. Please see <ref type="figure">Figure 6</ref> for architecture modifications for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">DocFormer Multi-Modal Self-Attention</head><p>In <ref type="figure" target="#fig_5">Figure 7</ref> we show a more detailed visual representation of the novel multi-modal self-attention introduced in this paper. For reference we also show the original selfattention used by Vaswani et al. <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">FUNSD Vizualizations</head><p>DocFormer achieves state-of-the-art performance of 83.34% F1-score (see Section 4.1) on FUNSD <ref type="bibr" target="#b17">[18]</ref> dataset amongst other multi-modal models its size. In this subsection we look at more visualizations by DocFormer on the test-set. One important aspect of this VDU we would like to mention is the OCR is not in human reading-order.</p><p>Please note that, we search for and present cases where mistakes were made by DocFormer with the aim of understanding mistakes. Legend for the colors used in images is, Header-label: Red, Question: Blue, Answer: Green, Other: Grey color. Please see <ref type="bibr">Figures 8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10.</ref> In <ref type="figure">Figure 11</ref>, we show one specific pattern that Doc-Former learns through its novel multi-modal self-attention. We show that DocFormer automatically learns repetitive local patterns even though it was not explicitly taught this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">CORD Visualizations</head><p>DocFormer matches the state-of-the-art performance of 96.33% F1-score on CORD <ref type="bibr" target="#b46">[47]</ref> dataset (previous state-ofthe-art model TILT-large consists of 780M parameters almost 4x the size of DocFormer ). Please see Section 4.3 in the main paper.</p><p>In this sub-section we look at CORD <ref type="bibr" target="#b46">[47]</ref> visualizations by DocFormer . We explicitly show hard-cases where Doc-Former does well, see <ref type="bibr">Figures 12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16</ref>. In order to be transparent, we also show an error scenario in <ref type="figure" target="#fig_5">Figure  17</ref>. Legend for the colors in images is, Menu items: Red, Total: Blue, Sub-total (pre-tax): Green, Void-menu: Cyan color, Other: grey.</p><p>... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLS</head><formula xml:id="formula_10">W W W x softmax y W t W t W t x text W t W j-i W t W j-i softmax W v W v W v x visual W v W j-i W v W j-i softmax y y t y v</formula><p>a) orignal self-attention b) multi-modal spatial relative self-attention in each layer of DocFormer</p><formula xml:id="formula_11">W s W s W s W s Q K Q K Q K Q Q K rel rel K rel rel Q K V V Q K V</formula><p>x text x visual</p><p>x visual-spatial x text-spatial  <ref type="bibr" target="#b52">[53]</ref>. Note the multi-head attention and feed-forward layers are omitted for brevity. Cross (X) is matrix-multiplication and (+) is elementwise addition. b) Right shows the proposed multi-modal self-attention layer. This comprises each layer of DocFormer . Notice, the spatial weights across text, vision are shared (RED color), thus helping DocFormer address the cross-modality feature correlation issue commonly faced in multi-modal training. The notation is consistent with Equations 1-7 in the main paper. Best viewed in color.  <ref type="figure">Figure 9</ref>: DocFormer slightly bad predictions for 82250337 0338 testfile on FUNSD dataset: Based on the predictions on the right (b), we can see that DocFormer was able to classify most of the sequence correctly. However, if we look at the orange bounding boxes we can spot the errors. "(Indicate Distributor's Cost per Carton)" is tagged as Other text in ground-truth but DocFormer incorrectly classified part of the tokens as Question. Best if viewed digitally and in color. <ref type="figure">Figure 10</ref>: DocFormer slightly bad predictions for 87528380 testfile on FUNSD dataset: Here, we focus the readers attention on two specific scenarios: FUNSD dataset has been known to have ground-truth annotation issues. We find on the left image the orange highlighted box "8650" is incorrectly annotated in GT as "other" text, however DocFormer correctly predicts it as "answer" token for the question "total". Scenario 2: The orange highlighted boxes on the right image are tokens which are actually sub-headers but DocFormer mis-classifies as "question" tokens. In this case, DocFormer likely gave more weight-age to language features and not so much to visual features and so ended up mis-classifying. We would like to point out that this is an ambiguous example as the language in mis-classified regions do look like "questions". Best viewed in color.</p><p>(a) Example FUNSD document with Ground Truth overlays (b) DocFormer prediction self-attention heatmap (last encoder layer, 2nd head). DocFormer has up to 512 tokens in each layer. Each point on the image shows the strength of attention from a token on the y-axis to a token on the x-axis. The bluer colors show more attention and the red less attention. <ref type="figure">Figure 11</ref>: DocFormer learns repetition and regularity: the yellow and purple boxes in the left figure matches the yellow and purple boxes in the right figure. The OCR is not in reading order. Hence the six occurrences of "DIVISION" appear together in front (among the top 25) -yellow box in <ref type="figure">Figure b</ref>) and they correspond to the yellow boxes in <ref type="figure">Figure a)</ref>. Similarly, the purple box in <ref type="figure">Figure b</ref>) corresponds to the purple boxes in <ref type="figure">Figure a)</ref>. DocFormer is able to pick up such repetitions as strong self-attention signals (blue colored pixels in the right self-attention figure) that help the model solve the task. This example shows that regular indentation and spacing help DocFormer understand the form better just as they would help humans parse a form. The orange boxed region in the heatmap also shows strong self-attention. We think that is due to DocFormer representing the blob of text as a single paragraph (in this case, as background text). Best viewed digitally and in color.   <ref type="figure">Figure 16</ref>: DocFormer perfect predictions on CORD dataset: Left image shows GT and right image is the prediction for file receipt 00051 made by DocFormer which perfectly matches with GT. Note that the faded out text which is hard to OCR is correctly classified due to multi-modal self-attention features.</p><p>(a) Ground Truth (b) DocFormer predictions <ref type="figure" target="#fig_5">Figure 17</ref>: DocFormer Partially correct predictions on CORD dataset: Left image shows GT and right image is the prediction for file receipt 00085 made by DocFormer with a misclassification of tokens of category SUBTOTAL with TOTAL items. This could be due to the rarity of SUBTOTAL tokens appearing below TOTAL tokens which DocFormer may not have encountered during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Conceptual Comparisons of Transformer Multi-Modal Encoder Architectures: The mechanisms differ in how the modalities are combined. Type A) Joint Multi-Modal: like VL-BERT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Final spatial embeddings are obtained by summing up all intermediate embeddings. All spatial embeddings are trainable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>DocFormer pre-training methodology. High level overview. Note: First bounding box token corresponding to [CLS], is meant for entire page coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>DocFormer Qualitative Examples: From DocFormer on FUNSD test-set DocFormer 83.34 F1 vs LayoutLMv1 78.66 F1.Legend: Red: Header-label, Blue: Question, Green: Answer. Row 1: "TARGET" is a Header-label which is very visual in nature. DocFormer correctly classifies it whereas a text + spatial model misses such visual cues. Row 2: This is a challenging scenario. Notice the word "Research" behind the signature. Text + spatial model gets confused and mis-classifies "Research" as Header, whereas DocFormer figured out that "Research" is part of "Marketing Research Director" in spite of visual occlusions. Row 3: Notice "Approvals" is partially hidden behind DATE. In spite of that DocFormer correctly labelled "APPROVALS" as Question, where as text+spatial model incorrectly labels it as Header. Best viewed in color and digitally. Snippets are from FUNSD file 86079776 9777, 89856243, and 87125460.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 3 7 8 27 #1D 31 32 48 #1D 52 53</head><label>2727314852</label><figDesc>#text kqv embeddings 4 key1 = Linear(d_model, n_head * d_k) 5 query1 = Linear(d_model, n_head * d_k) 6 value1 = Linear(d_model, n_head * d_v) #image kqv embeddings 9 key2 = Linear(d_model, n_head * d_k) 10 query2 = Linear(d_model, n_head * d_k) 11 value2 = Linear(d_model, n_head * d_v)<ref type="bibr" target="#b11">12</ref> 13 #spatial embeddings. note! shared by text, image 14 key3 = Linear(d_model, n_head * d_k) 15 query3 = Linear(d_model, n_head * d_k)<ref type="bibr" target="#b15">16</ref> 17 #See Eq. 6 and 7 in main paper for formulation<ref type="bibr" target="#b17">18</ref> def multi_modal_self_attention(emb, img_feat, spatial_feat):19   20    #self-attention of text (and prev. layers subseq.)21 k1,q1,v1 = emb,emb,emb 22 k1 = rearr(key1(k1), 'b t (head k) -&gt; head b t k') 23 q1 = rearr(query1(q1), 'b l (head k) -&gt; head b l k') 24 v1 = rearr(value1(v1), 'b t (head v) -&gt; head b t v') 25 attn1 = einsum('hblk,hbtk-&gt;hblt', [q1,k1])/sqrt(q1. shape[-1])26 relative pos. (query, key) 28 #note rel_pos_embed1 is learnt relative pos emb. nxn 29 rel_pos_key1 = einsum('bhrd,lrd-&gt;bhlr', k1, rel_pos_embed1) 30 rel_pos_query1 = einsum('bhld,lrd-&gt;bhlr', q1, rel_pos_embed1) #shared spatial -text/hidden features 33 sp_k1, sp_q1 = spatial_feat, spatial_feat 34 sp_k1=rearr(key3(sp_k1),'b t (head k) -&gt; head b t k') 35 sp_q1=rearr(query3(sp_q1),'b l (head k)-&gt;head b l k') 36 text_only_spatial_scores = einsum('hblk,hbtk-&gt;hblt', [ sp_q1,sp_k1])/sqrt(sp_q1.shape[-1]) of image (repeat of above for img feat) 42 k2,q2,v2 = img_feat,img_feat,img_feat 43 k2 = rearr(key2(k2), 'b t (head k) -&gt; head b t k') 44 q2 = rearr(query2(q2), 'b l (head k) -&gt; head b l k') 45 v2 = rearr(value2(v2), 'b t (head v) -&gt; head b t v') 46 attn2 = einsum('hblk,hbtk-&gt;hblt', [q2,k2])/sqrt(q2. shape[-1]) 47 relative pos. (query, key) 49 #note rel_pos_embed1 is learnt relative pos emb. nxn 50 rel_pos_key2 = einsum('bhrd,lrd-&gt;bhlr', k2, rel_pos_embed2) 51 rel_pos_query2 = einsum('bhld,lrd-&gt;bhlr', q2, rel_pos_embed2) #shared spatial -img features 54 sp_k2, sp_q2 = spatial_feat, spatial_feat 55 sp_k2=rearr(key3(sp_k2),'b t (head k) -&gt; head b t k') 56 sp_q2=rearr(query3(sp_q2),'b l (head k)-&gt;head b l k') 57 img_only_spatial_scores = einsum('hblk,hbtk-&gt;hblt', [ sp_q2,sp_k2])/sqrt(sp_q2.shape[-1])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Multi-Modal Self-Attention Layer: the image a) Left shows the traditional self-attention proposed in Vaswani et al</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>DocFormer perfect predictions for 82837252 testfile of FUNSD dataset: Left image shows GT and right image is the prediction made by DocFormer which perfectly matches with GT. Best viewed in color. (a) Ground Truth (b) DocFormer predictions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>DocFormer predictions on CORD: For file receipt 00053 (a) shows both ground-truth and predictions. DocFormer predicted correctly all the entity regions in the image. Best if viewed digitally and in color.(a) Ground Truth (left) and DocFormer predictions (right) DocFormer predictions on CORD: For file receipt 00044. Best if viewed digitally and in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>(a) Ground Truth (left) and DocFormer predictions (right) DocFormer predictions on CORD: For file receipt 00072. Best if viewed digitally and in color.(a) Ground Truth (b) DocFormer predictions DocFormer perfect predictions on CORD dataset: Left image shows GT and right image is the prediction for file receipt 00004 made by DocFormer which perfectly matches with the GT despite the presence of distortion and background text.(a) Ground Truth (b) DocFormer predictions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>have been proposed recently. Tanaka et al. propose Layout-T5 [50] for a question answering task on a database of web article document images whereas Powalski et al. propose TILT [41] combining convolutional features with the T5 architecture to perform various downstream document understanding tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, 3, 4, use the following notation. T: Text features, S: spatial features. I: image features. Bold indicates SOTA. Underline indicates second best. ? denotes the use of Encoder + Decoder transformer layers. * signifies approximate estimation.Implementation details: We summarize details for pretraining and fine-tuning inTable 1in supplemental. We em-</figDesc><table><row><cell cols="3">T3) Text describe Image Yes/No</cell><cell cols="4">T1) Multi-modal Masked Language Modeling</cell><cell cols="2">T2) Learn to reconstruct</cell></row><row><cell></cell><cell>Binary classifier</cell><cell></cell><cell></cell><cell cols="2">Predict "Account"</cell><cell></cell><cell cols="2">Image decoder CNN</cell></row><row><cell></cell><cell>CLS</cell><cell>M2</cell><cell></cell><cell cols="2">M3</cell><cell>M4</cell><cell>M5</cell><cell>...</cell><cell>M511</cell><cell>M512</cell></row><row><cell></cell><cell cols="2">12th layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DocFormer: Vision and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Language</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transformer</cell></row><row><cell></cell><cell>1st layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Text tokens</cell></row><row><cell></cell><cell>CLS</cell><cell cols="2">Customer</cell><cell cols="2">[Mask]</cell><cell>Number</cell><cell>23878</cell><cell>...</cell><cell>City</cell><cell>SF</cell></row><row><cell>spatial embedding</cell><cell>(0,0) 1</cell><cell>(x,y) 2</cell><cell></cell><cell cols="2">(x,y) 3</cell><cell>(x,y) 4</cell><cell>(x,y) 5</cell><cell>...</cell><cell>(x,y) 511 (x,y) 512 Visual tokens Bounding boxes</cell></row><row><cell></cell><cell>V1</cell><cell>V2</cell><cell></cell><cell cols="2">V3</cell><cell>V4</cell><cell>V5</cell><cell>...</cell><cell>V511</cell><cell>V512</cell></row><row><cell></cell><cell></cell><cell>Linear layer</cell><cell>Image</cell><cell>feature</cell><cell>extractor</cell><cell></cell><cell cols="2">OCR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. DocFormer -base achieves 83.34% F1 score which is better than comparable models: LayoutLMv2-base (+0.58), BROS (+2.13), LayoutLMv1base (+4.07). Story repeats for DocFormer -large inspite of it trained only with 5M pages.</figDesc><table><row><cell>Model</cell><cell cols="3">#param (M) Precision Recall</cell><cell>F1</cell></row><row><cell cols="3">methods based on only text / (text + spatial) features:</cell><cell></cell></row><row><cell>BERT-base [15]</cell><cell>109</cell><cell>54.69</cell><cell cols="2">61.71 60.26</cell></row><row><cell>RoBERTa-base [35]</cell><cell>125</cell><cell>63.49</cell><cell cols="2">69.75 66.48</cell></row><row><cell>UniLMv2-base [4]</cell><cell>125</cell><cell>63.49</cell><cell cols="2">69.75 66.48</cell></row><row><cell>LayoutLMv1-base [56]</cell><cell>113</cell><cell>76.12</cell><cell cols="2">81.55 78.66</cell></row><row><cell>BROS-base [26]</cell><cell>139</cell><cell>80.56</cell><cell cols="2">81.88 81.21</cell></row><row><cell>BERT-large [15]</cell><cell>340</cell><cell>61.13</cell><cell cols="2">70.85 65.63</cell></row><row><cell>RoBERTa-large [35]</cell><cell>355</cell><cell>67.80</cell><cell cols="2">73.91 70.72</cell></row><row><cell>UniLMv2-large [4]</cell><cell>355</cell><cell>67.80</cell><cell cols="2">73.91 70.72</cell></row><row><cell>LayoutLMv1-large [56]</cell><cell>343</cell><cell>75.36</cell><cell cols="2">80.61 77.89</cell></row><row><cell cols="3">methods based on image + text + spatial features:</cell><cell></cell></row><row><cell>LayoutLMv1-base [56]</cell><cell>160</cell><cell>76.77</cell><cell cols="2">81.95 79.27</cell></row><row><cell>LayoutLMv2-base [57]</cell><cell>200</cell><cell>80.29</cell><cell cols="2">85.39 82.76</cell></row><row><cell>LayoutLMv2-large [57]</cell><cell>426</cell><cell>83.24</cell><cell cols="2">85.19 84.20</cell></row><row><cell>DocFormer-base (T+S)</cell><cell>149</cell><cell>77.63</cell><cell cols="2">83.69 80.54</cell></row><row><cell>DocFormer-base (I+T+S)</cell><cell>183</cell><cell>80.76</cell><cell cols="2">86.09 83.34</cell></row><row><cell>DocFormer-large (T+S)</cell><cell>536</cell><cell>81.33</cell><cell cols="2">85.44 83.33</cell></row><row><cell>DocFormer-large (I+T+S)</cell><cell>536</cell><cell>82.29</cell><cell cols="2">86.94 84.55</cell></row></table><note>FUNSD performance vs Pre-training samples: We also measure the performance of DocFormer -base with in-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>DocFormer -base achieves state-of-the-art performance of 96.17%. DocFormer gives superior performance to all existing base and large transformer variants. Some models greater than 4x in number of parameters (TILT-large, 780M parameters gives 94.02% (-2.15% gap).</figDesc><table><row><cell>Model</cell><cell cols="2">#param (M) Accuracy (%)</cell></row><row><cell cols="2">methods based on only images:</cell><cell></cell></row><row><cell>CNN ensemble [20]</cell><cell>*60</cell><cell>89.80</cell></row><row><cell>VGG-16 [1]</cell><cell>138</cell><cell>88.33</cell></row><row><cell>AlexNet [51]</cell><cell>61</cell><cell>90.94</cell></row><row><cell>GoogLeNet [10]</cell><cell>13</cell><cell>90.70</cell></row><row><cell>Single Vision model [11]</cell><cell>*140</cell><cell>91.11</cell></row><row><cell>Ensemble [11]</cell><cell>-</cell><cell>92.21</cell></row><row><cell>InceptionResNetV2 [49]</cell><cell>56</cell><cell>92.63</cell></row><row><cell>LadderNet [45]</cell><cell>-</cell><cell>92.77</cell></row><row><cell cols="2">methods based on text / (text + spatial) features:</cell><cell></cell></row><row><cell>BERT-base [15]</cell><cell>110</cell><cell>89.81</cell></row><row><cell>UniLMv2-base [4]</cell><cell>125</cell><cell>90.06</cell></row><row><cell>LayoutLMv1-base [56]</cell><cell>113</cell><cell>91.78</cell></row><row><cell>BROS-base  ? [26]</cell><cell>139</cell><cell>95.58</cell></row><row><cell>BERT-large [15]</cell><cell>340</cell><cell>89.92</cell></row><row><cell>UniLMv2-large [4]</cell><cell>355</cell><cell>90.20</cell></row><row><cell>LayoutLMv1-large [56]</cell><cell>343</cell><cell>91.90</cell></row><row><cell cols="3">methods based on image + text + spatial features:</cell></row><row><cell>Single Modal [12]</cell><cell>-</cell><cell>93.03</cell></row><row><cell>Ensemble [12]</cell><cell>-</cell><cell>93.07</cell></row><row><cell>TILT-base  ? [41]</cell><cell>230</cell><cell>93.50</cell></row><row><cell>LayoutLMv1-base [56]</cell><cell>160</cell><cell>94.42</cell></row><row><cell>LayoutLMv2-base [57]</cell><cell>200</cell><cell>95.25</cell></row><row><cell>LayoutLMv1-large [56]</cell><cell>390</cell><cell>94.43</cell></row><row><cell>TILT-large  ? [41]</cell><cell>780</cell><cell>94.02</cell></row><row><cell>LayoutLMv2-large [57]</cell><cell>426</cell><cell>95.65</cell></row><row><cell>DocFormer-base (I+T+S)</cell><cell>183</cell><cell>96.17</cell></row><row><cell>DocFormer-large (I+T+S)</cell><cell>536</cell><cell>95.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>RVL-CDIP dataset [20] comparison: We report classi- fication accuracy on the test set. DocFormer gets the highest clas- sification accuracy and outperforms TILT-large by +2.15 which is almost 4x its size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>CORD dataset [47] comparison. We present entity-level Precision, Recall, F1 on test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Kleister-NDA dataset<ref type="bibr" target="#b16">[17]</ref> comparison: We present entity-level Precision, Recall, F1 on validation set. DocFormer gives the best performance, out-performing other *-large models trained with 2.5x the learning capacity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Spatial Weight Sharing: In w/o shared spatial, vision and language get their own spatial weights Ws.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6</head><label>6</label><figDesc>).</figDesc><table><row><cell>Dataset</cell><cell>Train samples</cell><cell>with pre-train then 100 epochs (F1)</cell><cell>w/o pre-train 100 epochs (F1)</cell></row><row><cell>FUNSD [18]</cell><cell>149</cell><cell>83.34</cell><cell>4.18</cell></row><row><cell>CORD [47]</cell><cell>800</cell><cell>96.33</cell><cell>0.54</cell></row><row><cell cols="2">RVL-CDIP [20] 320,000</cell><cell>96.17</cell><cell>93.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Effect of Pre-training</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Deeper Projection Head</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Ablation on DocFormer Components: We show the impact of various architectural components used in DocFormer on two downstream tasks (FUNSD and CORD).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Implementation Details: Hyper-parameters used for pre-training DocFormer and fine-tuning for downstream tasks. Training epochs vary for down-stream tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Complexity analysis: Here n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Omitting number of attention heads h for brevity. Here assume h = 1. In addition, MMSA: multi-modal self-attention.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>DocFormer architecture for various downstream tasks: Image on Left (a) is the architecture for document classification [CLS] is a pooling layer (fn ReLU fn) to get a pooled representation used for document classification task. Image on Right (b) is the architecture used for entity and sequence labeling tasks. Note, only a single linear layer is added for all downstream tasks. Also, all components of DocFormer are fine-tuned for each of the downstream tasks.</figDesc><table><row><cell></cell><cell>Document Type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Token type</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classifier</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell></cell><cell>M511</cell><cell>M512</cell><cell></cell><cell></cell><cell></cell><cell>CLS</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell>...</cell><cell>M511</cell><cell>M512</cell></row><row><cell></cell><cell cols="2">12th layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">12th layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DocFormer: Vision and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DocFormer: Vision and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Language</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Language</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1st layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1st layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>spatial embedding</cell><cell>V1 (0,0) 1</cell><cell>V2 (x,y) 2</cell><cell>V3 (x,y) 3</cell><cell>V4 (x,y) 4</cell><cell>V5 (x,y) 5</cell><cell>... ...</cell><cell cols="2">V511 (x,y) 511 (x,y) 512 V512 Visual tokens Text tokens Bounding boxes</cell><cell>Linear layer</cell><cell>Image OCR feature extractor</cell><cell>spatial embedding</cell><cell>V1 (0,0) 1</cell><cell>V2 (x,y) 2</cell><cell>V3 (x,y) 3</cell><cell>V4 (x,y) 4</cell><cell>V5 (x,y) 5</cell><cell>... ...</cell><cell cols="2">V511 (x,y) 511 (x,y) 512 V512 Visual tokens Bounding boxes Text tokens</cell><cell>Linear layer</cell><cell>Image OCR feature extractor</cell></row><row><cell></cell><cell>CLS</cell><cell>Customer</cell><cell>[Mask]</cell><cell>Number</cell><cell>23878</cell><cell>...</cell><cell>City</cell><cell>SF</cell><cell></cell><cell></cell><cell></cell><cell>CLS</cell><cell>Customer</cell><cell>[Mask]</cell><cell>Number</cell><cell>23878</cell><cell>...</cell><cell>City</cell><cell>SF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>OCR tokens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>OCR tokens</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">(a) Architecture for downstream Document Classification Task</cell><cell cols="10">(b) Architecture for downstream Sequence and Entity Labeling Tasks</cell></row><row><cell>Figure 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This is the supplemental material for the main Doc-Former paper<ref type="bibr" target="#b1">[2]</ref>. Please read the main paper for model formulation, performance numbers on various datasets and further analysis and ablation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Ground Truth (b) DocFormer predictions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Ground Truth (left) and DocFormer predictions (right)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cutting the error by half: Investigation of very deep cnn and advanced training strategies for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Muhammad Zeshan Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>K?lsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="883" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Docformer: End-to-end transformer for document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikar</forename><surname>Appalaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavan</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Bhargava Urala Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards good practices in self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikar</forename><surname>Appalaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>Feh?rv?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS Self-Supervision Workshop 2020</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ICML. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved Baselines with Momentum Contrastive Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almazan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01076</idno>
		<title level="m">What is the right way to represent document images? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document image classification with intradomain transfer learning and stacked generalization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swapan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3180" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modular multimodal architecture for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Dauphinee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rashidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04376</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visual template-free form parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bertgrid: Contextualized embedding for 2d document representation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reisswig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04948</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lambert: Layout-aware language modeling using bert for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Garncarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Stanis?awek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Grali?ski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08087</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Grali?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Stanis?awek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Wr?blewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Lipi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Kaliska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulina</forename><surname>Rosalska</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02356</idno>
		<title level="m">Bartosz Topolski, and Przemys?aw Biecek. Kleister: A novel task for information extraction involving long documents with complex layout</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hazim Kemal Ekenel. Funsd: A dataset for form understanding in noisy scanned documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe Thiran Guillaume</forename><surname>Jaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accepted to ICDAR-OST</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One-shot field spotting on colored forms using subgraph isomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maroua</forename><surname>Hammami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Poulain D&amp;apos;andecy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="586" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Ufkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards the systematic reporting of the energy and carbon footprints of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Ru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Romoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno>abs/2002.05651</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe? Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02349</idno>
		<title level="m">Tapas: Weakly supervised table parsing via pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bros: A pre-trained language model for understanding texts in document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teakgyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingi</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://openreview.net/references/pdf?id=uCz3OR6CJT" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Wonseok Hwang, Daehyun Nam, and Sungrae Park</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bros: A pretrained language model for understanding texts in document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teakgyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=punMXQEsPr0" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Icdar2019 competition on scanned receipt ocr and information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Spatial dependency parsing for semi-structured document information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyeong</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Anoop Raveendra Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordula</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>H?hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baptiste Faddoul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08799</idno>
		<title level="m">Chargrid: Towards understanding 2d documents</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Supervised multimodal bitransformers for classifying images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrat</forename><surname>Bhooshan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02950</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hamed Firooz, and Davide Testuggine</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a test collection for complex document information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gady</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferson</forename><surname>Heard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scatter: selective context attentional scene text recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Mazor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11962" to="11972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An invoice reading system using a graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devashish</forename><surname>Lohani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Bela?d</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yolande</forename><surname>Bela?d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representation learning for information extraction from form-like documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Bodhisattwa Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Potti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Bradley</forename><surname>Tata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 58th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6495" to="6504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Jurkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Pietruszka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Pa?ka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09550</idno>
		<title level="m">Going full-tilt boogie on document understanding with textimage-layout transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Table detection in invoice documents by graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjan</forename><surname>Pau Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Goldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llad?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="122" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deterministic routing between layout abstractions for multi-scale classification of visually rich documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritesh</forename><surname>Sarkhel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
	<note>Andreas Dengel, and Sheraz Ahmed</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cord: A consolidated receipt dataset for post-ocr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename><surname>Seunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Bado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Junyeop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surh</forename><surname>Jaeheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo</forename><surname>Minjoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Hwalsuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visualmrc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11272</idno>
		<title level="m">Machine reading comprehension on document images</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Analysis of convolutional neural networks for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="388" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolf</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debut</forename><surname>Lysandre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanh</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaumond</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delangue</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moi</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cistac</forename><surname>Pierric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rault</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louf</forename><surname>R?mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06431</idno>
		<title level="m">Residual networks behave like ensembles of relatively shallow networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14740</idno>
		<title level="m">Multi-modal pre-training for visually-rich document understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Publaynet: largest dataset ever for document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Jimeno</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">irmp: From printed forms to relational data model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1394" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
