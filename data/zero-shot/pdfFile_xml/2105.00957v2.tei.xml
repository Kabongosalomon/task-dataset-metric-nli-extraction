<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNIVERSAL WEAKLY SUPERVISED SEGMENTATION BY PIXEL-TO-SEGMENT CONTRASTIVE LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Wei</forename><surname>Ke</surname></persName>
							<email>twke@berkeley.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<email>stellayu@berkeley.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNIVERSAL WEAKLY SUPERVISED SEGMENTATION BY PIXEL-TO-SEGMENT CONTRASTIVE LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image. We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity. They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose. Our code is publicly available at https://github.com/twke18/SPML. arXiv:2105.00957v2 [cs.CV] 11 May 2021 Published as a conference paper at ICLR 2021 image image tags bounding boxes labeled points scribbles SOTA methods CAM + refine box-wise CAM CRF loss CRF loss our method single pixel-to-segment contrastive learning loss formulation our relative gain +8.6% +4.7% +24.7% +1.4%</p><p>Figure 2: We propose a unified framework for weakly supervised semantic segmentation with different types of annotations. We demonstrate consistent performance gains compared to the state-ofthe-art (SOTA) methods: Chang et al. (2020) for image tags, Song et al. (2019) for bounding boxes, and Tang et al. (2018b) for points and scribbles. For tags and boxes, Class Activation Maps (CAM) (Zhou et al., 2016) are often used to localize semantics as an initial mask and iteratively refine the segmentation model, whereas for labeled points and scribbles, Conditional Random Fields (CRF) are used to propagate semantic labels to unlabeled regions based on low-level image similarity.</p><p>by individual body parts, even though the ground-truth segmentation is not known during training. This task is challenging, as not only a single body part could contain several visually distinctive areas (e.g., head consists of eyes, nose, mouth, beard), but two adjacent body parts could also have the same visual appearance (e.g., upper arm, lower arm, and hand have the same skin appearance). Once the segmenter is learned, it can be applied to a test image without any annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Consider the task of learning a semantic segmenter given sparsely labeled training images ( <ref type="figure">Fig. 1)</ref>: Each body part is labeled with a single seed pixel and the task is to segment out the entire person weak supervision image baseline ours ground-truth <ref type="figure">Figure 1</ref>: Our task learns a segmenter given partially labeled training images and applies it to test images. A common baseline is to propagate labels within an image based on feature similarity. We model it as semi-supervised metric learning and learn the pixel-wise feature by contrasting it within and across images. Our results are fuller and more accurate, approaching the ground-truth.</p><p>This task belongs to a family of weakly supervised segmentation problems, the goal of which is to assign a label to each pixel despite that only partial supervision is available during training. It addresses the practical issue of learning segmentation from minimum annotations. Such weak supervision takes many forms, e.g., image tags <ref type="bibr">(Kolesnikov &amp; Lampert, 2016;</ref><ref type="bibr" target="#b0">Ahn &amp; Kwak, 2018;</ref><ref type="bibr" target="#b16">Huang et al., 2018;</ref><ref type="bibr">Lee et al., 2019)</ref>, bounding boxes <ref type="bibr" target="#b9">(Dai et al., 2015;</ref><ref type="bibr">Khoreva et al., 2017;</ref><ref type="bibr" target="#b19">Song et al., 2019)</ref>, keypoints <ref type="bibr" target="#b5">(Bearman et al., 2016)</ref>, and scribbles <ref type="bibr">(Lin et al., 2016;</ref><ref type="bibr" target="#b21">Tang et al., 2018a;</ref><ref type="bibr">b)</ref>. Tags and boxes are coarse annotations that lack precise pixel localization whereas points and scribbles are sparse annotations that lack broad region coverage.</p><p>Weakly supervised semantic segmentation can be regarded as a semi-supervised pixel classification problem: Some pixels or pixel sets have labels, most don't, and the key is how to propagate and refine annotations from coarsely and sparsely labeled pixels to unlabeled pixels.</p><p>Existing methods tackle two types of weak supervision differently: Class Activation Maps (CAM) <ref type="bibr">(Zhou et al., 2016)</ref> are used to localize coarse labels, generate pseudo pixel-wise labels, and iteratively refine the segmentation model, whereas Conditional Random Fields (CRF) <ref type="bibr">(Kr?henb?hl &amp; Koltun, 2011</ref>) are used to propagate sparse labels to the entire image. These ideas can be incorporated as an additional unsupervised loss on the feature learned for segmentation <ref type="bibr" target="#b22">(Tang et al., 2018b)</ref>: While labeled pixels receive supervision, unlabeled pixels in different segments shall have distinctive feature representations.</p><p>We propose a Semi-supervised Pixel-wise Metric Learning (SPML) model that can handle all these weak supervision varieties with a single pixel-to-segment contrastive learning formulation <ref type="figure">(Fig. 2)</ref>. Instead of classifying pixels, our metric learning model learns a pixel-wise feature embedding based on common grouping relationships that can be derived from any form of weak supervision.</p><p>Our key insight is to integrate unlabeled pixels into both supervised labeling and discriminative feature learning. They shall participate not only in data-driven grouping within each image, but also in discriminative feature learning within and more importantly across images. Intuitively, labeled pixels receive supervision not only for themselves, but also for their surround pixels that share visual similarity. On the other hand, unlabeled pixels are not just passively brought into discriminative learning induced by sparsely labeled pixels, they themselves are organized based on bottom-up grouping cues (such as grouping by color similarity and separation by strong contours). When they are examined across images, repeated patterns of frequent occurrences would also form a cluster that demand active discrimination from other patterns.</p><p>We capture the above insight in a single pixel-wise metric learning objective for segmentation, the goal of which is to map each pixel into a point in the feature space so that pixels in the same (different) semantic groups are close (far) in the feature space. Our model extends SegSort <ref type="bibr">(Hwang et al., 2019)</ref> from its fully supervised and unsupervised segmentation settings to a universal weaklysupervised segmentation setting. With a single consistent feature learning criterion, such a model sorts pixels discriminatively within individual images and sorts segment clusters discriminatively across images, both steps minimizing the same feature discrimination loss.</p><p>Our experiments on Pascal VOC <ref type="bibr" target="#b11">(Everingham et al., 2010)</ref> and DensePose <ref type="bibr" target="#b1">(Alp G?ler et al., 2018)</ref> demonstrate consistent gains over the state-of-the-art (SOTA), and the gain is substantial especially for the sparsest keypoint supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Semi-supervised learning. <ref type="bibr" target="#b28">Weston et al. (2012)</ref> treats it as a joint learning problem with both labeled and unlabeled data. One way is to capture the underlying structure of unlabeled data with generative models <ref type="bibr">(Kingma et al., 2014;</ref><ref type="bibr">Rasmus et al., 2015)</ref>. Another way is to regularize feature learning through a consistency loss, e.g., adversarial ensembling <ref type="bibr">(Miyato et al., 2018)</ref>, imitation learning and distillation <ref type="bibr" target="#b23">(Tarvainen &amp; Valpola, 2017)</ref>, cross-view ensembling <ref type="bibr" target="#b8">(Clark et al., 2018)</ref>. These methods are most related to transductive learning <ref type="bibr">(Joachims, 2003;</ref><ref type="bibr">Zhou et al., 2004;</ref><ref type="bibr" target="#b13">Fergus et al., 2009;</ref><ref type="bibr">Liu et al., 2019)</ref>, where labels are propagated to unlabeled data via clustering in the pretrained feature space. Our work does transductive learning in an adaptively learned feature space.</p><p>Weakly-supervised semantic segmentation. Partial annotations include scribbles <ref type="bibr">(Lin et al., 2016;</ref><ref type="bibr" target="#b21">Tang et al., 2018a;</ref><ref type="bibr">b;</ref>, bounding boxes <ref type="bibr" target="#b9">(Dai et al., 2015;</ref><ref type="bibr">Khoreva et al., 2017;</ref><ref type="bibr" target="#b19">Song et al., 2019)</ref>, points <ref type="bibr" target="#b5">(Bearman et al., 2016)</ref>, or image tags <ref type="bibr">(Papandreou et al., 2015;</ref><ref type="bibr">Kolesnikov &amp; Lampert, 2016;</ref><ref type="bibr" target="#b0">Ahn &amp; Kwak, 2018;</ref><ref type="bibr" target="#b16">Huang et al., 2018;</ref><ref type="bibr">Li et al., 2018;</ref><ref type="bibr">Lee et al., 2019;</ref><ref type="bibr" target="#b18">Shimoda &amp; Yanai, 2019;</ref><ref type="bibr" target="#b33">Yao &amp; Gong, 2020;</ref><ref type="bibr" target="#b6">Chang et al., 2020;</ref><ref type="bibr" target="#b2">Araslanov &amp; Roth, 2020;</ref><ref type="bibr" target="#b12">Fan et al., 2020;</ref><ref type="bibr" target="#b20">Sun et al., 2020)</ref>. <ref type="bibr" target="#b32">Xu et al. (2015)</ref> formulates all types of weak supervision as linear constraints on a SVM. <ref type="bibr">Papandreou et al. (2015)</ref> bootstraps segmentation predictions via EM-optimization. Recent works <ref type="bibr">(Lin et al., 2016;</ref><ref type="bibr">Kolesnikov &amp; Lampert, 2016;</ref><ref type="bibr">Pathak et al., 2015)</ref> typically use CAM <ref type="bibr">(Zhou et al., 2016)</ref> to obtain an initial dense mask and then train a model iteratively. <ref type="bibr">GAIN (Li et al., 2018)</ref> utilizes image tags or bounding boxes to refine these class-specific activation maps. <ref type="bibr" target="#b20">Sun et al. (2020)</ref> considers within-image relationships and explores the idea of co-segmentation. <ref type="bibr" target="#b12">Fan et al. (2020)</ref> estimates the foreground and background for each category, with which the network learns to generate more precise CAMs. Regularization is enforced at either the image level <ref type="bibr">(Lin et al., 2016;</ref><ref type="bibr">Kolesnikov &amp; Lampert, 2016;</ref><ref type="bibr">Pathak et al., 2015)</ref> or the feature level <ref type="bibr" target="#b21">(Tang et al., 2018a;</ref><ref type="bibr">b)</ref> to produce better dense masks. We incorporate this concept into adaptive feature learning and train the model only once. All types of weak annotations are dealt with in a single contrastive learning framework.</p><p>Non-parametric segmentation. Prior to deep learning, non-parametric models <ref type="bibr" target="#b17">(Russell et al., 2009;</ref><ref type="bibr" target="#b24">Tighe &amp; Lazebnik, 2010;</ref><ref type="bibr">Liu et al., 2011)</ref> usually use designed features with statistical or graphical models to segment images. Recently, inspired by non-parametric models for recognition <ref type="bibr" target="#b30">(Wu et al., 2018b;</ref><ref type="bibr">a)</ref>, <ref type="bibr">SegSort (Hwang et al., 2019)</ref> captures pixel-to-segment relationships via a pixel-wise embedding and develops the first deep non-parametric semantic segmentation for supervised and unsupervised settings. Building upon SegSort, our work has the flexibility of a non-parametric model at capturing data relationships and modeling subclusters within a category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SEMI-SUPERVISED PIXEL-WISE METRIC LEARNING METHOD</head><p>Metric learning develops a feature representation based on data grouping and separation cues. Our method ( <ref type="figure">Fig. 3)</ref> segments an image by learning a pixel-wise embedding with a contrastive loss between pixels and segments: For each pixel i, we learn a latent feature ?(i) such that i is close to its positive segments (exemplars) and far from its negative ones in that feature space.</p><p>In the fully supervised setting, we can define pixel i's positive and negative sets, denoted by C + and C ? respectively, as pixels in the same (different) category. However, this idea is not applicable <ref type="figure">Figure 3</ref>: Overall method diagram. We develop pixel-wise embeddings with contrastive learning between pixels and segments. We derive various forms of positive and negative segments for each pixel. Our goal is to attract (blue inward arrows) the pixel with positive segments, while repelling (red outward arrows) it from negative segments in the feature space. to weakly-or un-supervised settings where the label is not available on every pixel. In the labeled points setting, C + and C ? would only contain a few exemplars according to the sparse pixel labels.</p><p>Our basic idea is to enlarge the sets of C + and C ? to improve the feature learning efficacy. By exploring different relationships and assumptions in the image data, we are able to generate abundant positive and negative segments for any pixel at the same time, providing more supervision in the latent feature space. We propose four types of relationships between pixels and segments ( <ref type="figure" target="#fig_0">Fig. 4</ref>):</p><p>1. Low-level image similarity: We impose a spatial smoothness prior on the pixel-wise feature to keep pixels together in visually coherent regions. The segment pixel i belongs to based on low-level image cues is a positive segment to pixel i; any other segments are negative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Semantic annotation:</head><p>We expand the semantics from labeled points and scribbles to pseudolabels inferred from image-or box-wise CAM. The label of a segment can be estimated by majority vote among pixels; if it is the same as pixel i's, the segment is a positive segment to i.</p><p>3. Semantic co-occurrence: We expand the semantics by assuming that pixels in similar semantic contexts tend to be grouped together. If a segment appears in an image that shares any of the semantic classes as pixel i's image, it is a positive segment to i and otherwise a negative one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Feature affinity:</head><p>We impose a featural smoothness prior assuming that pixels and segments of the same semantics form a cluster in the feature space. We propagate the semantics within and across images from pixel i to its closest segment s in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PIXEL-TO-SEGMENT CONTRASTIVE GROUPING RELATIONSHIPS</head><p>Our goal is to propagate known semantics from labeled data C to unlabeled data U with the aforementioned priors. C and U denote the sets of segment indices respectively. We detail how to augment positive / negative segment sets using both C and U for each type of relationships ( <ref type="figure" target="#fig_0">Fig. 4</ref>).</p><p>Low-level image similarity. To propagate labels within visually coherent regions, we generate a low-level over-segmentation. Following SegSort (Hwang et al., 2019), we use the HED contour detector <ref type="bibr" target="#b31">(Xie &amp; Tu, 2015)</ref> (pre-trained on BSDS500 dataset <ref type="bibr" target="#b3">(Arbelaez et al., 2010)</ref>) and gPb-owtucm <ref type="bibr" target="#b3">(Arbelaez et al., 2010)</ref> to generate a segmentation without semantic information. We define i's positive and negative segments as i's own segment and all the other segments, denoted as V + and V ? respectively. We only consider segments in the same image as pixel i's. We align the contour-based over-segmentations with segmentations generated by K-Means clustering as in SegSort.</p><p>Semantic annotation. Image tags and bounding boxes do not provide pixel-wise localization. We derive pseudo labels from image-or box-wise CAM and align them with oversegmentations induced by the pixel-wise feature. Pixel i's positive (negative) segments are the ones with the same (different) semantic category, denoted by C + and C ? respectively. We ignore all the unlabeled segments.</p><p>Semantic co-occurrence. Semantic context characterizes the co-occurrences of different objects, which can be used as a prior to group and separate pixels. We define semantic context as the union of object classes in each image. Even without the pixel-wise localization of semantic labels, we can leverage semantic context to impose global regularization on the latent feature: The feature should separate images without any overlapping object categories.</p><p>Let O + (O ? ) denote the set of segments in images with (without) overlapping categories as pixel i's image. That is, if the image of pixel i and another image share any semantic labels ( <ref type="figure" target="#fig_0">Fig. 4c</ref>: {cat, sofa, table, chair} for the pixel in the Row 2 image vs. {sofa} for the Row 1 image), then all the segments from that image are positive segments to i and included in O + ; otherwise they are considered negative segments in O ? <ref type="figure" target="#fig_0">(Fig. 4c</ref>: all the segments in the Row 3 image). In particular, all the segments in pixel i's image are in O + of i. This semantic context relationship does not require localized annotations yet imposes regularization on pixel feature learning.</p><p>Feature affinity. Our goal is to learn a pixel-wise feature that indicates semantic segmentation. It is thus reasonable to assume that pixels and segments of the same semantics form a cluster in the feature space, and we reinforce such clusters with a featural smoothness prior: We find nearest neighbours in the feature space and propagate labels accordingly.</p><p>Specifically, we assign a semantic label to each unlabeled segment by finding its nearest labeled segment in the feature space. We denote this expanded labeled set by?. For pixel i, we define its positive (negative) segment set? + (? ? ) according to whether a segment has the same label as i.</p><p>Our feature affinity relationship works best when: 1) the original labeled set is large enough to cover the feature space, 2) the labeled segments are distributed uniformly in the feature space, and 3) the pixel-wise feature already encodes certain semantic information. We thus only apply to DensePose keypoint annotations in our experiments, where each body part is annotated by a point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PIXEL-WISE METRIC LEARNING LOSS</head><p>SegSort <ref type="bibr">(Hwang et al., 2019)</ref> is an end-to-end segmentation model that generates a pixel-wise feature map and a resulting segmentation. Assuming independent normal distributions for individual segments, SegSort seeks a maximum likelihood estimation of the feature mapping, so that the feature induced partitioning in the image and clustering across images provide maximum discrimination among segments. During inference, the segment label is predicted by K-Nearest Neighbor retrievals.</p><p>The feature induced partitioning in each image is calculated via spherical K-Means clustering <ref type="bibr" target="#b4">(Banerjee et al., 2005)</ref>. Let e e e i denote the feature vector at pixel i, which contains the mapped feature ?(i) and i's spatial coordinates. Let z i denote the index of the segment that pixel i belongs to, R R R s the set of pixels in segment s, and ? ? ? s the segment feature calculated as the spherical cluster centroid of segment s. In the Expectation-Maximization (EM) procedure for spherical K-means, the E-step calculates the most likely segment pixel i belongs to: z i = arg max s ? ? ? Existing methods train a pixel-wise classifier using only labeled pixels and propagate labels within each image. c) Our method leverages four types of pixel-to-segment semantic relationships to augment the labeled sets, includes unlabeled pixels (fuller segments than just thin scribbles) and unlabeled segments (e.g. desk outlined in magenta), forms dynamic contrastive relationships between segments (e.g. the desk can be positive, negative, or to be ignored to the sofa in different relations.</p><p>Let s denote the resulting segment that pixel i belongs to per spherical clustering. The posterior probability of pixel i in segment s can be evaluated over the set of all segments S as:</p><formula xml:id="formula_0">p(z i = s|e e e i , ? ? ?) = exp(? ? ? ? s e e e i ) t?S exp(? ? ? ? t e e e i )<label>(1)</label></formula><p>where ? is a concentration hyper-parameter. SegSort minimizes the negative log-likelihood loss:</p><p>L SegSort (i) = ? log p(z i = s|e e e i , ? ? ?) = ? log exp(? ? ? ? s e e e i ) t?S exp(? ? ? ? t e e e i )</p><p>.</p><p>(2) SegSort adopts soft neighborhood assignment <ref type="bibr" target="#b14">(Goldberger et al., 2005)</ref> to further strengthen the grouping of same-category segments. Let C + (C ? ) denote the index set of segments in the same (different) category as pixel i except s -the segment i belongs to. We have:</p><formula xml:id="formula_1">L SegSort + (i, C + , C ? ) = ? log t?C + p(z i = t|e e e i , ? ? ?) = ? log t?C + exp(? ? ? ? t e e e i ) t?C + ?C ? exp(? ? ? ? t e e e i )</formula><p>.</p><p>( <ref type="formula">3)</ref> For our weakly supervised segmentation, the total pixel-to-segment contrastive loss for pixel i consists of 4 terms, one for each of the 4 pixel-to-segment attraction and repulsion relationships:</p><formula xml:id="formula_2">L(i) = ? I L SegSort + (i, V + , V ? ) + ? C L SegSort + (i, C + , C ? ) + ? O L SegSort + (i, O + , O ? ) + ? A L SegSort + (i,? + ,? ? ),<label>(4)</label></formula><p>where ? C = 1. <ref type="figure">Fig. 5</ref> shows how our metric learning method utilizes labeled and unlabeled pixels and segments more extensively than existing classification methods: Our pseudo-labeled sets are fuller than labeled thin scribbles and include unlabeled segments; there are 3 more relationships other than semantic annotations; our segments participate in contrastive learning with dynamic roles in different relations. By easily integrating a full range of pixel-to-segment attraction and repulsion relationships from low-level image similarity to mid-level feature affinity, and to high-level semantic co-occurrence, we go far beyond the direct supervision from semantic annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS ON PASCAL VOC AND DENSEPOSE</head><p>Datasets. Pascal VOC 2012 <ref type="bibr" target="#b11">Everingham et al. (2010)</ref> includes 20 object categories and one background class. Following <ref type="bibr" target="#b7">Chen et al. (2017)</ref>, we use the augmented training set with 10,582 images and validation set with 1,449 images. We use the scribble annotations provided by Lin et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Annotation  For each type of annotations and dataset, we formulate four types of pixel-to-segment contrastive relationships and jointly optimize them in a single pixel-wise metric learning framework <ref type="figure">(Fig. 3)</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the data-driven selection of hyperparameters ? I , ? O and ? A for different task settings.</p><formula xml:id="formula_3">? I ? O ? A<label>Pascal</label></formula><p>Pascal: Image tag annotations. <ref type="table" target="#tab_1">Table 2</ref> shows that, without using additional saliency labels, our method outperforms existing methods with saliency by 4.4%, and those without saliency by 5.1%.</p><p>Pascal: Bounding box annotations.   Pascal: Scribble annotations. <ref type="table" target="#tab_3">Table 3</ref> shows that, our method consistently delivers the best performance among methods without or with CRF post-processing. We get 74.2% (76.1%) mIoU, achieving 97.5% ( 98.4%) of full supervision performance in these two categories respectively.</p><p>Pascal: Varying sparsity of scribble and point annotations. Exploiting metric learning with different relationships in the data frees us from the classification framework and delivers a more Figure 7: Our segmentation results get better with more types of regularizations. We compare visual results by adding more regularizations. As we introduce more relationships for regularization, we observe significant improvement and our results are visually closer to fully supervised counterparts.</p><p>powerful approach that requires fewer annotations. <ref type="table" target="#tab_3">Table 3</ref> shows that, as we shorten the length of scribbles from 100%, 80%, 50%, 30% to 0% (points), we reach 97.5%, 97.5%, 96.3%, 96.5% and 93.7% of full supervision performance. Compared to the full scribble annotations, our accuracy only drops 3.7% with point labels and is significantly better than the baseline. DensePose: Point annotations. We train our baseline using the code released by <ref type="bibr" target="#b22">Tang et al. (2018b)</ref>. <ref type="table">Table 4</ref> shows that, our method without CRF post-processing outperforms the baseline by 12.9% mIoU, reaching 77.1% of full supervision performance with only point supervision.</p><p>Visual quality and ablation study. <ref type="figure" target="#fig_3">Fig. 6</ref> shows that our results are better aligned with region boundaries and visually closer to fully-supervised counterparts. <ref type="figure">Fig. 7</ref> shows that our results improve significantly with different relationships for more regularization. See Appendix for more details and ablation studies.</p><p>Summary. We propose a novel weakly-supervised semantic segmentation method via Semisupervised Pixel-wise Metric Learning, based on four common types of pixel-to-segment attraction and repulsion relationships. It is universally applicable to various weak supervision settings, whether the training images are coarsely annotated by image tags or bounding boxes, or sparsely annotated by keypoints or scribbles. Our results on PASCAL VOC and DensePose show consistent and substantial gains over SOTA, especially for the sparsest keypoint supervision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>We propose a single pixel-to-segment contrastive learning loss formulation for weakly supervised semantic segmentation. We explore different types of visual relationships to group and separate pixels within and across images. We demonstrate state-of-the-art performance using our proposed method with different types of annotations. Here, we include more details on the following aspects:</p><p>? We present the visual results of our method in A.1.</p><p>? We showcase the semantic cues generated by CAM for image tag and bounding box annotations in A.2.</p><p>? We illustrate the data pre-processing used for DensePose dataset in A.3.</p><p>? We describe the details of our experimental settings, hyper-parameters and inference procedure in A.4.</p><p>? We present the ablation study regarding hyper-parameters in A.5.</p><p>? We present mIoU performance with varying sparsity of scribble annotations on Pascal dataset in A.6.</p><p>? We present per-category results with Pascal and DensePose dataset in A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 VISUALIZATION</head><p>We present the visual results on VOC (with image tags, bounding boxes and scribbles) and Dense-Pose (with keypoints) dataset in figure 8. We observe that our segmentation results are better aligned with image boundary. When visual evidence is prominent, our weakly-supervised results are even better than the fully-supervised counterpart.</p><p>We then demonstrate the efficacy of each visual relationship in <ref type="figure">figure 9</ref>. By adding semantic annotation, low-level image similarity and feature affinity progressively, we observe consistent improvement of our results. The predicted segmentation becomes more coherent and better aligned with image boundary. We lastly showcase that our method implicitly encodes semantic contexts. In <ref type="figure">figure 10</ref>, We observe that retrieved segments appear in the similar semantic context as the query segments. For examples, given a bottle next to a desktop, our model retrieves bottles also next to a desktop; a set of sofas in a living room can be retrieved using one sofa query example; screens of a desktop can also be retrieved likewise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SEMANTIC ANNOTATIONS</head><p>Since image tag and bounding box annotations do not provide any of precisely localized semantic information, we adopt CAM <ref type="bibr">(Zhou et al., 2016)</ref> to produce localized semantic cues. Without using additional saliency labels, we use the classifier trained by  to generate CAM. Let M c be the activation map of class c.</p><p>For image tag annotations, we follow <ref type="bibr" target="#b0">Ahn &amp; Kwak (2018)</ref> to normalize M c of the entire image within the range between 0 and 1, where M c = Mc maxc Mc . The background confidence M bg can then be estimated by M bg = (1 ? max c M c ) ? , where ? is the hyper-parameter adjusting background confidence. In our experiments, we set ? to 6 and confidence threshold to 0.2. The low-confidence pixels are considered as unlabeled regions.</p><p>For bounding box annotations, we simply normalize the CAM logits within each bounding box to the range between 0 and 1. We then set confidence threshold to 0.5 for selecting foreground pixels and unlabeled regions. We restrict all the regions outside bounding boxes as "background". See <ref type="figure">figure 11</ref> for more visual examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DATA PRE-PROCESSING FOR DENSEPOSE DATASET</head><p>We next illustrate our pre-processing to generate training labels given keypoint annotations in Dense-Pose dataset. As shown in figure 12, we first assume a Gaussian heat map from every keypoint. By thresholding, we derive 3 regions from every Gaussian blob: labelled, unknown and background region. In labelled region, pixels are annotated as each body part. We then propagate labels, including background class, to pixels in the unknown region. The std of Gaussian heat map is estimated from instance size, and we use ground-truth information in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HYPER-PARAMETERS AND EXPERIMENTAL SETUP</head><p>Architecture and training. For all the experiments on VOC, we base our architecture as DeepLab <ref type="bibr" target="#b7">(Chen et al., 2017)</ref> with ResNet101 <ref type="bibr" target="#b15">(He et al., 2016)</ref> as backbone network. For the experiments on DensePose dataset, we adopt PSPNet <ref type="bibr">(Zhao et al., 2017)</ref> as backbone network. We only use models pre-trained on ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009)</ref> dataset.  <ref type="bibr" target="#b22">Tang et al. (2018b)</ref>. The results from our weakly-supervised model is visually very close to its fully-supervised counterpart, or even better when visual cues are prominent. <ref type="figure">Figure 9</ref>: Our segmentation results get better with more types of regularizations. We compare visual results by adding more regularizations. As we introduce more relationships for regularization, we observe significant improvement and our results are visually closer to fully supervised counterparts.</p><p>We next describe the hyper-parameters used for each experiment. On Pascal VOC dataset, we set "batchsize" to 12 and 16 for scribble / point and image tag / bounding box annotations. On Dense-Pose dataset, "batchsize" is set to 16. For all the experiments, we train our models with 512 ? 512 "cropsize". Following <ref type="bibr" target="#b7">Chen et al. (2017)</ref>, we adopt poly learning rate policy by multiplying base learning rate by 1 ? ( iter max iter ) 0.9 . We set initial learning rate to 0.003, momentum to 0.9. For the hyper-parameters in SegSort framework, we use unit-length normalized embedding of dimension 64 and 32 on VOC and DensePose, respectively. We iterate K-Means clustering for 10 iterations and generate 36 and 144 clusters on VOC and DensePose dataset. We set the concentration parameter ? to different values for semantic annotation, low-level image similarity, semantic co-occurrence and feature affinity, respectively. Moreover, ? I , ? O and ? A are set to different values according to different types of annotations and datasets. ? C is set to 1 among all the experiments. The detailed hyper-parameter settings are summarized in table 5. We train for 30k and 45k iterations on VOC and DensePose dataset for all the experiments. We use additional memory banks to cache up previous 2 batches. For conducting experiments, we take advantage of XSEDE infrastructure <ref type="bibr" target="#b25">(Towns et al., 2014)</ref> that includes Bridges resources <ref type="bibr">(Nystrom et al., 2015)</ref>. <ref type="figure">Figure 10</ref>: Visual examples of nearest neighbor segment retrievals. We observe that retrieved segments (right) appear in the similar semantic context as the query segments <ref type="bibr">(left)</ref>. For examples, given a bottle next to a desktop, our model retrieves bottles also next to a desktop. <ref type="figure">Figure 11</ref>: Visual examples of semantic annotations used on VOC. For image tag and bounding box annotation, we use the classifier trained by  to infer CAM as semantic annotation. These semantic annotations are noisy, which do not precisely localize on the objects.</p><p>Inference and testing. We fix the learned pixel-wise embedding and train an additional softmax classifier for inference. Iterative training is adopted to bootstrap the semantic segmentation prediction. Notably, we do not propagate gradients to the segmentation CNN from the softmax classifier.</p><p>For scribbles / points / bounding boxes, we first learn an initial softmax classifier S 1 from the corresponding weak annotations. Following <ref type="bibr" target="#b0">Ahn &amp; Kwak (2018)</ref>, we apply random walk to refine the semantic logitsM generated by S 1 . The transition probability matrix T is formulated as follows: T i,j = ( exp(?e e e i e e ej ) j exp(?e e e i e e ej ) ) ? , where ? and ? are 20 and 5, respectively. The label propagation is given by:M = T M , whereM denotes refined semantic logits. The random walk process is iterated for 6 times. Next, we obtain the corresponding pseudo labels Y sc = arg max cM c . The pseudo labels are used to train the final softmax classifier S 2 for predicting semantic segmentation.</p><p>For image tag annotations, we adopt both within-image and across-image label propagation to generate optimal pseudo labels. Starting with CAM logits M, we conduct within-image label propagation thru random walk and obtain refined pseudo labels Y 1 cam . Across-image label propagation is carried out by nearest neighbor search thru the whole training set. We refer to <ref type="bibr">SegSort (Hwang et al., 2019)</ref> for more details. We then obtain refined pseudo labels Y 1 nn and train the initial softmax classifier S 1 . Similarly, we use S 1 to predict pseudo labels Y 2 sc from the training images. Followed by nearest neighbor search, we obtain our final pseudo labels Y 2 nn and train the final semantic classifier S 2 . <ref type="figure">Figure 12</ref>: Preparing training labels on DensePose dataset. From left to right are input image, our training labels and ground-truth mask. For each keypoint, a Gaussian heat map is applied to determine labelled, unknown and background region. The white region denotes unknown pixels, to which we propagate labels from annotated or background region.</p><p>The inference procedures for different annotations are summarized in algorithm 1 and 2, respectively. For image tags, we adopt multi-scale and horizontally flipping as data augmentation for predicting semantic segmentation. For scribbles / points / bounding boxes, we do not employ data augmentation during the final inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Annotation  </p><formula xml:id="formula_4">? I ? I ? C ? C ? O ? O ? A ? A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 ABLATION STUDY OF HYPER-PARAMETERS</head><p>We conduct ablation study over different regularizations on Pascal VOC dataset. As shown in table 6, we achieve the most optimal performance on Pascal VOC dataset with ? I = 0.1 and ? O = 0.5. We also observe performance drops 0.4 of mIoU by adding feature affinity regularization. We argue that scribble/box/point annotations are not uniformly distributed across object instance and background, and results in noisy label propagation.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Four types of pixel-to-segment attraction and repulsion relationships. A pixel is attracted to (repelled by) segments: a) of similar (different) visual appearances such as color or texture, b) of the same (different) class labels, c) in images with common (distinctive) labels, d) of nearby (far-away) feature embeddings. They form different positive and negative sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>s e e e i , and the M-Step updates the segment feature as the mean pixel-wise feature: ? ? ? s = i?R R Rs e e ei i?R R Rs e e ei . a) training data b) existing methods c) our SPML Figure 5: Our method uses labeled and unlabeled portions of the training data more extensively. a) Training images and their labeled scribbles are sparse and incomplete. b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Our results on Pascal and DensePose under various weak supervision settings are consistently better aligned with region boundaries and visually closer to fully supervised counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Visual comparison of baseline method (c), our SPML (d) and fully-supervised SegSort (e) on VOC and DensePose. On VOC (top 6 rows), our baseline method is based on Lee et al. (2019); Song et al. (2019); Tang et al. (2018b) for image tag, bounding box and scribble annotations, respectively. On DensePose (bottom 2 rows), our baseline is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameters for different types of annotations on Pascal and DensePose dataset. (2016) for training. DensePose (Alp G?ler et al., 2018) is a human pose parsing dataset based on MSCOCO (Lin et al., 2014). The dataset is annotated with 14 body part classes. We extract the keypoints from the center of each part segmentation. The training set includes 26,437 images and we use minival2014 set for testing, which includes 1,508 images. See Appendix for more details.Architecture, training and testing. For all the experiments on PASCAL VOC, we base our architecture on DeepLab<ref type="bibr" target="#b7">(Chen et al., 2017)</ref> with ResNet101<ref type="bibr" target="#b15">(He et al., 2016)</ref> as the backbone network.</figDesc><table /><note>For the experiments on DensePose, we adopt PSPNet (Zhao et al., 2017) as the backbone network. Our models are pre-trained on ImageNet (Deng et al., 2009) dataset. See Appendix for details on our inference procedure and hyper-parameter selection for training and testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Pascal: Image tags</cell><cell>Saliency</cell><cell>val</cell><cell>test</cell><cell></cell></row><row><cell>Huang et al. (2018) Lee et al. (2019) Zhang et al. (2019) Yao &amp; Gong (2020) Chang et al. (2020)</cell><cell>--</cell><cell cols="2">61.4 63.2 64.9 65.3 66.3 66.5 67.1 67.2 66.1 65.9</cell><cell>Pascal: Bounding boxes Khoreva et al. (2017) Song et al. (2019) Our SPML</cell><cell>val 69.4 70.2 73.5 74.7 test --</cell></row><row><cell>Our SPML</cell><cell>-</cell><cell cols="2">69.5 71.6</cell><cell></cell></row></table><note>shows that, with the same DeepLab/ResNet101 back- bone network, our method outperforms existing methods by 3.2%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Pascal VOC 2012 dataset with image tag (left) and bounding box (right) annotations.</figDesc><table><row><cell>Pascal: Scribbles</cell><cell cols="2">CRF Full Weak WvF</cell></row><row><cell>Tang et al. (2018a)</cell><cell>75.6 72.8</cell><cell>96.3</cell></row><row><cell>Tang et al. (2018a)</cell><cell>76.8 74.5</cell><cell>97.0</cell></row><row><cell>Tang et al. (2018b)</cell><cell>75.6 73.0</cell><cell>96.6</cell></row><row><cell>Tang et al. (2018b)</cell><cell>76.8 75.0</cell><cell>97.7</cell></row><row><cell>Wang et al. (2019)</cell><cell>75.6 73.2</cell><cell>96.8</cell></row><row><cell>Wang et al. (2019)</cell><cell>76.8 76.0</cell><cell>99.0</cell></row><row><cell>Our SPML</cell><cell>76.1 74.2</cell><cell>97.5</cell></row><row><cell>Our SPML</cell><cell>77.3 76.1</cell><cell>98.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pascal VOC 2012 dataset using scribble annotations. Left: mIoU on validataion (white) and test (gray) set. WvF denotes relative mIoU w.r.t full supervision. Right: Relative mIoU performance w.r.t full supervision on different lengths of scribbles.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In ICCV, 2019. Thorsten Joachims. Transductive learning via spectral graph partitioning. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 290-297, 2003. Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2921-2929, 2016.</figDesc><table><row><cell>Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly supervised instance and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 876-885, 2017. Hengshuang Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch?lkopf. Learn-</cell></row><row><cell>ing with local and global consistency. In Advances in neural information processing systems, pp.</cell></row><row><cell>Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised 321-328, 2004.</cell></row><row><cell>learning with deep generative models. In Advances in neural information processing systems, pp.</cell></row><row><cell>3581-3589, 2014.</cell></row><row><cell>Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles</cell></row><row><cell>for weakly-supervised image segmentation. In European Conference on Computer Vision, pp.</cell></row><row><cell>695-711. Springer, 2016.</cell></row><row><cell>3159-3167, 2016.</cell></row><row><cell>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr</cell></row><row><cell>Doll?r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European</cell></row><row><cell>conference on computer vision, pp. 740-755. Springer, 2014.</cell></row><row><cell>Bin Liu, Zhirong Wu, Han Hu, and Stephen Lin. Deep metric transfer for label propagation with</cell></row><row><cell>limited annotated data. In Proceedings of the IEEE International Conference on Computer Vision</cell></row><row><cell>Workshops, pp. 0-0, 2019.</cell></row><row><cell>Ce Liu, Jenny Yuen, and Antonio Torralba. Nonparametric scene parsing via label transfer. PAMI,</cell></row><row><cell>2011.</cell></row><row><cell>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a</cell></row><row><cell>regularization method for supervised and semi-supervised learning. IEEE transactions on pattern</cell></row><row><cell>analysis and machine intelligence, 41(8):1979-1993, 2018.</cell></row><row><cell>Nicholas A Nystrom, Michael J Levine, Ralph Z Roskies, and J Ray Scott. Bridges: a uniquely</cell></row><row><cell>flexible hpc resource for new communities and data analytics. In Proceedings of the 2015 XSEDE</cell></row><row><cell>Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure, pp. 1-8, 2015.</cell></row><row><cell>George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-</cell></row><row><cell>supervised learning of a deep convolutional network for semantic image segmentation. In Pro-</cell></row><row><cell>ceedings of the IEEE international conference on computer vision, pp. 1742-1750, 2015.</cell></row></table><note>Philipp Kr?henb?hl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In Advances in neural information processing systems, pp. 109-117, 2011. Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon. Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5267-5276, 2019. Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look: Guided attention inference network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9215-9223, 2018. Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolu- tional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks for weakly supervised segmentation. In Proceedings of the IEEE international conference on computer vision, pp. 1796-1804, 2015. Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi- supervised learning with ladder networks. In Advances in neural information processing systems, pp. 3546-3554, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters for different types of annotations on Pascal and DensePose dataset. Fixed pixel-wise embedding e e e of the input image and weak annotations Y weak . Output: Semantic segmentation prediction Y pred . / * Train the initial softmax classifier * / 1 Train the softmax classifier S 1 using Y weak . / * Train the final softmax classifier * / 2 Predict semantic logits from initial softmax classifier:M = S 1 (e e e). Calculate pixel-wise transition probability matrix T from e e e. Refine semantic logits by random walk propagation:M = T ? ... ? T M . Derive pseudo labels from refined semantic logits: Y sc = arg max cM c . Train the softmax classifier S 2 using Y sc . Predict final semantic segmentation Y pred from S 2 . Inference procedure for semantic segmentation using image-level tags.Input: Fixed pixel-wise embedding e e e of the input image and CAM logits M. Output: Semantic segmentation prediction Y pred . / * Train the initial softmax classifier * / 1 Calculate pixel-wise transition probability matrix T from e e e. Refine CAM by random walk propagation: M = T ? ... ? T M. Derive pseudo labels from refined CAM: Y 1 cam = arg max c M c . Predict new pseudo labels Y 1 nn from Y 1 cam using nearest neighbor retrievals. Train the softmax classifier S 1 using Y 1 nn . / * Train the final softmax classifier * / 6 Predict pseudo labels Y 2 sc from initial softmax classifier S 1 . Predict new pseudo labels Y 2 nn from Y 2 sc using nearest neighbor retrievals. Train the softmax classifier S 2 using Y 2 nn . Predict final semantic segmentation Y pred from S 2 .</figDesc><table><row><cell>Algorithm 1: Inference procedure for semantic segmentation using scribble / point / bounding</cell></row><row><cell>box annotations.</cell></row><row><cell>Input: Algorithm 2:</cell></row></table><note>345672345789</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of different weighting parameters for each objective function on Pascal VOC validation dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">CRF Full 100% 80% 50% 30% 0%</cell></row><row><cell>Lin et al. (2016)</cell><cell>DeepLab-MSc-LargeFOV</cell><cell>68.5</cell><cell>63.1</cell><cell>61.8 58.5 54.3 51.6</cell></row><row><cell cols="2">Tang et al. (2018b) DeepLab-MSc-LargeFOV</cell><cell>68.7</cell><cell>66.0</cell><cell>65.5 64.2 62.7 57.2</cell></row><row><cell>Our SPML</cell><cell>DeepLab/ResNet101</cell><cell>76.1</cell><cell>74.2</cell><cell>74.2 73.3 73.4 71.3</cell></row><row><cell>Our SPML</cell><cell>DeepLab/ResNet101</cell><cell>77.3</cell><cell>76.1</cell><cell>75.8 74.8 75.0 73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>mIoU performance on Pascal VOC 2012 validation set on different lengths of scribble.</figDesc><table><row><cell>Backbone</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="5">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell cols="4">Tang et al. (2018b) 83.2 35.8 82.8 66.8 75.1 90.9 83.9 89.2 35.8 82.5 53.7 83.4 83.2</cell><cell>79.5</cell><cell>82.2</cell><cell>57.6</cell><cell>81.9 41.6 81.1 73.5</cell><cell>73.2</cell></row><row><cell>Our SPML</cell><cell cols="3">85.8 37.6 82.8 69.6 75.9 89.3 82.8 89.7 38.6 85.7 56.7 85.9 80.1</cell><cell>78.1</cell><cell>84.8</cell><cell>53.9</cell><cell>83.7 49.2 80.9 74.4</cell><cell>74.2</cell></row><row><cell cols="4">Tang et al. (2018b) 86.2 37.3 85.5 69.4 77.8 91.7 85.1 91.2 38.8 85.1 55.5 85.6 85.8</cell><cell>81.7</cell><cell>84.1</cell><cell>61.4</cell><cell>84.3 43.1 81.4 74.2</cell><cell>75.2</cell></row><row><cell>Our SPML</cell><cell cols="3">89.0 38.4 86.0 72.6 77.9 90.0 83.9 91.0 40.0 88.3 57.7 87.7 82.8</cell><cell>79.1</cell><cell>86.5</cell><cell>57.1</cell><cell>87.4 50.5 81.2 76.9</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Per-class results on Pascal VOC 2012 validation set. White-and gray-colored background denotes using without-and with-CRF post-processing for inference.</figDesc><table><row><cell cols="2">Annotations aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="5">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>Full mask</cell><cell cols="3">91.5 43.5 83.0 67.9 81.7 89.8 88.7 94.6 37.5 81.6 68.7 88.8 82.4</cell><cell>88.6</cell><cell>87.6</cell><cell>64.1</cell><cell>87.6 52.7 76.5 71.4</cell><cell>77.3</cell></row><row><cell>Scribbles</cell><cell cols="3">87.0 36.7 82.3 65.5 79.7 89.5 84.8 90.1 37.6 86.3 63.1 89.1 87.8</cell><cell>83.0</cell><cell>86.0</cell><cell>65.8</cell><cell>85.8 60.3 76.9 73.0</cell><cell>76.4</cell></row><row><cell>Points</cell><cell cols="3">83.5 37.0 78.4 61.9 74.8 86.4 83.2 86.9 37.9 85.3 62.4 87.2 84.2</cell><cell>81.1</cell><cell>83.1</cell><cell>64.3</cell><cell>85.1 59.1 74.0 66.3</cell><cell>74.0</cell></row><row><cell>Boxes</cell><cell cols="3">84.1 36.5 86.7 57.6 75.7 87.7 84.8 89.6 39.4 86.4 57.2 89.2 88.0</cell><cell>82.6</cell><cell>80.3</cell><cell>54.7</cell><cell>88.2 55.9 79.7 71.6</cell><cell>74.7</cell></row><row><cell>Tags</cell><cell cols="3">82.1 38.7 80.0 56.9 73.7 85.7 81.0 86.7 33.9 87.7 60.8 86.8 84.9</cell><cell>81.3</cell><cell>77.7</cell><cell>53.2</cell><cell>86.5 50.1 64.8 58.4</cell><cell>71.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Per-class results on Pascal VOC 2012 testing set. CRF post-processing is used for inference. A.6 MEAN IOU PERFORMANCE WITH VARYING SPARSITY OF SCRIBBLES. We report absolute mIoU performance by varying sparsity of scribbles on Pascal VOC 2012 validation set. The results are summarized in table 7. Our results are much better with sparser annotation. A.7 PER-CATEGORY MIOU ON PASCAL VOC AND DENSEPOSE DATASET. We next present per-category results on Pascal VOC and Denspose dataset. In table 8, we compare with<ref type="bibr" target="#b22">Tang et al. (2018b)</ref> on VOC validation set. Without-and with CRF post-processing, our method outperform the baseline method among most categories by large margin. We further conduct experiments on VOC testing set, using DeepLab as backbone network. In table 9, we can retrieve most performance w.r.t full supervision. We also compare per-category results on DensePose dataset in table 10. We train our baseline method using the code released by<ref type="bibr" target="#b22">Tang et al. (2018b)</ref>. We outperform the baseline method by large margin in every category.</figDesc><table><row><cell>mIoU WvF</cell></row></table><note>Method bg. torso RHand LHand LFoot RFoot RThigh LThigh RLeg LLeg LArm RArm LFarm RFarm Heaad</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Per-class results on DensePose minival 2014 set with keypoint annotations. White-and gray-colored background indicates using full and point supervision.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported, in part, by Berkeley Deep Drive and Berkeley AI Research Commons with Facebook. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von mises-fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cross-view training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08370</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning in gigantic image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmenting scenes by matching image composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1818" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On regularized losses for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Xsede: accelerating scientific discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Towns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cockerill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maytal</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Gaither</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Grimshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hazlewood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Lifka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregory D Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in science &amp; engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="62" to="74" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Boundary perception guidance: A scribble-supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency guided self-attention network for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14413" to="14423" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08039</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
