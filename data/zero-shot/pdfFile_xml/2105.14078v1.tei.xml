<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UCPhrase: Unsupervised Context-aware Quality Phrase Tagging KEYWORDS phrase mining; language models; unsupervised method ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>August 14-18, 2021. August 14-18, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
							<email>xiaotao2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Bi</surname></persName>
							<email>z1bi@ucsd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<email>yumeng5@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>jshang@ucsd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Bi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UCPhrase: Unsupervised Context-aware Quality Phrase Tagging KEYWORDS phrase mining; language models; unsupervised method ACM Reference Format</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;21)</title>
						<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;21) <address><addrLine>Singapore</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">August 14-18, 2021. August 14-18, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467397</idno>
					<note>2021. UCPhrase: Unsupervised Context-aware Quality Phrase Tagging. In Event, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3447548.3467397 * Equal Contribution. ACM ISBN 978-1-4503-8332-5/21/08. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying and understanding quality phrases from context is a fundamental task in text mining. The most challenging part of this task arguably lies in uncommon, emerging, and domain-specific phrases. The infrequent nature of these phrases significantly hurts the performance of phrase mining methods that rely on sufficient phrase occurrences in the input corpus. Context-aware tagging models, though not restricted by frequency, heavily rely on domain experts for either massive sentence-level gold labels or handcrafted gazetteers. In this work, we propose UCPhrase, a novel unsupervised context-aware quality phrase tagger. Specifically, we induce high-quality phrase spans as silver labels from consistently co-occurring word sequences within each document. Compared with typical context-agnostic distant supervision based on existing knowledge bases (KBs), our silver labels root deeply in the input domain and context, thus having unique advantages in preserving contextual completeness and capturing emerging, out-of-KB phrases. Training a conventional neural tagger based on silver labels usually faces the risk of overfitting phrase surface names. Alternatively, we observe that the contextualized attention maps generated from a Transformer-based neural language model effectively reveal the connections between words in a surface-agnostic way. Therefore, we pair such attention maps with the silver labels to train a lightweight span prediction model, which can be applied to new input to recognize (unseen) quality phrases regardless of their surface names or frequency. Thorough experiments on various tasks and datasets, including corpus-level phrase ranking, document-level keyphrase extraction, and sentence-level phrase tagging, demonstrate the superiority of our design over state-of-the-art pre-trained, unsupervised, and distantly supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Quality phrases refer to informative multi-word sequences that "appear consecutively in the text, forming a complete semantic unit in certain contexts or the given document" <ref type="bibr" target="#b9">[10]</ref>. Identifying and understanding quality phrases from context is a fundamental task in text mining. Automated quality phrase tagging serves as a cornerstone in a broad spectrum of downstream applications, including but not limited to entity recognition <ref type="bibr" target="#b34">[35]</ref>, text classification <ref type="bibr" target="#b0">[1]</ref>, and information retrieval <ref type="bibr" target="#b4">[5]</ref>.</p><p>The most challenging open problem in this task is how to recognize uncommon, emerging phrases, especially in specific domains. These phrases are essential in the sense of their significant semantic meanings and the large volume-following a typical Zipfian distribution, uncommon phrases can add up to a significant portion of quality phrases <ref type="bibr" target="#b36">[37]</ref>. Moreover, emerging phrases are critical in understanding domain-specific documents, such as scientific papers, since new terminologies often come along with transformative innovations. However, mining such sparse long-tail phrases is nontrivial, since a frequency threshold has long ruled them out in traditional phrase mining methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> due to the lack of reliable frequency-related corpus-level signals (e.g., the mutual information of its sub-ngrams). For instance, AutoPhrase <ref type="bibr" target="#b33">[34]</ref> only recognizes phrases with at least 10 occurrences by default.</p><p>For infrequent phrases, the tagging process largely relies on local context. Recent advances in neural language models have unleashed the power of sentence-level contextualized features in building chunking-and tagging-based models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>. These contextaware models can even recognize unseen phrases from new input texts, thus being no longer restricted by frequency. However, training a domain-specific tagger of reasonably high quality requires expensive, hard-to-scale effort from domain experts for massive sentence-level gold labels or handcrafted gazetteers.</p><p>In this work, we propose UCPhrase, a novel unsupervised contextaware quality phrase tagger. It first induces high-quality silver labels directly from the corpus under the unsupervised setting, and then trains a tailored Transformer-based neural model that can recognize quality phrases in new sentences. <ref type="figure" target="#fig_0">Figure 1</ref> presents an overview of UCPhrase. The two major steps are detailed as follows.</p><p>By imitating the reading process of humans, we derive supervision directly from the input corpus. Given a document, human readers can quickly recognize new phrases or terminologies from the consistently used word sequences within the document. The "document" here refers to a collection of sentences centered on the same topic, such as sentences from an abstract of a scientific paper and tweets mentioning the same hashtag. Inspired by this observation, we propose to extract core phrases from each document, which are maximal contiguous word sequences that appear in the document more than once. The "maximal" here means that if one expands this word sequence further towards the left or right, its frequency within this document will drop. To avoid uninformative  phrases (e.g., "of a"), we conduct simple filtering of stopwords before finalizing the silver labels. Note that our proposed silver label generation follows a per-document manner. Therefore, compared with typical context-agnostic distant supervision based on existing knowledge bases or dictionaries <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>, our supervision roots deeply in the input domain and context, thus having unique advantages in preserving contextual completeness of matched spans and capturing much more emerging phrases.</p><p>We further design a tailored neural tagger to fit our silver labels better. Training a conventional neural tagger based on silver labels usually faces a high risk of overfitting the observed labels <ref type="bibr" target="#b23">[24]</ref>. With access to the word-identifiable embedding features, it is easy for the model to achieve nearly zero training error by rigidly memorizing the surface names of training labels. Alternatively, we find that the contextualized attention distributions generated from a Transformer-based neural language model could capture the connections between words in a surface-agnostic way <ref type="bibr" target="#b18">[19]</ref>. Intuitively, the attention maps of quality phrases should reveal distinct patterns from ordinary word spans. Moreover, attention-based features block the direct access to the surface names of training labels, and force the model to learn about more general context patterns. Therefore, we pair such surface-agnostic features based on attention maps with the silver labels to train a neural tagging model, which can be applied to new input to recognize (unseen) quality phrases. Specifically, given an unlabeled sentence of words, we first encode the sentence with a pre-trained Transformer-based language model and obtain the attention maps as features. The ? matrices from different Transformer layers and attention heads can be viewed as images to be classified with multiple channels. A lightweight CNN-based classifier is then trained to distinguish quality phrases from randomly sampled negative spans.</p><p>Thorough experiments on various tasks and datasets, including corpus-level phrase ranking, document-level keyphrase extraction, and sentence-level phrase tagging, demonstrate the superiority of our design over state-of-the-art unsupervised, distantly supervised methods, and pre-trained off-the-shelf tagging models. It is noteworthy that our trained model is robust to the noise in the core phrases-our case studies in Section 4.7 show that the model can identify inferior training labels by assigning extremely low scores.</p><p>Efficiency wise, thanks to the rich semantic and syntactic knowledge in the pre-trained language model, we can simply use the generated attention maps as informative features without fine-tuning the language model. Hence we only need to update the lightweight classification model during training, making the training process as fast as one inference pass of the language model through the corpus with limited resource consumption.</p><p>To the best of our knowledge, UCPhrase is the first unsupervised context-aware quality phrase tagger. It enjoys the rich knowledge from the pre-trained neural language models. The learned phrase tagger works efficiently and effectively without reliance on human annotations, existing knowledge bases, or phrase dictionaries. We summarize our key contributions as follows:</p><p>? We propose to mine silver labels that root deeply in the input domain and context by recognizing core phrases, i.e., maximal word sequences that occur consistently in a per-document manner. ? We propose to replace the conventional contextualized word representations with surface-agnostic attention maps generated by pre-trained Transformer-based language models to alleviate the risk of overfitting silver labels. ? We conduct extensive experiments, ablation studies, and case studies to compare UCPhrase with state-of-the-art unsupervised, distantly supervised methods, and pre-trained off-the-shelf tagging models. The results verify the superiority of our method 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>Given a sequence of words [ 1 , . . . , ], a quality phrase is a contiguous span of words [ , . . . , + ] that form a complete and informative semantic unit in context. Though some studies also view unigrams as potential phrases, in this work, we focus on multi-word phrases ( &gt; 0), which are more informative, yet more challenging to get due to both diversity and sparsity.</p><p>To effectively capture phrases with potential overlaps, e.g., "information extraction" in "information extraction systems", we adopt the span prediction framework, where each possible span in the sentence is assigned a binary label. To avoid a quadratic growth of the size of candidate spans, we follow previous work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref> to set a maximum span length . We also explore alternative classifiers based on the sequence labeling framework in Section 3. <ref type="figure" target="#fig_0">Figure 1</ref> presents an overview of UCPhrase. As an unsupervised method, UCPhrase first mines core phrases directly from each document as silver labels and extracts surface-agnostic attention features with a pre-trained language model. A lightweight classifier is then trained with silver labels and randomly sampled negative labels. Algorithm 1 shows the detailed training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UCPHRASE: METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Silver Label Generation</head><p>As the first step, we seek to collect high-quality phrases in the input corpus following an unsupervised way, which will be our silver labels for the tagging model training. A common practice for automated label fetching is to conduct a context-agnostic matching between the corpus and a given quality phrase list, either mined    with unsupervised models or collected from an existing knowledge base (KB). Such methods, as we show later, can suffer from incomplete labels due to the negligence of context.</p><p>On the contrary, based on the definition of phrases, we look for consistently used word sequences in context. We propose to treat documents as context and collect high-quality phrases directly from each document. The "document" here refers to a collection of sentences centered on the same topic, such as sentences from an abstract of a scientific paper and tweets mentioning the same hashtag. This way, we expect to preserve better contextual completeness that reflects the original writing intention.</p><p>We view a document as a contiguous word sequence [ 1 , . . ., ] and then mine max contiguous sequential patterns. A valid pattern here is a word span [ , . . . , ] that appear more than once in the input sequence. One can easily adjust the frequency threshold to balance the quality and quantity of valid patterns. In this work, we simply use the minimum requirement of two occurrences without further tuning, and find it works well for both short documents like paper abstracts and long documents like news reports. To preserve completeness, we only leave max patterns that are not sub-patterns of any other valid patterns. Uninformative patterns like "of a" are removed with a stopword list widely used by previous work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. We treat the remaining max patterns as core phrases of document , and add them to the positive training samples P + . An equal number of negative samples are randomly drawn from the remaining spans in , denoted as P ? . <ref type="figure">Figure 2</ref> compares the silver labels generated by our core phrases with those by distant supervision, which follows a context-agnostic string matching from the Wikipedia entities. From the real example in <ref type="figure">Figure 2</ref>(a), "heat island effect" is not a Wikipedia entity, but "island effect" is one. Distant supervision hence generates a flawed label by partially matching the real phrase. Similar examples are quite common especially when it comes to compound phrases, like "biomedical data mining". Distant supervision would tend to favor those popular phrases and generate incomplete matches in context. On the contrary, our core phrase mining can generate labels with better contextual completeness. Core phrase mining can also dynamically capture concepts or expressions newly introduced in each document, such as the "core phrases" in this paper. <ref type="figure">Figure 2</ref>(b) confirms this by showing the distribution of unique phrases of the two types of silver labels, mined from the KP20k CS publication corpus and the KPTimes news corpus, with respect to their frequency. In particular, core phrase mining discovers more unique phrases with less than 10 occurrences in the corpus (30x on KP20k, 9x on KPTimes). As <ref type="figure">Figure 2</ref>(c) demonstrates, core phrases outnumber matched Wiki titles on all length ranges. Overall, core phrase mining discovers much more unique phrases than distant supervision (20x on KP20k, 6x on KPTimes).</p><p>Of course, there also inevitably exist noises in mined core phrases due to random word combinations consistently used in some documents, e.g., "countries including". Fortunately, since we collected core phrases from each document independently, such noisy labels will not spread and be amplified to the entire corpus. In fact, among the tagged core phrases randomly sampled from two datasets, the overall proportion of high-quality labels is over 90%. The large volume of reasonably high-quality silver labels provides a robust foundation for us to train a span classifier that learns about general context patterns to distinguish noisy spans. As Section 4.7 shows, the final classifier can assign extremely low scores to false-positive phrases in training labels.</p><p>In summary, document-level core phrase mining provides a simple and effective way to automatically fetch abundant contextaware silver labels of reasonably good quality without relying on external KBs. In ablation studies (Section 4.6) we show that models trained with such free silver labels can outperform the same models  trained with distant supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Surface-agnostic Feature Extraction</head><p>To build an effective context-aware tagger for quality phrases, in addition to labels, we need to figure out the contextualized feature representation for each span. Traditional word-identifiable features (e.g., contextualized embedding) make it easy for a classification model to overfit the silver labels by rigidly memorizing the surface names of training labels. A degenerated name-matching model can easily achieve zero training error without really learning about any useful, generalizable features. In principle, the recognition of a phrase should depend on the role that it plays in the sentence. Kim et al. <ref type="bibr" target="#b18">[19]</ref> show that the structure information of a sentence can be largely captured by its attention distribution. Therefore, we propose to obtain surfaceagnostic features from the attention distributions generated by a pre-trained Transformer-based language model encoder (LM), such as BERT <ref type="bibr" target="#b6">[7]</ref> and RoBERTa <ref type="bibr" target="#b25">[26]</ref>.</p><p>Given a sentence [ 1 , . . . , ], we encode it with a language model pre-trained on a massive, unlabeled corpus from the same domain (e.g., general domain and scientific domain). Suppose this language model has layers, and each layer has attention heads. Each attention head ? from layer produces an attention map A ,? ? R ? of the sentence. The aggregated attention map from attention heads of all layers is denoted as A ? R ? ?( ? ) , where A , ? R ? is a vector that contains the attention scores from to . Finally, for each candidate span = [ , . . . , ], we denote its feature as X = A ... , ... . Ideally, the attention maps of quality phrases should reveal distinct patterns of word connections. <ref type="figure" target="#fig_4">Figure 3</ref> shows a real example of the generated attention map of a sentence. The chunks on the attention map lead to a clear separation of different parts of the sentence. From all chunks, our final span classifier (Section 3.3) accurately distinguishes the quality phrases ("coal mine", "heat island effects") from ordinary spans (e.g., "We can"), indicating the informativeness of the attention features. Efficient Implementation. Thanks to the rich syntactic and semantic knowledge in the pre-trained language model, the generated attention maps are already informative enough for phrase tagging. <ref type="figure">Figure 4</ref>: An alternative classifier based on attention-level LSTM.</p><formula xml:id="formula_0">!"#$ !" !"#$ #" LM % $ ? % % ? % &amp; input attention LSTM &amp; $% &amp; $$ &amp; $&amp; ' $ ' % ' &amp; word LSTM ? ? !"#$ #" !"#$ !" feature ? ) $ ? ) % ? ) &amp; ? ? ? ? ? ? &amp; %% &amp; %$ &amp; %&amp; &amp; &amp;% &amp; &amp;$ &amp; &amp;&amp; ? ) $ ? ) % ? ) &amp;</formula><p>In this work, we adopt the RoBERTa model <ref type="bibr" target="#b25">[26]</ref>, one of the state-ofthe-art Transformer-based language models, as a feature extractor without the need for further fine-tuning. We only need to apply the pre-trained RoBERTa model for one inference pass through the target corpus for feature extraction. The overall efficiency now mainly depends on the size of the attention map, which is ? ? ( ? ). is restricted to the length of each span during training, and for inference, we apply sentencelevel encoding, with each sentence restricted to at most 64 tokens. Depth wise, existing studies have observed considerable redundancy in the outputs of different Transformer layers, including attention distributions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. For this reason, as the default setting of UCPhrase, we only preserve attention maps from the first 3 layers in RoBERTa (i.e., = 3). As RoBERTa has 12 layers in total, this saves 75% of resource consumption. We have quantitatively compared the final tagging performance of using 3 layers vs. using all 12 layers in Section 4.6. As the experimental results suggest, using 3 layers exhibits comparable performance with the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lightweight Span Classifier</head><p>With the labels and features in-house, we are ready to build a classifier to recognize spans of quality phrases. Our framework is general and compatible with various classification models. For the sake of efficiency, we wish to find a lightweight classifier.</p><p>Given the attention map of a -word span, an accurate classifier should effectively capture inter-word relationships from different LM layers and at different ranges. Naturally, the attention map can be viewed as a square image of pixels for both height and width, with ? channels. We can now transform the phrase classification problem into an image classification problem: given a multi-channel image (attention map), we want to predict whether the corresponding word span is a quality phrase. Specifically, we apply a two-layer convolutional neural network (CNN) model on the multi-channel attention map. The output is then fed to a logistic regression layer to assign a binary label for the corresponding span. During the training process, the classification model (?; ) parameterized by is learned by minimizing the loss over the training set {X P , P}:</p><formula xml:id="formula_1">= argmin 1 |P | | P | ?? =1 ? ( , (X ; )),</formula><p>where ? P = =1 {P ? , P + } represents the -th labeled span, and ? is the binary cross entropy loss function. The model is updated with minibatch-based stochastic gradient descent. Note that here only includes the parameters in the two CNN layers and the logistic regression layer during training, which makes   the training process efficient in terms of resource consumption. In fact, the checkpoint of training parameters from each epoch can be stored in a 22 KB file on disk.</p><p>Alternative Classifiers. In our study, we also considered some alternative classifiers. One intuitive choice here is LSTM-based models following the sequence labeling framework. We illustrate the general idea in <ref type="figure">Figure 4</ref>. Specifically, for sentence [ 1 , 2 , . . . , ], we first encode the attention map A ? R ? ?( ? ) with forward and backward runs of LSTM to get an attention-based word representation through the final output of both LSTMs as follows,</p><formula xml:id="formula_2">? ? R = LSTM(A , , A , +1 , . . . , A , ) last , ? ? R = LSTM(A , , A , ?1 , . . . , A ,1 ) last , R = [ ? ? R , ? ? R ], 1 ? ? .</formula><p>Another bidirectional LSTM layer is built upon the word representations R to extract the feature F, i.e., ? ? F 1,2,..., = LSTM(R 1 , R 2 , . . . , R ), ? ? F , ?1,...,1 = LSTM(R , R ?1 , . . . , R 1 ).</p><p>Scheme wise, there are two popular labeling schemes in sequence labeling: (1) Tie-or-Break, which is predicting whether each consecutive pair of words belong to the same phrase, and (2) BIO Tagging, which is tagging phrases in the sentence through a Begin-Inside-Outside scheme <ref type="bibr" target="#b31">[32]</ref>. We are not using BIOES <ref type="bibr" target="#b32">[33]</ref> as we focus on multi-word phrases. For the Tie-or-Break tagging scheme, we apply a 2-layer Multi-layer Perceptron followed by a Sigmoid classification function to predict whether the [ ? ? F , ? ? F +1 ] representation corresponds to a tie between word and word +1 or a break. For the BIO tagging scheme, in a word-wise manner, we concatenate the representations ? ? F 1,2,..., and ? ? F , ?1,...,1 into representations for each sentence, and then send them through a Conditional Random Field (CRF) layer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> to predict the BIO tags for phrases.</p><p>Other training procedures for both of the classifiers are the same as the aforementioned default span classifier. These alternative classifiers have comparable performance, as confirmed in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We compare our UCPhrase with previous studies on multi-word phrase mining tasks on two datasets and three tasks at different granularity: corpus-level phrase ranking, document-level keyphrase extraction, and sentence-level phrase tagging. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Tasks and Metrics</head><p>We evaluate all methods on the following three tasks. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates the tasks and evaluation metrics with some examples.</p><p>Task I: Phrase Ranking is a popular evaluation task in previous statistics-based phrase mining work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. Specifically, it evaluates the "global" rank list of phrases that a method finds from the input corpus. Since UCPhrase does not explicitly compute a "global" score for each phrase, we use the average logits of all occurrences of a predicted phrase to rank phrases.</p><p>In our experiments, for each method 2 on each dataset, we quantitatively evaluate the precision of the phrases found in the topranked 5,000 and 50,000 phrases, denoted as P@5K and P@50K. Since it is expensive to hire annotators to annotate all these phrases, we estimate the precision scores by randomly sampling 200 phrases from rank lists. Extracted phrases from different methods are shuffled and mixed before presenting to the annotators.</p><p>Task II: Keyphrase Extraction is a classic task to extract salient phrases that best summarize a document <ref type="bibr" target="#b8">[9]</ref>, which essentially has two stages: candidate generation and keyphrase ranking. At the first stage, we treat all compared methods as candidate phrase extractors and evaluate the recall of generated candidates. In each document, the recall measures how many gold keyphrases are extracted in the candidate list. For fair comparison, we preserve the same number of candidates from the rank list of each method for evaluation.</p><p>For the end-the-end performance, we apply the classic TF-IDF model to rank the candidate phrases extracted by different methods. In each document, we follow the standard evaluation method <ref type="bibr" target="#b12">[13]</ref> to calculate the 1 score of the top-10 ranked phrases (F 1 @10). The reported recall and 1 scores are averaged in a macro way across all documents in the same dataset.</p><p>Task III: Phrase Tagging is a fine-grained task that aims to find all occurrences of phrases in sentences. Specifically, it evaluates the extracted phrase spans for each sentence. We treat each phrase mining method as a sentence tagger that identifies starting and ending boundaries of phrases in a sentence. We randomly sample 200 sentences on each dataset and ask three annotators to tag all spans of multi-word phrases. Each sentence is annotated by all annotators independently, and the agreement between human annotations is around 90%. We then pool all annotations together, evaluate the predicted spans, and report the overall precision, recall and F 1 scores. Note that these scores are computed in a micro average fashion following previous work on entity recognition <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We adopt two commonly used datasets from different domains to evaluate all different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? KP20k [29] is a collection of titles &amp; abstracts from Computer</head><p>Science papers-527,090 for training and 20,000 for testing. ? KPTimes <ref type="bibr" target="#b11">[12]</ref> consists of news articles on New York Times from 2006 to 2017, supplemented with 10,000 more news articles from Japan Times. In total, there are 259,923 articles for training, and 20,000 articles for testing. Following Gururangan et al. <ref type="bibr" target="#b15">[16]</ref>, sentence separation and tokenization is conducted with Spacy <ref type="bibr" target="#b16">[17]</ref> for postprocessing. All three tasks are evaluated on the test sets. Statistics of the two datasets are shown in <ref type="table" target="#tab_4">Table 1</ref>. Note that 17% unique keyphrases in the test of KP20k never occur in the corresponding training corpus. On KPTimes the absence ratio is 33%. Hence, the task can be challenging for models relying on phrase frequencies and models rigidly memorizing training phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>We compare the proposed method with existing methods under the same scenario, where no gold annotations for training are available. This leads to three categories: unsupervised phrase mining methods, distantly supervised methods with an existing KB, and pre-trained off-the-shelf toolkits. For each method that requires training (i.e., all the unsupervised and distantly supervised ones), we use the unlabeled documents from the training set for model learning.</p><p>For unsupervised methods we consider:</p><p>? ToPMine <ref type="bibr" target="#b7">[8]</ref>, the state-of-the-art unsupervised phrase mining method building upon statistical features. ? UCPhrase, the proposed method in this work.</p><p>For distantly supervised methods, we use silver labels generated from the Wiki Entities, which is firstly used in <ref type="bibr" target="#b33">[34]</ref>.</p><p>? AutoPhrase <ref type="bibr" target="#b33">[34]</ref> leverages statistics-based phrase classifier and further enhances it with a POS-guided phrasal segmentation model for sentence tagging and phrase frequency rectification. ? Wiki+RoBERTa is a strong baseline that we propose here. It can be viewed as a variant of UCPhrase with the same span prediction framework and the same pre-trained LM as our method but following distant supervision. Also, it uses the output states from the last layer of the pre-trained RoBERTa as feature instead of attention maps. As shown in <ref type="bibr" target="#b23">[24]</ref>, stopping the model training early is an essential intervention for distantly supervised tagging models. To fully unleash the potential of the Wiki+RoBERTa baseline, we manually stop its training process after the first epoch to avoid overfitting. This indeed achieves a better test performance than stopping after more epochs.</p><p>For off-the-shelf toolkits we consider the linguistic-based methods that are pre-trained with labeled pos-tagging or parsing data.</p><p>? PKE [3] is a widely used toolkit for keyphrase extraction. Its phrase mining module is a chunking model based on a supervised POS-tagging model from NLTK <ref type="bibr" target="#b1">[2]</ref> and a set of grammar rules. ? Spacy <ref type="bibr" target="#b16">[17]</ref> is an industrial library with a pre-trained phrase chunking model based on supervised POS tagging and parsing. ? StanfordCoreNLP <ref type="bibr" target="#b26">[27]</ref> is a long recognized NLP package whose chunking model is based on dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Reproduction Details</head><p>For KPTimes, we use the official RoBERTa model pre-trained on documents from the general domain. On the KP20k dataset, we use the "allenai/cs_roberta_base" RoBERTa model <ref type="bibr" target="#b15">[16]</ref> 3 . The model is based on the standard pre-trained RoBERTa model, and then trained on unlabeled Computer Science publications. This domainadapted model performs slightly better on the KP20k dataset than the original model. We adopt the Adam <ref type="bibr" target="#b19">[20]</ref> optimizer with the default parameters for model training. The learning rate is set to 0.001. As described in Algorithm 1, we train the classifier until its performance on the 10% hold-out validation set D drops. Other details have been covered in Section 3. We will publish our data and code base for reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Results</head><p>From <ref type="table" target="#tab_5">Table 2</ref> we can see that UCPhrase achieves the best overall performance on all the three evaluation tasks. The performance gap becomes more vivid as the task becomes more fine-grained.  In the corpus-level phrase ranking task, most methods show very high precision (i.e., ? 95%) on the top 50,000 mined phrases from each dataset. Notably, UCPhrase significantly outperforms the only other unsupervised method ToPMine and is able to perform on par with distantly supervised methods.</p><p>In the document-level keyphrase extraction task, UCPhrase has better recall than most compared methods, demonstrating a coverage of high-quality phrases. Wiki+RoBERTa has slightly better recall on the KP20k dataset (0.1%) within a reasonable range, considering Wiki+RoBERTa has access to hundreds of thousands keyphrases from Wiki Entities. Note that UCPhrase outperforms all the compared methods on the end-to-end performance (i.e., 1 @10), which verifies its value to the application of keyphrase extraction.</p><p>In the sentence-level phrase tagging task, UCPhrase achieves 1 scores of more than 73% on both datasets, showing significant advantages (i.e., &gt; 10% in 1 ) over all the compared methods. This is truly encouraging given the facts that (1) UCPhrase is an unsupervised phrase mining model that requires no human effort, and (2) even human annotators cannot fully agree with each other on some particular phrases, and have around 10% disagreement on this task. This phrase tagging task makes clear that UCPhrase is able to find phrases much more accurately than compared methods. In Section 4.6 we apply comprehensive comparison between different models on real examples, for a more straightforward visualization of the pros and cons of compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation studies</head><p>To gain deeper insights, we apply extensive ablation studies to test model variants from several aspects, as summarized in <ref type="table" target="#tab_6">Table 3</ref>. For supervision, we compare the silver labels generated by unsupervised core phrase mining (core), and those generated by distant supervision with Wikipedia entities (Wiki). For the type of features, we compare the attention map features (attention), and the output states of RoBERTa (embedding).</p><p>Supervision: Core Phrase vs. Distant Supervision. When using the same type of feature, unsupervised models with core phrases as supervision significantly outperform distantly supervised models on most metrics by a clear gap. The better completeness and larger volume of core phrases bring unique advantages in training context-aware tagging models, not to mention the labels are fetched from the corpus for free without relying on an external KB. Moreover, the better diversity of core phrases effectively alleviates the risk of overfitting. It is also worth mentioning that different from the distantly supervised embedding model, the embedding-based model trained with core phrases does not require any manual early stopping to achieve satisfying performance.</p><p>Features: Attention vs. Embedding. When using the same type of supervision, models with attention features are almost always better than embedding-based features. This verifies our intuition that word-identifiable embeddings allow the classifier to easily overfit silver labels, while the surface-agnostic attention features force the model to learn about informative contextual features, and thus having a better ability of generalization.</p><p>Attention: First Few Layers vs. Full Layers. <ref type="table" target="#tab_7">Table 4</ref> compares UCPhrase trained with attention features aggregated from the first 3 layers of RoBERTa and those aggregated from all 12 layers, with intuitions explained in Section 3.2. The two models achieve comparable performance, while the small model only requires 25% resource consumption.</p><p>Alternative Classifiers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Studies</head><p>In spite of the reasonably high quality of the silver labels, we are curious about whether our final span classifier is robust to the noisy silver labels. To this end, we feed the silver labels to the span classifier and investigate the predicted probability scores. <ref type="table">Table 7</ref> presents the silver labels with probabilities below 1% and above 99% respectively. As it shows, our classifier successfully distinguishes high-quality core phrases from noisy spans, including typos (italic font) that happen to be used consistently in some document. The classifier draws a clear line between these two kinds of spans based on their attention features, which reflect their distinct roles in sentences. We have attempted to remove the low-score ones from the silver labels and re-train the classifier, however, the final performance changes little. This further verifies the robustness of our model, and its ability to capture general context features rather than rigid memorization. <ref type="table" target="#tab_10">Table 6</ref> presents sentences tagged with representative methods from each category. As it shows, pre-trained models like Spacy can hardly adapt to a new domain without human annotations. For instance, it fails to recognize "Varshamov bound" as a phrase for recognizing "bound" as a verb. Statistics-based methods like AutoPhrase tend to miss uncommon phrases in the corpus, such as "Varshamov graphs", "finite values", and "global profiles". The widely used distantly supervised methods based on word representations from a pre-trained language model (e.g., RoBERTa) can easily overfit the phrases in the KB, even though we have applied manual early stopping. The consequence of rigid memorization comes in two folds. First, the model can miss a lot of out-of-KB phrases, such as the terminologies in KP20k. Second, it can recognize false phrases just because they have similar surface names with real phrases. In the example from KPTimes, the model recognizes "taxes companies" and "but companies" as two phrases, while "taxes" is used as a verb in this sentence, and "but" is a conjunction word. Overall, the results generated by UCPhrase are more accurate. There is also an interesting case in the example from KPTimes, where RoBERTa and UCPhrase recognize "pharmaceutical companies" as a complete phrase, while Spacy and AutoPhrase think "technology and" is also part of the phrase. It is debatable which one is better: both results can contribute to a high-quality phrase vocabulary. In fact, even human annotators cannot achieve perfect agreement in their independent annotations. Dynamically adjusting the granularity of tagged phrases according to different end tasks remains a valuable research problem for further studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Phrase mining is a long studied task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. Due to the broad applicability of phrases to text-associated tasks, supervision signals would be expensive to obtain for vast domains. Unsupervised approaches have been proposed to extract phrases from many different angles, most importantly, language grammar <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> and text statistics <ref type="bibr" target="#b7">[8]</ref>. Our work utilizes contextualized features from Transformer-based language models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, therefore, lifts the unnecessary requirement of frequency in statistics-based methods and alleviates requirements of expert-crafted grammar rules. Through experiments of three different tasks (i.e., corpus-level <ref type="table">Table 7</ref>: Examples from silver training labels with extremely high and low quality scores ( ? : ) estimated by UCPhrase. The results show that UCPhrase is robust to noises in training labels. KP20k ( ? : ) &gt; 99% ( ? : ) &lt; 1% model identification, data structures, release dates, VLSI design, product development, network flow, finite precision, watermark detection, model selection, path planning, network security, data centers, source code, . . . times fewer, prescriptions implies, algorithms require estimating, significantly improves performance, including discontinuities, significantly reduce power consumption, factors include, considered byTitterington . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KPTimes</head><p>( ? : ) &gt; 99% ( ? : ) &lt; 1% Davis Cup, Ivy League, no-fly zone, Tour Championship, tax returns, City Hall, home runs, detention center, operating system, Ryder Cup, space stations, ice packs, White House, Jersey City, board games, tax cuts, . . .</p><p>PThe percentage, 11th title, departments began telling officers, category includes workers, attacks including, 74th career win, including political, countries including Spain, including banking, including mobile, . . . phrase ranking, document-level keyphrase extraction, and sentencelevel phrase tagging), our method shows great performance improvement over previous methods.</p><p>Another line of research studies on distant supervision signals, such as existing knowledge bases <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>. They typically use knowledge base entries (e.g., Wiki Entities from <ref type="bibr" target="#b33">[34]</ref>) to string-match a corpus to obtain supervision signals in their first step. Such matching does not take into account how n-grams exist in the corpus, and as we show, could lead to partial matching of phrases, thus bringing bias to the phrase mining tool trained (e.g., "heat island effect" is usually matched into "island effect"). Our core phrase mining method, while being unsupervised, looks into the context of each n-gram to find max patterns and is able to find more complete phrases that serve as a better supervision signal to UCPhrase contextualized feature based classifier.</p><p>We use attention maps from pre-trained Transformer-based language models to identify phrases since they carry inter-relation information of tokens <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. Clark et al. <ref type="bibr" target="#b3">[4]</ref> showed that a sufficient amount of linguistic knowledge, such as noun determiners and objects of verbs and prepositions, are captured by attention maps of BERT. Moreover, using only attention maps, one can train a model to perform dependency parsing <ref type="bibr" target="#b3">[4]</ref> and constituency tree construction <ref type="bibr" target="#b18">[19]</ref> relatively well. Our work utilizes this powerful nature of attention maps and treats them as the only feature to identify quality phrases. Furthermore, through comparing with the output states of RoBERTa, we show that using attention is less likely to overfit and has a more robust generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We explore phrase tagging in an unsupervised and context-aware manner. Our proposed method, UCPhrase, shows clear improvement on performance for three quality-measuring tasks on two datasets in different domains. Further experimental studies reveal the strength of our two major components: our unsupervised core phrase mining finds more diverse, complete phrases in context than string-matching from some knowledge bases; our use of attention features unleashes the rich linguistic knowledge contained in pretrained neural language models. By leveraging surface-agnostic context features, our model removes the frequency requirement in statistics-based models and alleviates the overfitting issue in embedding-based models.</p><p>We plan to explore the following directions in future studies. First, our study shows that the combination of silver labels and attention is robust and contains sufficient linguistic knowledge. This idea of unsupervised learning is worth exploring in other text mining tasks, such as coreference resolution <ref type="bibr" target="#b27">[28]</ref>, dependency parsing <ref type="bibr" target="#b20">[21]</ref>, and named entity recognition <ref type="bibr" target="#b29">[30]</ref>. Second, the imperfection of distant supervision calls for a more effective way to incorporate large-scale unlabeled corpus with existing knowledge bases for more accurate prediction and more intelligent reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of our UCPhrase: unsupervised context-aware quality phrase tagging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Algorithm 1 :</head><label>21</label><figDesc>Comparing core phrases with context-agnostic distant supervision. (a) An illustrative example with context. Our core phrases preserve better contextual completeness and discover emerging new concepts introduced in the document. (b) Distributions of the generated silver labels with their occurrences in the corpus. The X-axis represents bins of phrase occurrences in the corpus. The Y-axis (exponential) represents the number of unique phrases in each bin. (c) Distributions of the generated silver labels with their lengths (# of words). UCPhrase: unsupervised model training Input: A corpus of unlabeled documents, { } =1 ; a pre-trained language model (LM); the attention-based lightweight classification model ( ?; ) to be trained. // Generate silver labels as supervision (Sec. 3.1) for each document do P + = ? ( ). Randomly sample remaining spans as negative set P ? . // Generate surface-agnostic features using LM (Sec. 3.2) for each labeled span ? P = { P ? , P + } do , P). // Train a classifier based on features &amp; labels (Sec. 3.3) repeat Sample a minibatch {X P , P } from D . Update model parameters with loss ? ( (X P ; ), P ). until F 1 score on D drops; Output: The trained classification model .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the attention map generated by one of the pre-trained RoBERTa layers, averaged over all attention heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of the Three Evaluation Tasks and their Evaluation Metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The [heat island effect] is from ? The term heat island is also used ? [heat island effect] is found to be ? ? like other [cities including] [New York]? happens in [cities including] ? about [New York]. Core Phrases for Silver Labels unsupervised, per-document, could have noise (e.g., "cities including") Sentence Attention Maps no fine-tuning, one-pass only, captures the sentence structure Train a Lightweight Classifier core phrases vs. random negatives Final Tagged Quality Phrases both frequent &amp; uncommon phrases could correct noise from silver labels ?? The [heat island effect] is from ? The term [heat island] is also used ? [heat island effect] is found to be ? ? like other cities including [New York] ? happens in cities including ? about [New York].</figDesc><table><row><cell></cell><cell></cell><cell>li k e o th e r c it ie s in c lu d in g N e w Y o rk</cell></row><row><cell></cell><cell></cell><cell>like</cell></row><row><cell></cell><cell>?</cell><cell>other</cell></row><row><cell></cell><cell>?</cell><cell>cities</cell><cell>CNN,</cell></row><row><cell></cell><cell>?</cell><cell>including</cell><cell>LSTM,</cell></row><row><cell></cell><cell>?</cell><cell>New</cell><cell>or ?</cell></row><row><cell>??</cell><cell>Pre-trained Transformer LM</cell><cell>York</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>core phrases? Distant Supervision based on Wiki Entities Core Phrase Mining</head><label></label><figDesc>? study about heat [island effect] ? The heat [island effect] arises because the buildings?of their heat [island effect]? Doc2: ? propose to extract core phrases ? robust to potential noise in core phrases ? the surface names of</figDesc><table><row><cell>Doc1:</cell></row><row><cell>Doc1: ?a study about [heat island effect]? The [heat island effect]</cell></row><row><cell>arises because the buildings?of their [heat island effect]?</cell></row><row><cell>Doc2: ?propose to extract [core phrases]? robust to potential</cell></row><row><cell>noise in [core phrases]? the surface names of [core phrases]?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>We can think of them as the canary in the coal mine because of their heat island effects</head><label></label><figDesc></figDesc><table><row><cell>We can</cell><cell>think</cell><cell>of them</cell><cell>as</cell><cell>the can-</cell><cell>ary</cell><cell>in the</cell><cell>coal mine</cell><cell>beause</cell><cell>of</cell><cell>their heat</cell><cell>island effect</cell></row><row><cell>We can</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.20</cell></row><row><cell>think</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of them as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell></row><row><cell>the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>can-ary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in the coal mine because of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05 0.10</cell></row><row><cell>their</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>heat island effect</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Task II. Document-level Keyphrase Extraction3 = 80% Task III. Sentence-level Phrase Tagging80% Task I. Corpus-level Phrase Ranking Extracted Top Phrases</head><label></label><figDesc>? said Richard Healing, a former member of the Transportation Safety Board, who is now a safety consultant.</figDesc><table><row><cell>Corpus</cell><cell></cell><cell></cell></row><row><cell>Doc2: Support Vector Machine is a member of supervised classifiers widely used in information extraction systems... Doc1: Evaluated Model Human Annotators</cell><cell>-Support Vector Machine -information extraction -information extraction systems -supervised classifier -safety consultant -Richard Healing -member of -Transportation Safety Board -used in ?.. Prec. @ 10 =</cell><cell>Doc1 Gold Keyphrases: -Richard Healing -Transportation Safety Board Ranked by TF-IDF -Transportation Safety Board -Richard Healing -safety consultant Tagged phrases as candidates -Richard Healing -former member -Transportation Safety Board Rec. = 100% F 1 @ Human Annotators (* 3): [Support Vector Machine] is a member of [supervised classifiers] widely used in [information extraction systems] . System Prediction: [Support Vector Machine] is a [member of] [supervised classifiers] widely used in [information extraction] systems. Rec. = 66.7%, Prec. = 50%, F1 = 57.2% (average over all annotators)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics on KP20k and KPTimes.</figDesc><table><row><cell>Statistics</cell><cell cols="2">KP20k KPTimes</cell></row><row><cell></cell><cell></cell><cell>Train Set</cell></row><row><cell># documents</cell><cell cols="2">527,090 259,923</cell></row><row><cell># words per document</cell><cell>176</cell><cell>907</cell></row><row><cell></cell><cell></cell><cell>Test Set</cell></row><row><cell cols="2"># documents # multi-word keyphrases # unique # absent in training corpus 4,171 20,000 37,289 24,626</cell><cell>20,000 24,920 8,970 2,940</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results (%) of three tasks for all compared methods on datasets on two domains. P@50K Rec. F 1 @10 Rec. F 1 @10 Prec. Rec. F 1 Prec. Rec. F 1</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Task I: Phrase Ranking Task II: KP Extract.</cell><cell>Task III: Phrase Tagging</cell></row><row><cell>Method Type</cell><cell>Method Name</cell><cell cols="2">KP20k</cell><cell cols="2">KPTimes</cell><cell>KP20K KPTimes</cell><cell>KP20k</cell><cell>KPTimes</cell></row><row><cell cols="5">PKE [3] P@5K P@50K P@5K Pre-trained ---Spacy [17] ---</cell><cell>--</cell><cell cols="2">57.1 12.6 61.9 4.4 54.1 63.9 58.6 56.1 62.2 59.0 59.5 15.3 60.8 8.6 56.3 68.7 61.9 61.9 62.9 62.4</cell></row><row><cell></cell><cell cols="2">StanfordNLP [27] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">51.7 13.9 60.8 8.7 48.3 60.7 53.8 56.9 60.3 58.6</cell></row><row><cell>Distantly Supervised</cell><cell cols="7">AutoPhrase [34] 97.5 96.0 96.5 95.5 62.9 18.2 77.8 10.3 55.2 45.2 49.7 44.2 47.7 45.9 Wiki+RoBERTa 100.0 98.5 99.0 96.5 73.0 19.2 64.5 9.4 58.1 64.2 61.0 60.9 65.6 63.2</cell></row><row><cell>Unsupervised</cell><cell cols="7">TopMine [8] UCPhrase (ours) 96.5 96.5 96.5 95.5 72.9 19.7 83.4 10.9 69.9 78.3 73.9 69.1 78.9 73.5 81.5 78.0 85.5 71.0 53.3 15.0 63.4 8.5 39.8 41.4 40.6 32.0 36.3 34.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of UCPhrase model variants (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KP Extract.</cell><cell></cell><cell cols="3">Phrase Tagging</cell></row><row><cell></cell><cell cols="2">Design Choices</cell><cell></cell><cell cols="2">KP20k</cell><cell cols="2">KPTimes</cell><cell>KP20k</cell><cell></cell><cell>KPTimes</cell></row><row><cell></cell><cell>supervision</cell><cell>feature</cell><cell cols="6">fine-tune Rec. F 1 @10 Rec. F 1 @10 Prec. Rec.</cell><cell>F 1</cell><cell>Prec. Rec.</cell><cell>F 1</cell></row><row><cell>UCPhrase</cell><cell>core</cell><cell>attention</cell><cell>no</cell><cell>72.9</cell><cell cols="3">19.7 83.4 10.9</cell><cell cols="3">69.9 78.3 73.9 69.1 78.9 73.5</cell></row><row><cell>Variants</cell><cell>Wiki Wiki core core</cell><cell>attention embedding embedding embedding</cell><cell>no no no yes</cell><cell cols="2">68.7 73.0 79.3 80.3 19.7 17.7 19.2 19.7</cell><cell>79.4 64.5 78.7 73.9</cell><cell>10.7 9.4 10.2 9.9</cell><cell cols="2">72.1 71.9 72.0 60.9 65.6 63.2 68.4 74.6 71.4 68.6 74.8 71.6</cell><cell>64.1 60.9 55.7 53.3</cell><cell>67.6 65.8 65.6 63.2 64.8 59.9 64.5 59.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of attention feature aggregated from different numbers of Transformer layers, evaluated on KP20k (%).</figDesc><table><row><cell># Layers</cell><cell>KP Extract.</cell><cell cols="2">Phrase Tagging</cell></row><row><cell></cell><cell cols="2">Rec. F 1 @10 Prec. Rec.</cell><cell>F 1</cell></row><row><cell>3 12</cell><cell>72.9 81.8 20.6 19.7</cell><cell cols="2">69.9 78.3 73.9 69.4 76.8 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Exploring LSTM-based classifiers as alternatives based on Tie-or-Break and BIO labeling schemes, evaluated on KP20k (%).</figDesc><table><row><cell></cell><cell>KP Extract. Phrase Tagging</cell></row><row><cell>Classifier</cell><cell>Rec. F 1 @10 Prec. Rec. F 1</cell></row><row><cell cols="2">CNN (default in UCPhrase) 68.1 18.7 LSTM w/ Tie-or-Break 72.4 19.3 68.1 72.3 70.1 69.9 78.3 73.9 LSTM w/ BIO 66.2 18.1 71.0 76.7 73.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>compares our model with the the Tie-or-Break classifier and the BIO classifier as introduced in Section 3.3. Overall, the alternative classifiers have comparable performances, indicating the ability of our proposed method to generalize to different tagging schemes and model architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Sentences tagged with different methods described in Section 4.3. KPTimes Spacy We are interested in improving the Varshamov bound for [finite values] of length and [minimum distance] . We employ a [counting lemma] to this end which we find particularly useful in relation to [Varshamov graphs] . The [United States] , at least theoretically , taxes companies on their [global profits] . But companies with a lot of [intellectual property] -notably [technology and pharmaceutical companies] -get away with paying a fraction of that amount . AutoPhrase We are interested in improving the [Varshamov bound] for finite values of length and [minimum distance] . We employ a [counting lemma] to this end which we find particularly useful in relation to Varshamov graphs . The [United States] , at least theoretically , taxes companies on their global profits . But companies with a lot of [intellectual property] -notably [technology and pharmaceutical companies] -get away with paying a fraction of that amount . RoBERTa We are interested in improving the Varshamov bound for finite values of length and minimum distance . We employ a [counting lemma] to this end which we find particularly useful in relation to Varshamov graphs . The [United States] , at least theoretically , [taxes companies] on their [global profits] . [But companies] with a lot of [intellectual property] -notably technology and [pharmaceutical companies] -get away with paying a fraction of that amount . UCPhrase We are interested in improving the [Varshamov bound] for [finite values] of length and [minimum distance] . We employ a [counting lemma] to this end which we find particularly useful in relation to [Varshamov graphs] . The [United States] , at least theoretically , taxes companies on their [global profits] . But companies with a lot of [intellectual property] -notably technology and [pharmaceutical companies] -get away with paying a fraction of that amount .</figDesc><table><row><cell>KP20k</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and data: https://github.com/xgeric/UCPhrase-exp.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Except for methods that does not report any form of scores for ranking.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://huggingface.co/allenai/cs_roberta_base</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic text classification: A survey of past and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berna</forename><surname>Alt?nel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murat Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</title>
		<meeting>the COLING/ACL 2006 Interactive Presentation Sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PKE: an open source python-based keyphrase extraction toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What Does BERT Look at? An Analysis of BERT&apos;s Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of phrases and structured queries in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David D</forename><surname>Howard R Turtle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 14th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="32" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A nonparametric method for extraction of candidate phrasal terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable Topical Phrase Mining from Text Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanglei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noun-phrase analysis in unrestricted text for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Linguistic terms and concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Finch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Macmillan International Higher Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic recognition of multi-word terms:. the c-value/nc-value method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Frantzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Mima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal on digital libraries</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="130" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ygor</forename><surname>Gallina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B?atrice</forename><surname>Daille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
		<meeting>the 12th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="130" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale evaluation of keyphrase extraction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ygor</forename><surname>Gallina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B?atrice</forename><surname>Daille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020</title>
		<meeting>the ACM/IEEE Joint Conference on Digital Libraries in 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient training of bert by progressively stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12562</idno>
		<title level="m">On the Transformer Growth for Progressive BERT Training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Edmiston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficiently Mining High Quality Phrases from Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bond: Bert-assisted open-domain named entity recognition with distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siawpeng</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1054" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mining quality phrases from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2015 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1729" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using decision trees for conference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">G</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international joint conference on Artificial intelligence</title>
		<meeting>the 14th international joint conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Keyphrase Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An unsupervised model for joint phrase alignment and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="632" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Text chunking using transformation-based learning. In Natural language processing using very large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell P</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="157" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Named Entity Tagger using Domain-Specific Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mining Infrequent High-Quality Phrases from Domain-Specific Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1535" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zipf&apos;s law holds for phrases, not words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake Ryland</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Desu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">Sheridan</forename><surname>Danforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dodds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
