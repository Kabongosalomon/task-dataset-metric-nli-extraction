<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hydra: A System for Large Multi-Model Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Nagrecha</surname></persName>
							<email>knagrech@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<postCode>92023</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Kumar</surname></persName>
							<email>arunkk@eng.ucsd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<postCode>92023</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hydra: A System for Large Multi-Model Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scaling up model depth and size is now a common approach to raise accuracy in many deep learning (DL) applications, as evidenced by the widespread success of multi-billion or even trillion parameter models in natural language processing (NLP) research. Despite their success in DL research and at major technology companies, broader practical adoption of such large models among domain scientists and businesses is still bottlenecked by GPU memory limits, high costs of training or fine-tuning, and low GPU availability, even on public clouds. These resource challenges are further compounded by model selection needs: DL users often need to compare dozens of models with different hyper-parameter combinations and/or neural architectural design choices to suit their specific task and dataset. In this paper, we present HYDRA, a system designed to tackle such challenges by enabling out-of-the-box scaling for multi-large-model DL workloads on even commodity GPUs in a highly resource-efficient manner. HYDRA is the first approach to holistically optimize the execution of multi-model workloads for large DL models. We do this by adapting prior "model-parallel" execution schemes to work with scalable parameter offloading across the memory hierarchy and further hybridizing this approach with task-parallel job scheduling techniques. HYDRA decouples scalability of model parameters from parallelism of execution, thus enabling DL users to train even a 6-billion parameter model on a single commodity GPU. It also fully exploits the higher speedup potential offered by task parallelism in a multi-GPU setup, yielding near-linear strong scaling and in turn, making rigorous model selection perhaps more practical for such models. We evaluate end-to-end performance by fine-tuning GPT-2 for language modeling. We find that HYDRA offers between 50% and 100% higher training throughput than even the best settings of state-ofthe-art industrial frameworks such as DeepSpeed and GPipe for multi-large-model training.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The high profile success of DL at big technology companies has led to high interest in adopting state-of-the-art DL models at smaller companies in the Web, enterprise, and healthcare sectors, as well as among domain scientists and in digital humanities. Large neural architectures such as Transformers and other so-called "foundation models" <ref type="bibr" target="#b4">[5]</ref> now dominate NLP and have multiple billions of parameters, e.g., BERT-Large <ref type="bibr" target="#b7">[8]</ref>, GPT-3 <ref type="bibr" target="#b44">[45]</ref>, and Megatron-LM <ref type="bibr" target="#b42">[43]</ref>. Interest in such large models is also growing in computer vision (e.g., <ref type="bibr" target="#b8">[9]</ref>) and for tasks bridging NLP and tabular data <ref type="bibr" target="#b50">[51]</ref>. Moreover, the popularity of transfer learning using base models provided by public libraries such as HuggingFace <ref type="bibr" target="#b9">[10]</ref> is powering a massive shift toward "low-data large-model" training setups <ref type="bibr" target="#b4">[5]</ref>. Alas, three key systems-and economics-related bottlenecks are impeding the adoption of such powerful models by DL users outside of big technology companies: (1) GPU memory capacity trailing DL model sizes <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b1">(2)</ref> high computational/cost/energy footprints of GPU clusters, and (3) high demand for GPUs relative to supply, including on public clouds. Thus, ensuring overall resource efficiency, as well as enabling DL users to make do with fewer GPUs and/or cheaper GPUs is a pressing research concern to ensure that the potential of large-scale DL models are accessible to the many, not just the few.</p><p>The main approach practiced today to mitigate bottleneck (1) above (viz., GPU memory limits) is to partition the model's neural computational graph across multiple GPUs to lower its memory footprint on each GPU. This form of execution, known as model-parallelism, is increasingly popular <ref type="bibr" target="#b2">[3]</ref>. However, model-parallelism suffers from two fundamental issues.</p><p>First, sequential dependencies within the neural architecture causes resource idling (busy waiting) and thus, GPU underutilization. <ref type="figure">Figure 1</ref>(C) illustrates how devices can be blocked while waiting for intermediate data to be passed forward/backward by earlier stages of the model. However, this issue is mitigated to an extent by pipeline-parallelism, which shuttles different batches of data through different stages of the model in parallel. Another technique known as tensor-parallelism, which divides a model width-wise, can also help <ref type="bibr" target="#b21">[22]</ref>. We explain more about these techniques in Section 4. Nevertheless, some significant amount of resource idling is still inevitable in such techniques if one must preserve correctness (i.e., no heuristic approximations).</p><p>Second, most DL users do not train just one model in a vacuum but rather do it as part of a larger multi-model execution scenario. Model selection needs such as tuning hyper-parameters and/or fine-tuning some layers of the network is needed to control the balance of overfitting and underfitting on a new task and dataset <ref type="bibr" target="#b41">[42]</ref>. That leads to multi-model execution. Multi-tenant clusters also see multiple models being trained together. In such scenarios, task-parallelism, viz., a job scheduler assigning different models to different workers, helps raise throughput of execution. But pure modelparallelism works directly against task parallelism in such cases. Raising the per-model footprint to multiple GPUs reduces the number of tasks one can run in parallel on a given cluster and/or forces users to raise their cluster sizes by trying to get even more (expensive) GPUs.</p><p>Example. Consider a political scientist building a text classifier for sentiment analysis of tweets to understand polarization between gun rights and gun control supporters in the US. They download a state-of-the-art GPT-2 model from HuggingFace to fine-tune it. They decide to compare a few different learning rates and optimizers with a grid-search, leading to 48 different model configurations to train. Using an AWS P3 node in the N. Virginia region that offers Tesla V100 GPUs, they first try to train one model on a single 16GB GPU ($3.06/hr). Alas, the model's size causes out-of-memory (OOM) errors, with both PyTorch and TensorFlow crashing. So, they switch to 4-GPU node to train it in a model-parallel manner, costing $12.24/hr. But then they realize that fine-tuning even for a few epochs could take multiple hours and grid search in a serial fashion (one model after another) would be too slow and take weeks. They consider manually overlaying task-parallelism on top of model-parallelism, costing them up to $590/hr. But AWS rate-limiting policies prohibits them from obtaining 192 GPUs, forcing them to either move up to a much more expensive GPU and/or suffer much longer runtimes. Anecdotally, these sorts of frustrations are now common among DL users.</p><p>Overall, we observe that today's DL systems have a dichotomy of model-parallelism and taskparallelism for multi-large-model DL workloads. This leads to substantial resource idling and GPU underutilization, which in turn leads to higher runtimes, costs, and energy footprints.</p><p>In this paper, we start with a simple insight: the above dichotomy is a false dichotomy and we devise an approach that enables simultaneous task-parallel and model-parallel training of large DL models. We note that a key issue with today's model-parallelism is that it forces users to get multiple GPUs simply to store a model in the aggregate multi-GPU memory. We remove that bottleneck from first principles by using a "spilled" execution scheme that enables model-parallel scalability without the need for multiple GPUs. We do so by automatically rewriting a full model into shards (or sub-models) and promoting and demoting such shards between GPU memory and DRAM. This allows us to support very large feedforward models (e.g. Transformers, CNNs, MLPs) on even just a single GPU, decoupling scalability from parallelism. We leave non-feedforward-architectures such as graph neural networks and recurrent nets to future work.</p><p>Building on top of our above style of model spilling, we devise a novel hybrid of task-parallelism and model-parallelism we call Shard Alternator Parallelism (SHARP). SHARP offers the advantage of  <ref type="figure">Figure 1</ref>: Simplified illustration of training three models for a single minibatch with various techniques. We use {model}_{shard} format to describe fine-grained execution of shards; the additional postfix {microbatch} is used for pipeline parallelism. The suffix F or B indicates forward or backward pass. With SHARP, we exploit the efficiency of task parallelism and combine it with the scalability and fine-grained optimization of model parallelism to minimize runtimes and idling.</p><p>high throughput and compute-scalability (exploiting the higher degree of parallelism in multi-model workloads) of task-parallelism but does not suffer its disadvantage of needing to fit a full model into a GPU's memory. Likewise, SHARP offers the advantage of model-scalability of model-parallelism (not needing to fit a full model in a GPU's memory) but does not suffer its disadvantage of sequential dependencies leading to low throughput and poor compute-scalability.</p><p>We implement our above techniques into a system we name HYDRA on top of PyTorch. We offer it as an open-source library available under Apache License v2.0. We demonstrate HYDRA's benefits for multi-large-model DL workloads by performing a grid search to fine-tune GPT-2 for language modeling on the WikiText-2 <ref type="bibr" target="#b28">[29]</ref> dataset (available under Creative Commons License) on two different GPU setups. We find that HYDRA enables us to surpass the state-of-the-art perplexity results, while offering 1.5-4.8X faster runtimes and between 50% and 100% higher training throughput compared to DeepSpeed <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref> and GPipe <ref type="bibr" target="#b17">[18]</ref>, two state-of-the-art industrial-strength tools for large-model DL training. We also show that HYDRA is able to scale up DL model sizes on a commodity GPU by training a 6-billion parameter model on a 16GB GPU, while the other tools crash at much smaller model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Details</head><p>We now describe the interface and implementation details of HYDRA. HYDRA is provided to users as an open-source library available under the Apache License. Using HYDRA is relatively simple -it acts as a drop-in replacement for the typical PyTorch training loop, acting directly on top of PyTorch modules. This eases integration and adoption considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Interface</head><p>HYDRA takes as input a list of PyTorch models, PyTorch dataloaders, and training job specifications (e.g. loss functions and hyperparameters), then orchestrates execution. HYDRA automatically generates partitioning strategies and execution schedules with minimal user input. We provide more detail on the API usage in supplementary materials. Pretrained model libraries such as HuggingFace integrate easily with minimal development overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Partitioner</head><p>HYDRA begins by analyzing the memory footprint of each user-provided model with respect to GPU memory bounds. We introduce a simple automated partitioner, described in Algorithm 1, that runs a Algorithm sample minibatch through the model in a pilot pass and introduces "cut-points" when GPU memory is overloaded. In this way, HYDRA can shard a model into subgraphs of the original architecture's neural computational graph. These partitions are HYDRA's equivalent of model-parallel shards. The user is then provided with logging output informing them where their model was partitioned. So, in future runs with the same architecture they can reuse the same partitioning directly without a new pilot pass. The pilot pass also provides us with runtime statistics for future use.</p><p>Note that our algorithm assumes that the model graph is a chain architecture (sequence of layers). This structure suffices for most large-model architectures such as Transformers, CNNs, and MLPs. Recurrent and graph neural networks are out of scope for HYDRA. In model parallel execution, the generated shards would be placed on different GPUs to arrange the model across a network of devices. However, as we previously discussed, this execution strategy drives up compute requirements and minimizes the degree of task parallelism we can employ in multi-model workloads. As such, we now look for a novel execution strategy that will enable us to run our shards even if there is only one GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spilling</head><p>Spilling is a memory hierarchy utilization technique from the relational database management space that enables large data to be processed with low memory. We adapt this technique to enable scalable and flexible large-model execution. Essentially, we "chunk" model execution into sharded stages according to a partitioning scheme, then sequentially promote and demote model partitions and intermediate data between DRAM and GPU memory. <ref type="figure" target="#fig_1">Figure 2</ref>(A) illustrates. Because spilling directly replicates model parallel execution, and model parallel execution is known to be mathematically equivalent to standard execution <ref type="bibr" target="#b2">[3]</ref>, spilling is also mathematically equivalent to standard execution.</p><p>Our approach bears some resemblance to previous offloading designs explored in works such as ZeRO-Infinity <ref type="bibr" target="#b36">[37]</ref> and SwapAdvisor <ref type="bibr" target="#b16">[17]</ref> but generalizes the concept further to enable flexible multi-model scheduling. We discuss the differences in depth in Section 4.</p><p>Each shard is loaded to GPU memory twice, once during the forward pass and once during the backward pass. Backpropagation requires reuse of activations generated during the forward pass, but this would substantially increase CPU-GPU communication overheads. Instead, we make use of gradient checkpointing <ref type="bibr" target="#b6">[7]</ref>, saving activations at shard boundaries and recomputing in-shard intermediates during the backward pass. A similar approach was used to reduce memory bloat in GPipe <ref type="bibr" target="#b17">[18]</ref>. Even with checkpointing, communication latencies can be substantial. In our initial evaluations, naive spilling incurred a 3X overhead versus model parallel execution using fast GPU-GPU interconnects. To mitigate this, we use double buffering, a latency-hiding technique, to overlap communication with compute by prefetching shard parameters to a buffer space on the GPU while a different shard is still executing. This buffer space can be relatively small, as model parameters tend to be less than 10% of the model's overall memory footprint during execution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>. Empirical evaluations training GPT-2 on a single Tesla V100 with our approach took only 15% longer than model parallelism on 4 of these GPUs. Likewise, spilling on a cheaper K80 GPU was only 80% slower than model parallel execution on 8 of these GPUs. The slowdown factor is dependent on many things, including CPU-GPU interconnect speed, GPU-GPU interconnect speed, GPU memory, and GPU's processing speed.</p><p>Spilling enables us to scale to larger-than-GPU-memory models even on a single GPU as <ref type="figure" target="#fig_1">Figure  2</ref>(B) illustrates. More critically for HYDRA, being able to train large models even with just one GPU via sharding and spilling enables us to exploit task parallelism to its fullest extent. Since task parallelism offers linear strong scaling, HYDRA can hide the slowdowns we noted earlier in multi-GPU environments, surpassing model parallelism. Thus, spilling is beneficial for both the low-resource user (by enabling scaling) and for the high-resource user (by enabling maximal parallelism).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Shard Alternator Parallelism</head><p>Shard Alternator Parallelism, or SHARP for short, is our hybridization of spilling with task parallelism. We identify several desirable characteristics of task parallelism: zero-synchronization parallel execution and linear strong scaling when there are more tasks than processors. These are qualities we wish to preserve in our hybridization. However, we also identify a drawback: poor heterogeneous scheduling efficiency. When tasks have different execution lengths, task parallelism can only maximize parallelism for a limited period of time. Using fine-grained parallelism enables us to work around this issue so long as we have sufficient tasks to choose from. <ref type="figure">Figure 1</ref> illustrates these tradeoffs.</p><p>With SHARP, we aim to combine the scalability and fine-grained optimization of model parallelism with the lower resource idling of task parallelism. Note that this is only possible through the flexibility of spilling. To make the most of fine-grained parallelism, we must understand how to schedule multiple models at a fine-grained, sub-model level. This is especially critical for extreme scale models, where each individual minibatch potentially introduces hundreds of sharded execution tasks. After every shard completion, we must select a model to provide a shard for the newly freed device. Depending on the workload, the user could have one of several different scheduling objectives. In batched multi-model jobs, such as model selection, which is our focus, individual model training latency is less critical than overall completion time of the workload. We formalize the scheduling problem with completion time as the objective as an MILP in supplementary materials.</p><p>Using an optimal solver such as Gurobi <ref type="bibr" target="#b13">[14]</ref> for this task is not practical given the sheer number of shard execution tasks in our setting (even in the millions). Instead, we look for a fast and easy-toimplement dynamic scheduler. Intuitively, we can identify two settings that our scheduler encounters when training batched multi-model jobs. Initially, the workload will likely be have more model-tasks than GPUs. It is easy to maximize throughput and utilization in this setting, as every processor can be kept busy with a model. Over time though, tasks will complete, and there will be fewer tasks than devices. <ref type="figure">Figure 1(B)</ref> illustrates how this reduces the upper bound of our achievable degree of task parallelism.</p><p>We can minimize time spent in this reduced-efficiency setting by completing all tasks at approximately same time. This is a known property of "longest-remaining-time", or LRTF, schedulers <ref type="bibr" target="#b1">[2]</ref>. Unlike  <ref type="figure">Figure 3</ref>: Critical statistics recorded from language-modeling GPT-2 model selection jobs. We run a manual hybrid task parallelism over ZeRO-3 to simulate manual user-set task parallelism. GPipe and naive model parallelism crash due to GPU memory errors when run on 4 K80s or 2 V100s for this job, so we are not able to overlay manual task parallelism for those two approaches.</p><p>standard LRTF implementations which run tasks continuously with occasional pre-emptions, we treat each individual shard as its own atomic task with the time-cost being defined by the total model's running time. This maintains our desired scheduling behavior (even task completion times, maximal processor utilization) while fitting into the sharded nature of spilled execution. We name our dynamic greedy scheduler Sharded-LRTF. Empirical evaluations of Sharded-LRTF in our supplementary material demonstrate that Sharded-LRTF produces near-optimal execution times thanks to its finegrained scheduling while incurring minimal scheduling overheads. With these techniques -spilling, SHARP, and Sharded-LRTF -HYDRA is able to optimize multi-large-model workloads holistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Modeling Experiments</head><p>Dataset and Workload. We now evaluate HYDRA's performance on real-world workloads. Language modeling is a core task in the NLP space and fine-tuning pretrained models for language modeling is a common workload. We use HYDRA to run a model selection job for fine-tuning opensource HuggingFace GPT-2 models [10] on the WikiText-2 dataset <ref type="bibr" target="#b28">[29]</ref>. The workload compares 12 different hyperparameter configurations, obtained by a grid search with 2 batch sizes {16, 8} and 6 learning rates {0.0003, 0.0001, 0.00005, 0.00006, 0.00001, 0.00002} inspired by real-world GPT-2 fine-tuning jobs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref>. We use a context length of 512 tokens. We do not freeze parameters or take any steps to reduce the computational workload -we want HYDRA to undergo the full load of training.</p><p>We compare against two state-of-the-art industrial strength tools, ZeRO-3 <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> provided through the Microsoft DeepSpeed library and a PyTorch implementation of GPipe <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. <ref type="figure">Figure 3</ref> shows our end-to-end performance benchmarked against those two tools, a manual task parallel hybrid on ZeRO-3, and base model parallelism. The sheer scale of large language models also raises concerns of energy consumption and running costs. So, we report on these metrics in addition to the traditional runtime/utilization numbers. One of our core aims in this paper is to demonstrate HYDRA's usability on accessible hardware. Prior works demonstrating techniques for large-model training (e.g. Megatron <ref type="bibr" target="#b42">[43]</ref>, ZeRO-3) have generally focused on large-cluster, high-performance hardware configurations with expensive Nvidia DGX-2 servers. Our experiments are all run on AWS to create a reproducible environment. While HYDRA's techniques could certainly be applied to larger, more powerful hardware configurations, we do not focus on these settings in this paper.</p><p>End-to-End Results. We run two sets of jobs, one on 8 K80 GPUs, and another on 4 Tesla V100s. In both settings, we find that HYDRA reports the lowest execution times, costs, and energy consumption. Compared to ZeRO-3, HYDRA is 2.5X faster out-of-the-box on 8 K80s and 3.5X faster on 4 V100s. Applying a manual task parallel overlay on top of ZeRO-3 improves its performance but it still falls behind HYDRA's efficiency, demonstrating that our approach of integrating task parallelism from the ground-up outperforms top-down hybridizations. The closest competitor is GPipe, which makes  <ref type="table">Table 1</ref>: Fine-tuned model accuracy compared to zero-shot GPT-2 <ref type="bibr" target="#b35">[36]</ref> and BERT-Large-CAS <ref type="bibr" target="#b45">[46]</ref>. We only fine-tune for one epoch, but HYDRA could be easily be use to run more extensive model selection jobs or even build new architectures.</p><p>use of a fast NVLink connector for GPU-GPU communication. HYDRA's use of GPU-CPU-GPU communication with spilling should disadvantage it; yet HYDRA reports 50-90% lower runtimes and comparable or up to 2X lower energy consumption. Naive model parallelism produces by far the worst performance, about 4X slower than HYDRA in both settings.</p><p>We initially planned to benchmark against Megatron-style 3D parallelism <ref type="bibr" target="#b33">[34]</ref>, but based on their GitHub repository, we found that the only readily accessible implementation of 3D parallelism, provided by Microsoft's DeepSpeed library, is not yet usable for out-of-the-box model training <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> and is restricted to a limited set of training examples. Since our aim is to compare to typical large-model model selection options available to general DL users, Megatron-style 3D parallelism is not yet a practical candidate for comparison.</p><p>Accuracy. <ref type="table">Table 1</ref> compares the accuracy of our final model, with selected learning rate 0.0003 and batch size 8, to a few published examples. The full results of each configuration are available in supplementary materials. Please note that the aim of this experiment is not to claim that we have a better model than GPT-2. This is not a fair comparison -we are reporting against a zero-shot version of GPT-2. Fine-tuning will naturally improve results. We only report accuracy to demonstrate that HYDRA can be used to produce state-of-the-art results and advance DL research and practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Over the past several years, many systems have been released to support distributed execution for large-model training. Unlike prior approaches, HYDRA exploits task parallelism -it is the first system to holistically optimize multi-large-model training.</p><p>Alternative approaches to model parallelism (e.g. tensor parallelism) shard models in a more complex fashion, partitioning individual layers into pieces rather than dividing a model into shards. This increases complexity substantially but opens up more possibilities for parallel execution. Indeed, ZeRO <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> uses tensor parallelism in combination with data parallelism to offer higher training efficiency. We note, however, that tensor parallelism's complexity increases communication overheads and per-architecture implementation effort, especially when compared with the simplicity of spilling. In either case, these techniques are orthogonal to our goal of exploiting task parallelism in multimodel workloads. We leave it to future work to hybridize these techniques, but anticipate that communication challenges will be a challenge when combining tensor parallelism with spilling.</p><p>Hybridizations between model parallelism and data parallelism <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref> are now widely used to improve large-model training performance. ZeRO, for example, combines the multi-GPU requirements of intra-layer (tensor) parallelism with the multi-GPU requirements of data parallelism, eliminating the memory bloat of traditional data parallelism. Empirical evaluations with ZeRO demonstrate that it offers substantially better performance than naive model parallelism along with better scaling. However, the communication overheads of data parallelism weigh heavily on its performance, especially when compared with zero-synchronization task parallelism. Moreover, data parallelism requires the user to treat a training hyperparameter (batch size) as a control for efficiency in addition to model accuracy, which can be problematic in model selection workloads. We note that ZeRO's data parallelism could be hybridized with HYDRA to address the most substantial weakness of task parallelism -poor efficiency when there are fewer models than processors. We leave this additional hybridization to future, as explained further in Section 5. Model-task hybrids were initially proposed in a short abstract presented at a non-full length (2 page) venue <ref type="bibr" target="#b29">[30]</ref>. This paper expands those concepts into a complete problem setting with a full solution by fleshing out hybrid model-task parallelism along with a thorough empirical evaluation on real DL workloads.</p><p>Pipeline parallelism is one of the most popular modifications of model parallel execution. GPipe <ref type="bibr" target="#b17">[18]</ref> proposed using the sharded model as a staged-out pipeline, and shuttle mini-batch partitions, known as micro-batches, through the model. While this increases utilization and throughput over model parallelism, the bi-directional nature of model execution (i.e. prediction and backpropagation), forces pipeline flushes between directional changes. Other works such as PipeDream explore asynchronous pipelining <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b33">34]</ref>, a memory-efficient alternative to standard pipelining. However, these approaches are known to introduce accuracy degradation as they re-order execution stages of the model to minimize memory usage. We do not compare to asynchronous approaches in this paper since accuracy is a critical metric in model selection workloads -introducing any tradeoff between accuracy and efficiency complicates the objectives of model selection workloads. As such, we only compare to "exact" parallel approaches that are mathematically equivalent to standard execution.</p><p>Tensor offloading systems such as SwapAdvisor <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> enable large models to be trained on a single GPU. Other approaches such as ZeRO-Infinity <ref type="bibr" target="#b36">[37]</ref> and L2L <ref type="bibr" target="#b34">[35]</ref> introduce similar designs with further extensions, such as ZeRO-Infinity's CPU-based parameter updates and L2L's Transformer block-swapping design. All of these systems are heavily optimized for single-model execution, where GPU availability for a model is essentially guaranteed across the course of offloaded execution. Hybrid model-task parallelism requires the ability to temporarily "pause" model execution partway in favor of a different model. L2L in particular is restrictive in that it only works for Transformers. Both L2L and SwapAdvisor are only capable of using a single GPU and not targeted towards multi-GPU environments. The specialized nature of these designs prevents them from working in the more general context of a multi-model executor, though they can be beneficial for single-model execution.</p><p>Spilling's flexibility and generality are critical for our hybrid model-task parallelism, and it cannot be replaced by a different offloading design.</p><p>Parallelization strategy search tools such as FlexFlow <ref type="bibr" target="#b21">[22]</ref> and Alpa <ref type="bibr" target="#b51">[52]</ref> combine a variety of parallel execution strategies using simulators and solvers to identify a near-optimal approach to distributing a model architecture across devices. These approaches do not consider the possibility of task parallelism, instead optimizing each model individually. HYDRA could potentially be hybridized with these tools in the future to enable more holistic optimization for multi-model workloads, especially in cases where there are more devices than models.</p><p>Reducing model memory footprints has received much attention in DL systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Model quantization <ref type="bibr" target="#b18">[19]</ref> in particular has been a popular technique for reducing memory footprints at inference time. The goal of such systems is orthogonal to our own, and memory footprint reduction techniques could be integrated into HYDRA in the future. One system <ref type="bibr" target="#b49">[50]</ref> explores the possibility of transferring hyper-parameter selections from small models to larger models. Our focus is broader, tackling multi-large-model execution in general. Other work on machine teaching <ref type="bibr" target="#b46">[47]</ref> and data distillation <ref type="bibr" target="#b47">[48]</ref> aims to minimize the memory footprints of data, but these techniques address a different aspect of memory in DL systems.</p><p>Other optimizations for DL systems that exploit multi-task execution, e.g., systems such as Model-Batch <ref type="bibr" target="#b32">[33]</ref>, Cerebro <ref type="bibr" target="#b23">[24]</ref>, SystemML <ref type="bibr" target="#b3">[4]</ref>, Krypton <ref type="bibr" target="#b30">[31]</ref>, and ASHA <ref type="bibr" target="#b25">[26]</ref>. ModelBatch raises GPU utilization by altering the DL tool's internal execution kernels. Cerebro proposes a hybrid parallelism scheme named MOP combining task-and data-parallelism, akin to (but different from) SHARP's hybrid model-task parallelism. SystemML also hybridizes task-and data-parallelism, but for classical ML workloads rather than DL. Krypton applies task parallelism to multi-inference workloads with CNNs. ASHA is a new hyperparameter tuning algorithm that accounts for cluster size. None of them tackle larger-than-GPU-memory models, which is our focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work &amp; Ethical Implications</head><p>While HYDRA already is already the most resource-efficient among the systems we benchmarked on multi-large-model workloads, there are several areas for potential improvement. For example, spilling still has communication latencies. Although this tends to be outweighed by the task parallel speedups that spilling enables, it is can be a bottleneck in some cases. Making use of optimized low-level CUDA data transfer kernels has been shown to improve offloading performance in prior works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35]</ref>. We only use PyTorch-provided communication commands which helps with compatibility as PyTorch develops in the future, but hurts efficiency since they are not optimized for our use-case.</p><p>Another current limitation of HYDRA is that it inherits one of the restrictions of task parallelism; if there are fewer models than GPUs (e.g. single model training), then the degree of parallelism offered by Shard Alternator Parallelism is bounded by the number of models. Hybridizing with data parallelism, maybe even offloaded data parallelism like ZeRO-3, could enable us to optimize for this setting as well. We leave such complex hybrid-of-hybrid systems to future work.</p><p>In the current version of HYDRA we do not exploit spilling to disk and we do not yet support multi-node environments, which is typically needed for very large datasets. We will explore these optimizations in future work. HYDRA can also be used for inference with large models (not just training), although we have not explicitly optimized execution for that setting.</p><p>By and large, the ethical implications of HYDRA are mostly positive. We enable scalability for single GPU users, democratize access to large-scale models, and improve efficiency for multi-GPU users. This reduces the energy footprint of model selection, enables faster model development, and encourages replicable model selection practices. That being said, increased accessibility to powerful large-scale DL models must also be paired with increased caution and responsibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Building larger-than-GPU-memory DL models is a pressing need for researchers and practitioners. Such large models are increasingly being deployed in numerous applications outside of technology companies. Unfortunately, the computational demands of such models are impeding rigorous model selection practices such as hyperparameter searches or careful fine-tuning. To tackle this problem, we present HYDRA, a new system for multi-large-model DL training. We present the first-known hybrid of model-parallelism with task-parallelism to enable highly resource-efficient multi-large-model training on a GPU cluster, as well as out-of-the-box model scalability with even just one GPU. Overall, by optimizing multi-large-model workloads more holistically, our work helps make modern DL faster, cheaper, and more accessible to diverse user bases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A) Temporal schematic of a spilled forward pass. B) Demonstration of spilling's scalability versus popular techniques for single-GPU large-model training. We train scaled up GPT-2 models using a batch size of 1 and context length of 512 to explore the maximum trainable model size using different DL systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1</head><label></label><figDesc>Dynamic model partitioning algorithm.</figDesc><table><row><cell>Release all memory consumed by L[j]</cell></row><row><cell>Append i to A</cell></row><row><cell>end for</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note>Input: Model as a sequence of m layers L; data mini-batch B; GPU G Output: Array of partition indices A Append 0 to S for i = 0 to m ? 1 do Place L[i] and B on G B ? Forward pass through L[i] with B T ? New tensor with same shape as B Backpropagate T through L[i] without releasing memory if G out of memory then Append i to S for j = 0 to i ? 1 do</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Examples -transformers 2.0.0 documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Examples</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Operating Systems: Three Easy Pieces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
	<note>Arpaci-Dusseau Books, 1.00 edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Demystifying parallel and distributed deep learning: An in-depth concurrency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno>abs/1802.09941</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Systemml: Declarative machine learning on spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deron</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">V</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faraz</forename><surname>Makari Manshadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niketan</forename><surname>Pansare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthold</forename><surname>Reinwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><forename type="middle">R</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><forename type="middle">C</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirish</forename><surname>Tatikonda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1425" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hello, it&apos;s gpt-2 -how can i help you? towards the use of pretrained language models for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe?</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Training Deep Nets with Sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ampnet: Asynchronous model-parallel training for dynamic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maik</forename><surname>Riechert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Webster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Russian gpt-2. GitHub repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Grankin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Memory-Efficient Backpropagation Through Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audr?nas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">LLC Gurobi Optimization. Gurobi Optimizer Reference Manual</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pipedream: Fast and efficient pipeline parallel DNN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<idno>abs/1806.03377</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Module names gpt2modelpipe &amp; paralleltransformerlayerpipe is hardcoded in deepspeed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1341" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">GPipe: Efficient Training of Giant Neural Networks using Pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1811.06965</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Checkmate: Breaking the memory wall with optimal tensor rematerialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>abs/1910.02653</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>SOSP &apos;19</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">torchgpipe: On-the-fly pipeline parallelism for training giant models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungsub</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungryong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyuk</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boogeon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cerebro: A layered data platform for scalable deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supun</forename><surname>Nakandala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Side</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advitya</forename><surname>Gemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Nagrecha</surname></persName>
		</author>
		<ptr target="OnlineProceedings.www.cidrdb.org" />
	</analytic>
	<monogr>
		<title level="m">11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient Rematerialization for Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Svitkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Vee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15172" to="15181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<title level="m">Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chimera: efficiently training large-scale neural networks with bidirectional pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training deeper models by gpu memory optimization on tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ML Systems Workshop in NIPS</title>
		<meeting>of ML Systems Workshop in NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Model-parallel model selection for deep learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Nagrecha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2929" to="2931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Incremental and approximate computations for accelerating deep cnn inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supun</forename><surname>Nakandala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Nagrecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Papakonstantinou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Memoryefficient pipeline-parallel dnn training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accelerating model search with model batching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fourth Conference on Machine Learning and Systems (ML-Sys&apos;18)</title>
		<meeting>Fourth Conference on Machine Learning and Systems (ML-Sys&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno>abs/2104.04473</idno>
	</analytic>
	<monogr>
		<title level="j">Efficient Large-Scale Language Model Training on GPU Clusters. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Training large neural networks with constant memory using a new execution algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharadwaj</forename><surname>Pudipeddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maral</forename><surname>Mesmakhosroshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujeeth</forename><surname>Bharadwaj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Zeroinfinity: Breaking the gpu memory wall for extreme scale deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Zero-offload: Democratizing billion-scale model training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><forename type="middle">Yazdani</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A gpt-2 language model for biomedical texts in portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa Terumi Rubel</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Vitor Andrioli De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Bonescki Gumiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emerson</forename><forename type="middle">Cabrera</forename><surname>Paraiso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Activation checkpointing breaks for some layers in pipelinemodule. GitHub repository</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding Machine Learning: from Theory to Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shaleve-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Low-memory neural network training: A technical report. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nimit Sharad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">Richard</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Language Models are Few-Shot Learners. CoRR, abs</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Language models with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based algorithms for machine teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Nagrecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1387" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1811.10959</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Dataset Distillation. CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pipemare: Asynchronous pipeline parallel dnn training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Tabert: Pretraining for joint understanding of textual and tabular data. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12023</idno>
		<title level="m">Automating interand intra-operator parallelism for distributed deep learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
