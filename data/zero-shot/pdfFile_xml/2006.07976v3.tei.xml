<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of CST</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localizing persons and recognizing their actions from videos is a challenging task towards high-level video understanding. Recent advances have been achieved by modeling direct pairwise relations between entities. In this paper, we take one step further, not only model direct relations between pairs but also take into account indirect higher-order relations established upon multiple elements. We propose to explicitly model the Actor-Context-Actor Relation, which is the relation between two actors based on their interactions with the context. To this end, we design an Actor-Context-Actor Relation Network (ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and an Actor-Context Feature Bank to enable indirect relation reasoning for spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets show the advantages of modeling actor-context-actor relations, and visualization of attention maps further verifies that our model is capable of finding relevant higher-order relations to support action detection. Notably, our method ranks first in the AVA-Kinetics action localization task of ActivityNet Challenge 2020, outperforming other entries by a significant margin (+6.71 mAP). The code is available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Spatio-temporal action localization, which requires localizing persons and recognizing their actions from videos, is an important task that has drawn increasing attention in recent years <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. Unlike object detection which can be accomplished solely by observing visual appearances, activity recognition usually demands for reasoning about the actors' interactions with the surrounding context, including environments, other people and objects. Take <ref type="figure">Fig. 1</ref> as an example. To recognize the action "ride" of the person in the red bounding box, we need <ref type="bibr">Figure 1</ref>. We contrast our Actor-Context-Actor relation modeling with existing relation reasoning approaches for action localization. Reasoning relations between pairs of entities may not always be sufficient for correctly predicting the action labels of all individuals. Our method not only reasons relations between actors, but also models connections between different actor-context relations. As an illustration, the relation between the blue actor and the steering wheel (drive) serves as a crucial clue for recognizing the action being performed by the red actor (ride).</p><p>to observe that he is inside a car, and there is a driver next to him. Therefore, most recent progress in spatio-temporal action detection has been driven by the success of relation modeling. These approaches focus on modeling relationships in terms of pairwise interactions between entities.</p><p>However, it is not always the case that relations between elements can be formulated in terms of pairs; often, higherorder relations provide crucial clues for accurate action detection. In <ref type="figure">Fig. 1</ref>, it is difficult to infer the action of the red actor given only its relation with the blue actor, or only with the scene context (steering wheel). Instead, in order to identify that the red actor performs the action "ride", one has to reason over the interaction between the blue actor and the context (drive). In other words, it is necessary to capture the implicit second-order relation between the two actors based on their respective first-order relations with the context.</p><p>There were previous works that employ Graph Neural Networks (GNNs) to implicitly model higher-order interactions between actors and contextual objects <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b9">10]</ref>. However, in these approaches, an extra pre-trained object detector is required, and only located objects are used as context. Since bounding-box annotations of objects in spatio-temporal action localization datasets are generally not provided, the pre-trained object detector is limited to its original object categories and may easily miss various objects in the scenes. In addition, the higher-order relations in these methods are limited to be inferred solely from contextual objects, which might miss important environmental or background cues for action classification.</p><p>To tackle the above issues, we propose an Actor-Context-Actor Relation Network (ACAR-Net) which focuses on modeling second-order relations in the form of Actor-Context-Actor relation. It deduces indirect relations between multiple actors and the context for action localization. The ACAR-Net takes both actor and context features as inputs. We define actor features as the features pooled from the actor regions of interest, while for context features, we directly use spatio-temporal grid feature maps from our backbone network. The context that we adopt does not rely on any extra object detector with predefined categories, thus making our overall design much simpler and flexible. Moreover, grid feature maps are capable of representing scene elements of various levels (e.g. instance level and part level) and types (e.g. background, objects and object parts), which is useful for fine-grained action discrimination. The proposed ACAR-Net first encodes first-order actor-context relations, and then applies a High-Order Relation Reasoning Operator to model interactions between the first-order relations. The High-Order Relation Reasoning Operator is fully convolutional and operates on first order relational features maps without losing spatial layouts . For supporting actor-contextactor relation reasoning between actors and context at different time periods, we build an Actor-Context Feature Bank, which contains actor-context relations from different time steps across the whole video.</p><p>We conduct extensive experiments on the challenging Atomic Visual Actions (AVA) dataset <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> as well as the UCF101-24 dataset <ref type="bibr" target="#b34">[34]</ref> for spatio-temporal action localization. Our proposed ACAR-Net leads to significant improvements on recognizing human-object and human-human interactions. Qualitative visualization shows that our method learns to attend contextual regions that are relevant to the action of interest.</p><p>Our contributions are summarized as the three-fold:</p><p>? We propose to model actor-context-actor relations for spatio-temporal action localization. Such relations are mostly ignored by previous methods but crucial for achieving accurate action localization.</p><p>? We propose a novel Actor-Context-Actor Relation Network for improving spatio-temporal action localization by explicitly reasoning about higher-order relations between actors and the context.</p><p>? We achieve state-of-the-art performances with significant margins on the AVA and UCF101-24 datasets. At the time of submission, our method ranks first on the ActivityNet leaderboard <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. Research works on action recognition generally fall into three categories: action classification, temporal localization and spatio-temporal localization. Early works mainly focus on classifying a short video clip into an action class. 3D-CNN <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>, two-stream network <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b8">9]</ref> and 2D-CNN <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref> are the three dominant network architectures adopted for this task. While progress has been made for short trimmed video classification, the main research stream also moves forward to understand long untrimmed videos, which requires not only to recognize the category of each action instance but also to locate its start and end times. A handful of works <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b60">60]</ref> consider this problem as a detection problem in 1D temporal dimension by extending object detection frameworks.</p><p>Spatio-Temporal Action Localization. Recently, the problem of spatio-temporal action localization has drawn considerable attention from the research community, and datasets (such as AVA <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>) with atomic actions of all actors in the video being continuously annotated are introduced. It defines the action detection problem into a finer level, since the action instances need to be localized in both space and time. Typical approaches used by early works apply R-CNN detectors on 3D-CNN features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b22">23]</ref>. Wu et al. <ref type="bibr" target="#b47">[47]</ref> show that actor features obtained by running 3D-CNN backbone on top of the cropped and resized actor region from the original video preserve better spatial details than RoI-pooled actor features. Nevertheless, it has the limitation that computational costs and inference time almost increase linearly with the number of actors. Several more recent works have exploited graph-structured networks to leverage contextual information <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Relational Reasoning for Video Understanding. Relational reasoning has been studied in the domain of video understanding <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">38]</ref>. This is natural because recognizing the action of an actor depends on its relationships with other actors and objects. Zhou et al. <ref type="bibr" target="#b61">[61]</ref> extend Relation Network <ref type="bibr" target="#b31">[31]</ref> for modeling relations between video frames over time. Non-local Networks <ref type="bibr" target="#b44">[44]</ref> leverage self-attention mechanisms to capture long range dependencies between different entities. Wang et al. <ref type="bibr" target="#b45">[45]</ref> show that representing videos with Space-time Region Graph improves action classification accuracy. In the context of spatiotemporal localization, there are many traditional approaches that are dedicated to capturing spatio-temporal relationships in videos <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b17">18]</ref>. For deep neural networks based methods, Sun et al. <ref type="bibr" target="#b35">[35]</ref> propose Actor-Centric Relation Network that learns to aggregate actor and scene features. Girdhar et al. <ref type="bibr" target="#b11">[12]</ref> re-purpose the Transformer network <ref type="bibr" target="#b42">[42]</ref> for encoding pairwise relationships between every two actor proposals. Concurrently, Wu et al. <ref type="bibr" target="#b46">[46]</ref> use long-term feature banks (LFB) to provide temporal supportive information up to 60s for computing long range interaction between actors. Zhang et al. <ref type="bibr" target="#b58">[58]</ref> propose to explicity model interactions between actors and objects. However, their approach focuses on modeling actor-object and actor-actor relations separately. When deducing the action of a person, the interactions of other persons with contextual objects are ignored. In other words, they do not explicitly model the actor-context-actor relations. In contrast, our method emphasizes modeling those higher-order relations. Perhaps the most similar work to ours is <ref type="bibr" target="#b38">[38]</ref>, which aggregates multiple types of interactions with stacked units akin to Transformer Networks <ref type="bibr" target="#b42">[42]</ref>. Nonetheless, while this approach also supports actorcontext-actor interactions, it treats object detection results as context, which requires extra pre-trained object detectors with fixed object categories and ignores other important types of contexts (such as background, objects not in the predefined categories, and specific parts of some objects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we provide detailed descriptions of our proposed Actor-Context-Actor Relation Network (ACAR-Net). Our ACAR-Net aims at effectively modeling and utilizing higher-order relations built upon the basic actor-actor and actor-context relations for achieving more accurate action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Framework</head><p>We first introduce our overall framework for action localization, where the proposed actor-context-actor relation (ACAR) modeling is the key module. The framework is designed to detect all persons in an input video clip (?2s in our experiments) and estimate their action labels. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, following state-of-the-art methods <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">49]</ref>, the framework is built based on an off-the-shelf person detector (e.g. Faster R-CNN <ref type="bibr" target="#b30">[30]</ref>) and a video backbone network (e.g. I3D <ref type="bibr" target="#b1">[2]</ref>). Person and context features are then processed by the proposed ACAR module with a long-term Actor-Context Feature Bank for final action prediction.</p><p>In details, the person (actor) detector operates on the center frame (i.e. key frame) of the input clip and obtains N detected actors. The detected boxes are duplicated to neighboring frames of the key frame in the clip. In the meantime, the backbone network extracts a spatio-temporal feature volume from the input video clip. We perform average pooling along the temporal dimension to save follow-up computational cost, which results in a feature map X ? R C?H?W , and C, H, W are channel, height and width respectively. We apply RoIAlign <ref type="bibr" target="#b15">[16]</ref> (7 ? 7 spatial output) followed by spatial max pooling to the N actor features, producing a series of N actor features, A 1 , A 2 , . . . , A N ? R C , each of which describes the spatio-temporal appearance and motion of one Region of Interest (RoI).</p><p>The proposed Actor-Context-Actor Relation (ACAR) module is illustrated on the right side of <ref type="figure" target="#fig_0">Fig. 2</ref>. This module takes the aforementioned video feature map X and RoI</p><formula xml:id="formula_0">features {A i } N i=1</formula><p>as inputs, and outputs the final action predictions after relation reasoning. The ACAR module has two main operations. (1) It first encodes first-order actor-context relations between actors and spatial locations of the spatiotemporal context. Based on the actor-context relations, we further integrate a High-order Relation Reasoning Operator (HR 2 O) for modeling the interactions between pairs of first-order relations, which are indirect relations mostly ignored by previous methods. (2) Our reasoning operation is extended with an Actor-Context Feature Bank (ACFB). The bank contains actor-context relations at different time stamps, and can provide more complete spatio-temporal context than the existing long-term feature bank <ref type="bibr" target="#b46">[46]</ref> which only consists of features of actors. We will elaborate the two parts in the following sections. Notably, our high-order relation reasoning only requires action labels as supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Actor-Context-Actor Relation Modeling</head><p>First-order actor-context relation encoding. We adopt the Actor-Centric Relation Network (ACRN) <ref type="bibr" target="#b35">[35]</ref> as a module for encoding the first-order actor-context relations by combining RoI features A 1 , . . . , A N with the context feature X. More specifically, it replicates and concatenates each actor feature A i ? R C to all H ? W spatial locations of the context feature X ? R C?H?W to form a series of concatenated</p><formula xml:id="formula_1">feature map {F i } N i=1 ? R 2C?H?W</formula><p>. Actor-context relation features for each actor i can then be encoded by applying convolutions to this concatenated feature mapF i . High-order relation reasoning. We now discuss how to compute high-order relations between two actors based on their first-order interactions with the context. Let F i x,y record the first-order features between the actor A i and the scene context X at the spatial location (x, y). We propose to model the relationship between first-order actor-context relations, which are high-order relations encoding more informative scene semantics. However, since there are a large number of actor-context relation features,  ator (HR 2 O) that aims at learning the high-order relations between pairs of actor-context relations at the same spatial location (x, y), i.e., F i x,y and F j x,y . In this way, the proposed relational reasoning operator limits the relation learning to second-order actor-context-actor relations, i.e. two actors i and j can be associated via the same spatial context, denoted as i ? (x, y) ? j, to help the estimation of their actions.</p><formula xml:id="formula_2">F i x,y ? R C?1?1 , i ? {1, . . . , N }, x ? [1, H], y ? [1, W ],</formula><p>Our proposed HR 2 O takes as input a set of first-order actor-context relation feature maps</p><formula xml:id="formula_3">{F i } N i=1 . The operator outputs {H i } N i=1 = HR 2 O({F i } N i=1</formula><p>) that encode secondorder actor-context-actor relations for all actors. The operator is modeled as stacking several modified non-local blocks <ref type="bibr" target="#b44">[44]</ref>. For each non-local block, convolutions are used to convert the input first-order actor-context relation feature maps F i into query Q i , key K i and value V i embeddings of the same spatial size as F i . All feature maps are of dimension d = 512 in our implementation. It is worth noting that the use of convolutions is not only useful for aggregating local information but also makes the operator position and order-sensitive. The attention vectors are computed separately at every spatial location, and the Actor-Context-Actor Relation feature H i is given by the linear combination of all value features {V j } N j=1 according to their corresponding attention weights Att i,j . The overall process can be summarized by the following equations,</p><formula xml:id="formula_4">Q i , K i , V i = conv2D(F i ) Att i x,y = softmax j Q i x,y , K j x,y ? d , H i x,y = j Att i,j x,y V j x,y .<label>(1)</label></formula><p>Following <ref type="bibr" target="#b46">[46]</ref>, we also add layer normalization and dropout to our modified non-local block, H i = Dropout(Conv2D(ReLU(norm(H i )))),</p><formula xml:id="formula_5">F i = F i + H i ,<label>(2)</label></formula><p>where H i and the input actor-context features F i are fused via residual addition to obtain the actor-context-actor feature F i , which can be further processed by the following nonlocal block again. We also exploit another instantiation, which directly obtains second-order actor-context-actor interaction features from actor features {A i } N i=1 and the context feature X by a Relation Network <ref type="bibr" target="#b31">[31]</ref>. More specifically, we obtain the relation feature between actors A i , A j and context V x,y as</p><formula xml:id="formula_6">H i,j x,y = f ? ([A i , A j , V x,y ]),<label>(3)</label></formula><p>where [?, ?, ?] denotes concatenation along the channel dimension and f ? (?) is a stack of two convolutional layers. The high-order relation of an actor i is calculated as the average of all relation features related to that actor,</p><formula xml:id="formula_7">H i = 1 N j H i,j x,y .<label>(4)</label></formula><p>It is also fused with the input features to obtain actor-contextactor features via residual addition, i.e. F i = F i + H i . This method is computationally expensive when the number of actors N is large, since the number of feature triplets is proportional to N 2 . Action classifier. After the actor-context-actor feature maps</p><formula xml:id="formula_8">{F i } N i=1</formula><p>are obtained for all actors, a final action classifier is introduced as a single fully-connected layer with a nonlinearity function to output the confidence scores of each actor belonging to different action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Actor-Context Feature Bank</head><p>In order to support actor-context-actor relation reasoning between actors and context at different time periods in a long video, we propose an Actor-Context Feature Bank (ACFB), in which we store contextual information from both past and future. This is inspired by the Long-term Feature Bank (LFB) proposed in <ref type="bibr" target="#b46">[46]</ref>. Yet instead of providing relational features for long-term higher-order reasoning, the previous LFB only stores actor features for facilitating first-order actor-actor interaction recognition.</p><p>As is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, clips are evenly sampled (every 1 second) from an input video, and the clips (?2s) could overlap with each other. We first train a separate ACAR-Net without any feature bank following the descriptions in Section 3.2. First-order actor-context relation features F i of each actor in all clips of the entire video would be extracted by the separately pre-trained ACAR-Net and archived as the feature bank. To avoid confusion, we re-denote these acquired first-order features in the bank as L i .</p><p>To train a new ACAR-Net with the support of the longterm actor-context feature bank to conduct high-order relation reasoning at some current time step t, we retrieve all M archived actor-context relation features {L i } M i=1 from the frames within a time window [t ? w, t + w]. Actorcontext-actor interactions between short-term features (encoding first-order interactions at current time t) and longterm ones from the archived bank can be computed as</p><formula xml:id="formula_9">{H i } N i=1 = HR 2 O({F i } N i=1 , {L j } M j=1</formula><p>). Note that, the HR 2 O is the same as before, but the self-attention mechanism is replaced with the attention between current and long-term actor-context relations, where query features Q are still computed from short-term features {F i } N i=1 , but key and value features, K and V , are calculated with the long-term archived features {L i } M j=1 , i.e.,</p><formula xml:id="formula_10">Q i = conv2D(F i ), K j , V j = conv2D(L j ).<label>(5)</label></formula><p>Consequently, for any actor i at current time t, our ACAR-Net is now capable of reasoning about its higher-relations with actors and context over a much longer time span, and thus better captures what is happening in the temporal context for achieving more accurate action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on AVA</head><p>AVA <ref type="bibr" target="#b14">[15]</ref> is a video dataset for spatio-temporally localizing atomic visual actions. For AVA, box annotations and their corresponding action labels are provided on key frames of 430 15-minute videos with a temporal stride of 1 second. We use version 2.2 of AVA dataset by default. In addition to the current AVA dataset, Kinetics-700 <ref type="bibr" target="#b0">[1]</ref> videos with AVA <ref type="bibr" target="#b14">[15]</ref> style annotations are also introduced. The new AVA-Kinetics dataset <ref type="bibr" target="#b21">[22]</ref> contains over 238k unique videos and more than 624k annotated frames. However, only a single frame is annotated for each video from Kinetics-700. Following the guidelines of the benchmarks, we only evaluate 60 action classes with mean Average Precision (mAP) as the metric, using a frame-level IoU threshold of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Person Detector. For person detection on key frames, we use the human detection boxes from <ref type="bibr" target="#b46">[46]</ref>, which are generated by a Faster R-CNN <ref type="bibr" target="#b30">[30]</ref> with a ResNeXt-101-FPN <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b24">25]</ref> backbone. The model is pre-trained with Detectron <ref type="bibr" target="#b12">[13]</ref> on ImageNet <ref type="bibr" target="#b4">[5]</ref> as well as the COCO human keypoint images <ref type="bibr" target="#b25">[26]</ref>, and fine-tuned on the AVA dataset. Backbone Network. We use SlowFast networks <ref type="bibr" target="#b7">[8]</ref> as the backbone in our localization framework and increase the spatial resolution of res 5 by 2?. We conduct ablation experiments using a SlowFast R-50 8 ? 8 instantiation (without non-local blocks). The inputs are 64-frame clips, where we sample T = 8 frames with a temporal stride ? = 8 for the slow pathway, and ?T (? = 4) frames for the fast pathway. The backbone is pre-trained on the Kinetics-400 dataset 2 .</p><p>Training and inference. In AVA, actions are grouped into 3 major categories: poses (e.g. stand, walk), human-object and human-human interactions. Given that poses are mutually exclusive and interactions are not, we use softmax for poses and sigmoid for interactions before binary cross-entropy loss for training. We train all models end-to-end (except for the feature bank part) using synchronous SGD with a batch size of 32 clips. We train for 35k iterations with a base learning rate of 0.064, which is then decreased by a factor of 10 at iterations 33k and 34k. We perform linear warm-up <ref type="bibr" target="#b13">[14]</ref> during the first 6k iterations. We use a weight decay of 10 ?7 and Nesterov momentum of 0.9. We use both ground-truth boxes and predicted human boxes from <ref type="bibr" target="#b46">[46]</ref> for training. For inference, we scale the shorter side of input frames to 256 pixels and use detected person boxes with scores greater than 0.85 for final action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We conduct ablation experiments to investigate the effect of different components in our framework on AVA v2.2. mAP Baseline + STO <ref type="bibr" target="#b46">[46]</ref> 26.10 Baseline + ACRN <ref type="bibr" target="#b35">[35]</ref> 26.71 Baseline + AIA <ref type="bibr" target="#b38">[38]</ref> 26  <ref type="table">Table 1</ref>. Ablation study on AVA dataset. The "Baseline" of our framework only consists of the video backbone, actor detector and one-layer action classifier. HR 2 O: High-order Relation Reasoning Operator. ACFB: Actor-Context Feature Bank.</p><p>The baseline of our framework only consists of the video backbone (SlowFast R-50), the actor detector and the singlelayer action classifier (denoted as "Baseline" in <ref type="table">Table 1</ref>).</p><p>Relation Modeling -Comparison. In order to show the effectiveness of our actor-context-actor relation reasoning module, we compare against several previous approaches that leverage relation reasoning for action localization based on our baseline. Here we focus on validating the effect of relation modeling only, thus we disable long-term support in this study. We adapt their reasoning modules such that all methods use the same baseline as our ACAR-Net in order to fairly compare only the impact of relation reasoning.</p><p>We evaluate ACRN that focuses on learning actor-context relations; STO <ref type="bibr" target="#b46">[46]</ref> (a degraded version of LFB) that only captures actor interactions within the current short clip; AIA (w/o memory) <ref type="bibr" target="#b38">[38]</ref> that aggregates both actor-actor and actorobject interactions. As listed in <ref type="table">Table 1a</ref>, our proposed actor-context-actor relation modeling ("Baseline + HR 2 O" in <ref type="table">Table 1a</ref>) significantly improves over the compared methods. We observe that AIA with both actor and context relations performs better than ACRN and STO which only model one type of first-order relations, yet our method based on highorder relation modeling outperforms all compared methods by considerable margins. We further break down the performances of different relation reasoning modules into three major categories of the AVA dataset, which are poses (e.g. stand, sit, walk), humanobject interactions (e.g. read, eat, drive) and human-human interactions (e.g. talk, listen, hug). <ref type="figure">Fig. 4</ref> compares the gains of different approaches with respect to the baseline in terms of mAP on these major categories. We can see that our HR 2 O gives more performance boosts on two interaction categories compared to the pose category, which is consistent with our motivation to model indirect relations between actors and context. Once equipped with ACFB, our framework can further improve on the pose category as well.</p><p>Finally, we contrast our ACAR with existing relation reasoning approaches in AVA. We visualize attention maps from different reasoning modules over an example key frame in <ref type="figure">Fig. 5</ref>. Without needing object proposals, ACAR is capable of localizing free-form context regions for indirectly establishing relations between two actors (the actor of interest is listening to the supporting actor reading a report). In comparison, the attention weights of STO as well as AIA are distributed more diversely and do not have a clear focus point. Note that we do not show the attention map of ACRN since it assigns equal weights to all context regions.</p><p>Component Analysis. To validate our design, we first ablate the impacts of different components of our ACAR as shown in <ref type="table">Table 1b</ref>. We can observe that both HR 2 O and ACFB lead to significant performance gains over baseline.</p><formula xml:id="formula_11">HR 2 O Design.</formula><p>We test different instantiations of the Highorder Relation Reasoning Operator on top of our baseline in <ref type="table">Table 1c</ref>. Our modified non-local (denoted as "NL") mechanism works better than simply designing HR 2 O as an average function (denoted as "Avg"), i.e.</p><formula xml:id="formula_12">H i = 1 N i F i ,.</formula><p>In addition, the instantiation with relation network (RN) described in Section 3.2 also works alright. Nonetheless, the modified non-local attention is computationally more efficient than RN with feature triplets and has better performance.</p><p>Relation Ordering. There are two possible orders for reasoning actor-context-actor relations: 1) aggregating actoractor relations first, or 2) encoding actor-context relations first. Note that our ACAR-Net adopts the latter one. We implement the former order by performing self-attention between actor features with the modified non-local attention before incorporating context features in our baseline. The results in <ref type="table">Table 1d</ref> validate that context information should be aggregated earlier for better relation reasoning. <ref type="table">Table 1e</ref>, we observe that stacking two modified non-local blocks in HR 2 O has higher mAP than the one-layer version, yet adding one more non-local block produces worse performance, possibly due to overfitting. We therefore adopt two non-local blocks as the default setting.</p><formula xml:id="formula_13">HR 2 O Depth. In</formula><p>Actor-Context Feature Bank. In this set of experiments, we validate the effectiveness of the proposed ACFB. We set the "window size" 2w + 1 to 21s due to memory limitations, and longer temporal support is expected to perform better <ref type="bibr" target="#b46">[46]</ref>. As presented in <ref type="table">Table 1f</ref>, adding long-term support with ACFB significantly improves the baseline (HR 2 O's 27.83 ? HR 2 O + ACFB's 28.84). We also test replacing the ACFB in our framework with the long-term feature bank (LFB) <ref type="bibr" target="#b46">[46]</ref> (denoted as "HR 2 O + LFB"). However, LFB even fails to match the baseline performance. This drop might be because LFB encodes only "zeroth-order" actor features, which cannot provide enough relational information from neighboring frames for assisting interaction recognition. <ref type="figure">Figure 4</ref>. Gains of mAP on three major categories of the AVA dataset with respect to Baseline. Our ACAR consistently outperforms other relation reasoning methods, and achieves larger performance gains on the two interaction categories.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts on AVA</head><p>We compare our ACAR-Net with state-of-the-art methods on the validation set of both AVA v2.1 ( <ref type="table" target="#tab_2">Table 2</ref>) and v2.2 ( <ref type="table" target="#tab_3">Table 3</ref>). Note that we also provide results with more advanced video backbones, i.e. two SlowFast R-101 instantiations (with / without NL). On AVA v2.1, our framework achieves 30.0 mAP and outperforms all prior results with pretrained Kinetics-400 backbone. On AVA v2.2, our ACAR-Video Keyframe ACAR (actor-context-actor) STO (actor-actor) AIA (actor-actor, actor-object) <ref type="figure">Figure 5</ref>. Comparison of attention maps from different approaches of relation modeling for action detection. Our method is able to attend the contextual region (some document) that relates the actor of interest marked in red (performing "listen to") and the supporting actors in the green box (performing "read"), while other methods fail to achieve similar effects.</p><p>Net reaches 33.3 mAP with only single-scale testing, establishing a new state-of-the-art. Note that our method surpasses AIA <ref type="bibr" target="#b38">[38]</ref> with only 1/3 of temporal support. The results indicate that with proper modeling of higher-order relations, our approach extracts more informative cues from the context. We present our results on AVA-Kinetics in <ref type="table" target="#tab_4">Table 4</ref>. Our baseline is already highly competitive (?33 mAP). Yet integrating our ACAR modeling still leads to a significant gain of +2.86 mAP. This demonstrates that performance enhancement brought by high-order relation modeling can generalize to this new dataset. With an ensemble of models, we achieve 39.62 mAP on the test set, ranking first in the AVA-Kinetics task of ActivityNet Challenge 2020 and outperforming other entries by a large margin (+6.71 mAP). More details on our winning solution are provided in the technical report <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>Our proposed ACAR operates fully convolutionally on top of spatio-temporal features, and this allows us to visualize the actor-context-actor relation maps {Att i,j } generated by our High-order Relation Reasoning Operator. As shown in <ref type="figure" target="#fig_2">Fig. 6</ref>, the first two columns include the key frame as well as the corresponding relation map from the same clip, and the last three columns show the relation map denoting interactions with actors and context from a neighboring clip. We can observe that the attended regions usually include the actor of interest, supporting actors' body parts (i.e. head, hands and arm) and objects being in interaction with the actors. Take the first example on the left as an example. The green supporting actor A j is taking a package from the red actor of interest A i . Such information is well encoded by our ACAR-Net in the form of actor-context-actor relations: packages, hands and arms of both actors are highlighted. Heat maps illustrate the context regions' attention weights Att i,j from actor-context-actor relation reasoning. We observe that our model has learned to attend to useful relations between actors and context, and the context serves as the bridge for connecting actors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on UCF101-24</head><p>UCF101-24 is a subset of UCF101 <ref type="bibr" target="#b34">[34]</ref> that contains spatio-temporal annotations for 3,207 videos on 24 action classes. Following the evaluation settings of previous methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b54">54]</ref>. We experiment on the first split and report frame-mAP with an IoU threshold of 0.5.</p><p>Implementation Details. We also use SlowFast R-50 pretrained on Kinetics-400 as the video backbone, and adopt the person detector from <ref type="bibr" target="#b20">[21]</ref>. The temporal sampling for the slow pathway is changed to 8 ? 4 and the fast pathway takes as input 32 continuous frames.</p><p>For training, we train all the models end-to-end for 5.4k iterations with a base learning rate of 0.002, which is then decreased by a factor of 10 at iterations 4.9k and 5.1k. We perform linear warm-up during the first quarter of the training schedule. We only use ground-truth boxes for training, and use all boxes given by the detector for inference. Other hyper-parameters are similar to the experiments on AVA.</p><p>Results. As shown in <ref type="table">Table 5</ref>, ACAR surpasses the strong baseline with a considerable margin, which again indicates the importance of high-order relation reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Given the high complexity of realistic scenes encountered in the spatio-temporal action localization task which involve multiple actors and a large variety of contextual objects, we observe the demand for a more sophisticated form of  <ref type="table">Table 5</ref>. Comparison with previous works on UCF101-24. We evaluate frame-mAP on split 1. V and F refer to visual frames and optical flow respectively. relation reasoning than current ones which often miss important hints for recognizing actions. Therefore, we propose Actor-Context-Actor Relation Network for explicitly modeling higher-order relations between actors based on their interactions with the context. Extensive experiments on the action detection task show our ACAR-Net outperforms existing methods that leverage relation reasoning, and achieves state-of-the-art results on several challenging benchmarks of spatio-temporal action localization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Action Detection Framework. Videos are processed with a Backbone Network to produce spatio-temporal context features. For each actor proposal (person bounding box), we extract actor features from the context features by RoIAlign. Given the actor and context features, the ACAR-Net computes second-order relation between every two actors based on their interaction with the context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of ACAR-Net equipped with Actor-Context Feature Bank, where ACAR-Net* refers to the first-order relation extraction part of our proposed module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of actor-context-actor attention maps on AVA. Actors of interest are marked in red and supporting actors in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the number of their possible pairwise combinations is generally overwhelming. We therefore design a High-order Relation Reasoning Oper-</figDesc><table><row><cell></cell><cell cols="4">Actor-Context-Actor Relation Module</cell></row><row><cell>Actor Proposals</cell><cell>Actor Features</cell><cell>Actor-Context</cell><cell></cell><cell cols="2">Actor-Context-Actor</cell></row><row><cell>Input Video</cell><cell>ROI Align</cell><cell cols="2">Relation Features</cell><cell cols="2">Relational Features</cell><cell>Watch,</cell></row><row><cell></cell><cell>Tile + Concat</cell><cell></cell><cell></cell><cell></cell><cell>Talk to,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Listen to music</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Action</cell></row><row><cell></cell><cell></cell><cell>Conv.</cell><cell cols="2">HR 2 O</cell><cell>GAP</cell><cell>Classifier Play Instrument Stand,</cell></row><row><cell>Backbone Network</cell><cell>Context Features</cell><cell></cell><cell></cell><cell></cell><cell>Watch, Listen to music</cell></row><row><cell></cell><cell>Actor-Context</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feature Bank</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-arts on AVA v2.1. All models are pre-trained on Kinetics-400. V and F refer to visual frames and optical flow respectively.</figDesc><table><row><cell>model</cell><cell></cell><cell>inputs</cell><cell>val mAP</cell></row><row><cell>ACRN, S3D [35]</cell><cell></cell><cell>V+F</cell><cell>17.4</cell></row><row><cell>Zhang et al. [58], I3D</cell><cell></cell><cell>V</cell><cell>22.2</cell></row><row><cell>Action TX, I3D [12]</cell><cell></cell><cell>V</cell><cell>25.0</cell></row><row><cell>LFB, R-50+NL [46]</cell><cell></cell><cell>V</cell><cell>25.8</cell></row><row><cell>LFB, R-101+NL [46]</cell><cell></cell><cell>V</cell><cell>27.4</cell></row><row><cell>SlowFast, R-50, 8 ? 8 [8]</cell><cell></cell><cell>V</cell><cell>24.8</cell></row><row><cell cols="2">SlowFast, R-101, 8 ? 8 [8]</cell><cell>V</cell><cell>26.3</cell></row><row><cell>Ours, R-50, 8 ? 8</cell><cell></cell><cell>V</cell><cell>28.3</cell></row><row><cell>Ours, R-101, 8 ? 8</cell><cell></cell><cell>V</cell><cell>30.0</cell></row><row><cell>model</cell><cell></cell><cell>pre-train</cell><cell>val mAP</cell></row><row><cell>SlowFast, R-101+NL [8]</cell><cell cols="2">Kinetics-600</cell><cell>29.0</cell></row><row><cell>AIA, R-101+NL [38]</cell><cell cols="2">Kinetics-700</cell><cell>32.3</cell></row><row><cell>Ours, R-101+NL</cell><cell cols="2">Kinetics-600</cell><cell>31.4</cell></row><row><cell>Ours, R-101</cell><cell cols="2">Kinetics-700</cell><cell>33.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-arts on AVA 2.2. We do not conduct testing with multiple scales and flips. All models use T ? ? = 8 ? 8.</figDesc><table><row><cell>model</cell><cell>val mAP</cell><cell>test mAP</cell></row><row><cell>AIA++, ensemble [48]</cell><cell>-</cell><cell>32.91</cell></row><row><cell>MSF, ensemble [62]</cell><cell>-</cell><cell>31.88</cell></row><row><cell>SlowFast, R-101, 8 ? 8 (our impl.)</cell><cell>32.98</cell><cell>-</cell></row><row><cell>Ours, R-101, 8 ? 8</cell><cell>35.84</cell><cell>-</cell></row><row><cell>Ours++, R-101, 8 ? 8</cell><cell>36.36</cell><cell>-</cell></row><row><cell>Ours++, ensemble</cell><cell>40.49</cell><cell>39.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>AVA-Kinetics results. "++" refers to inference with 3 scales and horizontal flips. Models submitted to the test server are trained on both training and validation sets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The pre-trained SlowFast R-50 and SlowFast R-101+NL (in the following section) are downloaded from SlowFast's official repository.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09116</idno>
		<title level="m">1st place solution for ava-kinetics crossover in acitivitynet challenge 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10066</idno>
		<title level="m">A better baseline for ava</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Piotr Doll?r, and Kaiming He. Detectron</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (t-cnn) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5822" to="5831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A framework for recognizing multi-agent action from visual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">F</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action genome: Actions as compositions of spatiotemporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10236" to="10247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4405" to="4413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">You only watch once: A unified cnn architecture for realtime spatiotemporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06644</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghana</forename><surname>Thotakuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00214</idno>
		<title level="m">The avakinetics localized human actions video dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="303" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Something-else: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on</title>
		<meeting>the IEEE/CVF Conference on</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-agent event recognition in structured scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3289" to="3296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-region twostream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relational action forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex activity recognition using granger constrained dbn (gcdbn) in sports and surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Swears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="788" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Asynchronous interaction aggregation for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07485</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stage: Spatio-temporal attention on graph entities for video action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bronzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04316</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actor conditioned attention maps for video action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swati</forename><surname>Rallapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Context-aware rcnn: A baseline for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09861</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multiple attempts for ava-kinetics challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://static.googleusercontent.com/media/research.google.com/es//ava/2020/ByteDance-SJTU_AVA_report_2020.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04519</idno>
		<title level="m">Three branches: Detecting actions with richer features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Audiovisual slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08740</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Step: Spatio-temporal progressive learning for video action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Discovering spatio-temporal action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A structured model for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9975" to="9984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Modeling temporal interactions with interval temporal bayesian networks for complex activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Swears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Larios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2468" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08496</idno>
		<title level="m">Temporal relational reasoning in videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Obinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Tan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-scale spatiotemporal features for action localization</title>
		<ptr target="https://static.googleusercontent.com/media/research.google.com/es//ava/2020/Multi-scale_Spatiotemporal_Features_for_Action_Localization.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
