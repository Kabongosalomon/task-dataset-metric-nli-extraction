<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LT-OCF: Learnable-Time ODE-based Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongwhan</forename><surname>Choi</surname></persName>
							<email>jeongwhan.choi@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Jeon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noseong</forename><surname>Park</surname></persName>
							<email>noseong@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LT-OCF: Learnable-Time ODE-based Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482449</idno>
					<note>KEYWORDS collaborative filtering; neural ordinary differential equations ACM Reference Format: Jeongwhan Choi, Jinsung Jeon, Noseong Park. 2021. LT-OCF: Learnable-Time ODE-based Collaborative Filtering. In Proceedings of the 30th ACM Int&apos;l Conf. on Information and Knowledge Management (CIKM &apos;21), November 1-5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 10 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS ? Computing methodologies ? Machine learning; Neural net- works</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collaborative filtering (CF) is a long-standing problem of recommender systems. Many novel methods have been proposed, ranging from classical matrix factorization to recent graph convolutional network-based approaches. After recent fierce debates, researchers started to focus on linear graph convolutional networks (GCNs) with a layer combination, which show state-of-the-art accuracy in many datasets. In this work, we extend them based on neural ordinary differential equations (NODEs), because the linear GCN concept can be interpreted as a differential equation, and present the method of Learnable-Time ODE-based Collaborative Filtering (LT-OCF). The main novelty in our method is that after redesigning linear GCNs on top of the NODE regime, i) we learn the optimal architecture rather than relying on manually designed ones, ii) we learn smooth ODE solutions that are considered suitable for CF, and iii) we test with various ODE solvers that internally build a diverse set of neural network connections. We also present a novel training method specialized to our method. In our experiments with three benchmark datasets, our method consistently outperforms existing methods in terms of various evaluation metrics. One more important discovery is that our best accuracy was achieved by dense connections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Collaborative filtering (CF), which is to predict users' preferences from patterns, is a long-standing research problem in the field of recommender systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>. It is common Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM <ref type="bibr">'</ref>   to learn user and product embedding vectors and calculate their dot-products for recommendation. Matrix factorization is one such approach, which is well-known in the recommender system community <ref type="bibr" target="#b24">[25]</ref>. There have been proposed several other enhancements as well <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. Recently researchers started to focus on graph convolutional networks (GCNs) for the purpose of CF <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37]</ref>. GCNs had been proposed to process not only CF-related graphs but also other general graphs. GCNs are broadly categorized into the following two types: spectral GCNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> and spatial GCNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b33">34]</ref>. GCNs for CF fall into the first category due to its appropriateness for CF <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. However, there have been fierce debates about what is the optimal GCN architecture for CF. During its early phase, researchers utilized non-linear activations, such as ReLU, because they showed good accuracy in many machine learning tasks, e.g., classification, regression, and so on <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b44">45]</ref>. Surprisingly, however, it was recently reported that a linear GCN architecture with a layer combination, called LightGCN, works better than other non-linear GCNs for CF <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37]</ref>. Unlike other general graphs, user-product interaction bipartite graphs are frequently sparse and provide little information because they mostly do not include node/edge features. In <ref type="bibr" target="#b15">[16]</ref>, it was noted that, for the same reason, non-linear GCNs are arXiv:2108.06208v3 [cs.IR] 18 Aug 2021 quickly overfitted to training data and do not work well in general for CF.</p><p>Owing to the discovery, we propose the method of Learnable-Time ODE-based Collaborative Filtering (LT-OCF) in this paper. We redesign the linear GCN with the layer combination on top of the concept of the neural ordinary differential equations (NODEs) because linear GCNs, including LightGCN, can be theoretically interpreted as differential equations, i.e., heat equations (see Section 2.4). For instance, the main linear propagation layer of LightGCN is exactly the same as Newton's law of cooling.</p><p>Neural ordinary differential equations (NODEs) are to learn implicit differential equations from data. NODEs calculate</p><formula xml:id="formula_0">( 1 ) = ( 0 ) + ? 1 0 ( ( ), ; ) ,</formula><p>where is a neural network parameterized by that approximates <ref type="bibr">( )</ref> , to derive ( 1 ) from ( 0 ), where 1 &gt; 0 . We note that is trained from data -in other words, <ref type="bibr">( )</ref> is trained from data. The variable is called as time variable, which represents the layer concept of neural networks. Note that is a non-negative integer in conventional neural networks whereas it can be any arbitrary non-negative real number in NODEs. In this regard, NODEs are considered as continuous generalizations of neural networks.</p><p>Various ODE solvers can solve the integral problem, and it was also known that they can generalize various neural network architectures <ref type="bibr" target="#b6">[7]</ref>. For instance, the general form of the residual connection can be written as ( + 1) = ( ) + ( ( ); ), which is identical to the explicit Euler method to solve ODE problems. It is also known that the fourth-order Runge-Kutta (RK4) ODE solver is similar to dense convolutional networks and fractal neural networks <ref type="bibr" target="#b27">[28]</ref>.</p><p>The reason of our specific design choice to adopt NODEs for CF is threefold. In our proposed LT-OCF, firstly, is not only continuous but also trainable because we interpret linear GCNs as continuoustime differential equations. Therefore, we can learn the optimal layer combination construction rather than relying on a manually configured one. Let ? R ? and ? R ? , where is the number of users, is the number of products, and is the dimensionality of embedding space, be the user and product embeddings at layer , respectively. In recent GCN-based CF methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, for instance, the final user embeddings are calculated, owing to the layer combination technique, by 0 0 + 1 1 + 2 2 + ? ? ? + , where is a coefficient and is the number of layers. On the other hand, one contribution of LT-OCF is to calculate 0 (0) + 1 ( 1 ) + 2 ( 2 ) + ? ? ? + ( ), where ( ) corresponds to , is trainable for all , and &lt; if &lt; . Secondly, NODEs learn homeomorphic functions which we consider suitable for CF -see our discussion after Proposition 2.1. In the recent linear GCN architecture of CF <ref type="bibr" target="#b15">[16]</ref>, in addition, user/product embedding is simply a weighted sum of neighbors' embeddings, which can be solely written as matrix multiplications. We also use only matrix multiplications for defining our ODE formulation and matrix multiplication is an analytic operator. The Cauchy-Kowalevski theorem states, in such a case, that the optimal solution of ( ) always exists and is unique <ref type="bibr" target="#b11">[12]</ref>. Therefore, our ODE-based CF is a well-posed problem (see Section 3.3.1).</p><p>After formulating CF as an ODE problem, thirdly, we test with various ODE solvers that internally create a rich set of neural network connections. For instance, residual connections are the same t 0 t 1 X <ref type="figure">Figure 2</ref>: The locations of the yellow and blue points are inverted by the mapping from 0 to 1 . NODEs are not able to learn the yellow and blue trajectories at the same time, which cross each other, because their topology (i.e., their relative positions) cannot be changed after a mapping in NODEs, i.e., homeomorphic mapping.</p><p>as the explicit Euler method, dense connections are the same as RK4, and so on. We can test with various connections. The architecture of LT-OCF is shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>. To enable the proposed concept, we define dual co-evolving ODEs, whose detailed diagram is in <ref type="figure" target="#fig_5">Fig. 5</ref>. There exists an ODE for each of the user and product embeddings. However, they influence each other and co-evolve over time in our architecture. We also propose a novel training method to train LT-OCF because we have to train dual co-evolving ODEs with their time points { 1 , 2 , ? ? ? }.</p><p>We use three CF datasets, Gowalla, Yelp2018, and Amazon-Book, and compare LT-OCF with state-of-the-art methods, such as NGCF <ref type="bibr" target="#b36">[37]</ref>, LightGCN <ref type="bibr" target="#b15">[16]</ref>, and so forth, to name a few. Our method consistently outperforms all those methods in all cases. The biggest enhancement is made for Amazon-Book, i.e., a recall of 0.0411 by LightGCN vs. 0.0442 by LT-OCF and an NDCG of 0.0315 by Light-GCN vs. 0.0341 by LT-OCF. We also show that i) our method can be trained faster than LightGCN and ii) dense connections are better than linear connections for CF. To our knowledge, we are the first who reports that dense connections outperform linear connections in CF. Our contributions can be summarized as follows:</p><p>(1) We revisit state-of-the-art linear GCNs and propose the method of Learnable-Time ODE-based Collaborative Filtering (LT-OCF) based on NODEs. (2) In LT-OCF, we learn the optimal layer combination rather than relying on manually designed architectures. (3) We reveal that dense connections are better than linear connections for CF (see Section 5). To our knowledge, we first report this observation. <ref type="bibr" target="#b3">(4)</ref> We show that our formulation is theoretically well-posed, i.e., its solution always exists and is unique (see Section 3.3). (5) LT-OCF consistently outperforms all existing methods in three benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES &amp; RELATED WORK</head><p>We introduce our literature survey and preliminary knowledge to understand our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Ordinary Differential Equations (NODEs)</head><p>NODEs calculate ( +1 ) from ( ) by solving the following Riemann integral problem <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_1">( +1 ) = ( ) + ? +1 ( ( ), ; ) ,<label>(1)</label></formula><p>where the ODE function parameterized by is a neural network to approximate the time-derivative of , i.e., def = ( )</p><p>. To solve the problem, we typically rely on existing ODE solvers, e.g., the explicit Euler method, the Dormand-Prince (DOPRI) method, and so forth <ref type="bibr" target="#b8">[9]</ref>.</p><p>Let : R dim( ( 0 )) ? R dim( ( 1 )) be a mapping function from 0 to 1 created by Eq. (1). It is well-known that becomes a homeomorphic mapping: is bijective and continuous, and ?1 is also continuous for ? [0, ], where is the last value in the time domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref>. From this characteristic, the following proposition can be easily proved:</p><p>Proposition 2.1. The topology of the input space of is maintained in the output space, and as a result, the trajectories crossing each other cannot be learned by NODEs, e.g., <ref type="figure">Fig. 2</ref>.</p><p>While maintaining the topology, NODEs can perform downstream tasks and it was demonstrated that it actually enhances the robustness to adversarial attacks and out-of-distribution inputs <ref type="bibr" target="#b41">[42]</ref>. We conjecture that this characteristic is also suitable for learning reliable user/product representations, i.e., embeddings, when there is no abundant information. As mentioned earlier, CF typically includes only user-product interactions without additional user/product features. LightGCN <ref type="bibr" target="#b15">[16]</ref> showed that, in such a case, linear GCNs with zero non-linearity, which are known to be smooth <ref type="bibr" target="#b3">[4]</ref>, are appropriate. We conjecture that NODEs that learn smooth (homeomorphic) functions are also suitable for CF for the same reason. There are several other similar cases where NODEs work well <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</p><p>Instead of the backpropagation method, the adjoint sensitivity method can be adopted and its efficiency and theoretical correctness were already well proved <ref type="bibr" target="#b6">[7]</ref>. After letting ( ) = ( ) for a task-specific loss , it calculates the gradient of loss w.r.t model parameters with another reverse-mode integral as follows:</p><formula xml:id="formula_2">? = = ? ? 0 ( ) T ( ( ), ; ) .</formula><p>We customize the aforementioned adjoint sensitivity method to design our own training algorithm. In our framework, we learn both user/product embeddings and time points { 1 , 2 , ? ? ? } to construct a layer combination using the modified method.</p><p>It is known that NODEs have a couple of advantages. First, NODEs can sometimes significantly reduce the required number of parameters when building neural networks <ref type="bibr" target="#b29">[30]</ref>. Second, NODEs enable us to interpret the time variable as continuous, which is discrete in conventional neural networks <ref type="bibr" target="#b6">[7]</ref>. We fully enjoy the second advantage while designing our method. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the typical architecture of NODEs which we took from [10] -we assume a downstream classification task in this figure. There is a feature extraction layer which provides (0), and</p><p>(1) is calculated by the method described above. After that, there is a classification layer. In our case, however, we use the architecture in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, which has dual co-evolving ODEs only, because our task is not classification but CF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h(0)</head><p>Feature Extractor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h(1)</head><p>ODE Classifier</p><p>x prediction  <ref type="figure">Figure 4</ref>: The explicit Euler method and RK4 in a step. To derive ( + ) from ( ) with a step size , RK4 is four times more complicated than the explicit Euler method. Note that the explicit Euler method is the same as the residual connection and RK4 is the same as the dense connection when is a neural network layer.</p><formula xml:id="formula_3">+ f1 f2 + f3 + f4 + (b) RK4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Residual/Dense Connections and ODE Solvers</head><p>Many researchers discuss about the analogy between residual/dense connections and ODE solvers. ODE solvers discretize time variable and convert an integral into many steps of additions. For instance, the explicit Euler method can be written as follows in a step:</p><formula xml:id="formula_4">( + ) = ( ) + ? ( ( ), ; ),<label>(2)</label></formula><p>where , which is usually smaller than 1, is a configured step size of the Euler method. Note that this equation is identical to a residual connection when = 1.</p><p>Other ODE solvers use more complicated methods to update ( + ) from ( ). For instance, the fourth-order Runge-Kutta (RK4) method uses the following method:</p><formula xml:id="formula_5">( + ) = ( ) + 6 1 + 2 2 + 2 3 + 4 ,<label>(3)</label></formula><p>where 1 = ( ( ), ; ), 2 = ( ( ) + 2 1 , + 2 ; ), 3 = ( ( ) + 2 2 , + 2 ; ), and 4 = ( ( ) + 3 , + ; ).</p><p>It is also known that dense convolutional networks (DenseNets <ref type="bibr" target="#b46">[47]</ref>) and fractal neural networks (FractalNet <ref type="bibr" target="#b25">[26]</ref>) are similar to RK4 (as so are residual networks to the explicit Euler method) <ref type="bibr" target="#b27">[28]</ref>. For simplicity but without loss of generality, however, we use the explicit Euler method as our running example.</p><p>One more ODE solver that is worth mentioning is the implicit Adams-Moulto method which is written as follows:</p><formula xml:id="formula_6">( + ) = ( ) + 24 9 1 + 19 2 ? 5 3 + 4 ,<label>(4)</label></formula><p>where 1 = ( ( + ), + ; ), 2 = ( ( ), ; ), 3 = ( ( ? ), ? ; ), and 4 = ( ( ? 2 ), ? 2 ; ). Both 3 and 4 are from previous history and we do not need to newly evaluate them. This implicit method is different from the aforementioned explicit solvers in that i) it uses past multi-step history, i.e., 3 and 4 , to calculate a more robust derivative term and ii) it uses 1 in conjunction with the multi-step history. At the moment of time , however, it is before calculating ( + ) so evaluating 1 cannot be done in a naive way. It uses advanced methods, such as Newton's method, to solve for ( + ). The use of 1 is called implicit in the field of numerical methods to solve ODEs. Its analogy to neural network connection has not been studied yet due to the implicit nature of the method. However, it falls into the category of dense networks because it uses multi-step information.</p><p>For our experiments, we consider all those advanced solvers, which is one more advantage of our formulating the graph-based CF as the dual co-evolving ODEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Collaborative Filtering (CF)</head><p>Let 0 ? R ? and 0 ? R ? be the initial user and product embeddings, respectively. There are users and products, and embeddings are dimensions. Early CF methods include matrix factorization <ref type="bibr" target="#b24">[25]</ref>, SVD++ <ref type="bibr" target="#b22">[23]</ref>, neural attentive item similarity <ref type="bibr" target="#b16">[17]</ref>, and so on. All these methods utilize user-product interaction history <ref type="bibr" target="#b45">[46]</ref>.</p><p>Because user-product relationships can be represented by bipartite graphs, it recently became popular to adopt GCNs for CF <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37]</ref>. NGCF is one of the most popular GCN-based CF methods. It uses non-linear activations and transformation matrices to transform from the user embedding space to the product embedding space, and vice versa. At each layer, user and product embeddings are extracted as in the layer combination. However, it concatenates them instead of taking their sum. Its overall architecture is similar to the standard GCN <ref type="bibr" target="#b21">[22]</ref>. However, it was later noted that the adoption of the non-linear activation and the embedding space transformation are not necessary in CF due to the environmental dissimilarity between general graph-based downstream tasks and CF <ref type="bibr" target="#b15">[16]</ref>. That is, other graph-based tasks include abundant information, e.g., high-dimensional node features. However, CF frequently includes a bipartite graph without additional features. Even worse, the graph is sparse in CF. Due to the difference, non-linear GCNs are easily overfitted to training graphs and their testing accuracy is mediocre in many cases even with various countermeasures preventing it. It was also empirically proven that transforming between user and product embedding spaces is not helpful in CF <ref type="bibr" target="#b15">[16]</ref>.</p><p>After NGCF, several methods have been proposed. Among them, in particular, one recent graph-based method, called LightGCN, shows state-of-the-art accuracy in many datasets. In addition, it also showed that linear GCNs with layer combination work the best among many design choices. Its linear graph convolutional layer definition is as follows:</p><formula xml:id="formula_7">=?? ?1 , =?? ?1 ,<label>(5)</label></formula><p>where?? ? [0, 1] ? is a normalized adjacency matrix of the graph from products to users and?? ? [0, 1] ? is also defined in the same way but from users to products. LightGCN learns the initial embeddings, denoted 0 and 0 , and uses the layer combination, which can be written as follows:</p><formula xml:id="formula_8">= ?? =0 , = ?? =0 ,<label>(6)</label></formula><p>where is the number of layers, is a coefficient, and and are the final embeddings. The CF methods, including our method, LightGCN, and so on, learn the initial embeddings of users and products (and model parameters if any). After a series of graph convolutional layers, a graph-based CF algorithm derives and and use their dot products to predict , , a rating (or ranking score) by user to product , for all , . Ones typically use the following Bayesian personalized ranking (BPR) loss <ref type="bibr" target="#b31">[32]</ref> to train the initial embedding vectors (and model parameters if any) in the field of CF:</p><formula xml:id="formula_9">= ? ?? =1 ?? ?N ?? ?N ln ( , ? , ) + ? 0 ? 0 ? 2 ,<label>(7)</label></formula><p>where N is a set of products neighboring to , is a non-linear activation, and ? means the concatenation operator. We use the softplus for .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Linear GCNs and Newton's Law of Cooling</head><p>As a matter of fact, Eq. <ref type="formula" target="#formula_7">(5)</ref> is similar to the heat equation, which describes the law of thermal diffusive processes, i.e., Newton's Law of Cooling. The heat equation can be written as follows:</p><formula xml:id="formula_10">= ?? ,<label>(8)</label></formula><p>where ? is the Laplace operator and is a column vector which contains the temperatures of the nodes in a graph or a discrete grid at time . The Laplace operator ? is simply a matrix multiplication with the Laplacian matrix or the normalized adjacency matrix.</p><p>Therefore, the right-hand side of Eq. (5) can be reduced to Eq. (8) if we interpret each element of and as a temperature value -since they are -dimensional vectors, we can consider that different diffusive processes exist in Eq. <ref type="bibr" target="#b4">(5)</ref>. In this regard, we can consider that LightGCN models discrete thermal diffusive processes whereas our method describes continuous thermal diffusive processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In this section, we describe our proposed method. Our main idea is to design co-evolutionary ODEs of user and product embeddings with a continuous and learnable time variable .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>In <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, we show the overall architecture of LT-OCF. The two initial embeddings, 0 and 0 , are fed into the dual co-evolutionary ODEs. Then, we have the layer combination architecture to derive the final embeddings. The distinguished feature of LT-OCF lies in the dual ODE layer, where we can interpret the time variable as a continuous layer variable.</p><p>LT-OCF enjoys the continuous characteristic of and construct a more flexible architecture. In LightGCN and other existing GCNbased CF methods, we have to use pre-determined discrete architectures. However, LT-OCF can use any positive real numbers for and those numbers are even trainable in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ODE-based User and Product Embeddings</head><p>The user and product embedding co-evolutionary processes can be written as follows:</p><formula xml:id="formula_11">( ) = (0) + ? 0 ( ( )) , ( ) = (0) + ? 0 ( ( )) ,<label>(9)</label></formula><p>where ( ) ? R ? is a user embedding matrix and ( ) ? R ? is a product embedding matrix at time . ( ( )) outputs <ref type="bibr">( )</ref> and ( ( )) outputs <ref type="bibr">( )</ref> . (0) = 0 and (0) = 0 in our case because the initial embeddings are directly fed into the ODEs (cf. <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>). We note that ( ) and ( ) constitute a set of co-evolving ODEs. User embedding influences product embedding and vice versa. Therefore, our co-evolving ODEs are a reasonable design choice.</p><p>However, this formulation does not fully describe our proposed concept of learnable-time and we propose a more advanced formulation in the next paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Learnable-time Architecture. In our framework, we can learn how to construct the layer combination (rather than relying on a manually designed architecture). In order to adopt such an advanced option, we extract ( ) and ( ) with several different learnable time-points ? { 1 , ? ? ? , }, where is a hyperparameter, and 0 &lt; &lt; +1 &lt; for all . Therefore, Eq. (9) can be re-written as follows:</p><p>( 1 ) = (0)+  </p><formula xml:id="formula_12">( ) = ( )+ ? ( ( )) ,<label>(10)</label></formula><p>where is trainable for all . The parts of the equation highlighted in red are used to create residual connections (cf. the red residual connections inside the ODEs in <ref type="figure" target="#fig_5">Fig. 5</ref>). The final embeddings are calculated as follows:</p><formula xml:id="formula_13">= 0 (0) + ?? =1 ( ) + ( ), = 0 (0) + ?? =1 ( ) + ( ).<label>(11)</label></formula><p>Recall that what the explicit Euler method does internally is to generalize residual connections in a continuous manner. So, extracting intermediate ODE states (i.e., ( ) and ( ) with ? { 1 , ? ? ? , }) and creating a higher level of layer combination in Eq. (11) correspond to dual residual connections (cf. the blue layer combination outside the ODEs in <ref type="figure" target="#fig_5">Fig. 5</ref>).  If using other advanced ODE solvers, the connections inside the ODEs become more sophisticated, e.g., DenseNet or FractalNet connections if RK4 is used. In <ref type="table" target="#tab_1">Table 1</ref>, we summarize all those cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Non-parameterized and Non-time-dependent</head><p>ODEs. We need to define the two ODE functions, and . Being inspired by the recent success of linear graph convolutions, we use the following definition for and :</p><p>( ( )) =?? ( ),</p><formula xml:id="formula_14">( ( )) =?? ( ),<label>(12)</label></formula><p>where?means either the symmetric normalized Laplacian matrix or the normalized adjacency matrix. LightGCN uses the latter but our method based on the continuous thermal diffusive differential equation can use both of them. We note that our definitions for and will result in nonparameterized and non-time-dependent ODEs because our ODE functions do not require , , and as their input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relation with Linear GCN-based CF Methods.</head><p>There exist several linear GCNs. LightGCN studied about the similarity among various such linear GCN models and showed many other linear models can be approximated as a special case of LightGCN. In this subsection, we study about the similarity between our method and LightGCN.</p><p>Suppose the following setting in our method: i) is not trained but fixed to for all , ii) We use the explicit Euler method with its step size parameter = 1, and iii) We do not use the residual connection but the linear connection inside the ODEs, i.e., removing the red parts in Eq. (10). This specific setting can be written as follows:</p><p>(1) = ( (0)),</p><formula xml:id="formula_15">(1) = ( (0)), . . . ( ) = ( ( ? 1)), ( ) = ( ( ? 1)).<label>(13)</label></formula><p>After that, the linear combination yields = =0 ( ) and = =0 ( ). We note that these final embeddings are equivalent to Eq. <ref type="formula" target="#formula_8">(6)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How to Train.</head><p>Our proposed method includes a couple of sophisticated techniques and its training algorithm is inevitably more complicated than other cases. We also use the BPR loss, denoted , to train our model, which is common for many CF methods. We propose to alternately train the co-evolving ODEs and their intermediate time points. When training for one, we fix all other parts. This makes the gradient calculation by the adjoint sensitivity method simple because a fixed ODE/time point can be considered as constants at a moment of training time. The gradients of loss w.r.t. (0), which is identical to the initial embedding 0 , can be calculated via the following reverse-mode integration <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_16">(0) = ( ) ? ? 0 ( ) ? ( ( )) ( ) ,<label>(14)</label></formula><p>where ( ) def = ( ) . The gradients of loss w.r.t. (0), the initial embedding of products, can be done in the same way and we omit its description for space reasons. Calculating the gradients requires a space complexity of O (1) and a time complexity of O ( 1 ), where is the average step-size of underlying ODE solver which is fixed for the Euler method and RK4 and varied for DOPRI, because we use the adjoint sensitivity method.</p><p>The gradient of loss w.r.t. does not involve the adjoint sensitivity method but is defined directly as follows:</p><formula xml:id="formula_17">= ( ) ( ) + ( ) ( ) = ( ) ( ( )) + ( ) ( ( )),<label>(15)</label></formula><p>where its complexity is O ( ) to train all time-points. Our propose training algorithm is in Alg. 1. We alternately train each part until the BPR loss converges. Update for all ; 6 return 0 and 0 ; , there exists a unique solution of if is analytic (or locally Lipschitz continuous), i.e. the ODE problem is well-posed if is analytic <ref type="bibr" target="#b11">[12]</ref>. In our case, Eq. <ref type="bibr" target="#b11">(12)</ref>, which is to model <ref type="bibr">( )</ref> and <ref type="bibr">( )</ref> , uses matrix multiplications that are analytic. This implies that there will be only a unique optimal ODE for ( ), given fixed ( ) and vice versa. Because of i) the uniqueness of the solution and ii) our relatively simpler definitions of and in comparison with other NODE applications, we believe that our training method can find a good solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATIONS</head><p>In this section, we introduce our experimental environments and results. All experiments were conducted in the following software and hardware environments: Ubuntu 18.04 LTS, Python 3.6.6, Numpy 1.18.5, Scipy 1.5, Matplotlib 3.3.1, PyTorch 1.2.0, CUDA 10.0, and NVIDIA Driver 417.22, i9 CPU, and NVIDIA RTX Titan. Our source codes and data are at https://github.com/jeongwhanchoi/LT-OCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Environments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Baselines.</head><p>We use the three benchmark datasets used by previous works without any modifications: Gowalla, Yelp2018, and Amazon-Book <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37]</ref>. Their statistics are summarized in <ref type="table" target="#tab_3">Table 2</ref>. We consider the following baselines to compare with:</p><p>(1) MF <ref type="bibr" target="#b31">[32]</ref> is a matrix decomposition optimized by Bayesian Personalization Rank (BPR) loss, which utilizes the user-item direct interaction only as the target value of the interaction function.</p><p>(2) Neu-MF is a neural collaborative filtering method <ref type="bibr" target="#b17">[18]</ref>. This method uses multiple hidden layers above the element-wise concatenation of user and item embeddings to capture their non-linear feature interactions. (3) CMN <ref type="bibr" target="#b10">[11]</ref> is a state-of-the-art memory-based model. This method uses first-order connections to find similar users who interacted with the same items.</p><p>(4) HOP-Rec <ref type="bibr" target="#b42">[43]</ref> is a graph-based model, which uses the highorder user-item interactions by random walks to enrich the original training data. (5) GC-MC <ref type="bibr" target="#b32">[33]</ref> is a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. This method applies the GCN encoder on user-item bipartite graph and employs one convolutional layer to exploit the direct connections between users and items. (6) Mult-VAE is a variational autoencoder-based CF method <ref type="bibr" target="#b26">[27]</ref>.</p><p>We use a drop-out rate of {0, 0.2, 0.5} and of {0.2, 0.4, 0.6, 0.8}. The layer-wise dimensionality is 600, 200, and then 600 as recommended in the paper and the authors. <ref type="bibr" target="#b6">(7)</ref> GRMF is a matrix factorization method by adding the graph Laplacian regularizer <ref type="bibr" target="#b30">[31]</ref>. We change the original loss of GRMF to the BPR loss for fair comparison. GRMG-norm is a slight variation from GRMF by adding a normalization to graph Laplacian. (8) NGCF <ref type="bibr" target="#b36">[37]</ref> is a representative GCN-based CF method. It uses feature transformation and non-linear activations. (9) LR-GCCF <ref type="bibr" target="#b4">[5]</ref> and LightGCN <ref type="bibr" target="#b15">[16]</ref> are linear GCN-based CF methods. They currently show state-of-the-art accuracy.</p><p>We use the two standard evaluation metrics, Recall@20 and NDCG@20, with the all-ranking protocol, i.e., all items that do not have any interactions with a user are recommendation candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Hyperparameters.</head><p>Our method and the above baseline models have several common hyperparameters. In this paragraph, we introduce them.</p><p>(1) The regularization coefficient in all methods is in {1.0e?4, 1.0e?3, 1.0e?2}. (2) The dimensionality of embedding vectors is 64 as recommended in <ref type="bibr" target="#b15">[16]</ref>, and a Normal distribution of N (0, 0.1) is used to set initial embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we summarize the overall accuracy in terms of recall and NDCG. The non-linear GCN-based method, NGCF, shows good performance for a couple of cases in comparison with other non-GCNbased methods. After that, LightGCN shows the state-of-the-art accuracy in all cases among all baselines. It sometimes outperforms   other methods by large margins, e.g., a recall of 0.1830 in Gowalla by Light GCN vs. a recall of 0.1641 by Multi-VAE. In general, the three GCN-base methods, NGCF, LR-GCCF, and LightGCN, outperform other baseline methods by large margins. However, the best accuracy is straightly marked by our method, LT-OCF, in all cases. All those best results are achieved by RK4, which implies that the linear GCN architecture of LightGCN may not be the best option (see our discussion in Section 5). In particular, our method's NDCG in Amazon-Book shows an improvement of approximately 10% over LightGCN.</p><p>In <ref type="figure" target="#fig_9">Figure 6</ref>, we compare the training curve of LightGCN and LT-OCF in Gowalla. In general, our method provides a faster training speed in terms of the number of epochs than that of LightGCN. In <ref type="figure" target="#fig_9">Figure 6</ref> (d), we show that becomes larger (with a little fluctuation) as training goes on. It is because our model prefers embeddings   from deep layers when constructing the layer combination. 1 is more actively trained and 3 is not trained much. According to this training pattern, we can know that it is more important to have reliable early layers for the layer combination. In <ref type="figure" target="#fig_11">Figures 7 and 8</ref>, we show the results of the same experiment types for Yelp2018 and Amazon-Book. For Amazon-Book, LT-OCF shows remarkably smaller loss values than that of LightGCN as shown in <ref type="figure" target="#fig_12">Figure 8</ref> (a). In <ref type="figure" target="#fig_11">Figures 7 (b,c) and 8 (b,c)</ref>, our method shows faster training for recall and NDCG than LightGCN. In <ref type="figure" target="#fig_12">Figure 8 (d</ref> For other datasets, we can observe similar patterns. RK4 consistently outperforms the Euler method. Adams-Moulto shows almost the same performance as that of RK4. However, it is an implicit ODE solver that requires more computation than other explicit methods, e.g., RK4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Sensitivity on .</head><p>By varying , we also investigate how the model accuracy changes. The detailed results are in <ref type="figure" target="#fig_1">Figure 10</ref>. One point that is worth mentioning is that the fixed-time is sometimes more vulnerable to small than the learnable-time. In other words, the recall/NDCG gap between the fixed and the learnable-time at = 1 is larger than that in = 2, 3 for Gowalla. In general, the recall increases as we increase but it is stabilized after = 3. Therefore, our best setting for is 3 in our experiments, considering computational efficiency. We can observe similar patterns in Amazon-Book as well. <ref type="figure" target="#fig_1">Figure 11</ref>. Our best results are all made with = 4. As decreasing , we observe that performance also decreases. For instance, our method (RK4, learning ) achieves a recall/NDCG of 0.1833/0.1545 with = 3 and a recall/NDCG of 0.1823/0.1543 with = 2 in Gowalla. Similar patterns are observed in other two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Sensitivity on . By varying , we investigate how the model accuracy changes in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Fixed vs. Learnable-time.</head><p>Without learning , we fix = +1 and evaluate its accuracy. The learnable-time is one of the key concepts in our work. Without learning , our method ( =4, RK4) still outperforms LightGCN in many cases but is consistently worse than our method with learning . For instance, our method without learning achieves a recall/NDCG of 0.1859/0.1558 vs. a recall/NDCG of 0.1875/0.1574 by our method with learning in Gowalla. Similar patterns are observed in other two datasets.</p><p>In particular, a combination of = 2 and fixed-time shows poor performance in all cases of <ref type="figure" target="#fig_1">Figure 11</ref>. However, = 2 with learningtime surprisingly shows much improvement over it, which shows the efficacy of our proposed learnable-time concept.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Runtime Analyses</head><p>We solve ODE problems in our method and as a result, need longer time to train and infer than other methods. We use Amazon-Book, the largest and the most suitable dataset for this subsection, to analyze the runtime of various algorithms. Among many baselines in <ref type="table" target="#tab_4">Table 3</ref>, we mainly compare with LightGCN since it shows stateof-the-art accuracy and has a smaller complexity than other nonlinear methods. As shown in <ref type="figure" target="#fig_1">Fig. 12</ref>, LightGCN is the fastest method for both training and testing. However, our method with the Euler method provides better recall scores in 30 to 50% longer time. Even though RK4 shows the best accuracy, one can choose the Euler method to reduce the time complexity. In this regard, our proposed method is a versatile algorithm for CF, which provides a good tradeoff between complexity and accuracy. Our method shows similar runtime patterns in other datasets as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSIONS ON LINEAR VS. DENSE</head><p>We revisit recent debates on figuring out the best GCN architecture for CF. It was recently reported that linear layers with a layer combination work well. However, we found that dense layers with a layer combination are better. As reported in the previous section, our best accuracy was all achieved by RK4, which internally constructs connections similar to DenseNet or FractalNet as described in <ref type="table" target="#tab_1">Table 1</ref>  <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>. This is well aligned with the observation in ODEs that the explicit Euler method is inferior to RK4 in solve integral problems. We conjecture that dense connections are also optimal for non-ODE-based CF methods. We leave this as an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We tackled the problem of learnable-time ODE-based CF. Our method fundamentally differs from other methods in that we interpret the user and product embedding learning process of CF as dual co-evolving ODEs.</p><p>Owing to the continuous nature of time variable in NODEs, we propose to train a set of time points { 1 , 2 , ? ? ? , }, where we extract embedding vectors to construct a layer combination architecture. Our carefully designed training method guarantees a good solution by the well-posed nature of our formulation.</p><p>With the benchmark datasets, our method, LT-OCF, consistently outperforms all state-of-the-art methods in all cases. We also showed that our method can be trained faster than other methods.</p><p>One more key contribution in this paper is that we revealed that dense connections, which are created by RK4, are the best option for our method and leave it as an open question to apply dense connections to other CF methods. We hope that this discovery will inspire forthcoming research works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Noseong Park is the corresponding author. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-01361, Artificial Intelligence Graduate School Program (Yonsei University)).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The architecture of LightGCN. The outer loop created by a series of additions, denoted ?, is called layer combination. Our proposed LT-OCF. The detailed diagram of the ODE layer is inFig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) The linear architecture of LightGCN with a layer combination, and (b) Our proposed LT-OCF. In our design, two ODEs for user and product embeddings co-evolve over time and influence each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The typical architecture of NODEs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Our proposed dual co-evolving ODEs. In this figure, we assume the explicit Euler method. The residual connections inside the ODEs are in red and the layer combination is in blue. The time-points { 1 , ? ? ? , } to construct the layer combination are trained in our framework. In this figure, each ODE internally uses two steps (i.e., two red residual connections) to solve an integral problem but this is only for illustration purposes. In real environments, the number of steps for each ODE can be varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 : 3 Update</head><label>13</label><figDesc>How to train 0 and 0 Input: Rating matrix 1 Initialize 0 and 0 ; 2 while the BPR loss is not converged do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 3 )</head><label>3</label><figDesc>The layer combination coefficient = 1 1+ , where is the number of elements in the layer combination. (4) The number of elements is in {2,3,4}. (5) The number of learnable intermediate time points is in {1,2,3}. (6) We use the same early stopping criterion as that of NGCF and train with Adam and a learning rate in {1.0e?5, 1.0e?4, 1.0e?3, 1.0e?2}. (7) We consider the following ODE solvers: the explicit Euler method, RK4, Adams-Moulto, and DOPRI. The best configuration set in each data is as follows: In Gowalla, = 1.0e?4, learning rate = 1.0e?4, learning rate for time = 1.0e?6, = 4, = 3; In Yelp2018, = 1.0e?4, learning rate = 1.0e?5, learning rate for time = 1.0e?6, = 4, = 3; In Amazon-Book, = 1.0e?4, learning rate = 1.0e?4, learning rate = 1.0e?6, = 4, = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Training curves in Gowalla. (a) The loss curves, (b) The training curves of Recall, (c) The training curves of NDCG, (d) The training curves of 1 , 2 , 3 when = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Training curves in Yelp2018. (a) The loss curves, (b) The training curves of Recall, (c) The training curves of NDCG, (d) The training curves of 1 , 2 , 3 when = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Training curves in Amazon-Book. (a) The loss curves, (b) The training curves of Recall, (c) The training curves of NDCG, (d) The training curves of 1 , 2 , 3 when = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 2 3 T</head><label>3</label><figDesc>(# of time points to learn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Performance comparison by varying</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Performance comparison by varying</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Training and inference time in Amazon-Book</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>21, November 1-5, 2021, Virtual Event, Australia. ? 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00 https://doi.org/10.1145/3459637.3482449</figDesc><table><row><cell>initial user embedding</cell><cell>E0</cell><cell>Linear</cell><cell>E 1</cell><cell>E K?1</cell><cell>Linear</cell><cell>EK</cell><cell>final user embedding</cell></row><row><cell></cell><cell></cell><cell>Graph</cell><cell></cell><cell></cell><cell>Graph</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Conv.</cell><cell></cell><cell></cell><cell>Conv.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>E 1</cell><cell>E K?1</cell><cell></cell><cell>EK</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Various network architectures (inside and outside the ODEs in Fig. 5) created by various ODE solvers</figDesc><table><row><cell>ODE Solver</cell><cell cols="2">Inside the ODEs Outside the ODEs</cell></row><row><cell>Explicit Euler</cell><cell>Linear</cell></row><row><cell>(without the red parts in Eq. (10))</cell><cell>Connection</cell></row><row><cell>Explicit Euler</cell><cell>Residual</cell></row><row><cell>(with the red parts in Eq. (10))</cell><cell>Connection</cell></row><row><cell></cell><cell>DenseNet or</cell><cell>Layer</cell></row><row><cell>RK4</cell><cell></cell></row><row><cell></cell><cell>FractalNet</cell><cell>Combination</cell></row><row><cell></cell><cell>Implicit</cell></row><row><cell>Adams-Moulto</cell><cell></cell></row><row><cell></cell><cell>Connection</cell></row><row><cell></cell><cell>Adaptive</cell></row><row><cell>DOPRI</cell><cell></cell></row><row><cell></cell><cell>Connection</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets</figDesc><table><row><cell>Name</cell><cell cols="3">#Users #Items #Interactions</cell></row><row><cell>Gowalla</cell><cell cols="2">29,858 40,981</cell><cell>1,027,370</cell></row><row><cell>Yelp2018</cell><cell cols="2">31,668 38,048</cell><cell>1,561,406</cell></row><row><cell cols="3">Amazon-Book 52,643 91,599</cell><cell>2,984,108</cell></row><row><cell cols="4">3.3.1 On the Tractability of Training. The Cauchy-Kowalevski</cell></row><row><cell cols="2">theorem states that, given =</cell><cell>( )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Model performance comparison. LT-OCF significantly outperforms other methods in Amazon-Book.</figDesc><table><row><cell>Dataset</cell><cell>Gowalla</cell><cell>Yelp2018</cell><cell>Amazon-Book</cell></row><row><cell>Method</cell><cell cols="3">Recall NDCG Recall NDCG Recall NDCG</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>),<ref type="bibr" target="#b0">1</ref> is trained a lot more than other cases in Figures 6 (d) and7 (d)    4.3 Ablation and Sensitivity Studies as our default solver. As mentioned earlier, RK4 shows better accuracy than that of the Euler method in solving general ODE problems and we observe the same result. For instance, our method ( =4, learning ) with the Euler method achieves a recall/NDCG of 0.1834/0.1548 vs. 0.1875/0.1574 with RK4 in Gowalla.</figDesc><table><row><cell>Recall</cell><cell>0.02 0.04 0.06 0.08 0.10 0.18 0.16 0.14 0.12 0.00</cell><cell>Gowalla 0 20 40 60 80 100 120 140 160 DOPRI5 RK4 Euler Epoch</cell><cell>4.3.1 Euler vs. RK4 vs. DOPRI. We first compare various ODE solvers. Fig-ure 9 summarizes train-ing curves of various ODE solvers. In general, DOPRI and RK4 are al-most the same in terms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of recall and NDCG while</cell></row><row><cell cols="3">Figure 9: Various ODE solvers</cell><cell>RK4 has 33% smaller com-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>putation complexity. So,</cell></row><row><cell cols="3">we use RK4</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Context-Aware Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="191" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02136</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR. 1-14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Survey of Collaborative Filtering-Based Recommender Systems: From Traditional Methods to Hybrid Methods Based on Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="64301" to="64320" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09375</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A family of embedded Runge-Kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Augmented Neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collaborative Memory Network for Recommendation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerald B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Folland</surname></persName>
		</author>
		<title level="m">Introduction to Partial Differential Equations: Second Edition</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-Scale Learnable Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219947</idno>
		<idno type="arXiv">arXiv:1808.03965</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219947" />
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="2053" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NAIS: Neural Attentive Item Similarity Model for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Neural Collaborative Filtering. In WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LightMove: A Lightweight Next-POI Recommendation for Taxicab Rooftop Advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soyoung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minju</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noseong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyoung</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minju</forename><surname>Sheo Yon Jhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyong</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
		<title level="m">Jinsung Jeon, and Noseong Park. 2021. ACE-NODE: Attentive Co-Evolving Neural Ordinary Differential Equations. In KDD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OCT-GAN: Neural ODE-based Conditional Tabular GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TheWebConf (former WWW)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Jihyeon Hyeong, and Noseong Park</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factor in the neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FractalNet: Ultra-Deep Neural Networks without Residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TheWebConf (former WWW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<title level="m">Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations. In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08071</idno>
		<title level="m">Atsushi Yamashita, and Hajime Asama. 2020. Dissecting Neural ODEs</title>
		<imprint/>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural Ordinary Differential Equations for Semantic Segmentation of Individual Colon Glands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Pinckaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10470</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<title level="m">Collaborative Filtering with Graph Information: Consistency and Scalable Methods. In NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR 2018. 1-12</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Knowledge Graph Convolutional Networks for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TheWebConference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph Wavelet Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07785</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR. 1-13</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05513</idno>
		<title level="m">On Robustness of Neural Ordinary Differential Equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HOP-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
		<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Survey of Collaborative Filtering-Based Recommender Systems for Mobile Internet Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3273" to="3287" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep Learning Based Recommender System: A Survey and New Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08831</idno>
		<title level="m">Convolutional Neural Networks combined with Runge-Kutta Methods</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
