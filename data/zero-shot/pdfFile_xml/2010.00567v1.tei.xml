<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TH?SE Deep Learning For Time Series Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-01">1 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Prof</roleName><forename type="first">Jourdan</forename><surname>Laetitia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Prof</roleName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dr</roleName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Universit?</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haute-Alsace</forename><surname>R?sum?</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UNIVERSIT? DE HAUTE-ALSACE UNIVERSIT? DE STRASBOURG</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Docteur de l&apos;Universit? de Haute-Alsace ?cole Doctorale: Math?matiques, Sciences de l&apos;Information et de l&apos;Ing?nieur (</orgName>
								<address>
									<postCode>269)</postCode>
									<region>ED</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Universit? de Lille (Examinatrice)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">-Alsace (Directeur de th?se) Prof. Lhassane IDOUMGHAR, Universit? de Haute-Alsace (co-Directeur de th?se) Prof. Germain FORESTIER, Universit? de Haute-Alsace (co-Directeur de th?se)</orgName>
								<orgName type="institution">Universit? de Haute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Universit? de Haute-Alsace</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Facult? des Sciences et Techniques Institut de Recherche en Informatique, Math?matiques, Automatique et Signal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">UNIVERSIT? DE HAUTE-ALSACE</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TH?SE Deep Learning For Time Series Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-01">1 Oct 2020</date>
						</imprint>
					</monogr>
					<note>Ann?e 2020 N Pour l&apos;obtention du grade de Discipline : Informatique Pr?sent?e et soutenue publiquement par Hassan ISMAIL FAWAZ Le 21 Septembre 2020 Sous la direction du Prof. Pierre-Alain MULLER Jury Prof. Themis PALPANAS, Universit? de Paris (Rapporteur) Prof. Pierre-Fran?ois MARTEAU, Universit? Bretagne Sud (Rapporteur) Prof. Anthony BAGNALL, Universit? d&apos;East Anglia (Examinateur)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 An example illustrating the task of classifying an input time series from the GunPointAgeSpan dataset <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, where the goal is to classify whether or not a person is holding a gun (a time series corresponds to the hand's x coordinate of a person holding or not a gun) 14 tuning a pre-trained model. The rows' indexes correspond to the source datasets and the columns' indexes correspond to the target datasets. The red color shows the extreme case where the chosen pair of datasets (source and target) deteriorates the network's performance. Where on the other hand, the blue color identifies the improvement in accuracy when transferring the model from a certain source dataset and fine-tuning on another target dataset. The white color means that no change in accuracy has been identified when using the transfer learning method for two datasets. The matrix actually has a size of 85 ? 85 (instead of 85 ? 84) for visual clarity with its diagonal left out of the analysis. (Best viewed in color  <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> containing spectrographs of coffee beans). . . 87 2.18 Example of perturbing the classification of an input time series from the TwoLeadECG <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>  Chapitre 1: L'?tat de l'art de la classification de s?ries temporelles Au cours des deux derni?res d?cennies, la Classification de S?ries Temporelles (CST) a ?t? consid?r? comme l'un des probl?mes les plus difficiles dans la fouille de donn?es <ref type="bibr" target="#b256">(Yang and Wu, 2006;</ref><ref type="bibr" target="#b54">Esling and Agon, 2012)</ref>. Avec l'augmentation de la disponibilit? des donn?es temporelles <ref type="bibr" target="#b203">(Silva et al., 2018)</ref>, des centaines d'algorithmes de CST ont ?t? propos?s depuis 2015 . En raison de leur ordre temporel, les s?ries sont pr?sentes dans presque tout probl?me de fouille de donn?es <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. En fait, tout probl?me de classification, utilisant des donn?es enregistr?es ayant un ordre sp?cifique, peut ?tre converti en un probl?me de CST <ref type="bibr" target="#b45">(Cristian Borges Gamboa, 2017)</ref>. Les s?ries temporelles sont pr?sentes dans de nombreuses applications du monde r?el, allant des soins de sant? <ref type="bibr" target="#b70">(Gogolou et al., 2018</ref>) ? la reconnaissance de l'activit? humaine <ref type="bibr" target="#b154">Mathis, Ismail Fawaz, and Khamis, 2020)</ref> jusqu'? la classification des sc?nes acoustiques <ref type="bibr" target="#b167">(Nwe, Dat, and Ma, 2017)</ref> et la cybers?curit? <ref type="bibr" target="#b209">(Susto, Cenedese, and Terzi, 2018)</ref>. De plus, la diversit? des types d'ensembles de donn?es dans l'archive UCR/UEA <ref type="bibr" target="#b51">(Dau et al., 2019;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017)</ref> (la plus grande base de donn?es de r?f?rences de s?ries temporelles) montre les diff?rentes applications de la CST. Compte tenu de la n?cessit? de classer avec pr?cision les s?ries temporelles, les chercheurs ont propos? des centaines de m?thodes pour r?soudre cette t?che . L'une des approches de CST les plus populaires et traditionnelles est l'utilisation d'un classifieur NN coupl? ? une fonction de distance <ref type="bibr" target="#b136">(Lines and Bagnall, 2015)</ref>. En particulier, DTW lorsqu'il est utilis? avec un classifieur NN s'est r?v?l? ?tre une m?thode de r?f?rence tr?s solide . <ref type="bibr" target="#b136">Lines and Bagnall, 2015</ref> ont compar? plusieurs mesures de distance -telles que TWE <ref type="bibr" target="#b150">(Marteau, 2009</ref>) et MSM <ref type="bibr" target="#b207">(Stefan, Athitsos, and Das, 2013</ref>) -montrant qu'il n'y a pas de mesure de distance unique qui surpasse DTW. Ils ont ?galement montr? que l'ensembling des classifieurs NN individuels (avec diff?rentes mesures de distance) surpasse toutes les composantes individuelles de l'ensemble. Par cons?quent, les contributions r?centes se sont concentr?es sur le d?veloppement de m?thodes d'ensembling qui supplante consid?rablement le NN-DTW <ref type="bibr" target="#b81">Hills et al., 2014;</ref><ref type="bibr" target="#b30">Bostrom and Bagnall, 2015;</ref><ref type="bibr" target="#b135">Lines, Taylor, and Bagnall, 2016;</ref><ref type="bibr" target="#b200">Sch?fer, 2015;</ref><ref type="bibr" target="#b110">Kate, 2016;</ref><ref type="bibr" target="#b53">Deng et al., 2013;</ref><ref type="bibr" target="#b22">Baydogan, Runger, and Tuv, 2013)</ref>. Ces approches utilisent soit un ensemble d'arbres de d?cision (for?t al?atoire) <ref type="bibr" target="#b22">(Baydogan, Runger, and Tuv, 2013;</ref><ref type="bibr" target="#b53">Deng et al., 2013)</ref> ou un ensemble de diff?rents types de classifieurs (SVM, NN avec plusieurs distances) sur une ou plusieurs transformations <ref type="bibr" target="#b30">Bostrom and Bagnall, 2015;</ref><ref type="bibr" target="#b200">Sch?fer, 2015;</ref><ref type="bibr" target="#b110">Kate, 2016)</ref>. La plupart de ces approches obtiennent des r?sultats significativement meilleurs que NN-DTW  et partagent une propri?t? commune, qui est la phase de transformation des donn?es o? les s?ries temporelles sont transform?es en un nouvel espace de description (par exemple en utilisant des transformations shapelets <ref type="bibr" target="#b30">(Bostrom and Bagnall, 2015)</ref> ou des caract?ristiques DTW <ref type="bibr" target="#b110">(Kate, 2016)</ref>). Cette notion a motiv? le d?veloppement d'un ensemble de 35 classifieurs nomm? COTE  qui non seulement regroupe diff?rents classifieurs sur la m?me transformation, mais regroupe ? la place diff?rents classifieurs utilisant plusieurs transformations latentes de s?ries temporelles. <ref type="bibr" target="#b135">Lines, Taylor, and Bagnall, 2016;</ref><ref type="bibr" target="#b137">Lines, Taylor, and Bagnall, 2018</ref> ont ?tendu COTE avec HIVE-COTE qui permet une am?lioration significative par rapport ? COTE en tirant parti d'une nouvelle structure hi?rarchique avec un vote probabiliste, comprenant deux nouveaux classifieurs et deux domaines de transformation de repr?sentation suppl?mentaires. HIVE-COTE est actuellement consid?r? comme l'?tat de l'art pour la classification des s?ries temporelles  lorsqu'il est ?valu? sur les 85 jeux de donn?es de l'archive UCR/UEA.</p><p>Pour atteindre sa haute pr?cision, HIVE-COTE devient extr?mement intensif en calcul et peu pratique pour fonctionner sur un vrai probl?me d'exploration de donn?es volumineuses . L'approche n?cessite l'entrainement de 37 classifieurs ainsi que la validation crois?e de chaque hyperparam?tre de ces algorithmes, ce qui rend l'approche impossible ? appliquer dans certaines situations <ref type="bibr" target="#b143">(Lucas et al., 2018)</ref>. Pour souligner cette inaptitude, notons que l'un de ces 37 classifieurs est le ST <ref type="bibr" target="#b81">(Hills et al., 2014)</ref> dont la complexit? temporelle est O(n 2 ? l 4 ) avec n ?tant le nombre de s?ries temporelles dans l'ensemble de donn?es et l ?tant la longueur d'une s?rie temporelle. ? la complexit? du temps d'apprentissage s'ajoute le temps de classification ?lev? de l'un des 37 classifieurs: le plus proche voisin qui doit analyser l'ensemble d'apprentissage avant de prendre une d?cision au moment du test. Par cons?quent, ?tant donn? que le plus proche voisin constitue une composante essentielle de HIVE-COTE, son d?ploiement dans un environnement en temps r?el est encore limit?, voire impossible. Enfin, s'ajoutant ? l'?norme temps d'ex?cution de HIVE-COTE, la d?cision prise par 37 classifieurs ne peut pas ?tre interpr?t?e facilement par les experts du domaine, car il est d?j? difficile de comprendre les d?cisions prises par un unique classifieur. Notons que r?cemment, <ref type="bibr" target="#b18">Bagnall et al., 2020</ref> ont propos? une nouvelle version de HIVE-COTE qui est sensiblement plus rapide, montrant l'importance de pouvoir faire ?voluer les m?thodes de CST.</p><p>Apr?s avoir ?tabli l'?tat de l'art actuel des classifieurs non profonds pour la CST , nous discutons du succ?s de l'apprentissage profond <ref type="bibr">(Le-Cun, Bengio, and Hinton, 2015)</ref> dans diverses t?ches de classification qui ont motiv? l'utilisation r?cente de l'apprentissage profond pour la CST <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>. Les CNNs profonds ont r?volutionn? le domaine de la vision par ordinateur <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref>. Par exemple, en 2015, les CNNs ont ?t? utilis?s pour atteindre les performances au niveau humain dans les t?ches de reconnaissance d'image . Suite au succ?s des DNNs en vision par ordinateur, de nombreuses recherches ont propos? plusieurs architectures DNN pour r?soudre de nombreuses t?ches de NLP telles que la traduction automatique <ref type="bibr" target="#b211">(Sutskever, Vinyals, and Le, 2014;</ref><ref type="bibr" target="#b19">Bahdanau, Cho, and Bengio, 2015)</ref>, la repr?sentation vectorielle des mots  et la classification des documents <ref type="bibr" target="#b126">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b71">Goldberg, 2016)</ref>. Les DNNs ont ?galement eu un impact ?norme sur la communaut? de reconnaissance vocale <ref type="bibr" target="#b195">Sainath et al., 2013)</ref>. Il est int?ressant de noter que la similitude intrins?que entre le NLP et les t?ches de reconnaissance vocale est due ? l'aspect s?quentiel des donn?es qui est ?galement l'une des principales caract?ristiques des s?ries temporelles.</p><p>Dans ce contexte, ce premier chapitre cible les questions ouvertes suivantes: Quel est le meilleur DNN actuel pour la CST? Existe-t-il une approche DNN moins complexe que HIVE-COTE pouvant obtenir des r?sultats comp?titifs avec l'?tat de l'art ? Quel type d'architectures DNN fonctionne le mieux pour la t?che de CST? Comment l'initialisation al?atoire affecte-t-elle les performances des classifieurs d'apprentissage profond ? Et enfin: L'effet bo?te noire des DNNs pourrait-il ?tre ?vit? pour fournir une interpr?tabilit? des r?sultats ? ?tant donn? que ces derni?res questions n'ont pas ?t? abord?es par la communaut? de CST, il est surprenant de voir que les DNNs n'ont pas ?t? consid?r? comme classifieur pr?cis potentiel des s?ries temporelles <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref>. En fait, une ?tude empirique r?cente  a ?valu? 18 algorithmes de CST sur 85 jeux de donn?es, dont aucun n'?tait un mod?le d'apprentissage profond. Cela montre ? quel point la communaut? n'a pas une vue d'ensemble des performances actuelles des mod?les d'apprentissage profond pour r?soudre le probl?me de CST <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref>.</p><p>Dans ce chapitre, nous avons effectu? une ?tude comparative empirique des approches d'apprentissage profond les plus r?centes pour la CST. Avec la mont?e en puissance des GPUs, nous avons montr? comment les architectures profondes peuvent ?tre entra?n?es efficacement pour apprendre de bout en bout des fonctionnalit?s discriminantes cach?es, ? partir de s?ries temporelles brutes. De mani?re similaire ? <ref type="bibr">Bagnall et al., 2017, afin d'</ref>avoir une comparaison ?quitable entre les approches test?es, nous avons d?velopp? une plateforme en Python, Keras <ref type="bibr" target="#b41">(Chollet, 2015)</ref> et Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> pour entrainer les mod?les d'apprentissage profond sur un cluster de plus de 60 GPUs.</p><p>En plus de l'?valuation des ensembles de donn?es univari?s, nous avons test? les approches sur 12 jeux de donn?es MTS <ref type="bibr" target="#b23">(Baydogan, 2015)</ref>. L'?valuation multivari?e montre un autre avantage des mod?les d'apprentissage en profondeur, qui est la capacit? de g?rer la mal?diction de la dimensionnalit? <ref type="bibr" target="#b24">(Bellman, 2010;</ref><ref type="bibr" target="#b112">Keogh and Mueen, 2017)</ref> en exploitant diff?rents degr?s de fluidit? dans la fonction de composition <ref type="bibr" target="#b184">(Poggio et al., 2017)</ref> ainsi que les calculs parall?les des GPUs <ref type="bibr" target="#b142">(Lu et al., 2015)</ref>.</p><p>Quant ? la comparaison des classifieurs sur plusieurs ensembles de donn?es, nous avons suivi les recommandations <ref type="bibr">de Dem?ar, 2006</ref> et utilis? le test de Friedman <ref type="bibr" target="#b63">(Friedman, 1940)</ref> pour rejeter l'hypoth?se nulle. Une fois que nous avons ?tabli qu'il existe une diff?rence statistique dans les performances des classifieurs, nous avons suivi l'analyse post-hoc par paires recommand?e par <ref type="bibr" target="#b25">Benavoli, Corani, and Mangili, 2016</ref> o? la comparaison de rang moyen est remplac?e par un test de rang sign? Wilcoxon <ref type="bibr" target="#b247">(Wilcoxon, 1945)</ref> avec la correction alpha de Holm <ref type="bibr" target="#b85">(Holm, 1979;</ref><ref type="bibr" target="#b66">Garcia and Herrera, 2008)</ref>.</p><p>Dans cette ?tude, nous avons entrain? environ 1 milliard de param?tres dans 97 jeux de donn?es de s?ries temporelles univari?es et multivari?es. Malgr? le fait qu'un grand nombre de param?tres risquent de sur-apprendre <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref> les ensembles d'entrainement relativement petit dans les archives UCR/UEA, nos exp?riences ont montr? que non seulement les DNNs sont capables de surpasser consid?rablement le NN-DTW, mais sont ?galement capables d'obtenir des r?sultats qui ne sont pas significativement diff?rents de COTE et HIVE-COTE en utilisant une architecture de r?seau r?siduel profond <ref type="bibr" target="#b78">(He et al., 2016;</ref><ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017)</ref>. Enfin, nous avons analys? comment de mauvaises initialisations al?atoires peuvent avoir un effet significatif sur les performances d'un DNN.</p><p>En conclusion, avec les probl?mes de fouille de donn?es de plus en plus fr?quents, tirant parti d'architectures plus approfondies qui peuvent apprendre automatiquement de bout en bout des donn?es annot?es, l'apprentissage en profondeur est une approche tr?s attrayante. Dans ce chapitre, nous avons montr? le potentiel des r?seaux de neurones profonds pour le probl?me de CST, n?anmoins ces mod?les complexes d'apprentissage automatique peuvent encore b?n?ficier de nombreuses techniques de r?gularisation, qui est l'objectif principal du chapitre suivant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapitre 2: Regularisation des r?seaux de neurones profonds</head><p>Les mod?les d'apprentissage profond ont g?n?ralement plus de param?tres ? entrainer qu'il n'y a d'instances d'entrainement. N?anmoins, dans le chapitre pr?c?dent, nous avons montr? comment ces r?seaux de neurones artificiels sont capables d'atteindre de bonnes capacit?s de g?n?ralisation par rapport aux algorithmes de CST traditionnels. Pourtant, la plupart de ces mod?les n?cessitent une sorte de r?gularisation afin de minimiser l'erreur de g?n?ralisation (en d'autres termes minimiser la diff?rence entre l'erreur d'entrainement et d'erreur de test). Dans ce chapitre, nous pr?sentons les quatre principales techniques pour r?gulariser les DNNs pour la CST.</p><p>Apprentissage par transfert: Nous avons r?cemment montr? que les CNNs peuvent atteindre des performances similaires ? celles de l'?tat de l'art. Cependant, malgr? les performances ?lev?es de ces CNNs, les mod?les d'apprentissage profond sont toujours sujets au sur-apprentissage. Un exemple o? ces r?seaux de neurones ne parviennent pas ? g?n?raliser est lorsque l'ensemble des donn?es d'apprentissage est tr?s petit. Nous attribuons cette ?norme diff?rence de pr?cision au ph?nom?ne de sur-apprentissage, qui reste un domaine de recherche ouvert dans la communaut? <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. Ce probl?me est connu pour ?tre att?nu? ? l'aide de plusieurs techniques de r?gularisation telles que l'apprentissage par transfert <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref>, o? un mod?le entra?n? sur une premi?re t?che est ensuite affin? sur un ensemble de donn?es cible. L'apprentissage par transfert est actuellement utilis? dans presque tous les mod?les d'apprentissage en profondeur lorsque l'ensemble de donn?es cible ne contient pas suffisamment de donn?es ?tiquet?es <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref>. Malgr? son r?cent succ?s en vision par ordinateur <ref type="bibr" target="#b46">(Csurka, 2017)</ref>, l'apprentissage par transfert a rarement ?t? appliqu? aux mod?les d'apprentissage profond d?di?s aux s?ries temporelles. L'une des raisons de cette absence est probablement le manque d'un grand ensemble de donn?es g?n?ralistes similaire ? Im-ageNet <ref type="bibr" target="#b194">(Russakovsky et al., 2015)</ref> ou OpenImages <ref type="bibr" target="#b118">(Krasin et al., 2017)</ref> mais pour les s?ries temporelles. De plus, ce n'est que r?cemment que l'apprentissage en profondeur s'est av?r? efficace pour la CST <ref type="bibr" target="#b47">(Cui, Chen, and Chen, 2016</ref>) et il reste encore beaucoup ? explorer dans la construction de r?seaux de neurones profonds pour l'analyse de s?ries temporelles <ref type="bibr" target="#b45">(Cristian Borges Gamboa, 2017)</ref>. Comme le transfert de mod?les d'apprentissage en profondeur, entre les diff?rents jeux de donn?es des archives UCR/UEA <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, n'a pas ?t? ?tudi? de mani?re approfondie, nous avons d?cid? de nous y atteler dans le but ultime de d?terminer ? l'avance quels types de jeux de donn?es pourraient b?n?ficier du transfert de mod?les CNNs et am?liorer leur pr?cision.</p><p>Ensembling: Une autre fa?on d'am?liorer les classifieurs bas?s sur les r?seaux de neurones est de construire un ensemble de mod?les d'apprentissage profond. Cette id?e semble tr?s int?ressante pour les t?ches de CST car l'?tat de l'art ?volue vers des solutions d'ensemble <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018;</ref><ref type="bibr" target="#b136">Lines and Bagnall, 2015;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017;</ref><ref type="bibr" target="#b22">Baydogan, Runger, and Tuv, 2013)</ref>. De plus, les ensembles de r?seaux de neurones profonds semblent obtenir des r?sultats tr?s prometteurs dans de nombreux domaines de l'apprentissage automatique supervis? tels que la d?tection des l?sions cutan?es <ref type="bibr" target="#b74">(Goyal and Rajapakse, 2018)</ref>, la reconnaissance d'expression faciale <ref type="bibr" target="#b246">(Wen et al., 2017)</ref> et le remplissage automatique des seaux <ref type="bibr" target="#b49">(Dadhich, Sandin, and Bodin, 2018)</ref>. Par cons?quent, nous proposons de regrouper les mod?les actuels d'apprentissage profond pour la CST d?velopp?s dans le chapitre pr?c?dent, en construisant un mod?le compos? de 60 r?seaux de neurones profonds diff?rents: 6 architectures diff?rentes <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017;</ref><ref type="bibr" target="#b266">Zheng et al., 2014;</ref><ref type="bibr" target="#b265">Zhao et al., 2017;</ref><ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018)</ref> chacun avec 10 initialisations diff?rentes des poids du mod?le. En ?valuant sur les 85 jeux de donn?es de l'archive UCR/UEA, nous d?montrons une am?lioration significative par rapport aux classifieurs individuels tout en atteignant des performances tr?s similaires ? HIVE-COTE: m?thode ensembliste contenant 37 classifieurs diff?rents et repr?sentant actuellement l'?tat de l'art. Enfin, en nous inspirant de nos r?sultats sur l'apprentissage par transfert <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref>, nous rempla?ons les r?seaux initialis?s de mani?re al?atoire par un ensemble construit ? partir de mod?les affin?s ? partir des 84 autres jeux de donn?es de l'archive, et montrons une am?lioration significative pour la CST.</p><p>Augmentation de donn?es: Bien que les CNNs profonds r?cemment propos?s aient atteint des hautes performances pour la CST sur l'archive UCR/UEA <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>, ils montrent toujours de faibles capacit?s de g?n?ralisation sur certains petits jeux de donn?es tels que l'ensemble de donn?es CinCECGTorso avec 40 instances d'entra?nement. Cela est surprenant car le NN-DTW fonctionne exceptionnellement bien sur ce jeu de donn?es, ce qui montre la relative facilit? de cette t?che de classification. Ainsi, les similitudes entre s?ries temporelles dans ces petits jeux de donn?es ne peuvent pas ?tre captur?es par les CNNs en raison du manque d'instances ?tiquet?es, ce qui pousse l'algorithme d'optimisation du r?seau ? ?tre bloqu? dans des minimums locaux <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. Ce ph?nom?ne, ?galement connu sous le nom de sur-apprentissage dans la communaut? du machine learning, peut ?tre r?solu en utilisant diff?rentes techniques telles que la r?gularisation ou simplement la collecte de donn?es ?tiquet?es suppl?mentaires <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref> (qui, dans certains domaines, sont difficiles ? obtenir). Une autre technique bien connue est l'augmentation des donn?es, o? les donn?es synth?tiques sont g?n?r?es ? l'aide d'une m?thode sp?cifique. Par exemple, les images contenant des num?ros de rue sur des maisons peuvent ?tre l?g?rement pivot?es sans changer leur num?ro r?el <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref>. Pour les mod?les d'apprentissage en profondeur, ces m?thodes sont g?n?ralement propos?es pour les donn?es d'image et se g?n?ralisent mal aux s?ries temporelles <ref type="bibr" target="#b225">(Um et al., 2017)</ref>. Cela est probablement d? au fait que pour les images, une comparaison visuelle peut confirmer si la transformation (telle que la rotation) n'a pas modifi? la classe de l'image, tandis que pour les s?ries temporelles, on ne peut pas facilement confirmer l'effet de telles transformations ad hoc sur la nature d'une s?rie. Nous proposons de tirer parti d'une technique d'augmentation de donn?es bas?e sur DTW sp?cifiquement d?velopp?e pour les s?ries temporelles, afin d'am?liorer les performances d'un ResNet profond pour la CST. Nos exp?riences pr?liminaires r?v?lent que l'augmentation des donn?es peut am?liorer consid?rablement la pr?cision des CNNs sur certains jeux de donn?es tout en ayant un impact n?gatif faible sur d'autres jeux de donn?es.</p><p>Attaques adversaires: Comme nous l'avons d?j? discut?, la CST est utilis?e dans diverses t?ches d'exploration de donn?es du monde r?el, allant des soins de sant? et de la s?curit? <ref type="bibr" target="#b215">(Tan, Webb, and Petitjean, 2017;</ref><ref type="bibr" target="#b220">Tobiyama et al., 2016)</ref> ? la s?curit? alimentaire et surveillance de la consommation d'?nergie <ref type="bibr" target="#b172">(Owen and Foreman, 2012)</ref>. Avec des mod?les d'apprentissage en profondeur r?volutionnant de nombreux domaines de l'apprentissage automatique tels que la vision par ordinateur <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref> et le traitement du langage naturel <ref type="bibr" target="#b255">(Yang et al., 2018;</ref><ref type="bibr" target="#b236">Wang, Li, and Xu, 2018)</ref>, nous avons montr? dans le chapitre pr?c?dent que ces mod?les ont commenc? ? ?tre adopt? pour les t?ches de CST (Ismail <ref type="bibr" target="#b100">Fawaz et al., 2019d)</ref>. Apr?s l'av?nement du deep learning, les chercheurs ont commenc? ? ?tudier la vuln?rabilit? des r?seaux profonds aux attaques adversaires <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. Dans le cadre de la reconnaissance d'images, une attaque adversaire consiste ? modifier une image originale afin que les changements soient quasiment ind?tectables par un humain <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. L'image modifi?e est appel?e une image adversaire, qui sera mal class?e par le r?seau de neurone, tandis que l'image d'origine est correctement class?e. L'une des attaques les plus c?l?bres de la vie r?elle consiste ? modifier une image de panneau de signalisation afin qu'elle soit mal interpr?t?e par un v?hicule autonome <ref type="bibr" target="#b56">(Eykholt et al., 2018)</ref>. Une autre application est l'alt?ration du contenu ill?gal pour le rendre ind?tectable par les algorithmes de mod?ration automatique <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. Bien que ces approches aient ?t? intensivement ?tudi?es dans le contexte de la reconnaissance d'image, elles n'ont pas ?t? ?tudi?es en profondeur pour la CST. Cela est surprenant car les mod?les d'apprentissage en profondeur deviennent de plus en plus populaires pour classer les s?ries temporelles <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>. En outre, des attaques adverses potentielles sont pr?sentes dans de nombreuses applications o? l'utilisation de donn?es de s?ries temporelles est cruciale. Par exemple, dans ce chapitre on montre la similitude entre une s?rie temporelle originale et perturb?e de spectrographe de grains de caf?. Alors qu'un r?seau de neurones profond classe correctement la s?rie d'origine en tant que grains Robusta, l'ajout de petites perturbations le classe comme Arabica. Par cons?quent, ?tant donn? que les grains Arabica ont plus de valeur que les grains Robusta, cette attaque pourrait ?tre utilis?e pour tromper les tests de contr?le des aliments et, ?ventuellement, les consommateurs. Nous pr?sentons, transf?rons et adaptons les attaques qui se sont av?r?es efficaces sur les images, aux donn?es de s?ries temporelles. Nous pr?sentons ?galement une ?tude exp?rimentale utilisant les 85 jeux de donn?es de l'archive UCR/UEA <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> qui r?v?le que les r?seaux de neurones sont sensibles aux attaques adversaires. Nous mettons en ?vidence des cas d'utilisation concrets sp?cifiques pour souligner l'importance de ces attaques dans des situations r?elles, ? savoir la qualit? et la s?curit? des aliments, les capteurs des v?hicules et la consommation d'?lectricit?. Nos r?sultats montrent que les r?seaux profonds pour les donn?es de s?ries temporelles sont vuln?rables aux attaques adverses comme leurs homologues en vision par ordinateur. Par cons?quent, ce travail met en lumi?re la n?cessit? de se prot?ger contre de telles attaques, en particulier lorsque l'apprentissage profond est utilis? pour les applications sensibles de CST. Nous montrons ?galement que les s?ries temporelles adverses apprises ? l'aide d'une architecture de r?seau peuvent ?tre transf?r?es ? diff?rentes architectures. Nous discutons ensuite de certains m?canismes pour emp?cher ces attaques tout en renfor?ant la robustesse des mod?les aux attaques adversaires. Enfin, dans un esprit de r?gularisation des DNNs, nous montrons comment ces s?ries perturb?es peuvent ?tre exploit?es afin d'am?liorer la capacit? de g?n?ralisation d'un mod?le d'apprentissage en profondeur: une technique appel?e entra?nement adversaire <ref type="bibr" target="#b252">(Xie et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapitre 3: InceptionTime: Recherche d'AlexNet pour la classification de s?ries temporelles</head><p>Les industries allant des soins de sant? <ref type="bibr" target="#b61">(Forestier et al., 2018;</ref><ref type="bibr" target="#b132">Lee et al., 2018;</ref><ref type="bibr" target="#b99">Ismail Fawaz et al., 2019c)</ref> et de la s?curit? sociale <ref type="bibr" target="#b259">(Yi et al., 2018)</ref> ? la reconnaissance de l'activit? humaine <ref type="bibr" target="#b262">(Yuan et al., 2018)</ref> et ? la t?l?d?tection <ref type="bibr" target="#b179">(Pelletier, Webb, and Petitjean, 2019)</ref>, produisent toutes des s?ries temporelles d'une ?chelle jamais vue auparavant -? la fois en termes de longueur et de quantit? de s?ries. Cette croissance signifie ?galement une d?pendance accrue ? l'?gard de la classification automatique de ces donn?es s?quentielles et, id?alement, des algorithmes capables de le faire ? grande ?chelle.</p><p>Dans les chapitres pr?c?dents, nous avons montr? en quoi ces probl?mes de CST diff?rent consid?rablement de l'apprentissage supervis? traditionnel pour les donn?es structur?es, en ce que les algorithmes doivent ?tre capables de g?rer et d'exploiter les informations temporelles pr?sentes dans le signal. Il est facile d'?tablir des parall?les entre ce sc?nario et des probl?mes de vision par ordinateur tels que la classification d'images et la localisation d'objets, o? les algorithmes efficaces apprennent des informations spatiales contenues dans une image. En termes simples, le probl?me de classification des s?ries temporelles est essentiellement la m?me classe de probl?mes, juste avec une dimension de moins. Pourtant, malgr? cette similitude, les algorithmes d'?tat de l'art actuels des deux domaines partagent peu de ressemblance <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>.</p><p>Le deep learning a une longue histoire (en termes d'apprentissage automatique) en vision par ordinateur  mais sa popularit? a explos? avec AlexNet <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref>, apr?s quoi il a ?t? incontestablement la classe d'algorithmes avec le plus de r?ussite (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref>. Inversement, l'apprentissage en profondeur n'a commenc? ? gagner en popularit? que r?cemment parmi les chercheurs en exploration de donn?es temporelles <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>. Ceci est soulign? par le fait que ResNet, qui est actuellement consid?r? comme l'architecture de r?seau neurone d'?tat de l'art pour la CST lorsqu'elle est ?valu?e sur l'archive UCR/UEA <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, a ?t? initialement propos? simplement comme mod?le de base pour la t?che sous-jacent <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>. Compte tenu des similitudes dans les donn?es, il est facile de sugg?rer qu'il y a beaucoup d'am?lioration potentielle pour l'apprentissage profond dans la CST. Dans le chapitre pr?c?dent, nous avons montr? comment il est possible d'am?liorer la pr?cision d'une architecture d'apprentissage profond donn?e, en utilisant diverses techniques de r?gularisation telles que l'ensembling, l'apprentissage par transfert, l'augmentation des donn?es et l'apprentissage adversaire. Cependant, nous pensons qu'il y a encore place ? l'am?lioration en termes d'architecture de r?seau, qui peut ?tre consid?r?e comme une t?che orthogonale aux diff?rentes m?thodes de r?gularisation des DNNs.</p><p>Dans ce chapitre, nous franchissons une ?tape importante vers la recherche de l'?quivalent d'AlexNet pour la CST en pr?sentant InceptionTime -un nouvel ensemble d'apprentissage profond pour la CST. InceptionTime atteint la pr?cision d'?tat de l'art lorsqu'il est ?valu? sur l'archive UCR/UEA (actuellement le plus grand r?f?rentiel publiquement disponible pour la CST <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>) et passe mieux ? l'?chelle d? ? son co?t computationnel plus faible.</p><p>InceptionTime est un ensemble de cinq mod?les d'apprentissage profond pour la CST, chacun est cr?? en cascadant plusieurs modules Inception . Chaque classifieur individuel (mod?le) aura exactement la m?me architecture mais avec une initialisation des poids diff?rente et al?atoire. L'id?e centrale d'un module Inception est d'appliquer simultan?ment plusieurs filtres ? une s?rie temporelle en entr?e. Le module comprend des filtres de longueurs variables qui, comme nous le montrerons, permettent au r?seau d'extraire automatiquement les caract?ristiques pertinentes des s?ries temporelles longues et courtes. En fait, InceptionTime suit ici l'id?e d'ensemble pr?sent?e dans le chapitre pr?c?dent.</p><p>Apr?s avoir pr?sent? InceptionTime et ses r?sultats, nous effectuons une analyse des hyperparam?tres architecturaux des r?seaux de neurones profonds -profondeur, longueur du filtre, nombre de filtres -et les caract?ristiques du module Inception -le bottleneck et les connexions r?siduelles, afin de mieux comprendre pourquoi ce mod?le conna?t un tel succ?s. En fait, nous construisons des r?seaux avec des filtres plus grands que ceux utilis?s pour les t?ches de vision par ordinateur, profitant directement du fait que les s?ries temporelles pr?sentent une dimension de moins que les images.</p><p>En conclusion, l'apprentissage profond pour la classification des s?ries temporelles est toujours en retard par rapport aux r?seaux de neurones pour la reconnaissance d'images en termes d'?tudes exp?rimentales et de conceptions architecturales. Dans ce chapitre, nous comblons cette lacune en introduisant InceptionTime, inspir? par le r?cent succ?s des r?seaux bas?s sur Inception pour diverses t?ches de vision par ordinateur. Nous avons regroup? ces r?seaux pour produire de nouveaux r?sultats d'?tat de l'art pour la CST sur les 85 jeux de donn?es du UCR/UEA archive. Notre approche est hautement ?volutive, deux ordres de grandeur plus rapide que les mod?les d'?tat de l'art actuels tels que HIVE-COTE. L'ampleur de cette acc?l?ration est int?ressante dans un contexte Big Data ainsi que pour des s?ries temporelles plus longues avec un taux d'?chantillonnage ?lev?. Nous ?tudions en outre les effets sur la pr?cision globale de divers hyperparam?tres des architectures CNN. Pour ceux-ci, nous allons bien au-del? des pratiques standard pour les donn?es d'images et la conception de r?seaux avec de longs filtres. Nous les examinons en utilisant un ensemble de donn?es simul?es et encadrons notre ?tude en termes de d?finition du "receptive field" d'un CNN pour la CST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapitre 4: Analyse de s?ries temporelles pour la formation chirurgicale</head><p>Au cours des cent derni?res ann?es, la m?thodologie d'enseignement classique de "voir un, faire un, enseigner un" a domin? les syst?mes d'enseignement chirurgical dans le monde. Avec l'av?nement de salle d'op?ration 2.0, l'enregistrement des vid?os, des donn?es cin?matiques et de nombreux autres types de donn?es au cours d'une op?ration chirurgicale est devenu une t?che facile, permettant ainsi aux syst?mes d'intelligence artificielle d'?tre d?ploy?s et utilis?s dans la pratique chirurgicale et m?dicale. R?cemment, il a ?t? d?montr? que les donn?es des capteurs de mouvement (par exemple cin?matique) ainsi que les vid?os chirurgicales fournissent une structure de coaching permettant aux stagiaires d?butants d'apprendre des chirurgiens exp?riment?s en rejouant ces vid?os et/ou trajectoires cin?matiques. Dans ce chapitre, nous abordons deux probl?mes pr?sents dans le programme actuel de formation chirurgicale.</p><p>?valuation des comp?tences chirurgicales: L'id?e principale de la m?thodologie d'enseignement du Dr William Halsted est que l'?tudiant pourrait devenir un chirurgien exp?riment? en observant et en participant ? des chirurgies encadr?es <ref type="bibr" target="#b185">(Polavarapu et al., 2013)</ref>. Ces techniques de formation, bien que largement utilis?es, sont d?pourvues d'une m?thode objective d'?valuation des comp?tences chirurgicales <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>. L'?valuation standard des comp?tences chirurgicales est actuellement bas?e sur des listes de contr?le remplies par un expert observant la t?che chirurgicale <ref type="bibr" target="#b7">(Ahmidi et al., 2017)</ref>. Afin de pr?dire le niveau de comp?tence d'un stagiaire sans utiliser le jugement d'un chirurgien expert, l'OSATS a ?t? propos? et est actuellement adopt? comme pratique clinique standard <ref type="bibr" target="#b166">(Niitsu et al., 2013)</ref>. H?las, ce type de notation observationnelle souffre encore de plusieurs facteurs externes et subjectifs tels que la fiabilit? inter-?valuateurs, le processus de d?veloppement et le biais de la liste de contr?le et de l'?valuateur <ref type="bibr" target="#b76">(Hatala et al., 2015)</ref>. D'autres ?tudes ont d?montr? une corr?lation entre les comp?tences techniques d'un chirurgien et les r?sultats postop?ratoires <ref type="bibr" target="#b34">(Bridgewater et al., 2003)</ref>.</p><p>Cette derni?re approche souffre du fait que les suites d'une intervention chirurgicale d?pendent des attributs physiologiques du patient <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>. De plus, l'obtention de ce type de donn?es est tr?s ardue, ce qui rend ces techniques d'?valuation des comp?tences difficiles ? mettre en oeuvre pour la formation chirurgicale. Les progr?s r?cents en robotique chirurgicale tels que le syst?me chirurgical da Vinci (Intuitive Surgical Sunnyvale, 2018) ont permis l'enregistrement de donn?es vid?o et cin?matiques de diverses t?ches chirurgicales. Par cons?quent, un substitut des listes de contr?le et des approches bas?es sur les r?sultats, est de g?n?rer, ? partir de ces cin?matiques, des GMFs tels que la vitesse de la t?che chirurgicale, l'ach?vement du temps, la fluidit? du mouvement, la courbure et d'autres caract?ristiques holistiques <ref type="bibr" target="#b269">(Zia and Essa, 2018;</ref><ref type="bibr" target="#b108">Kassahun et al., 2016)</ref>. Bien que la plupart de ces techniques soient efficaces, il n'est pas ?vident de voir comment elles pourraient ?tre utilis?es pour soutenir le stagiaire avec une r?troaction d?taill?e et constructive, afin d'aller au-del? d'une classification na?ve dans un niveau de comp?tence <ref type="bibr">(c.-?-d. expert, interm?diaire, etc.)</ref> . Cela est probl?matique car le retour d'exp?rience sur la pratique m?dicale permet aux chirurgiens d'atteindre des niveaux de comp?tence plus ?lev?s tout en am?liorant leurs performances <ref type="bibr" target="#b94">(Islam et al., 2016)</ref>. Derni?rement, un domaine intitul? Surgical Data Science (Maier-Hein et al., 2017) est apparu motiv? par l'acc?s croissant ? une ?norme quantit? de donn?es complexes qui concernent le personnel, le patient et les capteurs pour capturer la proc?dure et les donn?es relatives au patient telles que les variables cin?matiques et les images <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>. Au lieu d'extraire des GMFs, les enqu?tes r?centes ont tendance ? d?composer manuellement les t?ches chirurgicales en segments plus fins appel?s "gestes", avant d'entra?ner le mod?le, et enfin ? estimer les performances des stagiaires en fonction de leur ?valuation au cours de ces gestes individuels <ref type="bibr" target="#b218">(Tao et al., 2012)</ref>. M?me si ces m?thodes ont obtenu des r?sultats prometteurs et pr?cis en termes d'?valuation des comp?tences chirurgicales, elles n?cessitent d'?tiqueter une ?norme quantit? de gestes avant de former l'estimateur <ref type="bibr" target="#b218">(Tao et al., 2012)</ref>. Nous avons soulign? deux limites majeures dans les techniques actuelles existantes qui estiment le niveau de comp?tence des chirurgiens ? partir de leurs variables cin?matiques correspondantes: premi?rement, l'absence d'un r?sultat interpr?table de la pr?diction de comp?tence qui peut ?tre utilis? par les stagiaires pour atteindre des niveaux de comp?tence chirurgicale plus ?lev?s; deuxi?mement, l'exigence de limites de gestes pr?d?finies par les annotateurs qui est sujette ? la fiabilit? inter-annotateurs et qui prend du temps ? pr?parer <ref type="bibr" target="#b227">(Vedula et al., 2016)</ref>. Dans cette premi?re partie du chapitre, nous concevons une nouvelle architecture bas?e sur les FCNs, d?di?e ? l'?valuation des comp?tences chirurgicales. En utilisant des noyaux unidimensionnels sur les s?ries temporelles cin?matiques, nous ?vitons d'avoir ? extraire de gestes peu fiables et sensibles. La structure hi?rarchique originale de notre mod?le nous permet de capturer des informations globales sp?cifiques au niveau de comp?tence chirurgicale, ainsi que de repr?senter les gestes dans des caract?ristiques latentes de bas niveau. De plus, pour fournir une r?troaction interpr?table, au lieu d'utiliser une couche dense comme la plupart des architectures traditionnelles d'apprentissage profond, nous pla?ons une couche GAP qui nous permet de profiter de la carte d'activation de classe, propos?e ? l'origine par <ref type="bibr" target="#b268">Zhou et al., 2016</ref>, pour localiser quelle partie de l'exercice a eu un impact sur la d?cision du mod?le lors de l'?valuation du niveau de comp?tence d'un chirurgien. En utilisant une configuration exp?rimentale standard sur le plus grand ensemble de donn?es publiques pour l'analyse des donn?es robotiques chirurgicales: le JHU-ISI Gesture and Skill Assessment Working Set <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>, nous montrons la pr?cision de notre mod?le FCN. Notre principale contribution est de d?montrer que l'apprentissage en profondeur peut ?tre mis ? profit pour comprendre les structures complexes et latentes lors de la classification des comp?tences chirurgicales et de la pr?vision du score OSATS d'une chirurgie, d'autant plus qu'il y a encore beaucoup ? apprendre sur ce qui constitue exactement une comp?tence chirurgicale <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>.</p><p>Alignement des vid?os chirurgicales: Les ?ducateurs ont toujours cherch? des moyens innovants d'am?liorer le taux d'apprentissage des apprenants. Alors que les cours classiques sont encore les plus utilis?s, les ressources multim?dias sont de plus en plus adopt?es <ref type="bibr" target="#b204">(Smith and Ransbottom, 2000)</ref>, en particulier dans les cours en ligne ouverts et massifs <ref type="bibr" target="#b156">(Means et al., 2009)</ref>. Dans ce contexte, les vid?os ont ?t? consid?r?es comme particuli?rement int?ressantes car elles peuvent combiner les images, textes, graphiques, audios et animations. Le domaine m?dical ne fait pas exception et l'utilisation de ressources vid?o est intensivement adopt?e dans le programme m?dical <ref type="bibr" target="#b153">(Masic, 2008)</ref>, en particulier dans le contexte de la formation chirurgicale <ref type="bibr" target="#b116">(Kneebone et al., 2002)</ref>. L'av?nement de la chirurgie robotique stimule ?galement cette tendance, car les robots chirurgicaux, comme le Da Vinci (Intuitive Surgical Sunnyvale, 2018), enregistrent g?n?ralement des flux vid?o pendant l'intervention. Par cons?quent, une grande quantit? de donn?es vid?o a ?t? enregistr?e au cours des dix derni?res ann?es <ref type="bibr" target="#b191">(Rapp et al., 2016)</ref>. Cette nouvelle source de donn?es repr?sente une opportunit? sans pr?c?dent pour les jeunes chirurgiens d'am?liorer leurs connaissances et leurs comp?tences <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>. En outre, la vid?o peut ?galement ?tre un outil pour les chirurgiens seniors pendant les p?riodes d'enseignement pour ?valuer les comp?tences des stagiaires. En fait, une ?tude r?cente de Mota et al., 2018 a montr? que les r?sidents passent plus de temps ? regarder des vid?os que des sp?cialistes, soulignant la n?cessit? pour les jeunes chirurgiens de profiter pleinement de cet outil. Dans Herrera-Almario et al., 2016, les auteurs ont montr? que les scores obtenus pour la t?che noeuds ainsi que leurs temps de r?alisation des exercices se sont consid?rablement am?lior?s pour les sujets qui ont regard? les vid?os de leur propre performance.</p><p>Cependant, lorsque les stagiaires sont pr?ts ? ?valuer leurs progr?s au cours de plusieurs essais de la m?me t?che chirurgicale en revoyant simultan?ment leurs vid?os chirurgicales enregistr?es, le fait que les vid?os soient d?synchronis?es rend la comparaison entre les diff?rents essais tr?s difficile, voire impossible. Ce probl?me se rencontre dans de nombreuses ?tudes de cas r?els, car les experts accomplissent en moyenne les t?ches chirurgicales en moins de temps que les chirurgiens d?butants <ref type="bibr" target="#b155">(McNatt and Smith, 2001)</ref>. Ainsi, lorsque les stagiaires am?liorent leurs comp?tences, leur fournir une r?troaction qui identifie la raison de l'am?lioration des comp?tences chirurgicales devient probl?matique car les vid?os enregistr?es pr?sentent une dur?e diff?rente et ne sont pas parfaitement align?es. Bien que la synchronisation des vid?os ait ?t? le centre d'int?r?t de plusieurs sites de recherche en vision par ordinateur, les contributions se concentrent g?n?ralement sur un cas particulier o? plusieurs vid?os enregistr?es simultan?ment (avec des caract?ristiques diff?rentes telles que les angles de vue et les facteurs de zoom) sont trait?es <ref type="bibr" target="#b248">(Wolf and Zomet, 2002;</ref><ref type="bibr" target="#b244">Wedge, Kovesi, and Huynh, 2005;</ref><ref type="bibr" target="#b173">Padua et al., 2010)</ref>. Un autre type de synchronisation de multiples vid?os utilise des fonctionnalit?s con?ues ? la main (telles que des trajectoires de points d'int?r?t) ? partir des vid?os <ref type="bibr" target="#b233">(Wang et al., 2014;</ref><ref type="bibr" target="#b55">Evangelidis and Bauckhage, 2011)</ref>, ce qui rend l'approche tr?s sensible ? la qualit? des caract?ristiques extraites. Ce type de techniques ?tait tr?s efficace car les vid?os brutes ?taient la seule source d'information disponible, alors que dans notre cas, l'utilisation de syst?mes chirurgicaux robotis?s permet de capturer un type de donn?es suppl?mentaire: les variables cin?matiques telles que les coordonn?es cart?siennes x, y, z des effecteurs terminaux du Da Vinci <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>. Dans cette deuxi?me partie du chapitre, nous proposons de tirer parti de l'aspect s?quentiel des donn?es cin?matiques enregistr?es par le syst?me chirurgical Da Vinci, afin de synchroniser leurs images vid?o correspondantes en alignant les donn?es de s?ries temporelles. Lors de l'alignement de deux s?ries temporelles, l'algorithme standard est DTW <ref type="bibr" target="#b196">(Sakoe and Chiba, 1978)</ref> que nous avons en effet utilis? pour aligner deux vid?os. Cependant, lors de l'alignement de plusieurs s?quences, cette derni?re technique ne se g?n?ralise pas de mani?re simple et r?alisable par calcul <ref type="bibr" target="#b180">(Petitjean et al., 2014)</ref>. Par cons?quent, pour la synchronisation de multiples vid?os, nous proposons d'aligner leurs s?ries temporelles correspondantes avec une s?rie temporelle moyenne, calcul?e en utilisant l'algorithme DBA. Ce processus est appel? NLTS et a ?t? initialement propos? pour trouver l'alignement multiple d'un ensemble de gestes chirurgicaux discr?tis?s <ref type="bibr" target="#b58">(Forestier et al., 2014)</ref>, que nous ?tendons dans ce travail ? des donn?es cin?matiques num?riques continues.</p><p>En conclusion, dans ce chapitre, nous avons abord? deux probl?mes diff?rents li?s aux comp?tences chirurgicales. Tout d'abord, en concevant un FCN, nous avons pu obtenir des r?sultats d'?tat de l'art pour l'?valuation des comp?tences chirurgicales (classification et r?gression). Ainsi, nous avons pu att?nuer l'effet de bo?te noire des DNNs, en utilisant la technique CAM afin de mettre en ?vidence ce qui permet d'identifier l'exp?rience du chirurgien. Le deuxi?me probl?me li? aux comp?tences chirurgicales ?tait d? au fait que les vid?os de formation chirurgicale n'?taient pas synchronis?es, ce qui rend difficile pour les stagiaires de comprendre et de comparer les vid?os entre diff?rents chirurgiens avec diff?rents niveaux de comp?tence. Nous avons abord? ce dernier probl?me en proposant l'utilisation de NLTS afin d'aligner et de synchroniser plusieurs vid?os simultan?ment. Ces deux projets ?taient orthogonaux dans le sens o? ils pouvaient ?galement se compl?ter: la synchronisation de la vid?o et des s?ries temporelles pourrait ?tre une ?tape de pr?traitement qui am?liorerait les mod?les d'?valuation des comp?tences chirurgicales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Time series data are omnipresent in many practical data science applications ranging from health care <ref type="bibr" target="#b65">(Gao et al., 2014)</ref> and stock market predictions <ref type="bibr" target="#b10">(Anghinoni et al., 2018)</ref> to social media analysis <ref type="bibr" target="#b253">(Xu, Chen, and Mao, 2018)</ref> and human activity recognition <ref type="bibr" target="#b250">(Xi et al., 2018)</ref>. In fact, any type of numerical acquisition of data with some notion of ordering will generate time series, making this type of data very common among data mining problems <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. Compared to traditional tabular data, each time series can be represented (under the tabular format) as a row with each attribute corresponding to one time stamp (numerical acquisition). However, analyzing time series data differs significantly from its tabular counterpart, as harnessing the temporal information is usually very important for the underlying task we are trying to solve . A concrete example of time series data in health care would be the acquisition of ECG heart signals <ref type="bibr" target="#b189">(Rajan and Thiagarajan, 2018)</ref>. In stock market analysis, a time series element would correspond to the value of a stock at a given time stamp <ref type="bibr" target="#b10">(Anghinoni et al., 2018)</ref>. In human activity recognition, the given Cartesian position of a hand in 3D space would constitute an element of a time series <ref type="bibr" target="#b91">(Ignatov, 2018)</ref>.</p><p>Since 2006, time series analysis has been considered one of the most challenging problems in data mining <ref type="bibr" target="#b256">(Yang and Wu, 2006)</ref>, and in a more recent poll it has been shown that 48% of data expert had analyzed time series data during their career, ahead of text and images <ref type="bibr" target="#b163">(Neamtu et al., 2018)</ref>. Under the time series analysis umbrella, there exists many orthogonal tasks, that can be grouped into four main categories:</p><p>Forecasting consists of training a model using some historical time series data, with the goal to predict the future observations of this time series <ref type="bibr" target="#b89">(Hyndman and Athanasopoulos, 2018)</ref>. Weather forecasting is one of the most common applications, where the training data consists of old observations, and the task is to predict the future weather behavior <ref type="bibr" target="#b219">(Taylor, McSharry, and Buizza, 2009</ref>). In finance, predicting the value of stock using its historical values is one very common application <ref type="bibr" target="#b113">(Kim, 2003)</ref>. Further examples and details of time series forecasting an be found in <ref type="bibr" target="#b89">Hyndman and Athanasopoulos, 2018.</ref> Anomaly detection is a very special task of time series analysis. Unlike forecasting, the goal is not to predict the future, but rather to determine if a given time series observation is normal or not <ref type="bibr" target="#b29">(Bl?zquez-Garc?a et al., 2020)</ref>. This task is also known as time series outlier detection. One of the most common application is called predictive maintenance, such as predicting anomalies in advance in order to prevent potential failures <ref type="bibr" target="#b188">(Rabatel, Bringay, and Poncelet, 2011)</ref>. The reader is referred to this excellent review by Bl?zquez-Garc?a et al., 2020 on time series anomaly/outlier detection.</p><p>Clustering is probably one of the most widely studied problems in unsupervised learning. The problem can be defined as follows: given a set of data points, partition them into a set of groups which are as similar as possible <ref type="bibr" target="#b5">(Aggarwal, 2014)</ref>. For time series data, traditional clustering approaches can provide a baseline, however many FIGURE 1: An example illustrating the task of classifying an input time series from the GunPointAgeSpan dataset <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, where the goal is to classify whether or not a person is holding a gun (a time series corresponds to the hand's x coordinate of a person holding or not a gun) researchers found that mining the temporal information provided by the time series data can be crucial for many tasks <ref type="bibr" target="#b182">(Petitjean and Gan?arski, 2012)</ref>. Applications range from discovering daily patterns of sales in marketing databases <ref type="bibr" target="#b198">(Sanwlani and Vijayalakshmi, 2013)</ref> to finding particular behaviors of solar magnetic wind in scientific databases <ref type="bibr" target="#b186">(Pravilovic et al., 2014)</ref>. For more details on time series clustering, we refer the interested reader to this recent survey by <ref type="bibr" target="#b6">Aghabozorgi, Shirkhorshidi, and Wah, 2015.</ref> Classification consists of predicting the correct class of a given data point using a labeled training set. For TSC, this data point is by itself a whole time series, and the task consists of predicting its correct label. This problem is encountered in various data mining fields such as patient risk identification in health care <ref type="bibr" target="#b146">(Ma, Xiao, and Wang, 2018)</ref>, malware detection in cyber security <ref type="bibr" target="#b220">(Tobiyama et al., 2016)</ref>, food safety evaluation in agriculture and livestock <ref type="bibr" target="#b162">(Nawrocka and Lamorska, 2013)</ref>. Similar to unsupervised learning, traditional classification approaches can provide a basic baseline for solving this underlying TSC task. However over the past two decades, research has shown that designing algorithms that can exploit the temporal information is a need in order to achieve high classification accuracy . <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an example of the TSC task.</p><p>In this thesis, we chose to focus on the latter TSC problem. The reason behind choosing this field of time series analysis was motivated by our interest in a very particular problem: surgical skills evaluation from kinematic data. This specific task can be cast as a TSC problem: given an input time series (kinematic data registered through time) predict the correct label (skill level of the surgeon performing the surgery). The reader can find more details on this problem in Chapter 4. For solving this surgical skills evaluation problem, we started looking into the state-ofthe-art TSC algorithms of that epoch. At the beginning of this thesis in 2017, we found that most TSC approaches were inspired by traditional machine learning algorithms such as NN classifiers coupled with a bespoke distance -such as DTW instead of ED. Some algorithms were inspired by traditional text mining approaches such as the Time Series Bag-Of-Features developed by <ref type="bibr" target="#b22">Baydogan, Runger, and Tuv, 2013</ref>. Other TSC methods were inspired by Fourier analysis from signal processing such as BOSS (proposed by <ref type="bibr" target="#b200">Sch?fer, 2015)</ref>. One type of classifier focused on extracting discriminative subsequence from the time series called Shapelets, which are later used for classifying the input time series <ref type="bibr" target="#b81">(Hills et al., 2014)</ref>. Meanwhile in 2017, the computer vision community has already achieved tremendous human level performance with DNNs, followed by the much more recent success of deep learning for various NLP tasks. However, we have noticed that the TSC community have not considered DNNs as potential classifiers of time series data, which is evident in the great TSC bake-off paper . This is very surprising as given the success of deep learning with image classification problems, coupled with the intrinsic similarity between 2D patterns in images and 1D patterns in series, one should consider the potential of deep learning for TSC problems. We therefore started investigating and benchmarking the recent work proposing the use of DNNs for classifying time series data, which is the main focus of Chapter 1 <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>.</p><p>Following this thorough review of the recent advances in deep learning architectures for TSC, we started looking into the different techniques to improve the accuracy of a given neural network model. This type of technique is also known as regularization, which enables us to improve the generalization capabilities of a given machine learning model. These methods range from transfer learning <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref> and ensembling <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019e)</ref> to data augmentation <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018b)</ref> and adversarial training <ref type="bibr">(Ismail Fawaz et al., 2019b)</ref>. The latter techniques were the main focus of Chapter 2 of this thesis.</p><p>Having identified the current state-of-the-art architectures for TSC in Chapter 1, followed by the main techniques on how to improve the generalization capability of a given deep learning model in Chapter 2, we took a further step into designing a new type of neural network architecture for TSC based on the famous Inception module proposed by <ref type="bibr" target="#b213">Szegedy et al., 2015.</ref> Although we achieved similar results to other non deep learning based approaches for TSC, the main focus of Chapter 3 was to motivate the use of the Inception module, with a focus on its running time, a severe bottleneck of the current state-of-the-art algorithm for TSC .</p><p>Finally, with the recent advances in deep learning for TSC being presented in the first three chapters, we turned our attention in Chapter 4 to our initial motivation: evaluating surgical skills from kinematic data using DNNs. We also focused on achieving state-of-the-art results while providing interpretability of our deep learning model, allowing us to leverage the high accuracy from DNNs while mitigating their black-box effect.</p><p>In order to have a thorough and fair experimental evaluation of all approaches, we used the whole UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> which contained 85 univariate time series datasets at that time. Other than the fact of being publicly available, the choice of validating on the UCR/UEA archive is motivated by having datasets from different domains which have been broken down into seven different categories (Image Outline, Sensor Readings, Motion Capture, Spectrographs, ECG, Electric Devices and Simulated Data) in <ref type="bibr" target="#b17">Bagnall et al., 2017.</ref> In fact, the UCR/UEA  archive has evolved over the years. In 2015 the benchmark contained 44 datasets, then in 2017 UCR and UEA collaborated and provided the community with an even larger archive containing 85 datasets, which represents the version that we have used in this work. In 2018, <ref type="bibr" target="#b51">Dau et al., 2019</ref> published the most recent version of the archive containing 128 datasets. We did our best to be consistent in the datasets used for our experiments to make our results easily comparable to other state-ofthe-art methods. The evolution of the UCR during this thesis can bring some sort of confusion in the datasets used in our different papers. Therefore <ref type="table" target="#tab_4">Table 1</ref> has the objective to clarify what has been used when our papers were published and what is now available on the companion GitHub pages. In any case, the source code for each paper is available to the community and can be easily reused to follow further evolution of the archive or other challenges. We are aware of the limitations of using the UCR/UEA archive as a sole reference to compare methods and we share the thoughts about this issue discussed by the authors in <ref type="bibr" target="#b51">Dau et al., 2019</ref>. However, we believe that having a way to objectively compare the methods between them is very important and we are thankful to all the people involved in making the UCR/UEA archive publicly available. As for the experiments regarding the surgical evaluation skills, we have used the publicly available JIGSAWS dataset published in <ref type="bibr" target="#b65">Gao et al., 2014</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 1</head><p>The state of the art for time series classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Introduction</head><p>During the last two decades, TSC has been considered as one of the most challenging problems in data mining <ref type="bibr" target="#b256">(Yang and Wu, 2006;</ref><ref type="bibr" target="#b54">Esling and Agon, 2012)</ref>. With the increase of temporal data availability <ref type="bibr" target="#b203">(Silva et al., 2018)</ref>, hundreds of TSC algorithms have been proposed since 2015 . Due to their natural temporal ordering, time series data are present in almost every task that requires some sort of human cognitive process <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. In fact, any classification problem, using data that is registered taking into account some notion of ordering, can be cast as a TSC problem (Cristian Borges Gamboa, 2017). Time series are encountered in many real-world applications ranging from health care <ref type="bibr" target="#b70">(Gogolou et al., 2018)</ref> and human activity recognition <ref type="bibr" target="#b154">Mathis, Ismail Fawaz, and Khamis, 2020)</ref> to acoustic scene classification <ref type="bibr" target="#b167">(Nwe, Dat, and Ma, 2017)</ref> and cyber security <ref type="bibr" target="#b209">(Susto, Cenedese, and Terzi, 2018)</ref>. In addition, the diversity of the datasets' types in the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017</ref>) (the largest repository of time series datasets) shows the different applications of the TSC problem.</p><p>Given the need to accurately classify time series data, researchers have proposed hundreds of methods to solve this task . One of the most popular and traditional TSC approaches is the use of an NN classifier coupled with a distance function <ref type="bibr" target="#b136">(Lines and Bagnall, 2015)</ref>. Particularly, DTW when used with an NN classifier has been shown to be a very strong baseline . Lines and Bagnall, 2015 compared several distance measures -such as TWE <ref type="bibr" target="#b150">(Marteau, 2009</ref>) and MSM <ref type="bibr" target="#b207">(Stefan, Athitsos, and Das, 2013)</ref> -showing that there is no single distance measure that significantly outperforms DTW. They also showed that ensembling the individual NN classifiers (with different distance measures) outperforms all of the ensemble's individual components. Hence, recent contributions have focused on developing ensembling methods that significantly outperforms the NN-DTW <ref type="bibr" target="#b81">Hills et al., 2014;</ref><ref type="bibr" target="#b30">Bostrom and Bagnall, 2015;</ref><ref type="bibr" target="#b135">Lines, Taylor, and Bagnall, 2016;</ref><ref type="bibr" target="#b200">Sch?fer, 2015;</ref><ref type="bibr" target="#b110">Kate, 2016;</ref><ref type="bibr" target="#b53">Deng et al., 2013;</ref><ref type="bibr" target="#b22">Baydogan, Runger, and Tuv, 2013)</ref>. These approaches use either an ensemble of decision trees (random forest) <ref type="bibr" target="#b22">(Baydogan, Runger, and Tuv, 2013;</ref><ref type="bibr" target="#b53">Deng et al., 2013)</ref> or an ensemble of different types of discriminant classifiers (SVM, NN with several distances) on one or several feature spaces <ref type="bibr" target="#b30">Bostrom and Bagnall, 2015;</ref><ref type="bibr" target="#b200">Sch?fer, 2015;</ref><ref type="bibr" target="#b110">Kate, 2016)</ref>. Most of these approaches significantly outperform the NN-DTW  and share one common property, which is the data transformation phase where time series are transformed into a new feature space (for example using shapelets transform <ref type="bibr" target="#b30">(Bostrom and Bagnall, 2015)</ref> or DTW features <ref type="bibr" target="#b110">(Kate, 2016)</ref>). This notion motivated the development of an ensemble of 35 classifiers named COTE ) that does not only ensemble different classifiers over the same transformation, but instead ensembles different classifiers over different time series representations. <ref type="bibr" target="#b135">Lines, Taylor, and Bagnall, 2016;</ref><ref type="bibr" target="#b137">Lines, Taylor, and Bagnall, 2018</ref> extended COTE with HIVE-COTE which has been shown to achieve a significant improvement over COTE by leveraging a new hierarchical structure with probabilistic voting, including two new classifiers and two additional representation transformation domains. HIVE-COTE is currently considered the state-of-the-art algorithm for time series classification  when evaluated over the 85 datasets from the UCR/UEA archive.</p><p>To achieve its high accuracy, HIVE-COTE becomes hugely computationally intensive and impractical to run on a real big data mining problem . The approach requires training 37 classifiers as well as cross-validating each hyperparameter of these algorithms, which makes the approach infeasible to train in some situations <ref type="bibr" target="#b143">(Lucas et al., 2018)</ref>. To emphasize on this infeasibility, note that one of these 37 classifiers is the ST <ref type="bibr" target="#b81">(Hills et al., 2014</ref>) whose time complexity is O(n 2 ? l 4 ) with n being the number of time series in the dataset and l being the length of a time series. Adding to the training time's complexity is the high classification time of one of the 37 classifiers: the nearest neighbor which needs to scan the training set before taking a decision at test time. Therefore since the nearest neighbor constitutes an essential component of HIVE-COTE, its deployment in a real-time setting is still limited if not impractical. Finally, adding to the huge runtime of HIVE-COTE, the decision taken by 37 classifiers cannot be interpreted easily by domain experts, since researchers already struggle with understanding the decisions taken by an individual classifier. We should note that recently, <ref type="bibr" target="#b18">Bagnall et al., 2020</ref> proposed a new version of HIVE-COTE that is substantially faster, showing the importance of having a scalable TSC algorithm.</p><p>After having established the current state-of-the-art of non deep classifiers for TSC ), we discuss the success of deep learning (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref> in various classification tasks which motivated the recent utilization of deep learning models for TSC <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>. Deep CNNs have revolutionized the field of computer vision <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref>. For example, in 2015, CNNs were used to reach human level performance in image recognition tasks . Following the success of DNNs in computer vision, a huge amount of research proposed several DNN architectures to solve many NLP tasks such as machine translation <ref type="bibr" target="#b211">(Sutskever, Vinyals, and Le, 2014;</ref><ref type="bibr" target="#b19">Bahdanau, Cho, and Bengio, 2015)</ref>, learning word embeddings  and document classification <ref type="bibr" target="#b126">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b71">Goldberg, 2016)</ref>. DNNs also had a huge impact on the speech recognition community <ref type="bibr" target="#b195">Sainath et al., 2013)</ref>. Interestingly, we should note that the intrinsic similarity between the NLP and speech recognition tasks is due to the sequential aspect of the data which is also one of the main characteristics of time series data.</p><p>In this context, this chapter targets the following open questions: What is the current state-of-the-art DNN for TSC? Is there a current DNN approach that reaches stateof-the-art performance for TSC and is less complex than HIVE-COTE? What type of DNN architectures works best for the TSC task? How does the random initialization affect the performance of deep learning classifiers? And finally: Could the black-box effect of DNNs be avoided to provide interpretability? Given that the latter questions have not been addressed by the TSC community, it is surprising how a small amount of papers have considered DNNs to be a potential accurate classifier of time series data <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref>. In fact, a recent empirical study  evaluated 18 TSC algorithms on 85 time series datasets, none of which was a deep learning model. This shows how much the community lacks of an overview of the current performance of deep learning models for solving the TSC problem <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref>.</p><p>In this chapter, we performed an empirical comparative study of the most recent deep learning approaches for TSC. With the rise of GPUs, we show how deep architectures can be trained efficiently to learn hidden discriminative features from raw time series in an end-to-end manner. Similarly to <ref type="bibr">Bagnall et al., 2017, in order</ref> to have a fair comparison between the tested approaches, we developed a common framework in Python, Keras <ref type="bibr" target="#b41">(Chollet, 2015)</ref> and Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> to train the deep learning models on a cluster of more than 60 GPUs.</p><p>In addition to the univariate datasets' evaluation, we tested the approaches on 12 MTS datasets <ref type="bibr" target="#b23">(Baydogan, 2015)</ref>. The multivariate evaluation shows another benefit of deep learning models, which is the ability to handle the curse of dimensionality <ref type="bibr" target="#b24">(Bellman, 2010;</ref><ref type="bibr" target="#b112">Keogh and Mueen, 2017)</ref> by leveraging different degrees of smoothness in compositional function <ref type="bibr" target="#b184">(Poggio et al., 2017)</ref> as well as the parallel computations of the GPUs <ref type="bibr" target="#b142">(Lu et al., 2015)</ref>.</p><p>As for comparing the classifiers over multiple datasets, we followed the recommendations in <ref type="bibr" target="#b52">Dem?ar, 2006 and</ref><ref type="bibr">used the Friedman test (Friedman, 1940)</ref> to reject the null hypothesis. Once we have established that a statistical difference exists within the classifiers' performance, we followed the pairwise post-hoc analysis recommended by <ref type="bibr" target="#b25">Benavoli, Corani, and Mangili, 2016</ref> where the average rank comparison is replaced by a Wilcoxon signed-rank test <ref type="bibr" target="#b247">(Wilcoxon, 1945)</ref> with Holm's alpha correction <ref type="bibr" target="#b85">(Holm, 1979;</ref><ref type="bibr" target="#b66">Garcia and Herrera, 2008)</ref>.</p><p>In this study, we have trained about 1 billion parameters across 97 univariate and multivariate time series datasets. Despite the fact that a huge number of parameters risks overfitting <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref> the relatively small train set in the UCR/UEA archive, our experiments showed that not only DNNs are able to significantly outperform the NN-DTW, but are also able to achieve results that are not significantly different than COTE and HIVE-COTE using a deep residual network architecture <ref type="bibr" target="#b78">(He et al., 2016;</ref><ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017</ref>). Finally, we analyze how poor random initializations can have a significant effect on a DNN's performance.</p><p>The rest of the chapter is structured as follows. In Section 1.2, we provide some background materials concerning the main types of architectures that have been proposed for TSC. In Section 1.3, the tested architectures are individually presented in details. We describe our experimental open source framework in Section 1.4. The corresponding results and the discussions are presented in Section 1.5. In Section 1.6, we describe in detail a couple of methods that mitigate the black-box effect of the deep learning models. Finally, we present a conclusion in Section 1.7 to summarize our findings and discuss future directions.</p><p>The main contributions presented in this chapter can be summarized as follows:</p><p>? We explain with practical examples, how deep learning can be adapted to one dimensional time series data.</p><p>? We propose a unified taxonomy that regroups the recent applications of DNNs for TSC in various domains under two main categories: generative and discriminative models.</p><p>? We detail the architecture of nine end-to-end deep learning models designed specifically for TSC. ? We evaluate these models on the univariate UCR/UEA archive benchmark and 12 MTS classification datasets.</p><p>? We provide the community with an open source deep learning framework for TSC in which we have implemented all nine approaches.</p><p>? We investigate the use of CAM in order to reduce DNNs' black-box effect and explain the different decisions taken by various models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Background</head><p>In this section, we start by introducing the necessary definitions for ease of understanding. We then follow by an extensive theoretical background on training DNNs for the TSC task. Finally we present our proposed taxonomy of the different DNNs with examples of their application in various real world data mining problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">Time series classification</head><p>Before introducing the different types of neural networks architectures, we go through some formal definitions for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1.</head><p>A univariate time series X = [x 1 , x 2 , . . . , x T ] is an ordered set of real values. The length of X is equal to the number of real values T.</p><formula xml:id="formula_0">Definition 2. An M-dimensional MTS, X = [X 1 , X 2 , . . . , X M ] consists of M different univariate time series with X i ? R T . Definition 3. A dataset D = {(X 1 , Y 1 ), (X 2 , Y 2 ), . . . , (X N , Y N )} is a collection of pairs (X i , Y i )</formula><p>where X i could either be a univariate or multivariate time series with Y i as its corresponding one-hot label vector. For a dataset containing K classes, the one-hot label vector Y i is a vector of length K where each element j ? [1, K] is equal to 1 if the class of X i is j and 0 otherwise.</p><p>The task of TSC consists of training a classifier on a dataset D in order to map from the space of possible inputs to a probability distribution over the class variable values (labels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Deep learning approaches for time series classification</head><p>In this chapter, we focus on reviewing various approaches tackling the TSC task  using DNNs, which are considered complex machine learning models (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref>. A general deep learning framework for TSC is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.1. These networks are designed to learn hierarchical representations of the data. A deep neural network is a composition of L parametric functions referred to as layers where each layer is considered a representation of the input domain <ref type="bibr" target="#b176">(Papernot and McDaniel, 2018)</ref>. One layer l i , such as i ? 1 . . . L, contains neurons, which are small units that compute one element of the layer's output. The layer l i takes as input the output of its previous layer l i?1 and applies a nonlinearity (such as the sigmoid function) to compute its own output. The behavior of these non-linear transformations is controlled by a set of parameters ? i for each layer. In the context of DNNs, these parameters are called weights which link the input of the previous layer to the output of the current layer. Hence, given an input x, a neural network performs the following computations to predict the class:</p><formula xml:id="formula_1">f L (? L , x) = f L?1 (? L?1 , f L?2 (? L?2 , . . . , f 1 (? 1 , x))) (1.1)</formula><p>where f i corresponds to the non-linearity applied at layer l i . For simplicity, we will omit the vector of parameters ? and use f (x) instead of f (?, x). This process is also referred to as feed-forward propagation in the deep learning literature. During training, the network is presented with a certain number of known inputoutput (for example a dataset D). First, the weights are initialized randomly , although a robust alternative would be to take a pre-trained model on a source dataset and fine-tune it on the target dataset <ref type="bibr" target="#b174">(Pan and Yang, 2010)</ref>. This process is known as transfer learning which we do not study empirically, rather we discuss the transferability of each model with respect to the architecture in Section 1.3. After the weight's initialization, a forward pass through the model is applied: using the function f the output of an input x is computed. The output is a vector whose components are the estimated probabilities of x belonging to each class. The model's prediction loss is computed using a cost function, for example the negative log likelihood. Then, using gradient descent , the weights are updated in a backward pass to propagate the error. Thus, by iteratively taking a forward pass followed by backpropagation, the model's parameters are updated in a way that minimizes the loss on the training data.</p><p>During testing, the probabilistic classifier (the model) is tested on unseen data which is also referred to as the inference phase: a forward pass on this unseen input followed by a class prediction. The prediction corresponds to the class whose probability is maximum. To measure the performance of the model on the test data (generalization), we adopted the accuracy measure (similarly to <ref type="bibr" target="#b17">Bagnall et al., 2017)</ref>. One advantage of DNNs over non-probabilistic classifiers (such as NN-DTW) is that a probabilistic decision is taken by the network <ref type="bibr" target="#b125">(Large, Lines, and Bagnall, 2017)</ref>, thus allowing to measure the confidence of a certain prediction given by an algorithm.</p><p>Although there exist many types of DNNs, in this review we focus on three main DNN architectures used for the TSC task: MLP, CNN and ESN. These three types of architectures were chosen since they are widely adopted for end-to-end deep learning (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref> models for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi Layer Perceptrons</head><p>An MLP constitutes the simplest and most traditional architecture for deep learning models. This form of architecture is also known as an FC network since the neurons in layer l i are connected to every neuron in layer l i?1 with i ? [1, L]. These connections are modeled by the weights in a neural network. A general form of applying a non-linearity to an input time series X can be seen in the following equation:</p><formula xml:id="formula_2">A l i = f (? l i * X + b) (1.2)</formula><p>with ? l i being the set of weights with length and number of dimensions identical to X's, b the bias term and A l i the activation of the neurons in layer l i . Note that the number of neurons in a layer is considered a hyperparameter. One impediment from adopting MLPs for time series data is that they do not exhibit any spatial invariance, which can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>.2. In other words, each time stamp has its own weight and the temporal information is lost: meaning time series elements are treated independently from each other. For example the set of weights w d of neuron d contains T ? M values denoting the weight of each time stamp t for each dimension of the M-dimensional input MTS of length T. Then by cascading the layers we obtain a computation graph similar to equation 1.1.</p><p>For TSC, the final layer is usually a discriminative layer that takes as input the activation of the previous layer and gives a probability distribution over the class variables in the dataset. Most deep learning approaches for TSC employ a softmax layer which corresponds to an FC layer with softmax as activation function f and a number of neurons equal to the number of classes in the dataset. Three main useful properties motivate the use of the softmax activation function: the sum of probabilities is guaranteed to be equal to 1, the function is differentiable and it is an adaptation of logistic regression to the multinomial case. The result of a softmax function can be defined as follows:</p><formula xml:id="formula_3">Y j (X) = e A L?1 * ? j +b j ? K k=1 e A L?1 * ? k +b k (1.3)</formula><p>with? j denoting the probability of X having the class Y equal to class j out of K classes in the dataset. The set of weights w j (and the corresponding bias b j ) for each class j are linked to each previous activation in layer l L?1 . The weights in equations (1.2) and (1.3) should be learned automatically using an optimization algorithm that minimizes an objective cost function. In order to approximate the error of a certain given value of the weights, a differentiable cost (or loss) function that quantifies this error should be defined. The most used loss function in DNNs for the classification task is the categorical cross entropy as defined in the following equation:</p><formula xml:id="formula_4">L(X) = ? K ? j=1 Y j log? j (1.4)</formula><p>with L denoting the loss or cost when classifying the input time series X. Similarly, the average loss when classifying the whole training set of D can be defined using the following equation:</p><formula xml:id="formula_5">J(?) = 1 N N ? n=1 L(X n ) (1.5)</formula><p>with ? denoting the set of weights to be learned by the network (in this case the weights w from equations 1.2 and 1.3). The loss function is minimized to learn the weights in ? using a gradient descent method which is defined using the following equation:</p><formula xml:id="formula_6">? = ? ? ? ?J ?? | ? ? ? ? (1.6)</formula><p>with ? denoting the learning rate of the optimization algorithm. By subtracting the partial derivative, the model is actually auto-tuning the parameters ? in order to reach a local minimum (or a saddle point) of J in case of a non-linear classifier (which is almost always the case for a DNN). We should note that when the partial derivative cannot be directly computed with respect to a certain parameter ?, the chain rule of derivative is employed which is in fact the main idea behind the backpropagation algorithm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Networks</head><p>Since AlexNet <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref> won the ImageNet competition in 2012, deep CNNs have seen a lot of successful applications in many different domains (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref> such as reaching human level performance in image recognition problems  as well as different natural language processing tasks <ref type="bibr" target="#b211">(Sutskever, Vinyals, and Le, 2014;</ref><ref type="bibr" target="#b19">Bahdanau, Cho, and Bengio, 2015)</ref>. Motivated by the success of these CNN architectures in these various domains, researchers have started adopting them for time series analysis (Cristian Borges Gamboa, 2017). A convolution can be seen as applying and sliding a filter over the time series. Unlike images, the filters exhibit only one dimension (time) instead of two dimensions (width and height). The filter can also be seen as a generic non-linear transformation of a time series. Concretely, if we are convoluting (multiplying) a filter of length 3 with a univariate time series, by setting the filter values to be equal to [ 1 3 , 1 3 , 1 3 ], the convolution will result in applying a moving average with a sliding window of length 3. A general form of applying the convolution for a centered time stamp t is given in the following equation:</p><formula xml:id="formula_7">C t = f (? * X t?l/2:t+l/2 + b) | ? t ? [1, T] (1.7)</formula><p>where C denotes the result of a convolution (dot product * ) applied on a univariate time series X of length T with a filter ? of length l, a bias parameter b and a final non-linear function f such as the ReLU. The result of a convolution (one filter) on an input time series X can be considered as another univariate time series C that underwent a filtering process. Thus, applying several filters on a time series will result in a multivariate time series whose dimensions are equal to the number of filters used. An intuition behind applying several filters on an input time series would be to learn multiple discriminative features useful for the classification task. Unlike MLPs, the same convolution (the same filter values w and b) will be used to find the result for all time stamps t ? [1, T]. This is a very powerful property (called weight sharing) of the CNNs which enables them to learn filters that are invariant across the time dimension.</p><p>When considering an MTS as input to a convolutional layer, the filter no longer has one dimension (time) but also has dimensions that are equal to the number of dimensions of the input MTS. Thus, the filter can be considered to be multivariate itself.</p><p>Finally, instead of setting manually the values of the filter ?, these values should be learned automatically since they depend highly on the targeted dataset. For example, one dataset would have the optimal filter to be equal to <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">2]</ref> whereas another dataset would have an optimal filter equal to [2, 0, ?1]. By optimal we mean a filter whose application will enable the classifier to easily discriminate between the dataset classes (see <ref type="figure" target="#fig_0">Figure 1</ref>.3). In order to learn automatically a discriminative filter, the convolution should be followed by a discriminative classifier, which is usually preceded by a pooling operation that can either be local or global.</p><p>Local pooling such as average or max pooling takes an input time series and reduces its length T by aggregating over a sliding window of the time series. For example if the sliding window's length is equal to 3 the resulting pooled time series will have a length equal to T 3 -this is only true if the stride is equal to the sliding window's length. With a global pooling operation, the time series will be aggregated over the whole time dimension resulting in a single real value. In other words, this is similar to applying a local pooling with a sliding window's length equal to the length of the input time series. Usually a global aggregation is adopted to reduce drastically the number of parameters in a model thus decreasing the risk of overfitting while enabling the use of CAM to explain the model's decision .</p><p>In addition to pooling layers, some deep learning architectures include normalization layers to help the network converge quickly. For time series data, the batch normalization operation is performed over each channel therefore preventing the internal covariate shift across one mini-batch training of time series (Ioffe and Szegedy, 2015). Another type of normalization was proposed by <ref type="bibr" target="#b224">Ulyanov, Vedaldi, and Lempitsky, 2016</ref> to normalize each instance instead of a per batch basis, thus learning the mean and standard deviation of each training instance for each layer via gradient descent. The latter approach is called instance normalization and mimics learning the z-normalization parameters for the time series training data.</p><p>The final discriminative layer takes the representation of the input time series (the result of the convolutions) and give a probability distribution over the class variables in the dataset. Usually, this layer is comprised of a softmax operation similarly to the MLPs. Note that for some approaches, we would have an additional non-linear FC layer before the final softmax layer which increases the number of parameters in a network. Finally in order to train and learn the parameters of a deep CNN, the process is identical to training an MLP: a feed-forward pass followed by backpropagation . An example of a CNN architecture for TSC with three convolutional layers is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Echo State Networks</head><p>Another popular type of architectures for deep learning models is the RNN. Apart from time series forecasting, we found that these neural networks were rarely applied for time series classification which is mainly due to three factors: (1) the type of this architecture is designed mainly to predict an output for each element (time stamp) in the time series <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>; (2) RNNs typically suffer from the vanishing gradient problem due to training on long time series <ref type="bibr" target="#b178">(Pascanu, Mikolov, and Bengio, 2012)</ref>; (3) RNNs are considered hard to train and parallelize which led the researchers to avoid using them for computational reasons <ref type="bibr" target="#b178">(Pascanu, Mikolov, and Bengio, 2013)</ref>.</p><p>Given the aforementioned limitations, a relatively recent type of recurrent architecture was proposed for time series: ESNs <ref type="bibr" target="#b64">(Gallicchio and Micheli, 2017)</ref>. ESNs were first invented by <ref type="bibr" target="#b102">Jaeger and Haas, 2004</ref> for time series prediction in wireless communication channels. They were designed to mitigate the challenges of RNNs by eliminating the need to compute the gradient for the hidden layers which reduces the training time of these neural networks thus avoiding the vanishing gradient problem. These hidden layers are initialized randomly and constitutes the reservoir: the core of an ESN which is a sparsely connected random RNN. Each neuron in the reservoir will create its own nonlinear activation of the incoming signal. The inter-connected weights inside the reservoir and the input weights are not learned via gradient descent, only the output weights are tuned using a learning algorithm such as logistic regression or Ridge classifier <ref type="bibr" target="#b84">(Hoerl and Kennard, 1970)</ref>. To better understand the mechanism of these networks, consider an ESN with input dimensionality M, neurons in the reservoir N r and an output dimensionality K equal to the number of classes in the dataset. Let X(t) ? R M , I(t) ? R N r and Y(t) ? R K denote the vectors of the input M-dimensional MTS, the internal (or hidden) state and the output unit activity for time t respectively. Further let W in ? R N r ?M and W ? R N r ?N r and W out ? R C?N r denote respectively the weight matrices for the input time series, the internal connections and the output connections as seen in <ref type="figure" target="#fig_0">Figure 1</ref>.5. The internal unit activity I(t) at time t is updated using the internal state at time step t ? 1 and the input time series element at time t. Formally the hidden state can be computed using the following recurrence:</p><formula xml:id="formula_8">I(t) = f (W in X(t) + W I(t ? 1)) | ? t ? [1, T]</formula><p>(1.8) with f denoting an activation function of the neurons, a common choice is tanh(?) applied element-wise <ref type="bibr" target="#b217">(Tanisaro and Heidemann, 2016)</ref>. The output can be computed according to the following equation:</p><formula xml:id="formula_9">Y(t) = W out I(t) (1.9)</formula><p>thus classifying each time series element X(t). Note that ESNs depend highly on the initial values of the reservoir that should satisfy a pre-determined hyperparameter: the spectral radius. <ref type="figure" target="#fig_0">Figure 1</ref>.5 shows an example of an ESN with a univariate input time series to be classified into K classes. Finally, we should note that for all types of DNNs, a set of techniques was proposed by the deep learning community to enhance neural networks' generalization capabilities. Regularization methods such as l2-norm weight decay <ref type="bibr" target="#b29">(Bishop, 2006)</ref> or Dropout <ref type="bibr" target="#b206">(Srivastava et al., 2014)</ref> aim at reducing overfitting by limiting the activation of the neurons. Another popular technique is data augmentation, which tackles the problem of overfitting a small dataset by increasing the number of training instances <ref type="bibr" target="#b20">(Baird, 1992)</ref>. This method consists in cropping, rotating and blurring images which have been shown to improve the DNNs' performance for computer vision tasks <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. Although two approaches in this survey include a data augmentation technique, the study of its impact on TSC is currently limited (Ismail <ref type="bibr" target="#b96">Fawaz et al., 2018b)</ref>. Finally we should note that Chapter 2 of this thesis is dedicated to study several regularization techniques of DNNs for the underlying TSC task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.3">Generative or discriminative approaches</head><p>Deep learning approaches for TSC can be separated into two main categories: the generative and the discriminative models (as proposed in <ref type="bibr" target="#b124">L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. We further separate these two groups into sub-groups which are detailed in the following subsections and illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative models</head><p>Generative models usually exhibit an unsupervised training step that precedes the learning phase of the classifier <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. This type of network has been referred to as Model-based classifiers in the TSC community . Some of these generative non deep learning approaches include auto-regressive models <ref type="bibr" target="#b16">(Bagnall and Janacek, 2014)</ref>, hidden Markov models (Kotsifakos and Papapetrou, 2014) and kernel models <ref type="bibr" target="#b40">(Chen et al., 2013)</ref>. For all generative approaches, the goal is to find a good representation of time series prior to training a classifier <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. Usually, to model the time series, classifiers are preceded by an unsupervised pre-training phase such as SDAEs <ref type="bibr" target="#b87">Hu, Zhang, and Zhou, 2016)</ref>. A generative CNN-based model was proposed in <ref type="bibr" target="#b239">Wang et al., 2016b;</ref><ref type="bibr" target="#b160">Mittelman, 2015</ref> where the authors introduced a deconvolutional operation followed by an upsampling technique that helps in reconstructing a multivariate time series. DBNs were also used to model the latent features in an unsupervised manner which are then leveraged to classify univariate and multivariate time series <ref type="bibr" target="#b21">Banerjee et al., 2017)</ref>. In <ref type="bibr" target="#b157">Mehdiyev et al., 2017;</ref><ref type="bibr" target="#b149">Malhotra et al., 2018;</ref><ref type="bibr" target="#b189">Rajan and Thiagarajan, 2018</ref>, an RNN auto-encoder was designed to first generate the time series then using the learned latent representation, they trained a classifier (such as SVM or Random Forest) on top of these representations to predict the class of a given input time series.</p><p>Other studies such as in <ref type="bibr">Aswolinskiy, Reinhart, and Steil, 2017;</ref><ref type="bibr" target="#b28">Bianchi et al., 2018;</ref><ref type="bibr" target="#b43">Chouikhi, Ammar, and Alimi, 2018;</ref><ref type="bibr">Ma et al., 2016 used self-predict modeling</ref> for time series classification where ESNs were first used to re-construct the time series and then the learned representation in the reservoir space was utilized for classification. We refer to this type of architecture by traditional ESNs in Figure 1.6. Other ESN-based approaches <ref type="bibr" target="#b39">(Chen et al., 2015;</ref><ref type="bibr" target="#b40">Chen et al., 2013;</ref><ref type="bibr" target="#b38">Che et al., 2017b)</ref> define a kernel over the learned representation followed by an SVM or an MLP classifier. In <ref type="bibr" target="#b72">Gong et al., 2018;</ref><ref type="bibr" target="#b231">Wang, Wang, and Liu, 2016</ref>, a meta-learning evolutionary-based algorithm was proposed to construct an optimal ESN architecture for univariate and multivariate time series. For more details concerning generative ESN models for TSC, we refer the interested reader to a recent empirical study <ref type="bibr" target="#b12">(Aswolinskiy, Reinhart, and Steil, 2016</ref>) that compared classification in reservoir and model-space for both multivariate and univariate time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative models</head><p>A discriminative deep learning model is a classifier (or regressor) that directly learns the mapping between the raw input of a time series (or its hand engineered features) and outputs a probability distribution over the class variables in a dataset. Several discriminative deep learning architectures have been proposed to solve the TSC task, but we found that this type of model could be further sub-divided into two groups: (1) deep learning models with hand engineered features and (2) end-to-end deep learning models.</p><p>The most frequently encountered and computer vision inspired feature extraction method for hand engineering approaches is the transformation of time series into images using specific imaging methods such as Gramian fields <ref type="bibr">(Wang and Oates, 2015b;</ref><ref type="bibr" target="#b240">Wang and Oates, 2015a)</ref>, recurrence plots <ref type="bibr" target="#b77">(Hatami, Gavet, and Debayle, 2017;</ref><ref type="bibr" target="#b221">Tripathy and Acharya, 2018)</ref> and Markov transition fields <ref type="bibr" target="#b237">(Wang and Oates, 2015)</ref>. Unlike image transformation, other feature extraction methods are not domain agnostic. These features are first hand-engineered using some domain knowledge, then fed to a deep learning discriminative classifier. For example in Uemura et al., 2018, several features (such as the velocity) were extracted from sensor data placed on a surgeon's hand in order to determine the skill level during surgical training. In fact, most of the deep learning approaches for TSC with some hand engineered features are present in human activity recognition tasks <ref type="bibr" target="#b91">(Ignatov, 2018)</ref>. For more details on the different applications of deep learning for human motion detection using mobile and wearable sensor networks, we refer the interested reader to a recent survey by <ref type="bibr" target="#b168">Nweke et al., 2018</ref> where deep learning approaches (with or without hand engineered features) were thoroughly described specifically for the human activity recognition task.</p><p>In contrast to feature engineering, end-to-end deep learning aims to incorporate the feature learning process while fine-tuning the discriminative classifier <ref type="bibr" target="#b168">(Nweke et al., 2018)</ref>. Since this type of deep learning approach is domain agnostic and does not include any domain specific pre-processing steps, we decided to further separate these end-to-end approaches using their neural network architectures.</p><p>In <ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017;</ref><ref type="bibr" target="#b67">Geng and Luo, 2018</ref>, an MLP was designed to learn from scratch a discriminative time series classifier. The problem with an MLP approach is that temporal information is lost and the features learned are no longer time-invariant. This is where CNNs are most useful, by learning spatially invariant filters (or features) from raw input time series <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>. During our study, we found that CNN is the most widely applied architecture for the TSC problem, which is probably due to their robustness and the relatively small amount of training time compared to their counterpart architectures such as RNNs or MLPs. Several variants of CNNs have been proposed and validated on a subset of the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017)</ref> such as ResNets <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017;</ref><ref type="bibr" target="#b67">Geng and Luo, 2018)</ref> which add linear shortcut connections for the convolutional layers potentially enhancing the model's accuracy <ref type="bibr" target="#b78">(He et al., 2016)</ref>. In <ref type="bibr" target="#b127">Le Guennec, Malinowski, and Tavenard, 2016;</ref><ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016;</ref><ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017;</ref><ref type="bibr">Zhao et al., 2017, traditional</ref> CNNs were also validated on the UCR/UEA archive. More recently in , the architectures proposed in Wang, Yan, and Oates, 2017 were modified to leverage a filter initialization technique based on the Daubechies 4 Wavelet values <ref type="bibr" target="#b193">(Rowe and Abbott, 1995)</ref>. Outside of the UCR/UEA archive, deep learning has reached state-of-the-art performance on several datasets in different domains <ref type="bibr" target="#b124">(L?ngkvist, Karlsson, and Loutfi, 2014)</ref>. For spatio-temporal series forecasting problems, such as meteorology and oceanography, DNNs were proposed in <ref type="bibr">Ziat et al., 2017. Strodthoff and</ref><ref type="bibr" target="#b208">Strodthoff, 2019</ref> proposed to detect myocardial infractions from electrocardiography data using deep CNNs. For human activity recognition from wearable sensors, deep learning is replacing the feature engineering approaches <ref type="bibr" target="#b168">(Nweke et al., 2018)</ref> where features are no longer hand-designed but rather learned by deep learning models trained through backpropagation. One other type of time series data is present in Electronic Health Records, where a recent generative adversarial network with a CNN <ref type="bibr" target="#b37">(Che et al., 2017a)</ref> was trained for risk prediction based on patients historical medical records. In Ismail Fawaz et al., 2018c, CNNs were designed to reach state-of-the-art performance for surgical skills identification. Liu, Hsaio, and Tu, 2018 leveraged a CNN model for multivariate and lag-feature characteristics in order to achieve stateof-the-art accuracy on the Prognostics and Health Management 2015 challenge data. Finally, a recent review of deep learning for physiological signals classification revealed that CNNs were the most popular architecture <ref type="bibr" target="#b57">(Faust et al., 2018)</ref> for the considered task. We mention one final type of hybrid architectures that showed promising results for the TSC task on the UCR/UEA archive datasets, where mainly CNNs were combined with other types of architectures such as Gated Recurrent Units <ref type="bibr" target="#b134">(Lin and Runger, 2018)</ref> and the attention mechanism <ref type="bibr" target="#b201">(Serr?, Pascual, and Karatzoglou, 2018)</ref>. The reader may have noticed that CNNs appear under Auto Encoders as well as under End-to-End learning in <ref type="figure" target="#fig_0">Figure 1</ref>.6. This can be explained by the fact that CNNs when trained as Auto Encoders have a complete different objective function than CNNs that are trained in an end-to-end fashion. Now that we have presented the taxonomy for grouping DNNs for TSC, we introduce in the following section the different approaches that we have included in our experimental evaluation. We also explain the motivations behind the selection of these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Benchmarking deep learning for time series classification</head><p>In this section, we start by explaining the reasons behind choosing discriminative end-to-end approaches for this empirical evaluation. We then describe in detail the nine different deep learning architectures with their corresponding advantages and drawbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Why discriminative end-to-end approaches ?</head><p>As previously mentioned in Section 1.2, the main characteristic of a generative model is fitting a time series self-predictor whose latent representation is later fed into an off-the-shelf classifier such as Random Forest or SVM. Although these models do sometimes capture the trend of a time series, we decided to leave these generative approaches out of our experimental evaluation for the following reasons:</p><p>? This type of method is mainly proposed for tasks other than classification or as part of a larger classification scheme ;</p><p>? The informal consensus in the literature is that generative models are usually less accurate than direct discriminative models <ref type="bibr" target="#b165">Nguyen, Gsponer, and Ifrim, 2017)</ref>;</p><p>? The implementation of these models is usually more complicated than for discriminative models since it introduces an additional step of fitting a time series generator -this has been considered a barrier with most approaches whose code was not publicly available such as <ref type="bibr" target="#b72">Gong et al., 2018;</ref><ref type="bibr" target="#b38">Che et al., 2017b;</ref><ref type="bibr" target="#b43">Chouikhi, Ammar, and Alimi, 2018;</ref><ref type="bibr" target="#b238">Wang et al., 2017;</ref><ref type="bibr"></ref> ? The accuracy of these models depends highly on the chosen off-the-shelf classifier which is sometimes not even a neural network classifier <ref type="bibr" target="#b189">(Rajan and Thiagarajan, 2018)</ref>.</p><p>Given the aforementioned limitations for generative models, we decided to limit our experimental evaluation to discriminative deep learning models for TSC. In addition to restricting the study to discriminative models, we decided to only consider end-to-end approaches, thus further leaving classifiers that incorporate feature engineering out of our empirical evaluation. We made this choice because we believe that the main goal of deep learning approaches is to remove the bias due to manually designed features <ref type="bibr" target="#b170">(Ord?nez and Roggen, 2016)</ref>, thus enabling the network to learn the most discriminant useful features for the classification task. This has also been the consensus in the human activity recognition literature, where the accuracy of deep learning methods depends highly on the quality of the extracted features <ref type="bibr" target="#b168">(Nweke et al., 2018)</ref>. Finally, since our goal is to provide an empirical study of domain agnostic deep learning approaches for any TSC task, we found that it is best to compare models that do not incorporate any domain knowledge into their approach.</p><p>As for why we chose the nine approaches (described in the next section), it is first because among all the discriminative end-to-end deep learning models for TSC, we wanted to cover a wide range of architectures such as CNNs, Fully CNNs, MLPs, ResNets, ESNs, etc. Second, since we cannot cover an empirical study of all approaches validated in all TSC domains, we decided to only include approaches that were validated on the whole (or a subset of) the univariate time series UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017)</ref> and/or on the MTS archive (Baydogan, 2015). Finally, we chose to work with approaches that do not try to solve a sub task of the TSC problem such as in <ref type="bibr" target="#b67">Geng and Luo, 2018</ref> where CNNs were modified to classify imbalanced time series datasets. To justify this choice, we emphasize that imbalanced TSC problems can be solved using several techniques such as data augmentation <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018b)</ref> and modifying the class weights <ref type="bibr" target="#b67">(Geng and Luo, 2018)</ref>. However, any deep learning algorithm can benefit from this type of modification. Therefore if we did include modifications for solving imbalanced TSC tasks, it would be much harder to determine if it is the choice of the deep learning classifier or the modification itself that improved the accuracy of the model. Another sub task that has been at the center of recent studies is early time series classification <ref type="bibr" target="#b235">(Wang et al., 2016a)</ref> where deep CNNs were modified to include an early classification of time series. More recently, a deep reinforcement learning approach was also proposed for the early TSC task <ref type="bibr" target="#b152">(Martinez et al., 2018)</ref>. For further details, we refer the interested reader to a recent survey on deep learning for early time series classification <ref type="bibr" target="#b197">(Santos and Kern, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Compared approaches</head><p>After having presented an overview over the recent deep learning approaches for time series classification, we present the nine architectures that we have chosen to compare in this chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi Layer Perceptron</head><p>The MLP, which depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.2, is considered the most traditional form of DNNs proposed in Wang, Yan, and Oates, 2017 as a baseline architecture for TSC. The network contains 4 layers in total where each one is fully connected to the output of its previous layer. The final layer is a softmax classifier, which is fully connected to its previous layer's output and contains a number of neurons equal to the number of classes in a dataset. All three hidden FC layers are composed of 500 neurons with ReLU as the activation function. Each layer is preceded by a dropout operation (Srivastava et al., 2014) with a rate equal to 0.1, 0.2, 0.2 and 0.3 for respectively the first, second, third and fourth layer. Dropout is one form of regularization that helps in preventing overfitting <ref type="bibr" target="#b206">(Srivastava et al., 2014)</ref>. The dropout rate indicates the percentage of neurons that are deactivated (set to zero) in a feed forward pass during training.</p><p>MLP does not have any layer whose number of parameters is invariant across time series of different lengths (denoted by #invar in <ref type="table" target="#tab_4">Table 1</ref>.1) which means that the transferability of the network is not trivial: the number of parameters (weights) of the network depends directly on the length of the input time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully Convolutional Neural Network</head><p>FCNs, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.4, were first proposed in Wang, Yan, and Oates, 2017 for classifying univariate time series and validated on 44 datasets from the UCR/UEA archive. FCNs are mainly convolutional networks that do not contain any local pooling layers which means that the length of a time series is kept unchanged throughout the convolutions. In addition, one of the main characteristics of this architecture is the replacement of the traditional final FC layer with a GAP layer which reduces drastically the number of parameters in a neural network while enabling the use of the CAM  that highlights which parts of the input time series contributed the most to a certain classification.</p><p>The architecture proposed in Wang, Yan, and Oates, 2017 is first composed of three convolutional blocks where each block contains three operations: a convolution followed by a batch normalization <ref type="bibr" target="#b93">(Ioffe and Szegedy, 2015)</ref> whose result is fed to a ReLU activation function. The result of the third convolutional block is averaged over the whole time dimension which corresponds to the GAP layer. Finally, a traditional softmax classifier is fully connected to the GAP layer's output.</p><p>All convolutions have a stride equal to 1 with a zero padding to preserve the exact length of the time series after the convolution. The first convolution contains 128 filters with a filter length equal to 8, followed by a second convolution of 256 filters with a filter length equal to 5 which in turn is fed to a third and final convolutional layer composed of 128 filters, each one with a length equal to 3. We can see that FCN does not hold any pooling nor a regularization operation. In addition, one of the advantages of FCNs is the invariance (denoted by #invar in <ref type="table" target="#tab_4">Table 1</ref>.1) in the number of parameters for 4 layers (out of 5) across time series of different lengths. This invariance (due to using GAP) enables the use of a transfer learning approach where one can train a model on a certain source dataset and then fine-tune it on the target dataset <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Network</head><p>The third and final proposed architecture in Wang, Yan, and Oates, 2017 is a relatively deep ResNet. For TSC, this is the deepest architecture with 11 layers of which the first 9 layers are convolutional followed by a GAP layer that averages the time series across the time dimension. The main characteristic of ResNet is the shortcut residual connection between consecutive convolutional layers. In fact, the difference with the usual convolutions (such as in FCNs) is that a linear shortcut is added to link the output of a residual block to its input thus enabling the flow of the gradient directly through these connections, which makes training a DNN much easier by reducing the vanishing gradient effect <ref type="bibr" target="#b78">(He et al., 2016)</ref>.</p><p>The network is composed of three residual blocks followed by a GAP layer and a final softmax classifier whose number of neurons is equal to the number of classes in a dataset. Each residual block is first composed of three convolutions whose output is added to the residual block's input and then fed to the next layer. The number of filters for all convolutions is fixed to 64, with the ReLU activation function that is preceded by a batch normalization operation. In each residual block, the filter's length is set to 8, 5 and 3 respectively for the first, second and third convolution.</p><p>Similarly to the FCN model, the layers (except the final one) in the ResNet architecture have an invariant number of parameters across different datasets. That being said, we can easily pre-train a model on a source dataset, then transfer and fine-tune it on a target dataset without having to modify the hidden layers of the network. As we have previously mentioned and since this type of transfer learning approach can give an advantage for certain types of architecture, we leave the exploration of this area of research for future work. The ResNet architecture proposed by Wang, Yan, and Oates, 2017 is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Originally proposed by <ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018</ref>, Encoder is a hybrid deep CNN whose architecture is inspired by FCN <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref> with a main difference where the GAP layer is replaced with an attention layer. In <ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018</ref>, two variants of Encoder were proposed: the first approach was to train the model from scratch in an end-to-end fashion on a target dataset while the second one was to pre-train this same architecture on a source dataset and then fine-tune it on a target dataset. The latter approach reached higher accuracy thus benefiting from the transfer learning technique. On the other hand, since almost all approaches can benefit to certain degree from a transfer learning method, we decided to implement only the end-to-end approach (training from scratch) which already showed high performance in the author's original paper. Similarly to FCN, the first three layers are convolutional with some relatively small modifications. The first convolution is composed of 128 filters of length 5; the second convolution is composed of 256 filters of length 11; the third convolution is composed of 512 filters of length 21. Each convolution is followed by an instance normalization operation <ref type="bibr" target="#b224">(Ulyanov, Vedaldi, and Lempitsky, 2016</ref>) whose output is fed to the PReLU <ref type="bibr" target="#b79">(He et al., 2015)</ref> activation function. The output of PReLU is followed by a dropout operation (with a rate equal to 0.2) and a final max pooling of length 2. The third convolutional layer is fed to an attention mechanism <ref type="bibr" target="#b19">(Bahdanau, Cho, and Bengio, 2015)</ref> that enables the network to learn which parts of the time series (in the time domain) are important for a certain classification. More precisely, to implement this technique, the input MTS is multiplied with a second MTS of the same length and number of channels, except that the latter has gone through a softmax function. Each element in the second MTS will act as a weight for the first MTS, thus enabling the network to learn the importance of each element (time stamp). Finally, a traditional softmax classifier is fully connected to the latter layer with a number of neurons equal to the number of classes in the dataset.</p><p>In addition to replacing the GAP layer with the attention layer, Encoder differs from FCN in three main core changes: (1) the PReLU activation function where an additional parameter is added for each filter to enable learning the slope of the function, (2) the dropout regularization technique and (3) the max pooling operation. One final note is that the careful design of Encoder's attention mechanism enabled the invariance across all layers which encouraged the authors to implement a transfer learning approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Convolutional Neural Network</head><p>Originally proposed by <ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016</ref>, MCNN is the earliest approach to validate an end-to-end deep learning architecture on the UCR Archive. MCNN's architecture is very similar to a traditional CNN model: with two convolutions (and max pooling) followed by an FC layer and a final softmax layer. On the other hand, this approach is very complex with its heavy data pre-processing step. <ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016</ref> were the first to introduce the WS method as a data augmentation technique. WS slides a window over the input time series and extract subsequences, thus training the network on the extracted subsequences instead of the raw input time series. Following the extraction of a subsequence from an input time series using the WS method, a transformation stage is used. More precisely, prior to any training, the subsequence will undergo three transformations: (1) identity mapping;</p><p>(2) down-sampling and (3) smoothing; thus, transforming a univariate input time series into a multivariate input time series. This heavy pre-processing would question the end-to-end label of this approach, but since their method is generic enough we incorporated it into our developed framework.</p><p>For the first transformation, the input subsequence is left unchanged and the raw subsequence will be used as an input for an independent first convolution. The down-sampling technique (second transformation) will result in shorter subsequences with different lengths which will then undergo another independent convolutions in parallel to the first convolution. As for the smoothing technique (third transformation), the result is a smoothed subsequence whose length is equal to the input raw subsequence which will also be fed to an independent convolution in parallel to the first and the second convolutions.</p><p>The output of each convolution in the first convolutional stage is concatenated to form the input of the subsequent convolutional layer. Following this second layer, an FC layer is deployed with 256 neurons using the sigmoid activation function. Finally, the usual softmax classifier is used with a number of neurons equal to the number of classes in the dataset.</p><p>Note that each convolution in this network uses 256 filters with the sigmoid as an activation function, followed by a max pooling operation. Two architecture hyperparameters are cross-validated, using a grid search on an unseen split from the training set: the filter length and the pooling factor which determines the pooling size for the max pooling operation. The total number of layers in this network is 4, out of which only the first two convolutional layers are invariant (transferable). Finally, since the WS method is also used at test time, the class of an input time series is determined by a majority vote over the extracted subsequences' predicted labels. <ref type="figure" target="#fig_0">Figure 1</ref>.9 shows the MCNN's architecture for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Le-Net</head><p>T-LeNet was originally proposed by <ref type="bibr" target="#b127">Le Guennec, Malinowski, and Tavenard, 2016</ref> and inspired by the great performance of LeNet's architecture for the document recognition task . This model can be considered as a traditional CNN with two convolutions followed by an FC layer and a final softmax classifier. There are two main differences with the FCNs: (1) an FC layer and (2) local maxpooling operations. Unlike GAP, local pooling introduces invariance to small perturbations in the activation map (the result of a convolution) by taking the maximum value in a local pooling window. Therefore for a pool size equal to 2, the pooling operation will halve the length of a time series by taking the maximum value between each two time steps.</p><p>For both convolutions, the ReLU activation function is used with a filter length equal to 5. For the first convolution, 5 filters are used and followed by a max pooling of length equal to 2. The second convolution uses 20 filters followed by a max pooling of length equal to 4. Thus, for an input time series of length l, the resulting output of these two convolutions will divide the length of the time series by 8 = 4 ? 2. The convolutional blocks are followed by a non-linear fully connected layer which is composed of 500 neurons, each one using the ReLU activation function. Finally, similarly to all previous architectures, the number of neurons in the final softmax classifier is equal to the number of classes in a dataset.</p><p>Unlike ResNet and FCN, this approach does not have much invariant layers (2 out of 4) due to the use of an FC layer instead of a GAP layer, thus increasing drastically the number of parameters needed to be trained which also depends on the length of the input time series. Thus, the transferability of this network is limited to the first two convolutions whose number of parameters depends solely on the number and length of the chosen filters.</p><p>We should note that t-LeNet is one of the approaches adopting a data augmentation technique to prevent overfitting especially for the relatively small time series datasets in the UCR/UEA archive. Their approach uses two data augmentation techniques: WS and WW. The former method is identical to MCNN's data augmentation technique originally proposed in <ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016</ref>. As for the second data augmentation technique, WW employs a warping technique that squeezes or dilates the time series. In order to deal with multi-length time series the WS method is adopted to ensure that subsequences of the same length are extracted for training the network. Therefore, a given input time series of length l is first dilated (?2) then squeezed (? 1 2 ) resulting in three time series of length l, 2l and 1 2 l that are fed to WS to extract equal length subsequences for training. Note that in their original paper <ref type="bibr" target="#b127">(Le Guennec, Malinowski, and Tavenard, 2016)</ref>, WS' length is set to 0.9l. Finally similarly to MCNN, since the WS method is also used at test time, a majority vote over the extracted subsequences' predicted labels is applied. <ref type="figure" target="#fig_0">Figure 1</ref>.10 shows the details of T-LeNet's architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi Channel Deep Convolutional Neural Network</head><p>MCDCNN was originally proposed and validated on two multivariate time series datasets <ref type="bibr" target="#b266">(Zheng et al., 2014;</ref><ref type="bibr" target="#b266">Zheng et al., 2016)</ref>. The proposed architecture is mainly a traditional deep CNN with one modification for MTS data: the convolutions are applied independently (in parallel) on each dimension (or channel) of the input MTS. Each dimension for an input MTS will go through two convolutional stages with 8 filters of length 5 with ReLU as the activation function. Each convolution is followed by a max pooling operation of length 2. The output of the second convolutional stage for all dimensions is concatenated over the channels axis and then fed to an FC layer with 732 neurons with ReLU as the activation function. Finally, the softmax classifier is used with a number of neurons equal to the number of classes in the dataset. By using an FC layer before the softmax classifier, the transferability of this network is limited to the first and second convolutional layers. MCDCNN's architecture is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Convolutional Neural Network</head><p>Time-CNN approach was originally proposed by <ref type="bibr" target="#b265">Zhao et al., 2017</ref> for both univariate and multivariate TSC. There are three main differences compared to the previously described networks. The first characteristic of Time-CNN is the use of the MSE instead of the traditional categorical cross-entropy loss function, which has been used by all the deep learning approaches we have mentioned so far. Hence, instead of a softmax classifier, the final layer is a traditional FC layer with sigmoid as the activation function, which does not guarantee a sum of probabilities equal to 1. Another difference to traditional CNNs is the use of a local average pooling operation instead of local max pooling. In addition, unlike MCDCNN, for MTS data they apply one convolution for all the dimensions of a multivariate classification task. Another unique characteristic of this architecture is that the final classifier is fully connected directly to the output of the second convolution, which removes completely the GAP layer without replacing it with an FC non-linear layer.</p><p>The network is composed of two consecutive convolutional layers with respectively 6 and 12 filters followed by a local average pooling operation of length 3. The convolutions adopt the sigmoid as the activation function. The network's output consists of an FC layer with a number of neurons equal to the number of classes in the dataset. <ref type="figure" target="#fig_0">Figure 1</ref>.12 shows the relatively shallow architecture of Time-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Warping Invariant Echo State Network</head><p>TWIESN <ref type="bibr" target="#b217">(Tanisaro and Heidemann, 2016)</ref> is the only non-convolutional recurrent architecture tested and re-implemented in our study. Although ESNs were originally proposed for time series forecasting, Tanisaro and Heidemann, 2016 proposed a variant of ESNs that uses directly the raw input time series and predicts a probability distribution over the class variables.</p><p>In fact, for each element (time stamp) in an input time series, the reservoir space is used to project this element into a higher dimensional space. Thus, for a univariate time series, the element is projected into a space whose dimensions are inferred from the size of the reservoir. Then for each element, a Ridge classifier <ref type="bibr" target="#b84">(Hoerl and Kennard, 1970)</ref> is trained to predict the class of each time series element. During test time, for each element of an input test time series, the already trained Ridge classifier will output a probability distribution over the classes in a dataset. Then the a posteriori probability for each class is averaged over all time series elements, thus assigning for each input test time series the label for which the averaged probability is maximum. Following the original paper of Tanisaro and Heidemann, 2016, using a grid-search on an unseen split (20%) from the training set, we optimized TWIESN's three hyperparameters: the reservoir's size, sparsity and spectral radius. An example of TWIESN's architecture is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.3">Hyperparameters</head><p>Tables 1.1 and 1.2 show respectively the architecture and the optimization hyperparameters for all the described approaches except for TWIESN, since its hyperparameters are not compatible with the eight other algorithms' hyperparameters. We should add that for all the other deep learning classifiers (with TWIESN omitted), a model checkpoint procedure was performed either on the training set or a validation set (split from the training set). Which means that if the model is trained for 1000  epochs, the best one on the validation set (or the train set) loss will be chosen for evaluation. This characteristic is included in <ref type="table" target="#tab_4">Table 1</ref>.2 under the "valid" column. In addition to the model checkpoint procedure, we should note that all deep learning models in <ref type="table" target="#tab_4">Table 1</ref>.1 were initialized randomly using Glorot's uniform initialization method <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. All models were optimized using a variant of SGD such as Adam <ref type="bibr" target="#b115">(Kingma and Ba, 2015)</ref> and AdaDelta <ref type="bibr" target="#b263">(Zeiler, 2012)</ref>. We should add that for FCN, ResNet and MLP proposed in Wang, Yan, and Oates, 2017, the learning rate was reduced by a factor of 0.5 each time the model's training loss has not improved for 50 consecutive epochs (with a minimum value equal to 0.0001).</p><p>One final note is that we have no way of controlling the fact that those described architectures might have been overfitted for the UCR/UEA archive and designed empirically to achieve a high performance, which is always a risk when comparing classifiers on a benchmark . We therefore think that challenges where only the training data is publicly available and the testing data are held by the challenge organizer for evaluation might help in mitigating this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Experimental setup</head><p>We first start by presenting the datasets' properties we have adopted in this empirical study. We then describe in details our developed open-source framework of deep learning for time series classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Univariate archive</head><p>In order to have a thorough and fair experimental evaluation of all approaches, we tested each algorithm on the whole UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> which contains 85 univariate time series datasets. The datasets possess different varying characteristics such as the length of the series which has a minimum value of 24 for the ItalyPowerDemand dataset and a maximum equal to 2,709 for the HandOutLines dataset. One important characteristic that could impact the DNNs' accuracy is the size of the training set which varies between 16 and 8926 for respectively Diatom-SizeReduction and ElectricDevices datasets. We should note that twenty datasets contains a relatively small training set (50 or fewer instances) which surprisingly was not an impediment for obtaining high accuracy when applying a very deep architecture such as ResNet. Furthermore, the number of classes varies between 2 (for 31 datasets) and 60 (for the ShapesAll dataset). Note that the time series in this archive are already z-normalized .</p><p>Other than the fact of being publicly available, the choice of validating on the UCR/UEA archive is motivated by having datasets from different domains which have been broken down into seven different categories (Image Outline, Sensor Readings, Motion Capture, Spectrographs, ECG, Electric Devices and Simulated Data) in <ref type="bibr" target="#b17">Bagnall et al., 2017</ref>. Further statistics, which we do not repeat for brevity, were conducted on the UCR/UEA archive in <ref type="bibr" target="#b17">Bagnall et al., 2017</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multivariate archive</head><p>We also evaluated all deep learning models on Baydogan's archive <ref type="bibr" target="#b23">(Baydogan, 2015)</ref> that contains 13 MTS classification datasets. For memory usage limitations over a single GPU, we left the MTS dataset Performance Measurement System out of our experimentations. This archive also exhibits datasets with different characteristics such as the length of the time series which, unlike the UCR/UEA archive, varies among the same dataset. This is due to the fact that the datasets in the UCR/UEA archive are already re-scaled to have an equal length among one dataset .</p><p>In order to solve the problem of unequal length time series in the MTS archive we decided to linearly interpolate the time series of each dimension for every given MTS, thus each time series will have a length equal to the longest time series' length. This form of pre-processing has also been used by <ref type="bibr" target="#b192">Ratanamahatana and Keogh, 2005</ref> to show that the length of a time series is not an issue for TSC problems. This step is very important for deep learning models whose architecture depends on the length of the input time series (such as an MLP) and for parallel computation over the GPUs. We did not z-normalize any time series, but we emphasize that this traditional pre-processing step  should be further studied for univariate as well as multivariate data, especially since normalization is known to have a huge effect on DNNs' learning capabilities <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. Note that this process is only true for the MTS datasets whereas for the univariate benchmark, the time series are already z-normalized. Since the data is pre-processed using the same technique for all nine classifiers, we can safely say, to some extent, that the accuracy improvement of certain models can be solely attributed to the model itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2">Experiments</head><p>For each dataset in both archives (97 datasets in total), we have trained the nine deep learning models (presented in the previous section) with 10 different runs each. Each run uses the same original train/test split in the archive but with a different random weight initialization, which enables us to take the mean accuracy over the 10 runs in order to reduce the bias due to the weights' initial values. In total, we have performed 8730 experiments for the 85 univariate and 12 multivariate TSC datasets. Thus, given the huge number of models that needed to be trained, we ran our experiments on a cluster of 60 GPUs. These GPUs were a mix of four types of Nvidia graphic cards: GTX 1080 Ti, Tesla K20, K40 and K80. The total sequential running time was approximately 100 days, that is if the computation has been done on a single GPU. However, by leveraging the cluster of 60 GPUs, we managed to obtain the results in less than one month. We implemented our framework using the open source deep learning library Keras <ref type="bibr" target="#b41">(Chollet, 2015)</ref> with the Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> back-end 1 . Following <ref type="bibr" target="#b143">Lucas et al., 2018;</ref><ref type="bibr" target="#b60">Forestier et al., 2017b;</ref><ref type="bibr">Petitjean et al., 2016;</ref><ref type="bibr" target="#b75">Grabocka et al., 2014</ref> we used the mean accuracy measure averaged over the 10 runs on the test set. When comparing with the state-of-the-art results published in <ref type="bibr" target="#b17">Bagnall et al., 2017</ref> we averaged the accuracy using the median test error. Following the recommendation in <ref type="bibr" target="#b52">Dem?ar, 2006</ref> we used the Friedman test <ref type="bibr" target="#b63">(Friedman, 1940)</ref> to reject the null hypothesis. Then we performed the pairwise post-hoc analysis recommended by <ref type="bibr" target="#b25">Benavoli, Corani, and Mangili, 2016</ref> where the average rank comparison is replaced by a Wilcoxon signed-rank test <ref type="bibr" target="#b247">(Wilcoxon, 1945</ref>) with Holm's alpha (5%) correction <ref type="bibr" target="#b85">(Holm, 1979;</ref><ref type="bibr" target="#b66">Garcia and Herrera, 2008)</ref>. To visualize this type of comparison we used a critical difference diagram proposed by <ref type="bibr" target="#b52">Dem?ar, 2006</ref>, where a thick horizontal line shows a group of classifiers (a clique) that are not-significantly different in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Results</head><p>In this section, we present the accuracies for each one of the nine approaches. All accuracies are absolute and not relative to each other that is if we claim algorithm A is 5% better than algorithm B, this means that the average accuracy is 0.05 higher for algorithm A than B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.1">Results for univariate time series</head><p>We provide on the companion GitHub repository the raw accuracies over the 10 runs for the nine deep learning models we have tested on the 85 univariate time series datasets: the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>. The corresponding critical difference diagram is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.13. The ResNet significantly outperforms the other approaches with an average rank of almost 2. ResNet wins on 34 problems out of 85 and significantly outperforms the FCN architecture. This is in contrast to the original paper's results where FCN was found to outperform ResNet on 18 out of 44 datasets, which shows the importance of validating on a larger archive in order to have a robust statistical significance. We believe that the success of ResNet is highly due to its deep flexible architecture. First of all, our findings are in agreement with the deep learning for computer vision literature where deeper neural networks are much more successful than shallower architectures <ref type="bibr" target="#b78">(He et al., 2016)</ref>. In fact, in a space of 4 years, neural networks went from 7 layers in AlexNet 2012 <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref> to 1000 layers for ResNet 2016 <ref type="bibr" target="#b78">(He et al., 2016)</ref>. These types of deep architectures generally need a huge amount of data in order to generalize well on unseen examples <ref type="bibr" target="#b78">(He et al., 2016)</ref>. Although the datasets used in our experiments are relatively small compared to the billions of labeled images (such as ImageNet <ref type="bibr" target="#b194">(Russakovsky et al., 2015)</ref> and OpenImages <ref type="bibr" target="#b118">(Krasin et al., 2017)</ref> challenges), the deepest networks did reach competitive accuracies on the UCR/UEA archive benchmark.</p><p>We give two potential reasons for this high generalization capabilities of deep CNNs on the TSC tasks. First, having seen the success of convolutions in classification tasks that require learning features that are spatially invariant in a two dimensional space (such as width and height in images), it is only natural to think that discovering patterns in a one dimensional space (time) should be an easier task for CNNs thus requiring less data to learn from. The other more direct reason behind the high accuracies of deep CNNs on time series data is its success in other sequential data such as speech recognition  and sentence classification <ref type="bibr" target="#b114">(Kim, 2014)</ref> where text and audio, similarly to time series data, exhibit a natural temporal ordering. MCNN yielded very low accuracy compared to t-LeNet. The main common idea between both of these approaches is extracting subsequences to augment the training data. Therefore the model learns to classify a time series from a shorter subsequence instead of the whole one, then with a majority voting scheme the time series at test time are assigned a class label. The poor performance of MCNN compared to t-LeNet suggest that the architecture itself is not very well optimized for the underlying task. In addition to MCNN's WS technique, t-LeNet uses the WW data augmentation method. This suggest that t-LeNet benefited even more from the warping time series data augmentation that shows significant improvement compared to MCNN's simple window slicing technique. Nevertheless, more architecture independent experiments are needed to further verify these findings regarding data augmentation <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018b)</ref>.</p><p>Although MCDCNN and Time-CNN were originally proposed to classify MTS datasets, we have evaluated them on the univariate UCR/UEA archive. The MCD-CNN did not manage to beat any of the cl assifiers except for the ECG5000 dataset which is already a dataset where almost all approaches reached the highest accuracy. This low performance is probably due to the non-linear FC layer that replaces the GAP pooling of the best performing algorithms (FCN and ResNet). This FC layer reduces the effect of learning time invariant features which explains why MLP, Time-CNN and MCDCNN exhibit very similar performance.</p><p>One approach that shows relatively high accuracy is Encoder <ref type="bibr" target="#b201">(Serr?, Pascual, and Karatzoglou, 2018)</ref>. The statistical test indicates a significant difference between Encoder, FCN and ResNet. FCN wins on 17 datasets whereases Encoder wins only on 9 which suggests the superiority of the GAP layer compared to Encoder's attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.2">Comparing with state-of-the-art approaches</head><p>In this subsection, we compared ResNet (the most accurate DNN of our study) with the current state-of-the-art classifiers evaluated on the UCR/UEA archive in the great time series classification bake off . Note that our empirical study strongly suggests to use ResNet instead of any other deep learning algorithm -it is the most accurate one with similar runtime to FCN (the second most accurate DNN). Finally, since ResNet's results were averaged over ten different random initializations, we chose to take one iteration of ResNet (the median) and compare it to other state-of-the-art algorithms that were executed once over the original train/test split provided by the UCR/UEA archive.</p><p>Out of the 18 classifiers evaluated by <ref type="bibr" target="#b17">Bagnall et al., 2017</ref>, we have chosen the four best performing algorithms: (1) EE proposed by <ref type="bibr" target="#b136">Lines and Bagnall, 2015</ref> is an ensemble of nearest neighbor classifiers with 11 different time series similarity measures;</p><p>(2) BOSS published in Sch?fer, 2015 forms a discriminative bag of words by discretizing the time series using a Discrete Fourier Transform and then building a nearest neighbor classifier with a bespoke distance measure; (3) ST developed by <ref type="bibr" target="#b81">Hills et al., 2014</ref> extracts discriminative subsequences (shapelets) and builds a new representation of the time series that is fed to an ensemble of 8 classifiers; (4) COTE proposed by <ref type="bibr" target="#b17">Bagnall et al., 2017</ref> is basically a weighted ensemble of 35 TSC algorithms including EE and ST. We also include HIVE-COTE (proposed by <ref type="bibr" target="#b137">Lines, Taylor, and Bagnall, 2018)</ref> which improves significantly COTE's performance by leveraging a hierarchical voting system as well as adding two new classifiers and two additional transformation domains. In addition to these five state-of-the-art classifiers, we have included the classic nearest neighbor coupled with DTW and a warping window set through cross-validation on the training set (denoted by NN-DTW-WW), since it is still one of the most popular methods for classifying time series data <ref type="bibr" target="#b17">(Bagnall et al., 2017)</ref>. Finally, we added a recent approach named PF which is similar to Random Forest but replaces the attribute based splitting criteria by a random similarity measure chosen out of EE's elastic distances <ref type="bibr" target="#b143">(Lucas et al., 2018)</ref>. Note that we did not implement any of the non-deep TSC algorithms. We used the results provided by <ref type="bibr" target="#b17">Bagnall et al., 2017</ref> and the other corresponding papers to construct the critical difference diagram in <ref type="figure" target="#fig_0">Figure 1</ref>.14.  <ref type="figure" target="#fig_0">Figure 1</ref>.14 shows the critical difference diagram over the UEA benchmark with ResNet added to the pool of six classifiers. As we have previously mentioned, the state-of-the-art classifiers are compared to ResNet's median accuracy over the test set. Nevertheless, we generated the ten different average ranks for each iteration of ResNet and observed that the ranking of the compared classifiers is stable for the ten different random initializations of ResNet. The statistical test failed to find any significant difference between COTE/HIVE-COTE and ResNet which is the only TSC algorithm that was able to reach similar performance to COTE. Note that for the ten different random initializations of ResNet, the pairwise statistical test always failed to find any significance between ResNet and COTE/HIVE-COTE. PF, ST, BOSS and ResNet showed similar performances according to the Wilcoxon signedrank test, but the fact that ResNet is not significantly different than COTE suggests that more datasets would give a better insight into these performances <ref type="bibr" target="#b52">(Dem?ar, 2006)</ref>. NN-DTW-WW and EE showed the lowest average rank suggesting that these methods are no longer competitive with current state-of-the-art algorithms for TSC. It is worthwhile noting that cliques formed by the Wilcoxon Signed Rank Test with Holm's alpha correction do not necessary reflect the rank order <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref>. For example, if we have three classifiers (C 1 , C 2 , C 3 ) with average ranks (C 1 &gt; C 2 &gt; C 3 ), one can still encounter a case where C 1 is not significantly worse than C 2 and C 3 with C 2 and C 3 being significantly different. In our experiments, when comparing to state-of-the-art algorithms, we have encountered this problem with (ResNet&gt;COTE&gt;HIVE-COTE). Therefore we should emphasize that HIVE-COTE and COTE are significantly different when performing the pairwise statistical test.</p><p>Although HIVE-COTE is still the most accurate classifier (when evaluated on the UCR/UEA archive) its use in a real data mining application is limited due to its huge training time complexity which is O(N 2 ? T 4 ) corresponding to the training time of one of its individual classifiers ST. However, we should note that the recent work of <ref type="bibr" target="#b30">Bostrom and Bagnall, 2015</ref> showed that it is possible to use a random sampling approach to decrease significantly the running time of ST (HIVE-COTE's choke-point) without any loss of accuracy. On the other hand, DNNs offer this type of scalability evidenced by its revolution in the field of computer vision when applied to images, which are thousand times larger than time series data <ref type="bibr" target="#b194">(Russakovsky et al., 2015)</ref>. In addition to the huge training time, HIVE-COTE's classification time is bounded by a linear scan of the training set due to employing a nearest neighbor classifier, whereas the trivial GPU parallelization of DNNs provides instant classification. Finally we should note that unlike HIVE-COTE, ResNet's hyperparameters were not tuned for each dataset but rather the same architecture was used for the whole benchmark suggesting further investigation of these hyperparameters should improve DNNs' accuracy for TSC. These results should give an insight of deep learning for TSC therefore encouraging researchers to consider DNNs as robust real time classifiers for time series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The need of a fair comparison</head><p>In this paragraph, we highlight the fairness of the comparison to other machine learning TSC algorithms. Since we did not train nor test any of the state-of-theart non deep learning algorithms, it is possible that we allowed much more training time for the described DNNs. For example, for a lazy machine learning algorithm such as NN-DTW, training time is zero when allowing maximum warping whereas it has been shown that judicially setting the warping window <ref type="bibr" target="#b50">Dau et al., 2017</ref> can lead to a significant increase in accuracy. Therefore, we believe that allowing a much more thorough search of DTW's warping window would lead to a fairer comparison between deep learning approaches and other state-of-the-art TSC algorithms. In addition to cross-validating NN-DTW's hyper-parameters, we can imagine spending more time on data pre-processing and cleansing (e.g. smoothing the input time series) in order to improve the accuracy of NN-DTW <ref type="bibr" target="#b86">(H?ppner, 2016;</ref><ref type="bibr" target="#b51">Dau et al., 2019)</ref>. Ultimately, in order to obtain a fair comparison between deep learning and current state-of-the-art algorithms for TSC, we think that the time spent on optimizing a network's weights should be also spent on optimizing non deep learning based classifiers especially lazy learning algorithms such as the K nearest neighbor coupled with any similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.3">Results for multivariate time series</head><p>We provide on our companion repository 2 the detailed performance of the nine deep learning classifiers for 10 different random initializations over the 12 MTS classification datasets <ref type="bibr" target="#b23">(Baydogan, 2015)</ref>. Although Time-CNN and MCDCNN are the only architectures originally proposed for MTS data, they were outperformed by the three deep CNNs (ResNet, FCN and Encoder), which shows the superiority of these approaches on the MTS classification task. The corresponding critical difference diagram is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.15, where the statistical test failed to find any significant difference between the nine classifiers which is mainly due to the small number of datasets compared to their univariate counterpart. Therefore, we illustrated in <ref type="figure">Figure</ref> 1.16 the critical difference diagram when both archives are combined (evaluation on 97 datasets in total). At first glance, we can notice that when adding the MTS datasets to the evaluation, the critical difference diagram in <ref type="figure" target="#fig_0">Figure 1</ref>.16 is not significantly different than the one in <ref type="figure" target="#fig_0">Figure 1</ref>.13 (where only the univariate UCR/UEA archive was taken into consideration). This is probably due to the fact that the algorithms' performance over the 12 MTS datasets is negligible to a certain degree when compared to the performance over the 85 univariate datasets. These observations reinforces the need to have an equally large MTS classification archive in order to evaluate hybrid univariate/multivariate time series classifiers. The rest of the analysis is dedicated to studying the effect of the datasets' characteristics on the algorithms' performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.4">What can the dataset's characteristics tell us about the best architecture?</head><p>The first dataset characteristic we have investigated is the problem's domain. These themes were first defined in <ref type="bibr" target="#b17">Bagnall et al., 2017</ref>. Again, we can clearly see the dominance of ResNet as the best performing approach across different domains. One exception is the ECG datasets (7 in total) where ResNet was drastically beaten by the FCN model in 71.4% of ECG datasets. However, given the small sample size (only 7 datasets), we cannot conclude that FCN will almost always outperform the ResNet model for ECG datasets .</p><p>The second characteristic which we have studied is the time series length. Similar to the findings for non deep learning models in <ref type="bibr" target="#b17">Bagnall et al., 2017</ref>, the time series length does not give information on deep learning approaches' performance. <ref type="table" target="#tab_4">Table 1</ref>.5 shows the average rank of each DNN over the univariate datasets grouped by the datasets' lengths. One might expect that the relatively short filters (3) might affect the performance of ResNet and FCN since longer patterns cannot be captured by short filters. However, since increasing the number of convolutional layers will increase the path length viewed by the CNN model <ref type="bibr" target="#b226">(Vaswani et al., 2017)</ref>, ResNet and FCN managed to outperform other approaches whose filter length is longer (21) such as Encoder. For the recurrent TWIESN algorithm, we were expecting a poor  ResNet and FCN achieved the worst accuracy (30%) on this dataset while Time-CNN reached the best accuracy (95%). Interestingly, DiatomSizeReduction is the smallest datasets in the UCR/UEA archive (with 16 training instances), which suggests that ResNet and FCN are easily overfitting this dataset. This suggestion is also supported by the fact that Time-CNN is the smallest model: it contains a very small number of parameters by design with only 18 filters compared to the 512 filters of FCN. This simple architecture of Time-CNN renders overfitting the dataset much harder. Therefore, we conclude that the small number of filters in Time-CNN is the main reason behind its success on small datasets, however this shallow architecture is unable to capture the variability in larger time series datasets which is modeled efficiently by the FCN and ResNet architectures. One final observation that is in agreement with the deep learning literature is that in order to achieve high accuracies while training a DNN, a large training set is needed. <ref type="figure" target="#fig_0">Figure 1</ref>.17 shows the effect of the training size on ResNet's accuracy for the TwoPatterns dataset: the accuracy increases significantly when adding more training instances until it reaches 100% for 75% of the training data.</p><p>Finally, we should note that the number of classes in a dataset -although it yielded some variability in the results for the recent TSC experimental study conducted by Bagnall et al., 2017 -did not show any significance when comparing the classifiers based on this characteristic. In fact, most DNNs architectures, with the categorical cross-entropy as their cost function, employ mainly the same classifier: softmax which is basically designed for multi-class classification.</p><p>Overall, our results show that, on average, ResNet is the best architecture with FCN and Encoder following as second and third respectively. ResNet performed very well in general except for the ECG datasets where it was outperformed by FCN. MCNN was significantly worse than all the other approaches, with t-LeNet showing a superiority with the additional WW technique. We found small variance between the approaches that replace the GAP layer with an FC dense layer (MCDCNN, CNN) which also showed similar performance to TWIESN, MLP and t-LeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.5">Effect of random initializations</head><p>The initialization of deep neural networks has received a significant amount of interest from many researchers in the field (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref>. These advancement have contributed to a better understanding and initialization of deep learning models in order to maximize the quality of non-optimal solutions found by the gradient descent algorithm <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. Nevertheless, we observed in our experiments, that DNNs for TSC suffer from a significant decrease (increase) in accuracy when initialized with bad (good) random weights. Therefore, we study in this section, how random initializations can affect the performance of ResNet and FCN on the whole benchmark in a best and worst case scenario. <ref type="figure" target="#fig_0">Figure 1</ref>.18 shows the accuracy plot of ResNet versus FCN on the 85 univariate time series datasets when aggregated over the 10 random initializations using three different functions: the minimum, median and maximum. When first observing <ref type="figure" target="#fig_0">Figure 1</ref>.18 one can easily conclude that ResNet has a better performance than FCN across most of the datasets regardless of the aggregation method. This is in agreement with the critical difference diagram as well as the analysis conducted in the previous subsections, where ResNet was shown to achieve higher performance on most datasets with different characteristics. A deeper look into the minimum aggregation (red points in <ref type="figure" target="#fig_0">Figure 1</ref> taking the worst initial weight values. This is also in agreement with the average standard deviation of ResNet (1.48) which is less than FCN's (1.70). These observations would encourage a practitioner to avoid using a complex deep learning model since its accuracy may be unstable. Nevertheless we think that investigating different weight initialization techniques such as leveraging the weights of a pre-trained neural network would yield better and much more stable results <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref> or even simply leveraging this high variance by ensembling DNNs (Ismail Fawaz et al., 2019e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Visualization</head><p>In this section, we start by investigating the use of Class Activation Map to provide an interpretable feedback that highlights the reason for a certain decision taken by the classifier. We then propose another visualization technique which is based on Multi-Dimensional Scaling <ref type="bibr" target="#b121">(Kruskal and Wish, 1978)</ref> to understand the latent representation that is learned by the DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.1">Class Activation Map</head><p>We investigate the use of CAM which was first introduced by Zhou et al., 2016 to highlight the parts of an image that contributed the most for a given class identification. Wang, Yan, and Oates, 2017 later introduced a one-dimensional CAM with an application to TSC. This method explains the classification of a certain deep learning model by highlighting the subsequences that contributed the most to a certain classification. <ref type="figure" target="#fig_0">Figure 1</ref>.19 and 1.20 show the results of applying CAM respectively on GunPoint and Meat datasets. Note that employing the CAM is only possible for the approaches with a GAP layer preceding the softmax classifier . Therefore, we only considered in this section the ResNet and FCN models, who also achieved the best accuracies overall. Note that Wang, Yan, and Oates, 2017 was the only paper to propose an interpretable analysis of TSC with a DNN. We should emphasize that this is a very important research area which is usually neglected for the sake of improving accuracy: only 2 out of the 9 approaches provided a method that explains the decision taken by a deep learning model. In this section, we start by presenting the CAM method from a mathematical point of view and follow it with two interesting case studies on Meat and GunPoint datasets. By employing a Global Average Pooling layer, ResNet and FCN benefit from the CAM method , which makes it possible to identify which regions of an input time series constitute the reason for a certain classification. Formally, let A(t) be the result of the last convolutional layer which is an MTS with M variables. A m (t) is the univariate time series for the variable m ? [1, M], which is in fact the result of applying the m th filter. Now let w c m be the weight between the m th filter and the output neuron of class c. Since a GAP layer is used then the input to the neuron of class c (z c ) can be computed by the following equation:</p><formula xml:id="formula_10">z c = ? m w c m ? t A m (t) (1.10)</formula><p>The second sum constitutes the averaged time series over the whole time dimension but with the denominator omitted for simplicity. The input z c can be also written by the following equation:</p><formula xml:id="formula_11">z c = ? t ? m w c m A m (t) (1.11)</formula><p>Finally the Class Activation Map (CAM c ) that explains the classification as label c is given in the following equation:</p><formula xml:id="formula_12">CAM c (t) = ? m w c m A m (t) (1.12)</formula><p>CAM is actually a univariate time series where each element (at time stamp t ? [1, T]) is equal to the weighted sum of the M data points at t, with the weights being learned by the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GunPoint dataset</head><p>The GunPoint dataset was first introduced by <ref type="bibr" target="#b192">Ratanamahatana and Keogh, 2005</ref> as a TSC problem. This dataset involves one male and one female actor performing two actions (Gun-Draw and Point) which makes it a binary classification problem. For Gun-Draw (Class-1 in <ref type="figure" target="#fig_0">Figure 1.19)</ref>, the actors have first their hands by their sides, then draw a replicate gun from hip-mounted holster, point it towards the target for one second, then finally place the gun in the holster and their hands to their initial position. Similarly to Gun-Draw, for Point (Class-2 in <ref type="figure" target="#fig_0">Figure 1.19)</ref> the actors follow the same steps but instead of pointing a gun they point their index finger. For each task, the centroid of the actor's right hands on both X and Y axes were tracked and seemed to be very correlated, therefore the dataset contains only one univariate time series: the X-axis. We chose to start by visualizing the CAM for GunPoint for three main reasons. First, it is easy to visualize unlike other noisy datasets. Second, both FCN and ResNet models achieved almost 100% accuracy on this dataset which will help us to verify if both models are reaching the same decision for the same reasons. Finally, it contains only two classes which allow us to analyze the data much more easily.  <ref type="figure" target="#fig_0">Figure 1.19c and 1.19d)</ref>. At first glance, we can clearly see how both DNNs are neglecting the plateau non-discriminative regions of the time series when taking the classification decision. It is depicted by the blue flat parts of the time series which indicates no contribution to the classifier's decision. As for the highly discriminative regions (the red and yellow regions) both models were able to select the same parts of the time series which correspond to the points with high derivatives. Actually, the first most distinctive part of class-1 discovered by both classifiers is almost the same: the little red bump in the bottom left of <ref type="figure" target="#fig_0">Figure 1.19a and 1.19c</ref>. Finally, another interesting observation is the ability of CNNs to localize a given discriminative shape regardless where it appears in the time series, which is evidence for CNNs' capability of learning time-invariant warped features. An interesting observation would be to compare the discriminative regions identified by a deep learning model with the most discriminative shapelets extracted by other shapelet-based approaches. This observation would also be backed up by the mathematical proof provided by <ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016</ref>, that showed how the learned filters in a CNN can be considered a generic form of shapelets extracted by the learning shapelets algorithm <ref type="bibr" target="#b75">(Grabocka et al., 2014)</ref>. <ref type="bibr">Ye and Keogh, 2011</ref> identified that the most important shapelet for the Gun/NoGun classification occurs when the actor's arm is lowered (about 120 on the horizontal axis in <ref type="figure" target="#fig_0">Figure 1.19)</ref>. <ref type="bibr" target="#b81">Hills et al., 2014</ref> introduced a shapelet transformation based approach that discovered shapelets that are similar to the ones identified by <ref type="bibr">Ye and Keogh, 2011</ref>. For ResNet and FCN, the part where the actor lowers his/her arm (bottom right of <ref type="figure" target="#fig_0">Figure 1.19</ref>) seems to be also identified as potential discriminative regions for some time series. On the other hand, the part where the actor raises his/her arm seems to be also a discriminative part of the data which suggests that the deep learning algorithms are identifying more "shapelets". We should note that this observation cannot confirm which classifier extracted the most discriminative subsequences especially because all algorithms achieved similar accuracy on GunPoint dataset. Perhaps a bigger dataset might provide a deeper insight into the interpretability of these machine learning models. Finally, we stress that the shapelet transformation classifier <ref type="bibr" target="#b81">(Hills et al., 2014)</ref> is an ensemble approach, which makes unclear how the shapelets affect the decision taken by the individual classifiers whereas for an end-to-end deep learning model we can directly explain the classification by using the Class Activation Map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meat dataset</head><p>Although the previous case study on GunPoint yielded interesting results in terms of showing that both models are localizing meaningful features, it failed to show the difference between the two most accurate deep learning classifiers: ResNet and FCN. Therefore we decided to further analyze the CAM's result for the two models on the Meat dataset.</p><p>Meat is a food spectrograph dataset which are usually used in chemometrics to classify food types, a task that has obvious applications in food safety and quality assurance. There are three classes in this dataset: Chicken, Pork and Turkey corresponding respectively to classes 1, 2 and 3 in <ref type="figure" target="#fig_0">Figure 1</ref>.20. Al-Jowder, Kemsley, and Wilson, 1997 described how the data is acquired from 60 independent samples using Fourier transform infrared spectroscopy with attenuated total reflectance sampling.</p><p>Similarly to GunPoint, this dataset is easy to visualize and does not contain very noisy time series. In addition, with only three classes, the visualization is possible to understand and analyze. Finally, unlike for the GunPoint dataset, the two  approaches ResNet and FCN reached significantly different results on Meat with respectively 97% and 83% accuracy. <ref type="figure" target="#fig_0">Figure 1</ref>.20 enables the comparison between FCN's CAM (left) and ResNet's CAM (right). We first observe that ResNet is much more firm when it comes to highlighting the regions. In other words, FCN's CAM contains much more smoother regions with cyan, green and yellow regions, whereas ResNet's CAM contains more dark red and blue subsequences showing that ResNet can filter out nondiscriminative and discriminative regions with a higher confidence than FCN, which probably explains why FCN is less accurate than ResNet on this dataset. Another interesting observation is related to the red subsequence highlighted by FCN's CAM for class 2 and 3 at the bottom right of <ref type="figure" target="#fig_0">Figure 1</ref>.20c and 1.20e. By visually investigating this part of the time series, we clearly see that it is a non-discriminative part since the time series of both classes exhibit this bump. This subsequence is therefore filtered-out by the ResNet model which can be seen by the blue color in the bottom right of <ref type="figure" target="#fig_0">Figure 1.20d and 1.20f</ref>. These results suggest that ResNet's superiority over FCN is mainly due to the former's ability to filter-out non-distinctive regions of the time series. We attribute this ability to the main characteristic of ResNet which is composed of the residual connections between the convolutional blocks that enable the model to learn to skip unnecessary convolutions by dint of its shortcut links <ref type="bibr" target="#b78">(He et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.2">Multi-Dimensional Scaling</head><p>We propose the use of MDS <ref type="bibr" target="#b121">(Kruskal and Wish, 1978)</ref> with the objective to gain some insights on the spatial distribution of the input time series belonging to different classes in the dataset. MDS uses a pairwise distance matrix as input and aims at placing each object in a N-dimensional space such as the between-object distances are preserved as well as possible. Using the ED on a set of input time series belonging to the test set, it is then possible to create a similarity matrix and apply MDS to display the set into a two dimensional space. This straightforward approach supposes that the ED is able to strongly separate the raw time series, which is usually not the case evident by the low accuracy of the nearest neighbor when coupled with ED .</p><p>On the other hand, we propose to apply this MDS method to visualize the set of time series with its latent representation learned by the network. Usually in a deep neural network, we have several hidden layers and one can find several latent representation of the dataset. But since we are aiming at visualizing the class specific latent space, we chose to use the last latent representation of a DNN (the one directly before the softmax classifier), which is known to be a class specific layer <ref type="bibr" target="#b260">(Yosinski et al., 2014b)</ref>. We decided to apply this method only on ResNet and FCN for two reasons: (1) when evaluated on the UCR/UEA archive they reached the highest ranks; (2) they both employ a GAP layer before the softmax layer making the number of latent features invariant to the time series length.</p><p>To better explain this process, for each input time series, the last convolution (for ResNet and FCN) outputs a multivariate time series whose dimensions are equal to the number of filters (128) in the last convolution, then the GAP layer averages the latter 128-dimensional multivariate time series over the time dimension resulting in a vector of 128 real values over which the ED is computed. As we worked with the ED, we used metric MDS <ref type="bibr" target="#b121">(Kruskal and Wish, 1978)</ref> that minimizes a cost function called Stress which is a residual sum of squares:</p><formula xml:id="formula_13">Stress D (X 1 , . . . , X N ) = ? i,j d ij ? x i ? x j 2 ? i,j d 2 ij 1/2 (1.13)</formula><p>where d ij is the ED between the GAP vectors of time series X i and X j . Obviously, one has to be careful about the interpretation of MDS output, as the data space is highly simplified (each time series X i is represented as a single data point x i ). We should note that there exists many visualization algorithms that would provide some insights about the data. One of the most popular techniques is called t-SNE <ref type="bibr" target="#b147">(Maaten and Hinton, 2008)</ref>. This method projects high-dimensional data by giving each data point (or in our case a time series or its representation) a location in a two or three dimensional map. It is an optimized derivative of Stochastic Neighbor Embedding <ref type="bibr" target="#b83">(Hinton and Roweis, 2003)</ref>. Although t-SNE has been used by many researchers, we decided to go with the MDS algorithm when analyzing time series representation. The reason behind this choice is that MDS is based on the pairwise distance matrix between the time series which is a very common way to cluster and analyze time series data <ref type="bibr">(Petitjean et al., 2016)</ref>, whereas t-SNE uses an iterative stochastic optimization process in order to preserve local structure at the expense of global structure in which we are interested. <ref type="figure" target="#fig_0">Figure 1</ref>.21 shows three MDS plots for the GunPoint dataset using: (1) the raw input time series <ref type="figure" target="#fig_0">(Figure 1.21a);</ref> (2) the learned latent features from the GAP layer for FCN <ref type="figure" target="#fig_0">(Figure 1.21b)</ref>; and (3) the learned latent features from the GAP layer for ResNet <ref type="figure" target="#fig_0">(Figure 1.21c</ref>). We can easily observe in <ref type="figure" target="#fig_0">Figure 1</ref>.21a that when using the raw input data and projecting it into a 2D space, the two classes are not linearly separable. On the other hand, in both <ref type="figure" target="#fig_0">Figures 1.21b and 1.21c</ref>, by applying MDS on the latent representation learned by the network, one can easily separate the set of time series belonging to the two classes. We note that both deep learning models (FCN and ResNet) managed to project the data from GunPoint into a linearly separable space which explains why both models performed equally very well on this dataset with almost 100% accuracy.</p><p>Although the visualization of MDS on GunPoint yielded some interesting results, it failed to pinpoint the difference between the two deep learning models FCN and ResNet. Therefore we decided to analyze another dataset where the accuracy of both models differed by almost 15%. <ref type="figure" target="#fig_0">Figure 1</ref>.22 shows three MDS plots for the Wine dataset using: (1) the raw input time series <ref type="figure" target="#fig_0">(Figure 1.22a);</ref> (2) the learned latent features from the GAP layer for FCN <ref type="figure" target="#fig_0">(Figure 1.22b)</ref>; and (3) the learned latent features from the GAP layer for ResNet <ref type="figure" target="#fig_0">(Figure 1.22c)</ref>. At first glimpse of <ref type="figure" target="#fig_0">Figure 1</ref>.22, the reader can conclude that all projections, even when using the learned representation, are not linearly separable which is evident by the relatively low accuracy of both models FCN and ResNet which is equal respectively to 58.7% and 74.4%. A thorough observation shows us that the learned hidden representation of ResNet <ref type="figure" target="#fig_0">(Figure 1.22c</ref>) separates the data from both classes in a much clearer way than the FCN <ref type="figure" target="#fig_0">(Figure 1.22b)</ref>. In other words, FCN's learned representation has too many data points close to the decision boundary whereas ResNet's hidden features enables projecting data points further away from the decision boundary. This observation could explain why ResNet achieves a better performance than FCN on the Wine dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7">Conclusion</head><p>In this chapter, we presented the largest empirical study of DNNs for TSC. We described the most recent successful deep learning approaches for TSC in many different domains such as human activity recognition and sleep stage identification. Under a unified taxonomy, we explained how DNNs are separated into two main categories of generative and discriminative models. We re-implemented nine recently published end-to-end deep learning classifiers in a unique framework which we make publicly available to the community. Our results show that end-to-end deep learning can achieve the current state-of-the-art performance for TSC with architectures such as Fully Convolutional Neural Networks and deep Residual Networks. Finally, we showed how the black-box effect of deep models which renders them uninterpretable, can be mitigated with a Class Activation Map visualization that highlights which parts of the input time series, contributed the most to a certain class identification. Although we have conducted an extensive experimental evaluation, deep learning for time series classification, unlike for computer vision and NLP tasks, still lacks a thorough study of data augmentation <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018b;</ref><ref type="bibr" target="#b60">Forestier et al., 2017b)</ref> and transfer learning <ref type="bibr">(Ismail Fawaz et al., 2018d;</ref><ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018</ref>). In addition, the time series community would benefit from an extension of this empirical study that compares in addition to accuracy, the training and testing time of these deep learning models. Furthermore, we think that the effect of z-normalization (and other normalization methods) on the learning capabilities of DNNs should also be thoroughly explored. In our future work, we aim to investigate and answer the aforementioned limitations by conducting more extensive experiments especially on multivariate time series datasets. In order to achieve all of these goals, one important challenge for the TSC community is to provide one large generic labeled dataset similar to the large images database in computer vision such as ImageNet <ref type="bibr" target="#b194">(Russakovsky et al., 2015)</ref> that contains 1000 classes.</p><p>In conclusion, with data mining repositories becoming more frequent, leveraging deeper architectures that can learn automatically from annotated data in an end-to-end fashion, makes deep learning a very enticing approach. In this chapter, we demonstrated the potential of deep neural networks for TSC, nevertheless these complex machine learning models can still benefit from many regularization techniques, which is the main focus of the following chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularizing deep neural networks 2.1 Introduction</head><p>Deep learning models usually have significantly more trainable parameters than the number of training instances. Nevertheless, in the previous chapter, we showed how these artificial neural networks are able to achieve good generalization capabilities compared to traditional TSC algorithms. Yet most of these models require some sort of regularization in order to ensure a small generalization error (i.e. a small difference between the training and testing error). In this chapter, we present four main techniques for regularizing DNNs for TSC: (1) transfer learning <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref>; (2) ensembling (Ismail Fawaz et al., 2019e); (3) data augmentation <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018b)</ref>; and adversarial training <ref type="bibr">(Ismail Fawaz et al., 2019b)</ref>; Our work reveals that these techniques improve the accuracy of the previously described deep learning models such as FCN and ResNet.</p><p>The chapter is divided into four main sections, each one describing in details the regularization technique. First we start by describing how transfer learning can further improve the performance of DNNs for the TSC task. We then showcase our findings regarding how ensembling deep learning models can significantly improve the performance of these time series classifiers. In the following section, we present how adopting a data augmentation method can help in improving the performance of neural networks for TSC. Finally, we define and investigate how DNNs are vulnerable to adversarial examples, and explore the possibility of leveraging those examples to regularize deep learning time series classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transfer learning</head><p>CNNs have recently been shown to significantly outperform the nearest neighbor approach coupled with the DTW algorithm (NN-DTW) on the UCR/UEA archive benchmark <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> for the TSC problem <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017</ref>). CNNs were not only able to beat the NN-DTW baseline, but in the previous chapter, we showed how they were also able to reach results that are not significantly different than COTE  -which is an ensemble of 35 classifiers. However, despite the high performance of these CNNs, deep learning models are still prone to overfitting. One example where these neural networks fail to generalize is when the training set of the time series dataset is very small. We attribute this huge difference in accuracy to the overfitting phenomena, which is still an open area of research in the deep learning community <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. This problem is known to be mitigated using several regularization techniques such as transfer learning <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref>, where a model trained on a source task is then finetuned on a target dataset. For example in <ref type="figure" target="#fig_30">Figure 2</ref>.1, we trained a model on the ElectricDevices dataset <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> and then fine-tuned this same model on the OSULeaf dataset <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, which significantly improved the network's generalization capability.</p><p>Transfer learning is currently used in almost every deep learning model when the target dataset does not contain enough labeled data <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref>. Despite its recent success in computer vision <ref type="bibr" target="#b46">(Csurka, 2017)</ref>, transfer learning has been rarely applied to deep learning models for time series data. One of the reasons for this absence is probably the lack of one big general purpose dataset similar to Im-ageNet <ref type="bibr" target="#b194">(Russakovsky et al., 2015)</ref> or OpenImages <ref type="bibr" target="#b118">(Krasin et al., 2017)</ref> but for time series. Furthermore, it is only recently that deep learning was proven to work well for TSC <ref type="bibr" target="#b47">(Cui, Chen, and Chen, 2016)</ref> and there is still much to be explored in building deep neural networks for mining time series data (Cristian Borges Gamboa, 2017).</p><p>Since transferring deep learning models, between the UCR/UEA archive datasets <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, have not been thoroughly studied, we decided to investigate this area of research with the ultimate goal to determine in advance which dataset transfers could benefit the CNNs and improve their TSC accuracy.</p><p>The intuition behind the transfer learning approach for time series data is also partially inspired by the observation of <ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016</ref>, where the authors showed that shapelets <ref type="bibr" target="#b257">(Ye and Keogh, 2009</ref>) (or subsequences) learned by the learning shapelets approach <ref type="bibr" target="#b75">(Grabocka et al., 2014)</ref> are related to the filters (or kernels) learned by the CNNs. We hypothesize that these learned subsequences might not be specific to one dataset and could occur in other unseen datasets with un/related classification tasks. Another observation for why transfer learning should work for time series data is its recent success in computer vision tasks <ref type="bibr" target="#b46">(Csurka, 2017)</ref>. Indeed, since time series data contain one temporal dimension (time) compared to two dimensions for images (width and height), it is only natural to think that if filters can successfully be transferred on images <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref>, they should also be transferable across time series datasets.</p><p>To evaluate the potential of transfer learning for TSC, we performed experiments where each pair of datasets in the UCR/UEA archive was tested twice: we pretrained a model for each dataset, then transferred and fine-tuned it on all the other datasets (a total of more than 7140 trained models). <ref type="figure" target="#fig_30">Figure 2</ref>.2 illustrates the architecture of our proposed framework of transfer learning for TSC on two datasets. The obtained results show that time series do exhibit some low level features that could be used in a transfer learning approach. They also show that using transfer learning reduces the training time by reducing the number of epochs needed for the network to converge on the train set. Motivated by the consensus that transferring models between similar datasets improves the classifier's accuracy <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>, we used the DTW algorithm as an inter-datasets similarity measure in order to quantify the relationship between the source and target datasets in our transfer learning framework. Our experiments show that DTW can be used to predict the best source dataset for a given target dataset. Our method can thus identify which datasets should be considered for transfer learning given a new TSC problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Background and related work</head><p>Before getting into the details of the recent applications for transfer learning, we give a formal definition of the latter <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>. Definition 1. Transfer learning for deep neural networks, is the process of first training a base network on a source dataset and task, and then transfer the learned features (the network's weights) to a second network to be trained on a target dataset and task. Throughout this section, we will refer to source dataset as the dataset we are transferring the pre-trained model from, and to target dataset as the dataset we are transferring the pre-trained model to.</p><p>Now that we have established the necessary definition, we will dive into the recent applications of transfer learning for time series data mining tasks. In fact, transfer learning is sometimes confused with the domain adaptation approach <ref type="bibr" target="#b174">(Pan and Yang, 2010;</ref><ref type="bibr" target="#b141">Long et al., 2015)</ref>. The main difference with the latter method is that the model is jointly trained on the source and target datasets <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>. The goal of using the target instances during training, is to minimize the discrepancy between the source's and target's instances. In Arief-Ang, Salim, and Hamilton, 2017, a domain adaptation approach was proposed to predict human indoor occupancy based on the carbon dioxide concentration in the room. In <ref type="bibr" target="#b109">Kasteren, Englebienne, and Kr?se, 2008</ref>, hidden Markov models' generative capabilities were used in a domain adaptation approach to recognize human activities based on a sensor network.</p><p>For time series anomaly detection, a transfer learning approach was used to determine which time series should be transferred from the source to the target dataset to be used with an NN-DTW classifier <ref type="bibr" target="#b228">(Vercruyssen, Meert, and Davis, 2017)</ref>. Similarly, Spiegel, 2016 developed a method to transfer specific training examples from the source dataset to the target dataset and hence compute the dissimilarity matrix using the new training set. As for time series forecasting, a transfer learning approach for an auto-encoder was employed to predict the wind-speed in a farm <ref type="bibr" target="#b87">(Hu, Zhang, and Zhou, 2016)</ref>. The authors proposed first to train a model on the historical wind-speed data of an old farm and fine-tune it using the data of a new farm. In <ref type="bibr" target="#b21">Banerjee et al., 2017</ref> restricted Boltzmann machines were first pre-trained for acoustic phoneme recognition and then fine-tuned for post-traumatic stress disorder diagnosis.</p><p>Perhaps the recent work in <ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018</ref> is the closest to ours in terms of using transfer learning to improve the accuracy of deep neural networks for TSC. In this work, the authors designed a CNN with an attention mechanism to encode the time series in a supervised manner. Before fine-tuning a model on a target dataset, the model is first jointly pre-trained on several source datasets with themes  that are different from the target dataset's theme which limits the choice of the source dataset to only one. Additionally, unlike Serr?, Pascual, and Karatzoglou, 2018, we take a pre-designed deep learning model without modifying it nor adding regularizers. This enabled us to solely attribute the improvement in accuracy to the transfer learning feature, which we describe in details in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Method</head><p>In this subsection, we present our proposed method of transfer learning for TSC. The adopted neural network architecture was FCN, for the simple reason of being three times faster than ResNet while being ranked the second most accurate model in the previous chapter. We then thoroughly explain how we adapted the network for the transfer learning process. Finally, we present our DTW based method that enabled us to compute the inter-datasets similarities, which we later use to guide the transfer learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network adaptation</head><p>After training FCN on the 85 datasets in the archive, we obtain 85 different neural networks. The only difference between these 85 neural network architectures lies in the output layer. The rest of the layers have the same number of parameters but with different values. In fact the last layer, which is a softmax classifier, depends on the number of classes in the dataset.</p><p>Thus, given a source dataset D s and a target dataset D t , we first train the network on D s . We then remove the last layer and replace it with another softmax layer whose number of neurons is equal to the number of classes in the target dataset D t . The added softmax layer's parameters are initialized randomly using Glorot's uniform initialization <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. This new network is then re-trained (finetuned) on D t .</p><p>We chose to fine-tune the whole network instead of training only the last newly added output layer. We tried to limit back-propagating the gradient to the last layer, but found that the network failed to converge. This is in compliance with the transfer learning literature <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref>, where re-training the whole network almost always leads to better results.</p><p>Finally, we should add that one of the advantages of using a global average pooling layer is that we do not need to re-scale the input time series when transferring models between time series of different length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-datasets similarity</head><p>One of the main challenges with transfer learning is choosing the source dataset. In <ref type="bibr" target="#b175">Pan et al., 2011, it</ref> was demonstrated that a learning algorithm trained with a certain source domain will not yield an optimal performance if the marginal distributions of the datasets' input are different. In our case, the total number of datasets in the UCR/UEA archive is 85. Therefore for each target dataset in the archive, we have 84 potential source datasets. This makes the trial and error based approach for transfer learning very costly in terms of computational resources. Hence, we propose to use the DTW distance to compute the similarities between the datasets, thus guiding the choice of a source dataset for a given target dataset.</p><p>Note that it is practically impossible to directly estimate the performance of a model learned on a source dataset by applying it on a target dataset's train set since the last layer of the network is specific <ref type="bibr" target="#b260">(Yosinski et al., 2014a)</ref> to the classes of the source dataset.</p><p>Before describing in details our method for computing inter-datasets similarity, we start by explaining the DTW distance.</p><p>DTW was first proposed for speech recognition when aligning two audio signals <ref type="bibr" target="#b196">(Sakoe and Chiba, 1978)</ref>. Suppose we want to compute the dissimilarity between two time series, X 1 = [x 1,1 , x 1,2 , . . . , x 1,m ] and X 2 = [x 2,1 , x 2,2 , . . . , x 2,n ]. The length of X 1 and X 2 are denoted respectively by m and n.</p><p>Let M(X 1 , X 2 ) be the m ? n point-wise dissimilarity matrix between X 1 and X 2 , where M i,j = ||x 1,i ? x 2,j || 2 . A warping path P = ((c 1 , d 1 ), (c 2 , d 2 ), . . . , (c s , d s )) is a series of points that define a crossing of M. The warping path must satisfy three conditions: (1) (c 1 , d 1 ) = (1, 1); (2) (c s , d s ) = (m, n); (3) 0 ? c i+1 ? c i ? 1 and 0 ? d j+1 ? d j ? 1 for all i &lt; m and j &lt; n. The DTW measure between two series corresponds to the path through M that minimizes the total distance. In fact, the distance for any path P is equal to D P (A, B) = ? s i=1 P i . Hence if P is the space of all possible paths, the optimal one -whose cost is equal to DTW(A, B) -is denoted by P * and can be computed using: min P?P D P <ref type="figure">(A, B)</ref>. The optimal warping path can be obtained efficiently by applying a dynamic programming technique to fill the cost matrix M.</p><p>Now that we have explained in details the DTW algorithm, which is usually used for computing a distance between two time series, we will describe the DBA algorithm which was first proposed by <ref type="bibr" target="#b183">Petitjean, Ketterlin, and Gan?arski, 2011</ref> in order to average in the DTW induced space (as opposed to the arithmetic mean in the Euclidean space).</p><p>DBA is an averaging method which consists in iteratively refining an initial series in order to minimize its squared DTW distance to the set of series we want to average. Technically for each refinement (or iteration), DBA consists of two main steps:</p><p>1. Computing the DTW between each individual series and the current temporary average that we want to refine. This is in order to find optimal associations between the elements of the average series and elements of the set of series to be averaged.</p><p>2. Now that we have these associations, we update each element of the average series as the barycenter of the elements associated to it during the previous step.</p><p>In order to compute the similarities between the datasets, we first reduce the number of time series for each dataset to one time series (or prototype) per class. The per class prototype is computed by averaging the set of time series in the corresponding class, using the previously described DBA algorithm as a data reduction step. The latter summarizing function was proposed and validated as an averaging method in the DTW induced space. In addition, DBA has been recently used as a data reduction technique where it was evaluated in a nearest centroid classification schema <ref type="bibr" target="#b180">(Petitjean et al., 2014)</ref>. Therefore, to generate the similarity matrix between the UCR datasets, we computed a distance between each pair of datasets. Finally, for simplicity and since the main goal of this project is not the inter-datasets similarity, we chose the distance between two datasets to be equal to the minimum distance between the prototypes of their corresponding classes. Note that other averaging methods such as soft-DTW <ref type="bibr" target="#b48">(Cutur and Blondel, 2017)</ref> and TEKA <ref type="bibr" target="#b151">(Marteau, 2019)</ref> could be used instead of DBA in our framework, but we leave such exploration for our future work.</p><p>Algorithm 1 shows the different steps followed to compute the distance matrix between the UCR datasets. The first part of the algorithm (lines 1 through 7) presents the data reduction technique similar to <ref type="bibr" target="#b180">Petitjean et al., 2014</ref>. For the latter step, we first go through the classes of each dataset (lines 1, 2 and 3) and then average the set of time series for each class. Following the recommendations in <ref type="bibr" target="#b180">Petitjean et al., 2014</ref>, the averaging method (DBA) was initialized to be equal to the medoid of the time series selected set (line 4). We fixed the number of iterations for the DBA algorithm to be equal to 10, for which the averaging method has been shown to converge <ref type="bibr" target="#b183">(Petitjean, Ketterlin, and Gan?arski, 2011)</ref>.</p><p>After having reduced the different sets for each time series dataset, we proceed to the actual distance computation step (lines 8 through 22). From line 8 to 10, we loop through every possible combination of datasets pairs. Lines 13 and 14 show the loop through each class for each dataset (at this stage each class is represented by one average time series thanks to the data reduction steps). Finally, lines 15 through 19 set the distance between two datasets to be equal to the minimum DTW distance between their corresponding classes.</p><p>One final note is that when computing the similarity between the datasets, the only time series data we used came from the training set, thus eliminating any bias due to having seen the test set's distribution.</p><p>Algorithm 1 Inter-datasets similarity Input: N time series datasets in an array D Output: N ? N datasets similarity matrix 1: Initialization : matrix M of size N ? N 2: data reduction step 3: for i = 1 to N do for c i = 1 to length(C i ) do <ref type="bibr">17:</ref> for c j = 1 to length(C j ) do 18: </p><formula xml:id="formula_14">cdist = DTW(C i [c i ], C j [c j ]) 19: dist = minimum(dist,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Experimental setup Datasets</head><p>We evaluate our developed framework thoroughly on the largest publicly available benchmark for time series analysis: the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, which consists of 85 datasets selected from various real-world domains. The time series in the archive are already z-normalized to have a mean equal to zero and a standard deviation equal to one. During the experiments, we used the default training and testing set splits provided by UCR. For pre-training a model, we used only the train set of the source dataset. We also fine-tuned the pre-trained model solely on the target dataset's training data. Hence the test sets were only used for evaluation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Experiments</head><p>For each pair of datasets (D 1 and D 2 ) in the UCR/UEA archive we need to perform two experiments:</p><p>? D 1 is the source dataset and D 2 is the target dataset.</p><p>? D 1 is the target dataset and D 2 is the source dataset.</p><p>Which makes it in total 7140 experiments for the 85 dataset in the archive. Hence, given the huge number of models that need to be trained, we ran our experiments on a cluster of 60 GPUs. These GPUs were a mix of three types of Nvidia graphic cards: GTX 1080 Ti, Tesla K20, K40 and K80. The total sequential running time was approximately 168 days, that is if the computation has been done on a single GPU. But by leveraging the cluster of 60 GPUs, we managed to obtain the results in less than one week. We implemented our framework using the open source deep learning library Keras <ref type="bibr" target="#b41">(Chollet, 2015)</ref> with the Tensorflow <ref type="bibr" target="#b1">(Abadi et al., 2016)</ref> backend. For reproducibility purposes, we provide the 7140 trained Keras models (in a HDF5 format) on the companion web page of this project 1 . We have also published the raw results and the full source code of our method to enable the time series community to verify and build upon our findings 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Results</head><p>The experiments described in the previous section yielded interesting yet hard-tounderstand results. In this section, we first present the result of the 85 ? 84 experiments in a form of a matrix (displayed as a heat map in <ref type="figure" target="#fig_30">Figure 2</ref>.3). We then empirically show how choosing the wrong source dataset for a given target dataset could decrease the network's performance. Therefore, we provide a DTW based solution to choose the best source dataset for a given target dataset. Finally, we detail a few interesting case studies where the behavior of the proposed method has a significant impact on the transfered model's accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer learning accuracy variation matrix</head><p>In order to have a fair comparison across the datasets, we illustrate the variation in the transferred model's accuracy based on the percentage of variation compared to  Where on the other hand, the blue color identifies the improvement in accuracy when transferring the model from a certain source dataset and fine-tuning on another target dataset. The white color means that no change in accuracy has been identified when using the transfer learning method for two datasets. The matrix actually has a size of 85 ? 85 (instead of 85 ? 84) for visual clarity with its diagonal left out of the analysis. (Best viewed in color).</p><p>the original accuracy (without transfer learning). For example, consider the original accuracy (equal to 74.6%) when training the neural network from scratch on the target dataset HandOutlines. Then instead of training the model from scratch (with random initializations) we obtain a 86.5% accuracy when initializing the network's weights to be equal to the weights of a pre-trained network on the source dataset MedicalImages. Hence, the percentage of accuracy variation with respect to the original value is equal to 100 ? (86.5 ? 74.6)/74.6 ? +16%. Thus negative values (red in <ref type="figure" target="#fig_30">Figure 2.</ref>3) indicate a decrease in performance when using the transfer learning approach. Whereas, a positive percentage (blue in <ref type="figure" target="#fig_30">Figure 2</ref>.3) indicates an increase in performance when fine-tuning a pre-trained model. When observing the heat map in <ref type="figure" target="#fig_30">Figure 2</ref>.3, one can easily see that fine-tuning a pre-trained model almost never hurts the performance of the CNN. This can be seen by the dominance of the white color in the heat map, which corresponds to almost no variation in accuracy.</p><p>On the other hand, the results which we found interesting are the two extreme cases (red and blue) where the use of transfer learning led to high variations in accuracy. Interestingly for a given target dataset, the choice of source dataset could deteriorate or improve the CNN's performance as we will see in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naive transfer learning</head><p>While observing the heat map in <ref type="figure" target="#fig_30">Figure 2</ref>.3, we can easily see that certain target datasets (columns) exhibit a high variance of accuracy improvements when varying the source datasets. Therefore, to visualize the worst and best case scenarios when fine-tuning a model against training from scratch, we plotted in <ref type="figure" target="#fig_30">Figure 2</ref>.4 a pairwise comparison of three aggregated accuracies {minimum, median, maximum}.</p><p>For each target dataset D t , we took its minimum accuracy among the source datasets and plot it against the model's accuracy when trained from scratch. This corresponds to the red dots in <ref type="figure" target="#fig_30">Figure 2</ref>.4. By taking the minimum, we illustrate how one can always find a bad source dataset for a given target dataset and decrease the model's original accuracy when fine-tuning a pre-trained network.</p><p>On the other hand, the maximum accuracy (blue dots in <ref type="figure" target="#fig_30">Figure 2</ref>.4) shows that there is also always a case where a source dataset increases the accuracy when using the transfer learning approach.</p><p>As for the median (yellow dots in <ref type="figure" target="#fig_30">Figure 2</ref>.4), it shows that on average, pretraining and then fine-tuning a model on a target dataset improves without significantly hurting the model's performance.</p><p>One extreme case, where the choice of the source dataset had a huge impact on the model's accuracy, is the OliveOil dataset. Precisely the accuracy decreased from 93.3% to 16.7% when choosing respectively MALLAT and FaceFour as source datasets.</p><p>This analysis showed us that blindly and naively using the transfer learning approach could drastically decrease the model's performance. Actually, this is largely due to the fact that the initial weights of the network have a significant impact on the training <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. This problem has been identified as negative transfer learning in the literature, where there still exists a need to quantify the amount of relatedness between the source and target datasets and whether an attempt to transfer knowledge from the source to the target domain should be made <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>. Therefore in the following paragraph, we show how our similarity based solution can quantify this relatedness between the source and the target, thus enabling us to predict the best source dataset for a given target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smart transfer learning</head><p>In order to know in advance which source dataset is suited for which target dataset, we propose to leverage the similarity between two datasets. Our method is designed specifically for time series data without any previous domain knowledge about the datasets. Using the method we described in subsection 2.2.2, we managed to compute a nearest neighbor for a target dataset and set this nearest neighbor to be the chosen source dataset for the current target dataset in question. The results showed that this proposed DTW based method will help in achieving what is called positive transfer <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>. As opposed to negative transfer, positive transfer learning means that the learning algorithm's accuracy increases when fine-tuning a pre-trained model compared to a training from scratch approach <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>. <ref type="figure" target="#fig_30">Figure 2</ref>.5 shows a pairwise accuracy plot for two approaches: a random selection process of the source dataset against a "smart" selection of the source dataset using a nearest neighbor algorithm with the distance calculated in algorithm 1. In order to reduce the bias due to the random seed, the accuracy for the random selection approach was averaged over 1000 iterations. This plot shows that on average, choosing the most similar dataset using our method is significantly better than a random selection approach (with p &lt; 10 ?7 for the Wilcoxon signed-rank test). Respectively our method wins, ties and loses on 71, 0 and 14 datasets against randomly choosing the source dataset. We should also note that for the two datasets DiatomSizeReduction and Wine, the nearest neighbor is not always the best choice. Actually, we found that the second nearest neighbor increases drastically the accuracy from 3.3% to 46.7% for DiatomSizeReduction and from 51.9% to 77.8% for Wine (see the 2 nd NN dots in <ref type="figure" target="#fig_30">Figure 2</ref>.5). This means that certain improvements could be incorporated to our inter-datasets similarity calculation such as adding a warping window <ref type="bibr" target="#b50">(Dau et al., 2017)</ref> or changing the number of prototypes for each class which we aim to study in our future work. Therefore, since in <ref type="figure" target="#fig_30">Figure 2</ref>.5 the most similar dataset is the only one that is considered as a potential source for a given target, another interesting study would be to analyze the accuracy on a given target dataset as a function of how dissimilar the source dataset is. However due to the huge number of datasets in the UCR/UEA archive compared to the space limitation, we chose to only study the most interesting cases where the results can be visually interpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interesting case studies</head><p>In this final analysis we chose to work with three interesting target datasets: Shapelet-Sim, HandOutlines and Meat. These datasets were chosen for different reasons such as the small size of the training set, the relatedness to shapelets and the transfer learning's accuracy variation.</p><p>ShapeletSim contains one of the smallest training sets in the UCR/UEA archive (with 20 training instances). Additionally, this dataset is a simulated dataset designed specifically for shapelets which makes it interesting to see how well CNNs can fine-tune (pre-learned) shapelets <ref type="bibr" target="#b47">(Cui, Chen, and Chen, 2016)</ref> when varying the source dataset. <ref type="figure" target="#fig_30">Figure 2</ref>.6 shows how the model's accuracy decreases as we go further from the target dataset. Precisely the average accuracy for the top 3 neighbors reaches 93% compared to the original accuracy of 76%. Actually, we found that the closest dataset to ShapeletSim is the RefrigerationDevices dataset which contains readings from 251 households with the task to identify three classes: Fridge, Refrigerator and Upright Freezer. This is very interesting since using other background knowledge one cannot easily predict that using RefrigerationDevices as a source for ShapeletSim will lead to better accuracy improvement. To understand better this source/target association, we investigated the shapes of the time series of each dataset and found that both datasets exhibit very similar spiky subsequences which is likely the cause for the transfer learning to work between these two datasets. HandOutlines is one of the datasets where fine-tuning a pre-trained model almost never improves the accuracy. Unlike ShapeletSim, this dataset contains enough labeled data for the learning algorithm to learn from (with 1000 time series in the training set). Surprisingly, we found that one could drastically increase the model's performance when choosing the best source dataset. <ref type="figure" target="#fig_30">Figure 2</ref>.7 shows a huge difference (10%) between the model's accuracy when fine-tuned using the most similar source dataset and the accuracy when choosing the most dissimilar source dataset.</p><p>HandOutlines is a classification problem that uses the outlines extracted from hand images. We found that the two most similar datasets (50words and WordsSynonyms) that yielded high accuracy improvements, are also words' outlines extracted from images of George Washington's manuscripts.</p><p>Meat is one of the smallest datasets (with 20 training instances) where the transfer learning approach was almost always beneficial. However, we would like to examine the possibility of improving the accuracy even for the case where the transfer learning seems to be positive <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref> for any choice of source dataset. <ref type="figure" target="#fig_30">Figure 2</ref>.8 shows that the accuracy reaches almost 95% for the top 3 closest datasets and then decrease the less similar the source and target datasets are. While investigating these similarities, we found the top 1 and 3 datasets to be respectively Strawberry and Beef which are all considered spectrograph datasets . As for the second most similar dataset, our method determined it was 50words. Given the huge number of classes (fifty) in 50words our method managed to find some latent similarity between the two datasets which helped in improving the accuracy of the transfer learning process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6">Conclusion</head><p>In this section, we investigated the transfer learning approach on a state of the art deep learning model for TSC problems. Our extensive experiments with every possible combination of source and target datasets in the UCR/UEA archive, were evidence that the choice of the source dataset could have a significant impact on the model's generalization capabilities. Precisely when choosing a bad source dataset for a given target dataset, the optimization algorithm can be stuck in a local optimum. This phenomena has been identified in the transfer learning literature by negative transfer learning which is still an active area of research <ref type="bibr" target="#b245">(Weiss, Khoshgoftaar, and Wang, 2016)</ref>. Thus, when deploying a transfer learning approach, the big data practitioner should give attention to the relationship between the target and the chosen source domains. These observations motivated us to examine the use of the well known time series similarity measure DTW, to predict the choice of the source dataset when finetuning a model on a time series target dataset. After applying this transfer learning guidance, we concluded that transferring deep CNNs on a target dataset works best when fine-tuning a network that was pre-trained on a similar source dataset. These findings are very interesting since no previous observation made the link between the space induced by the classic DTW and the features learned by the Convolutional Neural Networks.</p><p>Our results should motivate the big data practitioners to no longer train models from scratch when classifying time series, but instead to fine-tune pre-trained models. Especially because CNNs, if designed properly, can be adapted across different time series datasets with varying length.</p><p>In our future work, we aim again to reduce the deep neural network's overfitting phenomena by generating synthetic data using a Weighted DTW Barycenter Averaging method <ref type="bibr" target="#b60">(Forestier et al., 2017b)</ref>, since the latter distance gave encouraging results in guiding a complex deep learning tool such as transfer learning. Finally, with big data time series repositories becoming more frequent (Zoumpatianos, Idreos, and Palpanas, 2014), leveraging existing source datasets that are similar to, but not exactly the same as a target dataset of interest, makes a transfer learning method an enticing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ensembling</head><p>TSC tasks differ from traditional classification tasks by the natural temporal ordering of their attributes . To tackle this problem, a huge amount of research was dedicated into coupling and enhancing time series similarity measures with an NN classifier <ref type="bibr" target="#b50">(Dau et al., 2017;</ref><ref type="bibr" target="#b68">Gharghabi et al., 2018)</ref>. In Lines and Bagnall, 2015, ten elastic distances were compared to the traditional DTW algorithm to find out that no single measure could outperform the classic NN-DTW for TSC. These findings motivated the authors to construct a single EE classifier that includes all eleven different similarity measures, and achieve a significant improvement compared to the individual classifiers <ref type="bibr" target="#b136">(Lines and Bagnall, 2015)</ref>. Hence, recent contributions were focused on ensembling different discriminant classifiers such as decision trees <ref type="bibr" target="#b22">(Baydogan, Runger, and Tuv, 2013)</ref> and SVMs <ref type="bibr" target="#b30">(Bostrom and Bagnall, 2015)</ref> on different data representation techniques such shapelet transform <ref type="bibr" target="#b30">(Bostrom and Bagnall, 2015)</ref> or DTW features <ref type="bibr" target="#b110">(Kate, 2016)</ref>. These ideas gave rise to COTE (Bagnall et al., 2016) and its extended version HIVE-COTE <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref> where 37 different classifiers were ensembled over multiple time series data ? 1 ? n ? average prediction transformation techniques in order to reach current state-of-the-art performance for TSC .</p><p>With the advent of deep neural networks into industrial and commercial applications such as self-driving cars <ref type="bibr" target="#b187">(Qiu and Gu, 2018)</ref> and speech recognition systems <ref type="bibr" target="#b140">(Liul et al., 2018)</ref>, time series data mining practitioners started investigating the application of deep learning to TSC problems <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>. In the previous chapter, we showed how deep CNNs are able to achieve results that are not significantly different than current state-of-the-art algorithms for TSC problems when evaluated over the 85 time series datasets from the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>. These results suggest that building upon deep learning based solutions for TSC could further improve the current state-of-the-art performance of deep neural networks.</p><p>One way of improving neural network based classifiers is to build an ensemble of deep learning models. This idea seems very interesting for TSC tasks since the stateof-the-art is moving towards ensembled solutions <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018;</ref><ref type="bibr" target="#b136">Lines and Bagnall, 2015;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017;</ref><ref type="bibr" target="#b22">Baydogan, Runger, and Tuv, 2013)</ref>. In addition, deep neural network ensembles seem to achieve very promising results in many supervised machine learning domains such as skin lesions detection <ref type="bibr" target="#b74">(Goyal and Rajapakse, 2018)</ref>, facial expression recognition <ref type="bibr" target="#b246">(Wen et al., 2017)</ref> and automatic bucket filling <ref type="bibr" target="#b49">(Dadhich, Sandin, and Bodin, 2018)</ref>.</p><p>Therefore, we propose to ensemble the current state-of-the-art deep learning models for TSC developed in the previous chapter, by constructing one model composed of 60 different deep neural networks: 6 different architectures <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017;</ref><ref type="bibr" target="#b266">Zheng et al., 2014;</ref><ref type="bibr" target="#b265">Zhao et al., 2017;</ref><ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018)</ref> each one with 10 different initial weight values. By evaluating on the 85 datasets from the UCR/UEA archive, we demonstrate a significant improvement over the individual classifiers while also reaching very similar performance to HIVE-COTE: the current state-of-the-art ensemble of 37 non deep learning based time series classifiers. Finally, inspired by the our finding on transfer learning in the previous section, we replace ensembling randomly initialized networks with an ensemble constructed out of fine-tuned models from 84 different source datasets, which showed a significant improvement for TSC problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Background</head><p>In this subsection we present some work related to ensembling neural network classifiers, with a focus on time series data.</p><p>Constructing an ensemble of many deep learning classifiers has been shown to achieve high performance in many different fields. In <ref type="bibr" target="#b74">Goyal and Rajapakse, 2018</ref>, an ensemble of two neural networks was adopted: (1) Inception-v4 and (2) Inception-ResNet-v2. Both of these classifiers are learned with a joint meta-learning approach in an end-to-end manner. A forest CNN was proposed by <ref type="bibr" target="#b131">Lee, Kang, and Kang, 2017</ref> for image classification, where similarly to random forest, the ensemble is constructed by replacing the individual nodes with a CNN and finally the classifier's decision is taken by performing a majority voting scheme over the different decisions of the individual trees in the forest. Another ensemble of CNNs for facial expression recognition was proposed in <ref type="bibr" target="#b246">Wen et al., 2017</ref> where each individual classifier was trained independently to output a probability for each class and then the network's final decision was taken using a probability-based fusion method. In Dadhich, Sandin, and Bodin, 2018, an ensemble of neural networks was found to outperform other hybrid machine learning ensembles when solving an automatic bucket filling problem. Finally in Ienco and Pensa, 2018, deep auto-encoders were ensembled in order to learn an unsupervised latent representation of the input data over multiple resolutions, thus improving the quality of the produced clusters.</p><p>Although in almost all use cases ensembling deep neural networks almost always yields to better decisions, we did not find any approach using a neural network ensemble for domain agnostic TSC. Perhaps the work in <ref type="bibr" target="#b104">Jin and Dong, 2016</ref> is the closest to ours where a neural network based ensemble was used to perform biomedical TSC, where individual architectures were constructed with some domain knowledge specific to the classification problem at hand such as choosing the filter length with local and distorted views. Therefore, we decided to further explore ensembling deep neural networks for TSC, by combining multiple deep learning models in different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Methods</head><p>In this subsection, we start by presenting the six different architectures composing our ensembles of neural networks. For completeness, we describe the random initialization technique adopted for all models. Finally, we present a transfer learning based alternative to randomly initializing the weights of the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architectures</head><p>The average rank of the six chosen deep learning classifiers, over the 85 datasets from the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019;</ref><ref type="bibr" target="#b17">Bagnall et al., 2017)</ref> is listed in <ref type="table" target="#tab_14">Table 2</ref>.1. All of these architectures were implemented in a common framework during our empirical study presented in the previous chapter, containing originally 9 different deep learning approaches for TSC. However only 6 out of these 9 approaches were probabilistic classifiers whereas the three other classifiers performed a hard prediction: meaning an input time series is assigned a specific class rather than a probability distribution over all the classes in a dataset. Therefore, we chose to only ensemble</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Rank Wins</head><p>ResNet <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref> 1.88 41 FCN <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref> 2.49 18 Encoder <ref type="bibr" target="#b201">(Serr?, Pascual, and Karatzoglou, 2018)</ref> 3.34 10 MLP <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref> 4.08 4 Time-CNN <ref type="bibr" target="#b265">(Zhao et al., 2017)</ref> 4.38 4 MCDCNN <ref type="bibr" target="#b266">(Zheng et al., 2014)</ref> 4.83 3 the 6 probabilistic models, thus allowing us to combine the networks by averaging the a posteriori probability for each class over the individual classifiers' output. Finally, we present a brief description of these 6 different architectures and refer the interested reader to a more thorough explanation in the corresponding papers. All hyperparameters can be found in Ismail <ref type="bibr" target="#b100">Fawaz et al., 2019d</ref>.</p><p>Multi-Layer Perceptron (MLP) is the simplest form of deep neural networks and was proposed in Wang, Yan, and Oates, 2017 as a baseline architecture for TSC. The architecture contains three hidden layers, with each one fully connected to the output of its previous layer. The main characteristic of this architecture is the use of a Dropout layer <ref type="bibr" target="#b206">(Srivastava et al., 2014)</ref> to reduce overfitting. One disadvantage is that since the input time series is fully connected to the first hidden layer, the temporal information in a time series is lost <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>.</p><p>Fully Convolutional Neural Network (FCN), originally proposed in Wang, Yan, and Oates, 2017, is considered a competitive architecture yielding the second best results when evaluated on the UCR/UEA archive (see <ref type="table" target="#tab_14">Table 2</ref>.1). This network is comprised of three convolutional layers, each one performing a non-linear transformation of the input time series. A global average pooling operation is used before the final softmax classifier, thus reducing drastically the number of parameters in a network and allowing an architecture that is invariant to the length of the input time series. The latter characteristic motivated us to perform a transfer learning technique in Ismail <ref type="bibr">Fawaz et al., 2018d</ref>, and ensembling the resulting neural networks which is later discussed in this subsection.</p><p>Residual Network (ResNet) was originally proposed in Wang, Yan, and Oates, 2017 and showed similar performance to FCN when evaluated on 44 datasets from the archive. However, when evaluated over the 85 datasets, ResNet significantly outperformed FCN (see <ref type="table" target="#tab_14">Table 2</ref>.1). The main characteristic of ResNet is the addition of residual connections which enables a direct flow of the gradient <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>.</p><p>Encoder was originally proposed in <ref type="bibr" target="#b201">Serr?, Pascual, and Karatzoglou, 2018</ref> as a hybrid CNN that modifies the FCN architecture <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref> by mainly adding a Dropout layer <ref type="bibr" target="#b206">(Srivastava et al., 2014)</ref> and an attention mechanism. The latter operation enables Encoder to learn to localize which regions of the input time series are useful for a certain class identification.</p><p>Multi-Channels Deep Convolutional Neural Networks (MCDCNN) was originally proposed in <ref type="bibr" target="#b266">Zheng et al., 2014</ref> for multivariate TSC and adapted to univariate data by <ref type="bibr" target="#b100">Ismail Fawaz et al., 2019d</ref>. It consists of a traditional CNN, where each convolutional layer is followed by a max pooling operation, then a traditional fully connected layer is used before the final softmax classifier.</p><p>Time Convolutional Neural Network (Time-CNN) was originally proposed for univariate as well as multivariate TSC <ref type="bibr" target="#b265">(Zhao et al., 2017)</ref>. Similarly to MCDCNN, this network is a traditional CNN with one major exception: the use of the mean squared error instead of the traditional categorical cross-entropy loss function, which has been used by all the deep learning approaches we have mentioned so far. Therefore for Time-CNN, the sum of the output class probabilities is not guaranteed to be equal to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling models with random initial weights</head><p>We have described in the previous subsection, the architecture of six different classifiers. The weights for each network are initialized randomly using Glorot's uniform initialization method <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. This technique ensures a uniform distribution of the initial weight values. However due to non-convexity, networks with the same architecture but different initial weights could yield different validation accuracy. In <ref type="bibr" target="#b42">Choromanska et al., 2015</ref>, the authors showed that deeper networks are much more stable with respect to the randomness. This would suggest that ensembling relatively non deep architectures would yield to a much better improvement in accuracy than ensembling deeper architectures. Fortunately, for low dimensional time series data, current state-of-the-art architectures are much less deeper than their counterpart networks for high dimensional images. Therefore, we believe that we can leverage this instability of neural networks for time series data by ensembling the decision taken by the same network but with different random initializations, using the following equation:</p><formula xml:id="formula_15">y i,c = 1 n n ? j=1 ? c (x i , ? j ) | ?c ? [1, C] (2.1)</formula><p>with? i,c denoting the ensemble's output probability of having the input time series x i belonging to class c, which is equal to the logistic output ? c averaged over the n randomly initialized models. We should note that training an ensemble of the same architecture with different initial weight values has been shown to improve neural network's performance on many computer vision problems <ref type="bibr" target="#b246">(Wen et al., 2017)</ref>, however, we did not encounter any previous work that combines such classifiers for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer learning</head><p>An alternative to training a deep classifier from scratch is to fine-tune a model that has been already pre-trained on a un/related task <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref>, which was described in the previous section of this current chapter. This process is called transfer learning, where the network is first trained on a source dataset, then the final layer is removed and replaced with a new randomly initialized softmax layer whose number of neurons is equal to the number of classes in the target dataset. The pre-trained model is then fine-tuned or re-trained on the target dataset's training set. With 85 datasets in the archive, each target dataset will have 84 potential source datasets, which motivated us to ensemble the decision of these 84 FCN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Results</head><p>In this section we present the results of different ensembling schemes when evaluated on the 85 datasets from the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, which is currently the largest publicly available benchmark for time series analysis. In order to compare multiple classifiers over several datasets, we followed the same procedure described in the previous chapter. All experiments were conducted on a hybrid cluster of more than 60 NVIDIA GPUs comprised of GTX 1080 Ti, Tesla K20, K40 and K80. Note that the code and the raw results are publicly available on the project's companion repository 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling randomly initialized models</head><p>By ensembling randomly initialized networks, we are able to achieve a significant improvement in accuracy. <ref type="figure" target="#fig_30">Figure 2</ref>.10 shows a critical difference diagram where ten different random initializations of ResNet did not yield to significantly different results. However, by ensembling these different networks, we were able to demonstrate a significant improvement in the average rank over the 85 datasets. We should note that the latter phenomenon was also observed for the five other neural networks described in the previous subsection. Finally, we should emphasize that an ensembling technique will improve the stability of ResNet in terms of accuracy, in other words reducing the bias due to the initial weight values as well as the randomness induced by gradient descent based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling all neural networks</head><p>After demonstrating that using an ensemble of neural networks is always better than a single classifier, we sought to answer the following question: Could an ensemble of hybrid randomly initialized networks achieve even better performance? <ref type="figure" target="#fig_30">Figure 2</ref>.11 shows a critical difference diagram containing six ensembles of homogenized networks as well as the hybrid ensemble of all available networks. The latter classifier contains sixty different networks: each architecture (six in total) is initialized with ten different random weight values. The results show that ensembling all networks was able to outperform all classifiers. However the statistical test failed to find any significant difference between the full ensemble and individual ResNet/FCN ensembles. This would suggest that the ensemble is highly affected by the poor performance of Time-CNN, MLP and MCDCNN. The latter classifiers showed the worst average rank without any significant difference, thus suggesting that removing them would yield even better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Network Ensemble</head><p>The results in the previous section, suggest that choosing carefully the classifiers in the pool would yield to a better ensemble. Therefore, we construct an NNE comprised solely of ResNet, FCN and Encoder. These three architectures were the only ones to yield significantly different results when a homogenized ensemble was adopted <ref type="figure" target="#fig_0">(Figure 2.11)</ref>. Further investigations suggested that FCN performs better than ResNet on electrocardiography datasets <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>, which would motivate researchers to combine these two classifiers in order to have a robust algorithm that improves the accuracy over the whole datasets. However, for small datasets such as CinCECGTorso, both FCN and ResNet overfitted the dataset very easily with almost 82% test accuracy <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018b)</ref>, whereas Encoder managed to achieve very good performance with a 91% accuracy, therefore implying a combination of ResNet, FCN and Encoder would yield to better accuracy on a various range of TSC datasets. <ref type="figure" target="#fig_30">Figure 2</ref>.12 shows how NNE is able to outperform an ensemble of pure ResNets with 45 wins and 18 ties on 85 datasets from the archive. We believe that the combination of an FCN with ResNet and Encoder, enables the classifier to benefit respectively from the residual linear connections and the attention mechanism. To further understand how NNE is performing with respect to current state-ofthe-art TSC algorithms, we illustrate in <ref type="figure" target="#fig_30">Figure 2</ref>.13 a critical difference diagram containing NNE and seven other non deep learning based classiifers: (1) NN-DTW corresponds to the nearest neighbor coupled with the Dynamic Time Warping distance;</p><p>(2) EE is an ensemble of nearest neighbor classifiers with eleven elastic distances; (3) BOSS corresponds to the ensemble Bag-of-SFA-Symbols; (4) ST is another ensemble of off-the-shelf classifiers computed over the Shapelet Transform data domain; (5) PF or Proximity Forest is an ensemble of decision trees coupled with eleven elastic distances; finally (6) COTE and <ref type="formula" target="#formula_19">(7)</ref> HIVE-COTE are two ensembles of respectively 35 and 37 classifiers using multiple data transformation techniques. The results for these classifiers were taken from Bagnall et al., 2017 except for PF whose results were taken from the original paper <ref type="bibr" target="#b143">(Lucas et al., 2018)</ref>. <ref type="figure" target="#fig_30">Figure 2</ref>.13 clearly shows how our NNE is able to reach state-of-the-art performance for TSC, suggesting that CNNs are able to extract one dimensional discriminant features useful for classification in an end-to-end manner, as opposed to other hand-engineered features used by HIVE-COTE such as the Discrete Fourier Transform, DTW features and the Shapelet Transform. <ref type="figure" target="#fig_30">Figure 2</ref>.14 shows that ensembling fine-tuned FCNs is significantly better than ensembling randomly initialized FCN models that are trained from scratch. However, this transfer learning based ensemble did not manage to outperform ResNets' ensemble nor NNE. These results show that the choice of architecture is very crucial and suggest that an ensemble of transferred ResNets would demonstrate even better performance than an ensemble of pure ResNets or NNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling fine-tuned models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Conclusion</head><p>In this section, we showed how ensembling deep neural networks can achieve stateof-the-art performance for time series classification. We showed that it would be almost always beneficial to ensemble randomly initialized models rather than choosing one trained neural network out of the ensemble. Finally, we investigated an ensemble of transferred deep CNNs to demonstrate even better performance than ensembling randomly initialized networks. In the future, we would like to consider a meta-ensembling approach where the output logistics of individual deep learning models are fed to a meta-network that learns to map these inputs to the correct prediction. Ensemble of FCNs is better here Ensemble of fine-tuned FCNs is better here FIGURE 2.14: Ensembling fine-tuned models is significantly better than ensembling randomly initialized FCN models that are trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Data augmentation</head><p>Deep learning usually benefits from large training sets <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. However, for many applications only relatively small training data exist. In TSC, this phenomenon can be observed by analyzing the UCR/UEA archive's datasets <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, where 20 datasets have 50 or fewer training instances. These numbers are relatively small compared to the billions of labeled images in computer vision, where deep learning has seen its most successful applications (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref>.</p><p>Although the recently proposed deep CNNs reached state of the art performance in TSC on the UCR/UEA archive <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>, they still show low generalization capabilities on some small datasets such as the CinCECGTorso dataset with 40 training instances. This is surprising since the NN-DTW performs exceptionally well on this dataset which shows the relative easiness of this classification task. Thus, inter-time series similarities in such small datasets cannot be captured by the CNNs due to the lack of labeled instances, which pushes the network's optimization algorithm to be stuck in local minimums <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. <ref type="figure" target="#fig_30">Figure 2</ref>.15 illustrates on an example that the lack of labeled data can sometimes be compensated by the addition of synthetic data.</p><p>This phenomenon, also known as overfitting in the machine learning community, can be solved using different techniques such as regularization or simply collecting more labeled data <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref> (which in some domains are hard to obtain). Another well-known technique is data augmentation, where synthetic data are generated using a specific method. For example, images containing street numbers on houses can be slightly rotated without changing what number they actually are <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref>. For deep learning models, these methods are usually proposed for image data and do not generalize well to time series <ref type="bibr" target="#b225">(Um et al., 2017)</ref>. This is probably due to the fact that for images, a visual comparison can confirm if the transformation (such as rotation) did not alter the image's class, while for time series data, one cannot easily confirm the effect of such adhoc transformations on the nature of a time series. This is the main reason why data augmentation for TSC have been limited to mainly two relatively simple techniques: slicing and manual warping, which are further discussed in the next subsection.</p><p>In this section, we propose to leverage from a DTW based data augmentation technique specifically developed for time series, in order to boost the performance of a deep ResNet for TSC. Our preliminary experiments reveal that data augmentation can drastically increase the accuracy for CNNs on some datasets while having a small negative impact on other datasets. We finally propose to combine the decision of the two trained models and show how it can reduce significantly the rare negative effect of data augmentation while maintaining its high gain in accuracy on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Related work</head><p>The most used data augmentation method for TSC is the slicing window technique, originally introduced for deep CNNs in <ref type="bibr" target="#b47">Cui, Chen, and Chen, 2016</ref>. The method was originally inspired by the image cropping technique for data augmentation in computer vision tasks <ref type="bibr" target="#b264">(Zhang et al., 2017)</ref>. This data transformation technique can, to a certain degree, guarantee that the cropped image still holds the same information as the original image. On the other hand, for time series data, one cannot make sure that the discriminative information has not been lost when a certain region of the time series is cropped. Nevertheless, this method was used in several TSC problems, such as in <ref type="bibr" target="#b119">Krell, Seeland, and Kim, 2018</ref> where it improved the SVMs accuracy for classifying electroencephalographic time series. In <ref type="bibr" target="#b123">Kvamme et al., 2018,</ref> this slicing window technique was also adopted to improve the CNNs' mortgage delinquency prediction using customers' historical transactional data. In addition to the slicing window technique, jittering, scaling, warping and permutation were proposed in <ref type="bibr" target="#b225">Um et al., 2017</ref> as generic time series data augmentation approaches. The authors in <ref type="bibr" target="#b225">Um et al., 2017</ref> proposed an additional data augmentation method specific to wearable sensor time series data that rotates the trajectory of a person's arm around an axis (e.g. the x axis).</p><p>In <ref type="bibr" target="#b127">Le Guennec, Malinowski, and Tavenard, 2016</ref>, the authors proposed to extend the slicing window technique with a warping window that generates synthetic time series by warping the data through time. This method was used to improve the classification of their deep CNN for TSC, which was also shown to significantly decrease the accuracy of a NN-DTW classifier when compared to our adopted data augmentation algorithm <ref type="bibr" target="#b60">(Forestier et al., 2017b)</ref>. We should note that the use of a window slicing technique means that the model should classify each subsequence alone and then finally classify the whole time series using a majority voting approach. Alternatively, our method does not crop time series into shorter subsequences which enables the network to learn discriminative properties from the whole time series in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>We have chosen to improve the generalization capability of the deep ResNet proposed in <ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017</ref> for two main reasons, whose corresponding architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.7. First, by adopting an already validated architecture, we can attribute any improvement in the network's performance solely to the data augmentation technique. The second reason is that ResNet <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>, to the best of our knowledge, is the deepest neural network validated on large number of TSC tasks (such as the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>), which according to the deep learning literature will benefit the most from the data augmentation techniques as opposed to shallow architectures <ref type="bibr" target="#b26">(Bengio et al., 2011)</ref>. Deep ResNets were first proposed by <ref type="bibr" target="#b78">He et al., 2016</ref> for computer vision tasks. They are mainly composed of convolutions, with one important characteristic: the residual connections which acts like shortcuts that enable the flow of the gradient directly through these connections.</p><p>The input of this network is a univariate time series with a varying length l. The output consists of a probability distribution over the C classes in the dataset. The network's core contains three residual blocks followed by a Global Average Pooling layer and a final softmax classifier with C neurons. Each residual block contains three 1-D convolutions of respectively 8, 5 and 3 filter lengths. Each convolution is followed by a batch normalization <ref type="bibr" target="#b93">(Ioffe and Szegedy, 2015)</ref> and a ReLU as the activation function. The residual connection consists in linking the input of a residual block to the input of its consecutive layer with the simple addition operation. The number of filters in the first residual blocks is set to 64 filters, while the second and third blocks contain 128 filters.</p><p>All network's parameters were initialized using Glorot's Uniform initialization method <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. These parameters were learned using Adam <ref type="bibr" target="#b115">(Kingma and Ba, 2015)</ref> as the optimization algorithm. Following Wang, Yan, and Oates, 2017, without any fine-tuning, the learning rate was set to 0.001 and the exponential decay rates of the first and second moment estimates were set to 0.9 and 0.999 respectively. Finally, the categorical cross-entropy was used as the objective cost function during the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>The data augmentation method we have chosen to test with this deep architecture, was first proposed in <ref type="bibr" target="#b60">Forestier et al., 2017b</ref> to augment the training set for a 1-NN coupled with the DTW distance in a cold start simulation problem. In addition, the 1-NN was shown to sometimes benefit from augmenting the size of the train set even when the whole dataset is available for training. Thus, we hypothesize that this synthetic time series generation method should improve deep neural network's performance, especially that the generated examples in <ref type="bibr" target="#b60">Forestier et al., 2017b</ref> were shown to closely follow the distribution from which the original dataset was sampled. The method is mainly based on a weighted form of DBA technique <ref type="bibr" target="#b183">(Petitjean, Ketterlin, and Gan?arski, 2011;</ref><ref type="bibr">Petitjean et al., 2016;</ref><ref type="bibr" target="#b180">Petitjean et al., 2014)</ref>. The latter algorithm averages a set of time series in a DTW induced space and by leveraging a weighted version of DBA, the method can thus create an infinite number of new time series from a given set of time series by simply varying these weights. Three techniques were proposed to select these weights, from which we chose only one in our approach for the sake of simplicity, although we consider evaluating other techniques in our future work. The weighting method is called Average Selected which consists of selecting a subset of close time series and fill their bounding boxes.</p><p>We start by describing in details how the weights are assigned, which constitutes the main difference between an original version of DBA and the weighted version originally proposed in <ref type="bibr" target="#b60">Forestier et al., 2017b</ref>. Starting with a random initial time series chosen from the training set, we assign it a weight equal to 0.5. The latter randomly selected time series will act as the initialization of DBA. Then, we search for its 5 nearest neighbors using the DTW distance. We then randomly select 2 out these 5 neighbors and assign them a weight value equal to 0.15 each, making thus the total sum of assigned weights till now equal to 0.5 + 2 ? 0.15 = 0.8. Therefore, in order to have a normalized sum of weights (equal to 1), the rest of the time series in the subset will share the rest of the weight 0.2. We should note that the process of generating synthetic time series leveraged only the training set thus eliminating any bias due to having seen the test set's distribution.</p><p>As for computing the average sequence, we adopted the DBA algorithm in our data augmentation framework. Although other time series averaging methods exist in the literature, we chose the weighted version of DBA since it was already proposed as a data augmentation technique to solve the cold start problem when using a nearest neighbor classifier <ref type="bibr" target="#b60">(Forestier et al., 2017b)</ref>. Therefore we emphasize that other weighted averaging methods such as soft-DTW <ref type="bibr" target="#b48">(Cutur and Blondel, 2017)</ref> and TEKA <ref type="bibr" target="#b151">(Marteau, 2019)</ref> could be used instead of DBA in our framework, but we leave such exploration for our future work.</p><p>We did not test the effect of imbalanced classes in the training set and how it could affect the model's generalization capabilities. Note that imbalanced time series classification is a recent active area of research that merits an empirical study of its own <ref type="bibr" target="#b67">(Geng and Luo, 2018)</ref>. At last, we should add that the number of generated time series in our framework was chosen to be equal to double the amount of time series in the most represented class (which is a hyper-parameter of our approach that we aim to further investigate in our future work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Results</head><p>We evaluated the data augmentation method for ResNet on the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, which is the largest publicly available TSC benchmark. The archive is composed of datasets from different real world applications with varying characteristics such the number of classes and the size of the training set. Finally, for training the deep learning models, we leveraged the high computational power of more than 60 GPUs in one huge cluster <ref type="bibr">4</ref> We should also note that the same parameters' initial values were used for all compared approaches, thus eliminating any bias due to the random initialization of the network's weights.</p><p>Our results show that data augmentation can drastically improve the accuracy of a deep learning model while having a small negative impact on some datasets in the worst case scenario. <ref type="figure" target="#fig_30">Figure 2</ref>.16a shows the difference in accuracy between ResNet with and without data augmentation, it shows that the data augmentation technique does not lead a significant decrease in accuracy. Additionally, we observe a huge increase of accuracy for the DiatomSizeReduction dataset (the accuracy increases from 30% to 96% when using data augmentation).</p><p>This result is very interesting for two main reasons. First, DiatomSizeReduction has the smallest training set in the UCR/UEA archive <ref type="bibr">(Dau et al., 2019) (with 16</ref> training instances), which shows the benefit of increasing the number of training instances by generating synthetic time series. Secondly, the DiatomSizeReduction dataset is the one where ResNet yield the worst accuracy without augmentation. On the other hand, the NN coupled with DTW (or ED) gives an accuracy of 97% which shows the relative easiness of this dataset where time series exhibit similarities that can be captured by the simple Euclidean distance, but missed by the deep ResNet due to the lack of training data (which is compensated by our data augmentation technique). The results for the Wine dataset (57 training instances) also show an important improvement when using data augmentation.</p><p>While we did show that deep ResNet can benefit from synthetic time series on some datasets, we did not manage to show any significant improvement over the whole UCR/UEA archive (p-value &gt; 0.41 for the Wilcoxon signed rank test). Therefore, we decided to leverage an ensemble technique where we take into consideration the decisions of two ResNets (trained with and without data augmentation). In fact, we average the a posteriori probability for each class over both classifier outputs, then assign for each time series the label for which the averaged probability is maximum, thus giving a more robust approach to out-of-sample generated time series. The results in <ref type="figure" target="#fig_30">Figure 2</ref>.16b show that the datasets which benefited the most from data augmentation exhibit almost no change to their accuracy improvement. While on the other hand the number of datasets where data augmentation harmed the model's accuracy decreased from 30 to 21. The Wilcoxon signed rank test shows a significant difference (p-value &lt; 0.0005). The ensemble's results are in compliance with the recent consensus in the TSC community, where ensembles tend to improve the individual classifiers' accuracy .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Conclusion</head><p>In this section, we showed how overfitting small time series datasets can be mitigated using a recent data augmentation technique that is based on DTW and a weighted version of the DBA algorithm. These findings are very interesting since no previous observation made a link between the space induced by the classic DTW and the features learned by the CNNs, whereas our experiments showed that by providing enough time series, CNNs are able to learn time invariant features that are useful for classification.</p><p>In our future work, we aim to further test other variant weighting schemes for the DTW-based data augmentation technique, while providing a method that predicts when and for which datasets, data augmentation would be beneficial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Adversarial examples</head><p>As we have previously discussed, TSC problems are encountered in various real world data mining tasks ranging from health care <ref type="bibr" target="#b2">(Abdelfattah, Abdelrahman, and Wang, 2018;</ref><ref type="bibr" target="#b146">Ma, Xiao, and Wang, 2018;</ref><ref type="bibr" target="#b96">Ismail Fawaz et al., 2018c)</ref> and security <ref type="bibr" target="#b215">(Tan, Webb, and Petitjean, 2017;</ref><ref type="bibr" target="#b220">Tobiyama et al., 2016)</ref> to food safety <ref type="bibr" target="#b33">(Briandet, Kemsley, and Wilson, 1996;</ref><ref type="bibr" target="#b162">Nawrocka and Lamorska, 2013)</ref> and power consumption monitoring <ref type="bibr" target="#b172">(Owen and Foreman, 2012;</ref><ref type="bibr" target="#b267">Zheng et al., 2018)</ref>. With deep learning models revolutionizing many machine learning fields such as computer vision <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref> and natural language processing <ref type="bibr" target="#b255">(Yang et al., 2018;</ref><ref type="bibr" target="#b236">Wang, Li, and Xu, 2018)</ref>, we have shown in the previous chapter how researchers recently started to adopt these models for TSC tasks <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>. Following the advent of deep learning, researchers started to study the vulnerability of deep networks to adversarial attacks <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. In the context of image recognition, an adversarial attack consists in modifying an original image so that the changes are almost undetectable by a human <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. The modified image is called an adversarial image, which will be misclassified by the neural network, while the original one is correctly classified. One of the most famous reallife attacks consists in altering a traffic sign image so that it is misinterpreted by an autonomous vehicle <ref type="bibr" target="#b56">(Eykholt et al., 2018)</ref>. Another application is the alteration of illegal content to make it undetectable by automatic moderation algorithms <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. The most common attacks are gradient-based methods, where the attacker modifies the image in the direction of the gradient of the loss function with respect to the input image thus increasing the misclassification rate <ref type="bibr" target="#b73">(Goodfellow, Shlens, and Szegedy, 2015;</ref><ref type="bibr" target="#b122">Kurakin, Goodfellow, and Bengio, 2017;</ref><ref type="bibr" target="#b261">Yuan et al., 2017)</ref>.</p><p>While these approaches have been intensely studied in the context of image recognition (e.g. NeurIPS competition on Adversarial Vision Challenge), adversarial attacks haven not been thoroughly explored for TSC. This is surprising as deep learning models are getting more and more popular to classify time series <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019e;</ref><ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017;</ref><ref type="bibr" target="#b146">Ma, Xiao, and Wang, 2018;</ref><ref type="bibr" target="#b267">Zheng et al., 2018;</ref><ref type="bibr" target="#b96">Ismail Fawaz et al., 2018b;</ref><ref type="bibr">Ismail Fawaz et al., 2018d)</ref>. Furthermore, potential adversarial attacks are present in many applications where the use of time series data is crucial. For example, <ref type="figure" target="#fig_30">Figure 2</ref>.17 shows an original and perturbed time series of coffee beans spectrograph. While a deep neural network correctly classifies the original time series as Robusta beans, adding small perturbations makes it classify it as Arabica. Therefore, since Arabica beans are more valuable than Robusta beans, this attack could be used to deceive food control tests and eventually the consumers.</p><p>In this section, we present, transfer and adapt adversarial attacks that have been shown to work well on images to time series data. We also present an experimental study using the 85 datasets of the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref> which reveals that neural networks are prone to adversarial attacks. We highlight specific real-life use cases to stress the importance of such attacks in real-life settings, namely food quality and safety, vehicle sensors and electricity consumption. Our findings show that deep networks for time series data are vulnerable to adversarial attacks like their computer vision counterparts. Therefore, this work sheds the light on the need to protect against such attacks, especially when deep learning is used for sensitive TSC applications. We also show that adversarial time series learned using one network architecture can be transferred to different architectures. We then discuss some mechanisms to prevent these attacks while making the models more robust to adversarial examples. Finally, in the spirit of regularizing DNNs, we show how these perturbed time series can be leveraged in order to improve the generalization capability of a deep learning model: a technique called Adversarial Training <ref type="bibr" target="#b252">(Xie et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Background</head><p>In this subsection, we start with the necessary definitions for ease of understanding. We then follow by an overview of critical applications based on deep learning approaches for TSC where adversarial attacks could have serious and dangerous consequences. Finally, we present a brief survey of the current state-of-the-art methods for adversarial attacks which have been mainly proposed and validated on image datasets <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>.</p><p>Definition 1. f (?) ? F : R T ?? represents a deep learning model for TSC.</p><p>Definition 2. J f (?, ?) denotes the loss function (e.g. cross-entropy) of the model f .</p><p>Definition 3. X denotes the adversarial example, a perturbed version of X (the original instance) such that? =? and X ? X p ? .</p><p>In this section, we focus on the application of DNNs in crucial and sensitive decision making systems, thus motivating the investigation of neural network's vulnerabilities to adversarial examples. In <ref type="bibr" target="#b146">Ma, Xiao, and Wang, 2018</ref>, CNNs were used to mine temporal electronic health data for risk prediction and disease sub-typing. In situations where algorithms are taking the decision for reimbursement of medical treatment, tampering medical records in an imperceptible manner could eventually lead to fraud. Apart from the health care industry, deep CNNs are also being used when monitoring power consumption from houses or factories. For example in <ref type="bibr" target="#b267">Zheng et al., 2018</ref>, time series data from smart grids were analyzed for electricity theft detection, where in such use cases perturbed data can help thieves to avoid being detected. Other crucial decision making systems such as malware detection in smart-phones, leverage temporal data in order to classify if an Android application is malicious or not <ref type="bibr" target="#b220">(Tobiyama et al., 2016)</ref>. Using adversarial attacks, a hacker might generate synthetic data from his/her application allowing it to bypass the security systems and get it installed on the end user's smart-phone. Finally, when deep neural networks are deployed for road anomaly detection <ref type="bibr" target="#b36">(Cabral et al., 2018)</ref>, perturbing the data recorded by sensors placed on the road could help the entities responsible for such life threatening anomalies, to avoid being captured. We should note that this list of potential attacks is not exhaustive.  Adversarial attacks <ref type="bibr" target="#b212">Szegedy et al., 2014</ref> were the first to introduce adversarial examples against deep neural networks for image recognition tasks in 2014. Following these intriguing findings, a huge amount of research has been dedicated to generating, understanding and preventing adversarial attacks on deep neural networks <ref type="bibr" target="#b73">(Goodfellow, Shlens, and Szegedy, 2015;</ref><ref type="bibr" target="#b122">Kurakin, Goodfellow, and Bengio, 2017;</ref><ref type="bibr" target="#b56">Eykholt et al., 2018)</ref>. Most of these methods have been proposed for image recognition tasks <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. For example, in Goodfellow, Shlens, and Szegedy, 2015, a fast gradient-based attack was developed as an alternative to expensive optimization techniques <ref type="bibr" target="#b212">(Szegedy et al., 2014)</ref>, where the authors explained the presence of such adversarial examples with the hypothesis of linearity for deep learning models. This kind of attack was also extended by a more costly iterative procedure <ref type="bibr" target="#b122">(Kurakin, Goodfellow, and Bengio, 2017)</ref>, where the authors showed for the first time that even printed adversarial images (i.e. perceived by a camera) are able to fool a pretrained network. More recently, it has been shown that perturbing stop signs can trick autonomous vehicles into misclassifying it as a speed limit sign <ref type="bibr" target="#b56">(Eykholt et al., 2018)</ref>.</p><p>Other fields such as Natural Language Processing have also been investigated to create adversarial attacks such as adding distracting phrases at the end of a paragraph in order to show that deep learning-based reading comprehension systems were not able to distinguish subtle differences in text <ref type="bibr" target="#b103">(Jia and Liang, 2017)</ref>. For a review on the different adversarial attacks for deep learning systems, we refer the interested readers to a recent survey in <ref type="bibr" target="#b261">Yuan et al., 2017.</ref> For general TSC tasks, it is surprising how adversarial attack approaches have been ignored by the community. The only previous work mentioning attacks for TSC is <ref type="bibr" target="#b171">Oregi et al., 2018</ref>. By adapting a soft KNN coupled with DTW, the authors showed that adversarial examples could fool the proposed nearest neighbors classifier on a single simulated dataset (synthetic_control from the UCR/UEA archive <ref type="bibr" target="#b51">Dau et al., 2019)</ref>. However, the fact that the KNN classifier is no longer considered as the state-of-the-art classifier for time series data , we believe that it is important to investigate the generation of adversarial time series examples that deteriorate the accuracy of state-of-the-art classifiers such as ResNet <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d;</ref><ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017)</ref> and to validate it on the whole 85 datasets in the UCR/UEA archive. Finally, we formally define an adversarial attack on deep neural networks for TSC.</p><p>Definition 4. Given a trained deep learning model f and an original input time series X, generating an adversarial instance X can be described as a box-constrained optimization problem.</p><formula xml:id="formula_16">min X ? X ? X s.t. f (X ) =? , f (X) =? and? =? (2.2)</formula><p>Let ? = X ? X be the perturbation added to X, which corresponds to a very low amplitude signal. <ref type="figure" target="#fig_30">Figure 2</ref>.18 illustrates this process where the green time series corresponds to the added perturbation ?. The optimization problem in (2.2) minimizes the perturbation while misclassifying the input time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Adversarial attacks for time series</head><p>In this subsection, we present two attack methods that we then use to generate adversarial time series examples for the ResNet model described in Chapter 1. For testing adversarial examples, we used ResNet, which was originally proposed for TSC in Wang, Yan, and Oates, 2017, where it was validated on 44 datasets from the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>. In Chapter 1, we identified that ResNet achieved state-of-the-art performance for TSC, with results that are not significantly different than the HIVE-COTE, the current state-of-the-art classifier, which is an ensemble of 37 classifiers <ref type="bibr" target="#b137">(Lines, Taylor, and Bagnall, 2018)</ref>. Note that our adversarial attack methods are independent of the chosen network architecture, and that we chose ResNet for its robustness <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref> as well as its use in many critical domains such as malware detection <ref type="bibr" target="#b36">(Cabral et al., 2018)</ref>. In addition, adversarial examples are known to be transferable across different neural network architectures which enables the synthetic time series to fool other deep learning models: a technique known as black-box attack <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. Finally, we describe a very recent method called AdvProp <ref type="bibr" target="#b252">(Xie et al., 2020</ref>) that leverages adversarial training in order to improve image recognition systems, which we implement and apply to our TSC problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fast Gradient Sign Method</head><p>FGSM was first proposed in <ref type="bibr" target="#b73">Goodfellow, Shlens, and Szegedy, 2015</ref> to generate adversarial images that fooled the famous GoogLeNet model. FGSM is considered "fast" and replaces the expensive linear search method previously proposed in <ref type="bibr" target="#b212">Szegedy et al., 2014</ref>. The attack is based on a one step gradient update along the direction of the gradient's sign at each time stamp. The perturbation process (illustrated in <ref type="figure" target="#fig_30">Figure 2</ref>.18) can be expressed as:</p><formula xml:id="formula_17">? = ? sign(? x J(X,?)) (2.3)</formula><p>where denotes the magnitude of the perturbation (a hyperparameter). The adversarial time series X can be easily generated with X = X + ?. The gradient can be efficiently computed using back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Iterative Method</head><p>BIM extends FGSM by applying it multiple times with a small step size and clip the obtained time series elements after each step to ensure that they are in anneighborhood of the original time series <ref type="bibr" target="#b122">(Kurakin, Goodfellow, and Bengio, 2017)</ref>. In fact, by adding smaller changes or perturbations in an iterative manner, the method is able to generate adversarial examples that are closer to the original samples and have a better chance of fooling the network. Algorithm 2 shows the different steps of this iterative attack which requires setting three hyperparameters: (1) the number of iterations I;</p><p>(2) the amount of maximum perturbation and (3) the per step small perturbation ?. In our experiments we have set = 0.1 heuristically similarly to <ref type="bibr" target="#b73">Goodfellow, Shlens, and Szegedy, 2015;</ref><ref type="bibr" target="#b122">Kurakin, Goodfellow, and Bengio, 2017</ref> and the rest of BIM hyperparameters were left at their default value in the Cleverhans API .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Iterative Adversarial Attack</head><p>Parameter: I, , ? Input: original time series X &amp; its label? Output: perturbed time series X 1: X ? X 2: for i = 1 to I do 3: ? = ? ? sign(? x J(X ,?)) 4:</p><formula xml:id="formula_18">X = X + ? 5: X = min{X + , max{X ? , X }} 6: end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Training</head><p>Adversarial training consists of generating adversarial examples during the training process of a neural network classifier. These adversarial instances are used as training examples in order to defend against adversarial attacks <ref type="bibr" target="#b73">(Goodfellow, Shlens, and Szegedy, 2015)</ref>. This technique currently constitutes the foundation of state-of-thearts for defending against these attacks <ref type="bibr" target="#b251">(Xie et al., 2019)</ref>. Most previous approaches have witnessed a decrease in accuracy on clean original instances when the model is trained in adversarial setting <ref type="bibr" target="#b222">(Tsipras et al., 2018)</ref>, showing an inevitable trade-off between a model's accuracy and its robustness to adversarial attacks. However, <ref type="bibr" target="#b252">Xie et al., 2020</ref> were the first to show that one can leverage adversarial training to improve the test time accuracy of a DNN. They managed to achieve this by adding a using two different batch normalization layers <ref type="bibr" target="#b93">(Ioffe and Szegedy, 2015)</ref>: one for the clean original intact training instances and a second one for the perturbed examples. <ref type="bibr" target="#b252">Xie et al., 2020</ref> argued that the main reason behind the deterioration in accuracy when using adversarial training was that the perturbed instances come from a distribution that is significantly different than the clean original ones. Thus having different normalization layers should allow the network to adapt its weights for each distribution. In our case, we have implemented AdvProp and tested its effect when training DNNs with adversarial training for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>To train the deep neural network, we leveraged the parallel computation of a cluster of more than 60 GPUs (a mix of GTX 1080 Ti, Tesla K20, K40 and K80). All of our experiments were evaluated on the 85 datasets from the publicly available UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>. The model was trained/tested using the original training/testing splits provided in the archive. To perform the attacks, we have adapted the Cleverhans API  by extending the well known attacks for time series data and perturbed only the test instances without using the test labels, similarly to the computer vision literature <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>.</p><p>For reproducibility and to allow the time series community to verify and build on our findings, the source code for generating adversarial time series is publicly available on our GitHub repository 5 . In addition, we provide on our companion web page 6 the raw results, our pre-trained models as well as a set of perturbed time series for each dataset in the UCR/UEA archive. This would allow time series data mining practitioners to test the robustness of their machine learning models against adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial attacks on the whole UCR/UEA archive</head><p>For all datasets, both attacks managed to reduce ResNet's accuracy. One exception is the DiatomSizeReduction dataset which is the smallest one in the archive with an already low original accuracy equal to 30% due to overfitting <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>. <ref type="figure" target="#fig_30">Figure 2</ref>.19 shows the accuracy variation for both attacks with respect to the network's original accuracy on the UCR/UEA archive. On average, over the 85 datasets, FGSM and BIM managed to reduce the model's accuracy respectively by 43.2% and 56.89%. The Wilcoxon signed-rank test indicates that BIM is significantly better than FGSM in decreasing the model's accuracy, with a p-value ? 10 ?15 . However, we should note that FGSM is a fast approach allowing real-time generation of adversarial time series whereas BIM is time-consuming and requires a certain number of iterations I.</p><p>By analyzing the use-cases where both attacks failed to fool the classifier, we found out that the corresponding datasets have two interesting characteristics that could explain the classifier's robustness to adversarial examples. The first one is that 50% of the simulated datasets (CBF, Two_patterns and synthetic_control) in the archive are in the top six hardest datasets to attack. Perhaps since these are synthetic datasets generated by humans to serve some human intuition for TSC, small perturbations imperceptible by humans, are not enough to alter the classifier's decision. The second observation is that a network trained on datasets with time series of short length is harder to fool. This is rather expected since the less data points we have, the less amount of perturbation the attacker is allowed to add. For example ItalyPowerDemand contains the shortest sequences (T = 24) and is the second most hardest use-case for both attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Dimensional Scaling</head><p>We used Multi-Dimensional Scaling <ref type="bibr" target="#b121">(Kruskal and Wish, 1978;</ref><ref type="bibr" target="#b60">Forestier et al., 2017b)</ref> (explained in Chapter 1) with the objective to gain some insights on the spatial distribution of the perturbed time series compared to the original ones. Using the ED on a set of time series (original and perturbed), it is then possible to create a similarity matrix and apply MDS to display the set into a two dimensional space. The latter straightforward approach supposes that the ED is able to strongly separate the raw time series, which is usually not the case evident by the low accuracy of the nearest neighbor when coupled with the ED . Therefore, we decided to use the linearly separable representation of time series from the output of the GAP layer, which is used as input to the softmax linear classifier (multinomial logistic regression). More precisely, for each input time series, the last convolution outputs a multivariate time series whose dimensions are equal to the number of filters (128) in the last convolution, then the GAP layer averages the latter 128-dimensional multivariate time series over the time dimension resulting in a vector of 128 real values over which the ED is computed. This enables the MDS projection to be as close as possible to ResNet's latent representation of the time series. Obviously, one has to be careful about the interpretation of MDS output, as the data space is highly simplified (each time series X i is represented as a single data point in 2D space). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacks on food quality and safety</head><p>The determination of food quality, type and authenticity along with the detection of adulteration are major issues in the food industry <ref type="bibr" target="#b162">(Nawrocka and Lamorska, 2013)</ref>. With meat related product, authenticity checking concerns for example the identification of substitution of high value raw materials with cheaper materials like less costly cuts, offal, blood, water, eggs or other types of proteins. These substitutions not only decease the consumers but can also cause severe allergic responses as the substitute materials are hidden. Discriminating meat that has been frozenand-thawed from fresh meat is also an important issue, as refreezing food can result in an increased amount of bacteria. Spectroscopic methods have been historically very successful at evaluating the quality of agricultural products, especially food <ref type="bibr" target="#b162">(Nawrocka and Lamorska, 2013)</ref>. This technique is routinely used as a quality assurance tool to determine the composition of food ingredients. In this context, an adversarial attack could be used to modify recorded spectrographs (seen as time series) in order to hide the low qualify of the food. The Beef dataset (Al-Jowder, Kemsley, and <ref type="bibr" target="#b9">Wilson, 2002)</ref> (from the UCR/UEA archive) contains four classes of beef spectrograms, from pure and adulterated beef with varying degrees of potential adulterants (heart, tripe, kidney, and liver). An adversarial attack could thus consist in modifying an adulterated beef to make a network classify it as pure beef. For this dataset, FGSM and BIM reduced the model's accuracy respectively by 56.7% and 66.7%.</p><p>The Ham dataset <ref type="bibr" target="#b169">(Olias et al., 2006)</ref> contains measurements from 19 Spanish and 18 French dry-cured hams, with the goal to distinguish the provenance of the food. An adversarial attack could consist in perturbing the spectrograms to hide the real provenance of the food. <ref type="figure" target="#fig_30">Figure 2</ref>.20 shows the MDS projection of the original and perturbed instances for Ham's test set, where one can see that the adversarial examples are pushed toward the other class.</p><p>The Coffee dataset <ref type="bibr" target="#b33">(Briandet, Kemsley, and Wilson, 1996)</ref>  to distinguish between Robusta and Arabica coffee beans. Arabica beans are valued most highly by the trade, as they are considered to have a finer flavor than Robusta. An adversarial attack could consist in altering the spectrograms to make Robusta beans look like Arabica beans. <ref type="figure" target="#fig_30">Figure 2</ref>.21 shows the MDS representation of the original and perturbed time series from the test set. We can clearly see how the instances are pushed toward the class frontiers after performing the FGSM attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacks on vehicle sensors</head><p>The increase in the number of sensors and other electrical devices has drastically augmented the amount of data produced in the industry. These data are now routinely used to monitor systems or to perform predictive maintenance and prevent failures <ref type="bibr" target="#b210">(Susto et al., 2015)</ref>. The car industry is not an exception with the increasing number of sensors present in modern vehicles, especially for advanced driver assistance systems and autonomous driving. Data are also used to perform diagnostic on vehicles in order to detect engine problems or compliance with environmental regulations. In this context, an adversarial attack could consist in altering sensor readings in order to hide a specific problem or to pass a CO 2 emission test. The famous "dieselgate" (or "emissionsgate") <ref type="bibr" target="#b32">(Brand, 2016)</ref> made this kind of attack a reality as multiple automakers have been suspected of using emission control systems during laboratory emissions testing.</p><p>To illustrate this use case, we used the FordA datasets (from the UCR/UEA archive) that was was originally used in a competition in the IEEE World Congress on Computational Intelligence, 2008. The classification problem is to diagnose whether a certain symptom exists or not in an automotive subsystem. Each case consists of 500 measurements of engine noise and a class label. In this context, an attack could consist in hiding an engine problem. In practice for the FordA dataset,  the model's accuracy decreased by 57.9% and 70.2% when applying respectively the FGSM and BIM attacks. <ref type="figure" target="#fig_30">Figure 2</ref>.22 depicts the variations of ResNet's accuracy on FordA with respect to the amount of perturbation allowed for the FGSM and BIM attacks. As expected <ref type="bibr" target="#b122">(Kurakin, Goodfellow, and Bengio, 2017)</ref>, we found that FGSM fails to generate adversarial examples that can fool the network for larger values of , whereas the BIM produces perturbed time series that can reduce a model's accuracy to almost 0.0%. This can be explained by the fact that BIM adds a small amount of perturbation ? on each iteration whereas FGSM adds amount of noise for each data point in the series that may not be useful for misclassifying the test sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacks on electricity consumption</head><p>Smart meters are electronic devices that record electric power consumption while sending information to the electricity supplier for monitoring, billing and data analysis. These meters typically register energy hourly and report back at least once a day to the supplier by leveraging a two-way communication channel between the device and the supplier's central system. These smart meters have raised a set of concerns in public opinion especially because they send detailed information about how much electricity is being used for each time stamp. Precisely, it has been shown that it is possible to know exactly which type of electric device is or has been used from simply analyzing the electricity consumption data <ref type="bibr" target="#b172">Owen and Foreman, 2012.</ref> In this context, an attack could consist in modifying the electricity consumption time series of one device to make it recognized as another in order to hide which devices are actually used by a specific user.</p><p>To illustrate this use case, we used the SmallKitchenAppliances dataset from the UCR/UEA archive that was recorded as part of government sponsored study called Powering the Nation <ref type="bibr" target="#b172">(Owen and Foreman, 2012)</ref>. By collecting and analyzing behavioral data about consumers' daily use of electricity within their homes, the goal is to reduce the UK's carbon footprint. The dataset contains readings from 251 households recorded over a month. Each univariate time series has a length equal to 720 corresponding to 24 hours of readings taken every two minutes. The three classes are: Kettle, Microwave and Toaster. For this dataset, FGSM and BIM managed to reduce the classifier's accuracy respectively by 38.4% and 57%. The ItalyPowerDemand dataset <ref type="bibr" target="#b111">(Keogh et al., 2006)</ref>, another dataset from the UCR/UEA archive, contains twelve monthly electrical power demand time series from Italy. The task is to differentiate between instances that correspond to winter months (October to March) and summer months (April to September). This dataset contains the shortest time series in the archive (T = 24), thus requiring a higher amount of perturbation in order to be misclassified. <ref type="figure" target="#fig_30">Figure 2</ref>.23 shows the variation of the model's accuracy as well as the shape of a time series from the Italy-PowerDemand dataset with respect to the amount of noise that is added. For this example, both attacks needed higher values of perturbation ( ? 0.3) rather than the default setting ( = 0.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Are adversarial examples transferable?</head><p>To evaluate the transferability of perturbed time series, we used the FCN which was originally proposed in <ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017</ref> and was shown in Chapter 1 to be the second most accurate deep time series classifier when evaluated on the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>. We used the test sets altered with FGSM and BIM using ResNet and try to classify it with FCN (both were originally trained on the same train set). For both FGSM and BIM attacks, FCN's accuracy decreases respectively by 38.2% and 42.8% which shows that adversarial examples are capable of generalizing to a different network architecture. The Wilcoxon signed-rank test also shows that BIM is significantly better than FGSM in reducing FCN's accuracy, with a p-value ? 10 ?7 . This type of attacks is known as "black box" where the attackers do not have access to the target model's internal parameters (FCN) yet they are able to generate perturbed time series that fool the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How can we prevent such attacks?</head><p>Countermeasures for adversarial attacks <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref> follow two defense strategies: (1) reactive: identify the perturbed instance; (2) proactive: improve the network's robustness without generating adversarial examples. One of the most straightforward proactive methods is adversarial training, which consists of (re)training the classifier with adversarial examples. Other reactive techniques consist of detecting the adversarial examples during testing. However, most of these detectors are still prone to attacks that are designed specifically to fool the detectors <ref type="bibr" target="#b261">(Yuan et al., 2017)</ref>. Therefore, we think that the time series community would have much to offer in this area by leveraging the decades of research into nonprobabilistic classifiers such as the nearest neighbor coupled with DTW <ref type="bibr" target="#b216">(Tan et al., 2018)</ref>. Running classifiers against the adversarial examples that we provide here, is a first step toward identifying vulnerable models and making them more robust to such type of attacks. Finally, a recent approach proposed by Abdu-Aguye et al., 2020 defended against adversarial attacks by framing the problem as an outlier detection task. By constructing a normalcy model based on information and chaos-theoretic measures, Abdu-Aguye et al., 2020 were able to determine whether unseen time series instances are normal or adversarial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can we leverage adversarial examples to improve generalization?</head><p>As we have previously mentioned, <ref type="bibr" target="#b252">Xie et al., 2020</ref> were among the first to show that adversarial training can be leveraged to improve the generalization capabilities of a neural network, by adopting two batch normalization layers: one for the original images and another for the perturbed ones. When adopting their approach for TSC, we found similar results: using AdvProp was necessary in order to avoid deteriorating the accuracy of our model. <ref type="figure" target="#fig_30">Figure 2</ref>.24 illustrates the pairwise comparison between applying adversarial training with or without the AdvProp technique. We can clearly see how vanilla adversarial training will deteriorate significantly the accuracy compared to using the AdvProp method. These results are in line with the original AdvProp paper <ref type="bibr" target="#b252">(Xie et al., 2020)</ref>. However, <ref type="figure" target="#fig_30">Figure 2</ref>.25 shows that using adversarial training does not improve significantly the accuracy for TSC, unlike the observations in Xie et al., 2020 on the ImageNet dataset. We hope that these preliminary results will give some insights to time series data mining researchers looking to leverage adversarial training as a regularization technique of DNNs for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Conclusion</head><p>In this section, we introduced the concept of adversarial attacks on deep learning models for time series classification. We defined and adapted two attacks, originally proposed for image recognition, for the TSC task. We showed how adversarial perturbations are able to reduce the accuracy for the state-of-the-art deep learning classifier (ResNet) when evaluated on the UCR/UEA archive benchmark. With deep neural networks becoming frequently adopted by time series data mining practitioners in real-life critical decision making systems, we shed the light on some crucial use cases where adversarial attacks could have serious and dangerous consequences. Finally we presented our initial preliminary results and showed that vanilla adversarial training will deteriorate the accuracy, which is why the AdvProp method was necessary to maintain a high accurate classifier.</p><p>In the future, we would like to investigate countermeasures techniques to defend machine learning models against such attacks while exploring the transferability of adversarial examples to other non deep learning state-of-the-art classifiers. Finally, we would like to further explore the dozens of adversarial attacks that are published each year in order to identify and protect vulnerable deep learning models for TSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Conclusion</head><p>In this chapter, we have presented four main regularization techniques of deep learning models for TSC. First by implementing a transfer learning approach, coupled with an inter-dataset similarity selection algorithm, we were able to significantly improve the accuracy of FCN. We then proposed to leverage the high variance due to the stochastic nature of the optimization process of neural networks, by ensembling the decision of more than one network. Unlike transfer learning, we observed that ensembling will almost always improve the classifier's accuracy. We then presented a data augmentation technique based on the famous DTW algorithm, allowing us to increase the number of training samples, thus ameliorating the model's accuracy. Finally, we focused on the vulnerabilities of neural networks to adversarial attacks and gave many use case examples where such attacks might be catastrophic. Then in the spirit of regularizing DNNs, we showed how we can leverage these adversarial attacks to improve a network's generalization capabilities by leveraging a recent approach called AdvProp.</p><p>We believe that this chapter should motivate researchers into further looking into many types of regularization methods that were mostly applied to images. We should note that this list of techniques is not exhaustive, many other types of regularization still exist and constitute currently a very hot topic in machine learning such as self-supervised pre-training <ref type="bibr" target="#b164">(Newell and Deng, 2020)</ref>. Nevertheless, we believe that the current state-of-the-art neural networks architectures for TSC are suboptimal and still lack behind the current advances of deep learning in computer vision. Therefore, we present in the following chapter a novel architecture for TSC based on the famous Inception module proposed by Google for the ImageNet competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InceptionTime: Finding AlexNet for Time Series Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Recent times have seen an explosion in the magnitude and prevalence of time series data. Industries varying from health care <ref type="bibr" target="#b61">(Forestier et al., 2018;</ref><ref type="bibr" target="#b132">Lee et al., 2018;</ref><ref type="bibr" target="#b99">Ismail Fawaz et al., 2019c)</ref> and social security <ref type="bibr" target="#b259">(Yi et al., 2018)</ref> to human activity recognition <ref type="bibr" target="#b262">(Yuan et al., 2018)</ref> and remote sensing <ref type="bibr" target="#b179">(Pelletier, Webb, and Petitjean, 2019)</ref>, all now produce time series datasets of previously unseen scale -both in terms of time series length and quantity. This growth also means an increased dependence on automatic classification of time series data, and ideally, algorithms with the ability to do this at scale.</p><p>In the previous chapters, we have shown how these TSC problems, differ significantly to traditional supervised learning for structured data, in that the algorithms should be able to handle and harness the temporal information present in the signal. It is easy to draw parallels from this scenario to computer vision problems such as image classification and object localization, where successful algorithms learn from the spatial information contained in an image. Put simply, the time series problem is essentially the same class of problem, just with one less dimension. Yet despite this similarity, the current state-of-the-art algorithms from the two fields share little resemblance <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>.</p><p>Deep learning has a long history (in machine learning terms) in computer vision  but its popularity exploded with AlexNet <ref type="bibr" target="#b120">(Krizhevsky, Sutskever, and Hinton, 2012)</ref>, after which it has been unquestionably the most successful class of algorithms (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref>. Conversely, deep learning has only recently started to gain popularity amongst time series data mining researchers (see Chapter 1). This is emphasized by the fact that ResNet, which is currently considered the state-of-the-art neural network architecture for TSC when evaluated on the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, was originally proposed merely as a baseline model for the underlying task <ref type="bibr" target="#b238">(Wang, Yan, and Oates, 2017)</ref>. Given the similarities in the data, it is easy to suggest that there is much potential improvement for deep learning in TSC. In Chapter 2, we have shown how it is possible to improve the accuracy of a given deep learning architecture using various regularization techniques such as ensembling, transfer learning, data augmentation and adversarial training. However, we believe that there is still room for improvement in terms of network architecture, which can be considered an orthogonal task to the various DNNs regularization methods.</p><p>In this chapter, we take an important step towards finding the equivalent of 'AlexNet' for TSC by presenting InceptionTime -a novel deep learning ensemble for TSC. InceptionTime achieves state-of-the-art accuracy when evaluated on the UCR/UEA archive (currently the largest publicly available repository for TSC <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>) while also possessing ability to scale to a magnitude far beyond that of its strongest competitor.</p><p>InceptionTime is an ensemble of five deep learning models for TSC, each one created by cascading multiple Inception modules . Each individual classifier (model) will have exactly the same architecture but with different randomly initialized weight values. The core idea of an Inception module is to apply multiple filters simultaneously to an input time series. The module includes filters of varying lengths, which as we will show, allows the network to automatically extract relevant features from both long and short time series. In fact, the ensemble here follows the same ensembling idea presented in Chapter 2.</p><p>After presenting InceptionTime and its results, we perform an analysis of the architectural hyperparameters of deep neural networks -depth, filter length, number of filters -and the characteristics of the Inception module -the bottleneck and residual connection, in order to provide insight into why this model is so successful. In fact, we construct networks with filters larger than have ever been explored for computer vision tasks, taking direct advantage of the fact that time series exhibit one less dimension than images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Related work</head><p>In Chapter 1, we have shown that deeper CNN models coupled with residual connections such as ResNet can further improve the classification performance. In essence, since time series data exhibit only one structuring dimension (i.e. time, as opposed to two spatial dimensions for images), it is possible to explore more complex models that are usually computationally infeasible for image recognition problems: for example removing the pooling layers that throw away valuable information in favour of reducing the model's complexity. We therefore propose an Inception based network that applies several convolutions with various filters lengths. In contrast to networks designed for images, we are able to explore filters 10 times longer than recent Inception variants for image recognition tasks <ref type="bibr" target="#b214">(Szegedy et al., 2017)</ref>.</p><p>Inception was first proposed by <ref type="bibr" target="#b213">Szegedy et al., 2015</ref> for end-to-end image classification. Now the network has evolved to become Inceptionv4, where Inception was coupled with residual connections to further improve the performance <ref type="bibr" target="#b214">(Szegedy et al., 2017)</ref>. As for TSC a relatively competitive Inception-based approach was proposed in <ref type="bibr" target="#b106">Karimi-Bidhendi, Munshi, and Munshi, 2018,</ref> where time series where transformed to images using Gramian Angular Difference Field, and finally fed to an Inception model that had been pre-trained for (standard) image recognition. Unlike this feature engineering approach, by adopting an end-to-end learning from raw time series data, a one-dimensional Inception model was used for Supernovae classification using the light flux of a region in space as an input MTS for the network <ref type="bibr" target="#b35">(Brunel et al., 2019)</ref>. However, the authors limited the conception of their Inception architecture to the one proposed by Google for ImageNet <ref type="bibr" target="#b214">(Szegedy et al., 2017)</ref>. In our work, we explore much larger filters than any previously proposed network for TSC in order to reach state-of-the-art performance on the UCR benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">InceptionTime: an accurate and scalable time series classifier</head><p>In this section, we start by describing the proposed architecture we call Inception-Time for classifying time series data. Specifically, we detail the main component of our network: the Inception module. We then present our proposed model Incep-tionTime which consists of an ensemble of 5 different Inception networks initialized randomly. Finally, we adapt the concept of Receptive Field for time series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Inception Network: a novel architecture for TSC</head><p>The composition of an Inception network classifier contains two different residual blocks, as opposed to ResNet, which is comprised of three. For the Inception network, each block is comprised of three Inception modules rather than traditional fully convolutional layers. Each residual block's input is transferred via a shortcut linear connection to be added to the next block's input, thus mitigating the vanishing gradient problem by allowing a direct flow of the gradient <ref type="bibr" target="#b78">(He et al., 2016)</ref>. Following these residual blocks, we employed a GAP layer that averages the output multivariate time series over the whole time dimension. At last, we used a final traditional fully-connected softmax layer with a number of neurons equal to the number of classes in the dataset. <ref type="figure" target="#fig_50">Figure 3</ref>.1 depicts an Inception network's architecture showing 6 different Inception modules stacked one after the other. As for the Inception module, <ref type="figure" target="#fig_50">Figure 3</ref>.2 illustrates the inside details of this operation. Let us consider the input to be an MTS with M dimensions. The first major component of the Inception module is called the "bottleneck" layer. This layer performs an operation of sliding m filters of length 1 with a stride equal to 1. This will transform the time series from an MTS with M dimensions to an MTS with m M dimensions, thus reducing significantly the dimensionality of the time series as well as the model's complexity and mitigating overfitting problems for small datasets. Note that for visualization purposes, <ref type="figure" target="#fig_50">Figure 3</ref>.2 illustrates a bottleneck layer with m = 1. Finally, we should mention that this bottleneck technique allows the Inception network to have much longer filters than ResNet (almost ten times) with roughly the same number of parameters to be learned, since without the bottleneck layer, the filters will have M dimensions compared to m M when using the bottleneck layer. The second major component of the Inception module is sliding multiple filters of different lengths simultaneously on the same input time series. For example in <ref type="figure" target="#fig_50">Figure 3</ref>.2, three different convolutions with length l ? {10, 20, 40} are applied to the input MTS, which is technically the output of the bottleneck layer. Additionally, in order to make our model invariant to small perturbations, we introduce another parallel MaxPooling operation, followed by a bottleneck layer to reduce the dimensionality. The output of sliding a MaxPooling window is computed by taking the maximum value in this given window of time series. Finally, the output of each independent parallel convolution/MaxPooling is concatenated to form the output MTS. The latter operations are repeated for each individual Inception module of the proposed network. By stacking multiple Inception modules and training the weights (filters' values) via backpropagation, the network is able to extract latent hierarchical features of multiple resolutions thanks to the use of filters with various lengths. For completeness, we specify the exact number of filters for our proposed Inception module: 3 sets of filters each with 32 filters of length l ? {10, 20, 40} with MaxPooling added to the mix, thus making the total number of filters per layer equal to 32 ? 4 = 128 = M -the dimensionality of the output MTS. The default bottleneck size value was set to m = 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">InceptionTime: a neural network ensemble for TSC</head><p>Our proposed state-of-the-art InceptionTime model is an ensemble of 5 Inception networks, with each prediction given an even weight. In fact, during our experimentation, we have noticed that a single Inception network exhibits high standard deviation in accuracy, which is very similar to ResNet's behavior <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019e)</ref>. We believe that this variability comes from both the randomly initialized weights and the stochastic optimization process itself. This was an important finding for us, previously observed in <ref type="bibr" target="#b199">Scardapane and Wang, 2017</ref>, as rather than training only one, potentially very good or very poor, instance of the Inception network, we decided to leverage this instability through ensembling, creating InceptionTime. The following equation explains the ensembling of predictions made by a network with different initializations:? with? i,c denoting the ensemble's output probability of having the input time series x i belonging to class c, which is equal to the logistic output ? c averaged over the n randomly initialized models. More details on ensembling neural networks for TSC can be found in Chapter 2. As for our proposed model, we chose the number of individual classifiers to be equal to 5, which is justified in the next section. We should note that we have opted to a neural network ensemble given the small training size of the UCR/UEA archive datasets which are not well suited to deep learning approaches, thus allowing us to control and leverage the variance of the error, which is likely to reduce when increasing the training set's size.</p><formula xml:id="formula_19">i,c = 1 n n ? j=1 ? c (x i , ? j ) | ?c ? [1, C]<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Receptive field</head><p>The concept of Receptive Field is an essential tool to the understanding of deep CNNs <ref type="bibr" target="#b144">(Luo et al., 2016)</ref>. Unlike FC networks or MLPs, a neuron in a CNN depends only on a region of the input signal. This region in the input space is called the receptive field of that particular neuron. For computer vision problems this concept was extensively studied, such as in <ref type="bibr" target="#b139">Liu, Yu, and Han, 2018</ref> where the authors compared the effective and theoretical receptive fields of a CNN for image segmentation. For temporal data, the receptive field can be considered as a theoretical value that measures the maximum field of view of a neural network in a one-dimensional space: the larger it is, the better the network becomes (in theory) in detecting longer patterns. We now provide the definition of the RF for time series data, which is later used in our experiments. Suppose that we are sliding convolutions with a stride equal to 1. The formula to compute the RF for a network of depth d with each layer having a filter length equal to k i with i ? [1, d] is:</p><formula xml:id="formula_20">1 + d ? i=1 (k i ? 1) (3.2)</formula><p>By analyzing equation 3.2 we can clearly see that adding two layers to the initial set of d layers, will increase only slightly the value of RF. In fact in this case, if the old RF value is equal to RF , the new value RF will be equal to RF + 2 ? (k ? 1). Conversely, by increasing the filter length k i , ?i ? [1, d] by 2, the new value RF will be equal to RF + 2 ? d. This is rather expected since by increasing the filter length for all layers, we are actually increasing the RF for each layer in the network. <ref type="figure" target="#fig_50">Figure 3</ref>.3 illustrates the RF for a two layers CNN.</p><p>In this chapter, we chose to focus on the RF concept since it has been known for computer vision problems, that larger RFs are required to capture more context for class-1 class-2 object recognition <ref type="bibr" target="#b144">(Luo et al., 2016)</ref>. Following the same line of thinking, we hypothesize that detecting larger patterns from very long one-dimensional time series data, requires larger receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental setup</head><p>First, we detail the method to generate our synthetic dataset, which is later used in our architecture and hyperparameter study. For testing our different deep learning methods, we created our own synthetic TSC dataset. The goal was to be able to control the length of the time series data as well as the number of classes and their distribution in time. To this end, we start by generating a univariate time series using uniformly distributed noise sampled between 0.0 and 0.1. Then in order to assign this synthetic random time series to a certain class, we inject a pattern with an amplitude equal to 1.0 in a pre-defined region of the time series. This region will be specific to a certain class, therefore by changing the placement of this pattern we can generate an unlimited amount of classes, whereas the random noise will allow us to generate an unlimited amount of time series instances per class. One final note is that we have fixed the length of the pattern to be equal to 10% the length of the synthetic time series. An example of a synthetic binary TSC problem is depicted in <ref type="figure" target="#fig_50">Figure 3</ref>.4. All DNNs were trained by leveraging the parallel computation of a remote cluster of more than 60 GPUs comprised of GTX 1080 Ti, Tesla K20, K40 and K80. Local testing and development was performed on an NVIDIA Quadro P6000. The latter graphics card was also used for computing the training time of a model. When evaluating global accuracy and computational complexity, we have used the UCR/UEA archive <ref type="bibr" target="#b51">(Dau et al., 2019)</ref>, which is the largest publicly available archive for TSC.</p><p>The models were trained/tested using the original training/testing splits provided in the archive. To study the effect of different hyperparameters and architectural designs, we used in addition to the traditional UCR benchmark for TSC, the synthetic dataset whose generation is described in details in the previous paragraph. All time series data were z-normalized (including the synthetic series) to have a mean equal to zero and a standard deviation equal to one. This is considered a common bestpractice before classifying time series data . Finally, we should note that all models are trained using the Adam optimization algorithm <ref type="bibr" target="#b115">(Kingma and Ba, 2015)</ref> and all weights are initialized randomly using Glorot's uniform technique <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. When comparing with the state-of-the-art results published in <ref type="bibr" target="#b17">Bagnall et al., 2017</ref> we used the deep learning model's median test accuracy over the different runs, similarly to what we have done in Chapter 1. Following the recommendations in <ref type="bibr" target="#b52">Dem?ar, 2006</ref> we adopted the Friedman test <ref type="bibr" target="#b63">(Friedman, 1940)</ref> in order to reject the null hypothesis. We then performed the pairwise post-hoc analysis recommended by <ref type="bibr" target="#b25">Benavoli, Corani, and Mangili, 2016</ref> where we replaced the average rank comparison by a Wilcoxon signed-rank test with Holm's alpha (5%) correction <ref type="bibr" target="#b66">(Garcia and Herrera, 2008)</ref>. To visualize this type of comparison we used a critical difference diagram proposed by <ref type="bibr" target="#b52">Dem?ar, 2006,</ref> where a thick horizontal line shows a cluster of classifiers (a clique) that are not-significantly different in terms of accuracy.</p><p>In order to allow for the time series community to build upon and verify our findings, the source code for all these experiments was made publicly available on our companion repository 1 . In addition, we are planning on providing the pre-trained deep learning models, thus allowing data mining practitioners to leverage these networks in a transfer learning setting <ref type="bibr">(Ismail Fawaz et al., 2018d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Experiments: InceptionTime</head><p>In this section, we present the results of our proposed novel classifier called In-ceptionTime, evaluated on the 85 datasets of the UCR/UEA archive. We note that throughout this chapter (unless specified otherwise) InceptionTime refers to an ensemble of 5 Inception networks, while the "InceptionTime(n)" notation is used to denote an ensemble of n Inception networks. <ref type="figure" target="#fig_50">Figure 3</ref>.5 illustrates the critical difference diagram with InceptionTime added to the mix of the current state-of-the-art classifiers for time series data, whose results were taken from <ref type="bibr" target="#b17">Bagnall et al., 2017.</ref> We can see here that our InceptionTime ensemble reaches competitive accuracy with the class-leading algorithm HIVE-COTE, an ensemble of 37 TSC algorithms with a hierarchical voting scheme <ref type="bibr" target="#b135">(Lines, Taylor, and Bagnall, 2016)</ref>. While the two algorithms share the same clique on the critical difference diagram, the trivial GPU parallelization of deep learning models makes learning our InceptionTime model a substantially easier task than training the 37 different classifiers of HIVE-COTE, whose implementation does not trivially leverage the GPUs' computational power.</p><p>To further visualize the difference between the InceptionTime and HIVE-COTE,  InceptionTime, however the difference is not statistically significant as previously discussed. From <ref type="figure" target="#fig_50">Figure 3</ref>.6, we can also easily spot the two datasets for which In-ceptionTime noticeably under-performs (in terms of accuracy) with respect to HIVE-COTE: Wine and Beef. These two datasets contain spectrography data from different types of beef/wine, with the goal being to determine the correct type of meat/wine using the recorded time series data. In Chapter 2, we showed that transfer learning significantly increases the accuracy for these two datasets, especially when finetuning a dataset with similar time series data. Our results suggest that further potential improvements may be available for InceptionTime when applying a transfer learning approach, as recent discoveries in <ref type="bibr" target="#b107">Kashiparekh et al., 2019</ref> show that the various filter lengths of the Inception modules have been shown to benefit more from fine-tuning than networks with a static filter length. Now that we have demonstrated that our proposed technique is able to reach the current state-of-the-art accuracy for TSC problems, we will further investigate the time complexity of our model. Note that during the following experiments, we ran our ensemble on a single Nvidia Quadro P6000 in a sequential manner, meaning that for InceptionTime, 5 different Inception networks were trained one after the other. Therefore we did not make use of our remote cluster of GPUs. First we start by investigating how our algorithm scales with respect to the length of the input time series. <ref type="figure" target="#fig_50">Figure 3</ref>.7 shows the training time versus the length of the input time series. For this experiment, we used the InlineSkate dataset with an exponential re-sampling. We can clearly see that InceptionTime's complexity increases almost linearly with an increase in the time series' length, unlike HIVE-COTE, whose execution is almost two order of magnitudes slower. Having showed that Inception-Time is significantly faster when dealing with long time series, we now proceed to evaluating the training time with respect to a number of time series in a dataset. To this end, we used a Satellite Image Time Series dataset <ref type="bibr" target="#b215">(Tan, Webb, and Petitjean, 2017)</ref>. The data contain approximately one million time series, each of length 46 and labeled as one of 24 possible land-use classes (e.g. <ref type="bibr">'wheat', 'corn', 'plantation', 'urban')</ref>. From <ref type="figure" target="#fig_50">Figure 3</ref>.8 we can easily see how our InceptionTime is an order of magnitude faster than HIVE-COTE, and the trend suggests that this difference will only      continue to grow, rendering InceptionTime a clear favorite classifier in the Big Data era. Note that HIVE-COTE uses heuristics in its implementation, which explains why the complexity appears lower in the experiments than the expected O(T 4 ). To summarize, we believe that InceptionTime should be considered as one of the top state-of-the-art methods for TSC, given that it demonstrates equal accuracy to that of HIVE-COTE (see <ref type="figure" target="#fig_50">Figure 3</ref>.6) while being much faster (see <ref type="figure" target="#fig_50">Figure 3</ref>.7 and 3.8).</p><p>In order to further demonstrate the capability of InceptionTime to handle efficiently a large amount of training samples unlike its counterpart HIVE-COTE, we show in <ref type="figure" target="#fig_50">Figure 3</ref>.9 how the accuracy continues to increase with InceptionTime for larger training set sizes, where HIVE-COTE would take 100 times longer to run.</p><p>The pairwise accuracy plot in <ref type="figure" target="#fig_50">Figure 3</ref>.10 compares InceptionTime to a model we call ResNet(5), which is an ensemble of 5 different ResNet networks <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019e)</ref>. We found that InceptionTime showed a significant improvement over its neural network competitor, the previous best deep learning ensemble for TSC. Specifically, our results show a Win/Tie/Loss of 54/8/23 in favor of InceptionTime against ResNet(5) with a p-value &lt; 0.01, suggesting the significant gain in performance is mainly due to improvements in our proposed Inception network architecture. Additionally, in order to have a fair comparison between ResNet(5) and Incep-tionTime, we fixed the batch size of ResNet to 64 -equal to the default value used for InceptionTime. This would further highlight that the improvement is mainly due to the architectural design of our proposed network, and not due to some other optimization hyperparameter such as the batch size. Finally, we would like to note that when using the original batch size value proposed by <ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017</ref> for ResNet, we observed similar results: InceptionTime was significantly better than the original ResNet(5) with a Win/Tie/Loss of 53/7/25.</p><p>In order to better understand the effect of the randomness on the accuracy of our neural networks, we present in <ref type="figure" target="#fig_50">Figure 3</ref>.11 the critical difference diagram of different InceptionTime(x) ensembles with x ? {1, 2, 5, 10, 20, 30} denoting the number of individual networks in the ensemble. Note that InceptionTime(1) is equivalent to a single Inception network and InceptionTime is equivalent to InceptionTime(5).  By observing <ref type="figure" target="#fig_50">Figure 3</ref>.11 we notice how there is no significant improvement when x ? 5, which is why we chose to use an ensemble of size 5, to minimize the classifiers' training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Architectural Hyperparameter study</head><p>In this section, we will further investigate the hyperparameters of our deep learning architecture and the characteristics of the Inception module in order to provide insight for practitioners looking at optimizing neural networks for TSC. First, we start by investigating the batch size hyperparameter, since this will greatly influence training time of all of our models. Then we investigate the effectiveness of residual and bottleneck connections, both of which are present in InceptionTime. After this we will experiment on model depth, filter length, and number of filters. In all experiments the default values for InceptionTime are: batch size 64; bottleneck size 32; depth 6; filter length {10,20,40}; and, number of filters 32. Finally, since the train/test split (provided in the archive) does not help in estimating the generalization ability of our approach, we have conducted a sensitivity analysis that evaluates the second best value for each of the network's hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Batch size</head><p>We started by investigating the batch size hyperparameter on the UCR/UEA archive, since this will greatly influence training time of our models. The critical  difference diagram in <ref type="figure" target="#fig_50">Figure 3</ref>.12 shows how the batch size affects the performance of InceptionTime. The horizontal thick line between the different models shows a non significant difference between them when evaluated on the 85 datasets, with a small superiority to InceptionTime (batch size equal to 64). Finally, we should note that as we did not observe any significant impact on accuracy we did not study the effect of this hyperparameter on the simulated dataset and we chose to fix the batch size to 64 (similarly to InceptionTime) when experimenting on the simulated dataset below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Bottleneck and residual connections</head><p>In Chapter 1, compared to other deep learning classifiers, ResNet achieved the best classification accuracy when evaluated on the 85 datasets and as a result we chose to look at the specific characteristic of this architecture -its residual connections.</p><p>Additionally, we tested one of the defining characteristics of Inception -the bottleneck feature. For the simulated dataset, we did not observe any significant impact of these two connections, we therefore proceed with experimenting on the 85 datasets from the UCR/UEA archive. <ref type="figure" target="#fig_50">Figure 3</ref>.13 shows the pairwise accuracy plot comparing InceptionTime with/without the bottleneck. Similar to the experiments on the simulated dataset, we did not find any significant variation in accuracy when adding or removing the bottleneck layer.  In fact, using a Wilcoxon Signed-Rank test we found that InceptionTime with the bottleneck layer is only slightly better than removing the bottleneck layer (pvalue &gt; 0.1). In terms of accuracy, these results all suggest not to use a bottleneck layer, however we should note that the major benefit of this layer is to significantly decrease the number of parameters in the network. In this case, InceptionTime with the bottleneck contains almost half the number of parameters to be learned, and given that it does not significantly decrease accuracy, we chose to retain its usage. In a more general sense, these experiments suggest that choosing whether or not to use a bottleneck layer is actually a matter of finding a balance between a model's accuracy and its complexity. The latter observation is evident in <ref type="figure" target="#fig_50">Figure 3</ref>.14 where choosing smaller bottleneck size in order to reduce InceptionTime's runtime will result in small yet insignificant decrease in accuracy.</p><p>To test the residual connections, we simply removed the residual connection from InceptionTime. Thus, without any shortcut connection, InceptionTime will simply become a deep convolutional neural network with stacked Inception modules. <ref type="figure" target="#fig_50">Figure 3</ref>.15 shows how the residual connections have a minimal effect on accuracy when evaluated over the whole 85 datasets in the UCR/UEA archive with a p-value &gt; 0.2.</p><p>This result was unsurprising given that for computer vision tasks residual connections are known to improve the convergence rate of the network but not alter its test accuracy <ref type="bibr" target="#b214">(Szegedy et al., 2017)</ref>. However, for some datasets in the archive, the residual connections did not show any improvement nor deterioration of the network's convergence either. This could be linked to other factors that are specific to these data, such as the complexity of the dataset. One example of interest that we noticed was a significant decrease in Incep-tionTime's accuracy when removing the residual component for the ShapeletSim dataset. This is a synthetic dataset, designed specifically for shapelets discovery algorithms, with shapelets (discriminative subsequences) of different lengths <ref type="bibr" target="#b81">(Hills et al., 2014)</ref>. Further investigations on this dataset indicated that InceptionTime without the residual connections suffered from a severe overfitting.</p><p>While not the case here, some research has observed benefits of skip, dense or residual connections <ref type="bibr" target="#b88">(Huang et al., 2017)</ref>. Given this, and the small amount of labeled data available in TSC compared to computer vision problems, we believe that each case should be independently study whether to include residual connections. The latter observation suggests that a large scale general purpose labeled dataset similar to ImageNet <ref type="bibr" target="#b194">(Russakovsky et al., 2015)</ref> is needed for TSC. Finally, we should note that the residual connection has a minimal impact on the network's complexity <ref type="bibr" target="#b214">(Szegedy et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Depth</head><p>Most of deep learning's success in image recognition tasks has been attributed to how 'deep' the architectures are <ref type="bibr" target="#b129">(LeCun, Bengio, and Hinton, 2015)</ref>. Consequently, we decided to further investigate how the number of layers affects a network's accuracy. Unlike the previous hyperparameters, we present here the results on the simulated dataset. Apart from the depth parameter, we used the default values of InceptionTime. For this dataset we fixed the number of training instances to 256 and the number of classes to 2 (see <ref type="figure" target="#fig_50">Figure 3</ref>.4 for an example). The only dataset parameter we varied was the length of the input time series. <ref type="figure" target="#fig_50">Figure 3</ref>.16 illustrates how the model's accuracy varies with respect to the network's depth when classifying datasets of time series with different lengths. <ref type="table" target="#tab_4">Our  1  2  3  4  5  6  7  8  9  10  11  12   NN-DTW-WW  InceptionTime_1  EE  BOSS  ST  PF  InceptionTime_3  InceptionTime_12  InceptionTime_8  InceptionTime_9</ref> HIVE-COTE InceptionTime initial hypothesis was that as longer time series can potentially contain longer patterns and thus should require longer receptive fields in order for the network to separate the classes in the dataset. In terms of depth, this means that longer input time series will garner better results with deeper networks. And indeed, when observing <ref type="figure" target="#fig_50">Figure 3</ref>.16, one can easily spot this trend: deeper networks deliver better results for longer time series. In order to further see how much effect the depth of a model has on real TSC datasets, we decided to implement deeper and shallower InceptionTime models, by varying the depth between 1 layer and 12 layers. In fact, compared with the original architecture proposed by <ref type="bibr">Wang, Yan, and Oates, 2017, the deeper (shallower)</ref> version of InceptionTime will contain one additional (fewer) residual blocks each one comprised of three inception modules. By adding these layers, the deeper (shallower) InceptionTime model will contain roughly double (half) the number of parameters to be learned. <ref type="figure" target="#fig_50">Figure 3</ref>.17 depicts the critical difference diagram comparing the deeper and shallower InceptionTime models to the original InceptionTime.</p><p>Unlike the experiments on the simulated dataset, we did not manage to improve the network's performance by simply increasing its depth. This may be due to many reasons, however it is likely due to the fact that deeper networks need more data to achieve high generalization capabilities (LeCun, <ref type="bibr" target="#b129">Bengio, and Hinton, 2015)</ref>, and since the UCR/UEA archive does not contain datasets with a huge number of training instances, the deeper version of InceptionTime was overfitting the majority of the datasets and exhibited a small insignificant decrease in performance. On the other hand, the shallower version of InceptionTime suffered from a significant decrease in accuracy (see InceptionTime_3 and InceptionTime_1 in <ref type="figure" target="#fig_50">Figure 3</ref>.17). This suggests that a shallower architecture will contain a significantly smaller RF, thus achieving lower accuracy on the overall UCR/UEA archive.</p><p>From these experiments we can conclude that increasing the RF by adding more layers will not necessarily result in an improvement of the network's performance, particularly for datasets with a small training set. However, one benefit that we have observed from increasing the network's depth, is to choose an RF that is long enough to achieve good results without suffering from overfitting.</p><p>We therefore proceed by experimenting with varying the RF by changing the filter length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">Filter length</head><p>In order to test the effect of the filter length, we start by analyzing how the length of a time series influences the accuracy of the model when tuning this hyperparameter.</p><p>In these experiments we fixed the number of training time series to 256 and the number of classes to 2. <ref type="figure" target="#fig_50">Figure 3</ref>.18 illustrates the results of this experiment.  We can easily see that as the length of the time series increases, a longer filter is required to produce accurate results. This is explained by the fact that longer kernels are able to capture longer patterns, with higher probability, than shorter ones can. Thus, we can safely say that longer kernels almost always improve accuracy.</p><p>In addition to having visualized the accuracy as a function of both depth <ref type="figure" target="#fig_50">(Figure 3</ref>.16) and filter length <ref type="figure" target="#fig_0">(Figure 3.18)</ref>, we proceed by plotting the accuracy as function of the RF for the simulated time series dataset with various lengths. By observing <ref type="figure" target="#fig_50">Figure 3</ref>.19 we can confirm the previous observations that longer patterns require longer RFs, with length clearly having a higher impact on accuracy compared to the network's depth. Moreover, by using a large enough RF to cover the whole input time series, the usage of a GAP layer won't affect InceptionTime's ability to discriminate between the two patterns, because performing a GAP does not affect the model's RF.</p><p>There is a downside to longer filters however, in the potential for overfitting small datasets, as longer filters significantly increase the number of parameters in the network. To answer this question, we again extend our experiments to the real data from the UCR/UEA archive, allowing us to verify whether long kernels tend  <ref type="figure" target="#fig_50">Figure 3</ref>.20 illustrates a critical difference diagram showing how InceptionTime with longer filters will slightly decrease the network's performance in terms of accurately classifying the time series datasets. We also investigate the relationship between the length of the time series and the length of the network's filter.  <ref type="figure" target="#fig_50">Figure 3</ref>.20, we observe that almost for all time series length, InceptionTime with its default filter length (32) achieves the best or the second best overall accuracy. We can therefore summarize that the results from the simulated dataset do generalize (to some extent) to real datasets: longer filters will improve the model's performance as long as there is enough training data to mitigate the overfitting phenomena.</p><p>In summary, we can confidently state that increasing the receptive field of a model by adopting longer filters will help the network in learning longer patterns present in longer time series. However there is an accompanying disclaimer that it may negatively impact the accuracy for some datasets due to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.5">Number of filters</head><p>To provide some directions on how the number of filters affects the performance of the network, we experimented with varying this hyperparameter with respect to the number of classes in the dataset. To generate new classes in the simulated data, we varied the position and length of the patterns; for example, to create data with three classes, we inject patterns of the same length at three different positions. For this series of experiments, we fixed the length of the time series to 256 and the number of training examples to 256.  <ref type="figure" target="#fig_50">Figure 3</ref>.21 depicts the network's accuracy with respect to the number of filters for datasets with a differing number of classes. Our prior intuition was that the more classes, or variability, present in the training set, the more features are required to be extracted in order to discriminate the different classes, and this will necessitate a greater number of filters. This is confirmed by the trend displayed in <ref type="figure" target="#fig_50">Figure 3</ref>.21, where the datasets with more classes require more filters to be learned in order to be able to accurately classify the input time series.</p><p>After observing on the synthetic dataset that the number of filters significantly affects the performance of the network, we asked ourselves if the current implementation of InceptionTime could benefit/lose from a naive increase/decrease in the number of filters per layer. Our proposed InceptionTime model contains 32 filters per Inception module's component, while for these experiments we tested six ensembles, by varying the hyperparameter with a power of two. <ref type="figure" target="#fig_50">Figure 3</ref>.22 illustrates a critical difference diagram showing how increasing the number of filters per layer significantly deteriorated the accuracy of the network, whereas decreasing the number of filters did not significantly affect the accuracy. It appears that our InceptionTime model contains enough filters to separate the classes of the 85 UCR datasets, of which some have up to 60 classes (ShapesAll dataset).</p><p>Increasing the number of filters also has another side effect: it causes an explosion in the number of parameters in the network. The wider InceptionTime contains four times the number of parameters than the original implementation. We therefore conclude that naively increasing the number of filters is actually detrimental, as it will drastically increase the network's complexity and eventually cause overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.6">Sensitivity analysis</head><p>Working with open benchmarks such as the UCR/UEA archive has pushed the community towards publishing high quality TSC algorithms. The UCR/UEA archive provides a train/test split for the data, which has allowed researchers to directly benchmark their works with the ones of others, as well as providing splits that were potentially more challenging and realistic than assuming that both train and test data were sampled from the same population. Having the train/test split available has however also led to the potential issue that the techniques designed on this benchmark archive might overfit it. This is especially true of deep learning classifiers that contain dozens of optimization and architectural hyperparameters.</p><p>In an effort to give an idea of the sensitivity of InceptionTime to changes in its parameters, we have evaluated the performance of having chosen the second-best value of each of its parameters, that is the second-best value for the depth of the network (i.e. a value of 9 instead of the best value of 6), for its width (i.e. 16 instead of 32), for the length of the convolutions (final value of 32 instead of 40), for the batch size (i.e. 32 instead of 64), and for the bottleneck size (i.e. 64 instead of the default one 32). This gave us a new architecture -InceptionTime (second best) -which we then compared with InceptionTime and also other algorithms. <ref type="figure" target="#fig_50">Figure 3</ref>.23 depicts the average rank of current state-of-the-art TSC algorithms with both InceptionTime's default and second best hyperparameters added to the mix. We can clearly see that the effect is minimal: the ranking is a tiny bit lower but they are all well within the critical difference with HIVE-COTE (a post-hoc statistical test fails to reject the null hypothesis (p-value ? 0.71) making the difference between the default and second best hyperparameters non significant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Conclusion</head><p>Deep learning for time series classification still lags behind neural networks for image recognition in terms of experimental studies and architectural designs. In this chapter, we fill this gap by introducing InceptionTime, inspired by the recent success of Inception-based networks for various computer vision tasks. We ensemble these networks to produce new state-of-the-art results for TSC on the 85 datasets of the UCR/UEA archive. Our approach is highly scalable, two orders of magnitude faster than current state-of-the-art models such as HIVE-COTE. The magnitude of this speed up is consistent across both Big Data TSC repositories as well as longer time series with high sampling rate. We further investigate the effects on overall accuracy of various hyperparameters of the CNN architecture. For these, we go far beyond the standard practices for image data, and design networks with long filters. We look at these by using a simulated dataset and frame our investigation in terms of the definition of the receptive field for a CNN for TSC. In the future, we would like to explore how to design DNNs for multivariate TSC while investigating more recent architectural advancements that are being published each year for computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time series analysis for surgical training 4.1 Introduction</head><p>Over the past one hundred years, the classic teaching methodology of "see one, do one, teach one" has governed the surgical education systems worldwide. With the advent of Operation Room 2.0, recording video, kinematic and many other types of data during the surgery became an easy task, thus allowing artificial intelligence systems to be deployed and used in surgical and medical practice. Recently, motion sensor data (e.g. kinematics) as well as surgical videos has been shown to provide a structure for peer coaching enabling novice trainees to learn from experienced surgeons by replaying those videos and/or kinematic trajectories. In this chapter, we tackle two problems present in the current surgical training curriculum:</p><p>1. Manual feedback from senior surgeons observing less experienced trainees is a laborious task that is very expensive, time-consuming and prone to subjectivity.</p><p>2. The high inter-operator variability in surgical gesture duration and execution renders learning from comparing novice to expert surgical videos a very difficult task.</p><p>For the first problem, we designed a CNN (inspired by the FCN architecture explained in Chapter 1) to predict surgical skills by extracting latent patterns in the trainees' motions performed during robotic surgery. As for the second problem, we propose to align multiple videos based on the alignment of their corresponding kinematic multivariate time series data, by leveraging the multiple alignment procedure based on DBA (explained in Chapter 2). This chapter is divided into two main sections, each one tackling the aforementioned problem, before concluding with our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep learning for surgical skills evaluation</head><p>Over the last century, the standard training exercise of Dr. William Halsted has dominated surgical education in various regions of the world <ref type="bibr" target="#b185">(Polavarapu et al., 2013)</ref>. His training methodology of "see one, do one, teach one" is still one of the most adopted approaches to date <ref type="bibr" target="#b7">(Ahmidi et al., 2017)</ref>. The main idea is that the student could become an experienced surgeon by observing and participating in mentored surgeries <ref type="bibr" target="#b185">(Polavarapu et al., 2013)</ref>. These training techniques, although widely used, lack of an objective surgical skill evaluation method <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>. Standard assessment of surgical skills is presently based on checklists that are filled by an expert watching the surgical task <ref type="bibr" target="#b7">(Ahmidi et al., 2017)</ref>. In an attempt to predict a trainee's skill level without using on an expert surgeon's judgement, OSATS was proposed and is currently adopted for clinical practice <ref type="bibr" target="#b166">(Niitsu et al., 2013)</ref>. Alas, this type of observational rating still suffers from several external and subjective factors such as the inter-rater reliability, the development process and the bias of respectively the checklist and the evaluator <ref type="bibr" target="#b76">(Hatala et al., 2015)</ref>. Further studies demonstrated that a vivid relationship occurs between a surgeon's technical skill and the postoperative outcomes <ref type="bibr" target="#b34">(Bridgewater et al., 2003)</ref>. The latter approach suffers from the fact that the aftermath of a surgery hinges on the physiological attributes of the patient <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>. Furthermore, obtaining this type of data is very strenuous, which renders these skill evaluation techniques difficult to carry out for surgical education. Recent progress in surgical robotics such as the da Vinci surgical system (Intuitive Surgical Sunnyvale, 2018) enabled the recording of video and kinematic data from various surgical tasks. Ergo, a substitute for checklists and outcome-based approaches is to generate, from these kinematics, GMFs such as the surgical task's speed, time completion, motion smoothness, curvature and other holistic characteristics <ref type="bibr" target="#b269">(Zia and Essa, 2018;</ref><ref type="bibr" target="#b108">Kassahun et al., 2016)</ref>. While most of these techniques are efficacious, it is not perspicuous how they could be leveraged to support the trainee with a detailed and constructive feedback, in order to go beyond a naive classification into a skill level <ref type="bibr">(i.e., expert, intermediate, etc.)</ref>. This is problematic as feedback on medical practice enables surgeons to reach higher skill levels while improving their performance <ref type="bibr" target="#b94">Islam et al., 2016.</ref> Lately, a field entitled Surgical Data Science (Maier-Hein et al., 2017) has emerged by dint of the increasing access to a huge amount of complex data which pertain to the staff, the patient and sensors for capturing the procedure and patient related data such as kinematic variables and images <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>. Instead of extracting GMFs, recent inquiries have a tendency to break down surgical tasks into finer segments called "gestures", manually before training the model, and finally estimate the trainees' performance based on their assessment during these individual gestures <ref type="bibr" target="#b218">(Tao et al., 2012)</ref>. Even though these methods achieved promising and accurate results in terms of evaluating surgical skills, they necessitate labeling a huge amount of gestures before training the estimator <ref type="bibr" target="#b218">(Tao et al., 2012)</ref>. We pointed out two major limits in the actual existing techniques that estimate surgeons' skill level from their corresponding kinematic variables: firstly, the absence of an interpretable result of the skill prediction that can be used by the trainees to reach higher surgical skill levels; secondly, the requirement of gesture boundaries that are pre-defined by annotators which is prone to inter-annotator reliability and time-consuming <ref type="bibr" target="#b227">(Vedula et al., 2016)</ref>.</p><p>In this section, we design a novel architecture based on FCNs, dedicated to evaluating surgical skills. By employing one-dimensional kernels over the kinematic time series, we avoid the need to extract unreliable and sensitive gesture boundaries. The original hierarchical structure of our model allows us to capture global information specific to the surgical skill level, as well as to represent the gestures in latent lowlevel features. Furthermore, to provide an interpretable feedback, instead of using a dense layer like most traditional deep learning architectures , we place a GAP layer which allows us to take advantage from the class activation map, proposed originally by <ref type="bibr" target="#b268">Zhou et al., 2016</ref>, to localize which fraction of the trial impacted the model's decision when evaluating the skill level of a surgeon. Using a standard experimental setup on the largest public dataset for robotic surgical data analysis: the JHU-ISI Gesture and Skill Assessment Working Set <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>, we show the precision of our FCN model. Our main contribution is to demonstrate that deep learning can be leveraged to understand the complex and latent structures when classifying surgical skills and predicting the OSATS score of a surgery, especially since there is still much to be learned on what does exactly constitute a surgical skill <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Background</head><p>Here we turn our attention to the recent advances leveraging the kinematic data for surgical skills evaluation. The problem we are interested in requires an input that consists of a set of time series recorded by the da Vinci's motion sensors representing the input surgery and the targeted task is to attribute a skill level to the surgeon performing a trial. One of the earliest work focused on extracting GMFs from kinematic variables and training off-the-shelf classifiers to output the corresponding surgical skill level <ref type="bibr" target="#b108">(Kassahun et al., 2016)</ref>. Although these methods yielded impressive results, their accuracy depends highly on the quality of the extracted features. As an alternative to GMF-based techniques, recent studies tend to break down surgical tasks into smaller segments called surgical gestures, manually before the training phase, and assess the skill level of the surgeons based on their fine-grained performance during the surgical gestures, for example, using sparse hidden Markov model <ref type="bibr" target="#b218">(Tao et al., 2012)</ref>. Although the latter technique yields high accuracy, it requires manual segmentation of the surgical trial into fine-grained gestures, which is considered expensive and time-consuming. Hence, recent surgical skills evaluation techniques have focused on algorithms that do not require this type of annotation and are mainly data driven <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018c;</ref><ref type="bibr" target="#b269">Zia and Essa, 2018;</ref><ref type="bibr" target="#b242">Wang and Majewicz Fey, 2018;</ref><ref type="bibr" target="#b59">Forestier et al., 2017a)</ref>. For surgical skill evaluation, we distinguish two tasks. The first one is to output the discrete skill level of a surgeon such as novice (N), intermediate (I) or expert (E). For example, Zia and Essa, 2018 adopted the approximate entropy algorithm to extract features from each trial which are later fed to a nearest neighbor classifier. More recently, Wang and Majewicz Fey, 2018 proposed a CNN-based approach to classify sliding windows of time series; therefore, instead of outputting the class for the whole surgery, the network is trained to output the class in an online setting for each window. In <ref type="bibr" target="#b61">Forestier et al., 2018</ref>, the authors emphasized the lack of explainability for these latter approaches, by highlighting the fact that interpretable feedback to the trainees is important for a novice to become an expert surgeon <ref type="bibr" target="#b94">(Islam et al., 2016)</ref>. Therefore, the authors proposed an approach that uses a sliding window technique with a discretization method that transforms the time series into a bag of words and trains a nearest neighbor classifier coupled with the cosine similarity. Then, using the weight of each word, the algorithm is able to provide a degree of contribution for each sliding window and therefore give some sort of useful feedback to the trainees that explains the decision taken by the classifier. Although the latter technique showed interesting results, the authors did sacrifice the accuracy in favor of interpretability. On the other hand, using our fully convolutional neural network, we provide the trainee with an interpretable yet very accurate model by leveraging the class activation map algorithm, originally proposed for computer vision tasks by <ref type="bibr" target="#b268">Zhou et al., 2016</ref>. The second type of problem in surgical skill evaluation is to train a model that predicts the OSATS score for a certain surgical trial. For example, Zia and Essa, 2018 extended their ApEn model to predict the OSATS score, also known as global rating score. Interestingly, the latter extension to a regression model instead of a classification one enabled the authors to propose a technique that provides interpretability of the model's decision, whereas our neural network provides an explanation for both classification and regression tasks.</p><p>We present briefly the dataset used in this project as we rely on the features' definitions to describe our method. The JIGSAWS dataset, first published by <ref type="bibr" target="#b65">Gao et al., 2014</ref>, has been collected from eight right-handed subjects with three different surgical skill levels: novice (N), intermediate (I) and expert (E), with each group having reported, respectively, less than 10 h, between 10 and 100 h and more than 100 h of training on the Da Vinci. Each subject performed five trials of each one of the three surgical tasks: suturing, needle passing and knot tying. For each trial, the video and kinematic variables were registered. In this project, we focused solely on the kinematics which are numeric variables of four manipulators: right and left masters (controlled by the subject's hands) and right and left slaves (controlled indirectly by the subject via the master manipulators). These 76 kinematic variables are recorded at a frequency of 30 Hz for each surgical trial. Finally, we should mention that in addition to the three self-proclaimed skill levels (N,I,E), JIGSAWS also contains the modified OSATS score <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>, which corresponds to an expert surgeon observing the surgical trial and annotating the performance of the trainee. The main goal of this work is to evaluate surgical skills by considering either the self-proclaimed discrete skill level (classification) or the OSATS score (regression) as our target variable. We conceive each trial as a multivariate time series and designed a one-dimensional CNN dedicated to learn automatically useful features for surgical skill evaluation in an end-to-end manner <ref type="bibr" target="#b100">(Ismail Fawaz et al., 2019d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Method</head><p>Our approach takes inspiration of the recent success of CNNs for time series classification shown in Chapter 1. <ref type="figure" target="#fig_71">Figure 4</ref>.1 illustrates the fully convolutional neural network architecture, which we have designed specifically for surgical skill evaluation using temporal kinematic data. The network's input is an MTS with a variable length l and 76 channels. For the classification task, the output layer contains a number of neurons equal to three (N,I,E) with the softmax activation function, whereas for the regression task (predicting the OSATS score), the number of neurons in the last layer is equal to six: (1) "Respect for tissue"; (2) "Suture/needle handling"; (3) "Time and motion"; (4) "Flow of operation"; (5) "Overall performance"; (6) "Quality of final product" <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>, with a linear activation function.</p><p>Compared with convolutions for image recognition, where usually the model's input exhibits two spatial dimensions (height and width) and three channels (red, green and blue), the input to our network is a time series with one spatial dimension (surgical task's length l) and 76 channels (denoting the 76 kinematics: x, y, z, . . . ). One of the main challenges we have encountered when designing our architecture was the large number of channels (76) compared to the traditional red, green and blue channels (3) for the image recognition problem. Hence, instead of applying the filters over the whole 76 channels at once, we propose to carry out different convolutions for each group and subgroup of channels. We used domain knowledge when grouping the different channels, in order to decide which channels should be clustered together.</p><p>Firstly, we separate the 76 channels into four distinct groups, such as each group should contain the channels from one of the manipulators: the first, second, third and fourth groups correspond to the four manipulators (ML: master left, MR: master right, SL: slave left and SR: slave right) of the da Vinci surgical system. Thus, each group assembles 19 of the total kinematic variables. Next, each group of 19 channels is divided into five different subgroups each containing variables that we believe should be semantically clustered together. For each cluster, the variables are grouped into the following five sub-clusters:</p><p>? First sub-cluster with three variables for the Cartesian coordinates (x, y, z);</p><p>? Second sub-cluster with three variables for the linear velocity (x , y , z );</p><p>? Third sub-cluster with three variables for the rotational velocity (? , ? , ? );</p><p>? Fourth sub-cluster with nine variables for the rotation matrix R;</p><p>? Fifth sub-cluster with one variable for the gripper angular velocity (?). <ref type="figure" target="#fig_71">Figure 4</ref>.1 illustrates how the convolutions in the first layer are different for each subgroup of kinematic variables. Following the same line of thinking, the convolutions in the second layer are different for each group of variables (SL, SR, ML and MR). However, in the third layer, the same filters are applied for all dimensions (or channels), which corresponds to the traditional CNN.</p><p>To take advantage from the CAM method while reducing the number of parameters (weights) in our network, we employed a global average pooling operation after the last convolutional layer. In other words, the convolution's output (the MTS) will shrink from a length l to 1, while maintaining the same number of dimensions in the third layer. Without any sort of validation, we choose the following default hyperparameters. We used 8 kernels for the first convolution, and then we doubled the number of kernels, thus allowing us to balance the number of parameters for each layer as a function of its depth. We used ReLU as the nonlinear hidden activation function for all convolutional layers with a stride of 1 and a kernel length equal to 3.</p><p>We fixed our objective loss function to be the categorical cross-entropy to learn the network's parameters in an end-to-end manner for the classification task, and the mean squared error when learning a regressor to predict the OSATS score, which can be written as:  <ref type="bibr" target="#b218">(Tao et al., 2012)</ref> 97.4 n/a n/a 96.2 n/a n/a 94.4 n/a n/a ApEn <ref type="bibr" target="#b269">(Zia and Essa, 2018)</ref> 100 n/a 0.59 100 n/a 0.45 99.9 n/a 0.66 <ref type="bibr">Sax-Vsm (Forestier et al., 2017a)</ref> 89.7 86.7 n/a 96.3 95.8 n/a 61.1 53.3 n/a CNN <ref type="bibr" target="#b242">(Wang and Majewicz Fey, 2018)</ref> 93.4 n/a n/a 89.9 n/a n/a 84.9 n/a n/a <ref type="bibr">FCN (proposed)</ref> 100 100 0.60 100 100 0.57 92.1 93.2 0.65 The network's weights were optimized using the Adam optimization algorithm <ref type="bibr" target="#b115">(Kingma and Ba, 2015)</ref>. The default value of the learning rate was fixed to 0.001 as well as the first and second moment estimates were set to 0.9 and 0.999 respectively <ref type="bibr" target="#b41">(Chollet, 2015)</ref>. We initialized the weights using Glorot's uniform initialization <ref type="bibr" target="#b69">(Glorot and Bengio, 2010)</ref>. We randomly shuffled the training set before each epoch, whose maximum number was set to 1000 epochs. We then saved the model at each training iteration by choosing the network's state that minimizes the loss function on a random (non-seen) split from the training data. This process is also referred to as "model checkpoint" by the deep learning community <ref type="bibr" target="#b41">(Chollet, 2015)</ref>, allowing us to choose the best number of epochs based on the validation loss. Finally, to avoid overfitting, we added an l2 regularization parameter whose default value was fixed to 10 ?5 . For each surgical task, we have trained a different network, resulting in three different models. 1 We adopted for both classification and regression tasks a leave-one-super-trial-out scheme <ref type="bibr" target="#b7">(Ahmidi et al., 2017)</ref>. The use of a GAP layer allows us to employ the CAM algorithm, which was originally designed for image classification tasks by <ref type="bibr" target="#b268">Zhou et al., 2016</ref> and later introduced for time series data in <ref type="bibr" target="#b238">Wang, Yan, and Oates, 2017</ref>. Using the CAM, we are able to highlight which fractions of the surgical trial contributed highly to the classification. This method was discussed in details in Chapter 1. Finally, for the regression task, the CAM can be extended in a trivial manner: Instead of computing the contribution to a classification, we are computing the contribution to a certain score prediction (1 out of 6 in total).</p><formula xml:id="formula_21">MSE = 1 n n ? i=1 (Y i ?? i ) 2 .<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results</head><p>The first task consists in assigning a skill level for an input surgical trial out of the three possible levels: novice (N), intermediate (I) and expert (E). In order to compare with current state-of-the-art techniques, we adopted the micro and macro measures defined in <ref type="bibr" target="#b7">Ahmidi et al., 2017</ref>. The micro measure refers simply to the traditional accuracy metric. However, the macro takes into consideration the support of each class in the dataset, which boils down to computing the precision metric. <ref type="table" target="#tab_27">Table 4</ref>.1 reports the macro and micro metrics of five different models for the surgical skill classification of the three tasks: suturing, knot tying and needle passing. For the proposed FCN model, we average the accuracy over 40 runs to reduce the bias induced by the randomness of the optimization algorithm. From these results, it appears that FCN is much more accurate than the other approaches with 100% accuracy for the needle passing and suturing tasks. As for the knot tying task, we report 92.1% and 93.2%, respectively, for the micro and macro configurations. When comparing the other four techniques, for the knot tying surgical task, FCN exhibits relatively lower accuracy, which can be explained by the minor difference between the experts and intermediates for this task: Mean OSATS score is 17.7 and 17.1 for expert and intermediate, respectively.</p><p>An S-HMM was designed to classify surgical skills <ref type="bibr" target="#b218">(Tao et al., 2012)</ref>. Although this approach does leverage the gesture boundaries for training purposes, our method is much more accurate without the need to manually segment each surgical trial into finer gestures. <ref type="bibr" target="#b269">Zia and Essa, 2018</ref> introduced ApEn to generate characteristics from each surgical task, which are later given to a classical nearest neighbor classifier with a cosine similarity metric. Although ApEn and FCN achieved stateof-the-art results with 100% accuracy for the first two surgical tasks, it is still not obvious how ApEn could be used to give feedback for the trainee after finishing his/her training session. Forestier et al., 2017a introduced a sliding window technique with a discretization method to transform the MTS into bag of words. To justify their low accuracy, Forestier et al., 2017a insisted on the need to provide explainable surgical skill evaluation for the trainees. On the other hand, FCN is equally interpretable yet much more accurate; in other words, we do not sacrifice accuracy for interpretability. Finally, Wang and Majewicz Fey, 2018 designed a CNN whose architecture is dependent on the length of the input time series. This technique was clearly outperformed by our model which reached better accuracy by removing the need to pre-process time series into equal length thanks to the use of GAP.</p><p>We extended the application of our FCN model <ref type="bibr" target="#b96">(Ismail Fawaz et al., 2018c)</ref> to the regression task: predicting the OSATS score for a given input time series. Although the community made a huge effort toward standardizing the comparison between different surgical skills evaluation techniques <ref type="bibr" target="#b7">(Ahmidi et al., 2017)</ref>, we did not find any consensus over which evaluation metric should be adopted when comparing different regression models. However, Zia and Essa, 2018 proposed the use of Spearman's correlation coefficient (denoted by ?) to compare their 11 combination of regression models. The latter is a nonparametric measure of rank correlation that evaluates how well the relationship between two distributions can be described by a monotonic function. In fact, the regression task requires predicting six target variables; therefore, we compute ? for each target and finally report the corresponding mean over the six predictions. By adopting the same validation methodology proposed by Zia and Essa, 2018, we are able to compare our proposed FCN model to their best performing method. <ref type="table" target="#tab_27">Table 4</ref>.1 reports also the ? values for the three tasks, showing how FCN reaches higher ? values for two out of three tasks. In other words, the prediction and the ground truth OSATS score are more correlated when using FCN than the ApEn-based solution proposed by Zia and Essa, 2018 for the second task and equally correlated for the other two tasks.</p><p>The CAM technique allows us to visualize which parts of the trial contributes the most to a skill classification. By localizing, for example, discriminative behaviors specific to a skill level, observers can start to understand motion patterns specific to certain class of surgeons. To further improve themselves (the novice surgeons), the model, using the CAM's result, can pinpoint to the trainees their good/bad motor behaviors. This would potentially enable novices to achieve greater performance and eventually become experts.</p><p>By generating a heatmap from the CAM, we can see in <ref type="figure" target="#fig_71">Figure 4</ref>.2 how it is indeed possible to visualize the feedback for the trainee. In fact, we examine a trial of an expert and novice surgeon: The expert's trajectory is illustrated in <ref type="figure" target="#fig_71">Figure 4</ref>.2a while the novice's trajectory is depicted in <ref type="figure" target="#fig_71">Figure 4</ref>.2b. In this example, we can see how the model was able to identify which motion (red subsequence) is the main reason  for identifying a subject as a novice. Concretely, we can easily spot a pattern that is being recognized by the model when outputting the classification of subject H's skill level: The orange and red 3D subsequences correspond to same surgical gesture "pulling suture" and are exhibiting a high influence over the model's decision. This feedback could be used to explain to a young surgeon which movements are classifying him/her as a novice and which ones are classifying another subject as an expert. Thus ultimately, this sort of feedback could guide the novices into becoming experts. After having shown how our classifier can be interpreted to provide feedback to the trainees, we now present the result of applying the same visualization (based on the CAM algorithm) in order to explain the OSATS score prediction. <ref type="figure" target="#fig_71">Figure 4</ref>.3 depicts the trajectory with its associated heatmaps for subject E performing the second trial of the knot-tying task. <ref type="figure" target="#fig_71">Figure 4</ref>.3a and 4.3b illustrates the trajectory's heatmap, respectively, for "suture/needle handling" and "quality of the final product" OS-ATS score predictions. At first glimpse, one can see how a prediction that requires focusing on the whole surgical trial leverages more than one region of the input surgery-this is depicted by the multiple red subsequences in <ref type="figure" target="#fig_71">Figure 4</ref>.3b. However, when outputting a rating for a specific task such as "suture/needle handling"-the model is focusing on less parts of the input trajectory which is shown in <ref type="figure" target="#fig_71">Figure 4</ref>.3a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Conclusion</head><p>In this project, we proposed a deep learning-based method for surgical skills evaluation from kinematic data. We achieved state-of-the-art accuracy by designing a specific FCN, while providing explainability that justifies a certain skill evaluation, thus allowing us to mitigate the CNN's black-box effect. Furthermore, by extending our architecture we were able to provide new state-of-the-art performance for predicting the OSATS score from the input kinematic time-series data. In the following section, we present a technique for automatic alignment of surgical videos from kinematic time series <ref type="bibr" target="#b99">(Ismail Fawaz et al., 2019c)</ref> for surgical skills evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automatic alignment of surgical videos using kinematic time series data</head><p>Educators have always searched for innovative ways of improving apprentices' learning rate. While classical lectures are still most commonly used, multimedia resources are becoming more and more adopted <ref type="bibr" target="#b204">(Smith and Ransbottom, 2000)</ref> especially in Massive Open Online Courses <ref type="bibr" target="#b156">(Means et al., 2009)</ref>. In this context, videos have been considered as especially interesting as they can combine images, text, graphics, audio and animation. The medical field is no exception, and the use of video-based resources is intensively adopted in medical curriculum <ref type="bibr" target="#b153">(Masic, 2008)</ref> especially in the context of surgical training <ref type="bibr" target="#b116">(Kneebone et al., 2002)</ref>. The advent of robotic surgery also simulates this trend as surgical robots, like the Da Vinci (Intuitive Surgical Sunnyvale, 2018), generally record video feeds during the intervention. Consequently, a large amount of video data has been recorded in the last ten years <ref type="bibr" target="#b191">(Rapp et al., 2016)</ref>. This new source of data represent an unprecedented opportunity for young surgeons to improve their knowledge and skills <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>. Furthermore, video can also be a tool for senior surgeons during teaching periods to assess the skills of the trainees. In fact, a recent study by <ref type="bibr" target="#b161">Mota et al., 2018</ref> showed that residents spend more time viewing videos than specialists, highlighting the need for young surgeons to fully benefit from the procedure. In Herrera-Almario et al., 2016, the authors showed that knot-tying scores and times for task completion improved significantly for the subjects that watched the videos of their own performance. However, when the trainees are willing to asses their progress over several trials of the same surgical task by re-watching their recorded surgical videos simultaneously, the problem of videos being out-of-synch makes the comparison between different trials very difficult if not impossible. This problem is encountered in many real life case studies, since experts on average complete the surgical tasks in less time than novice surgeons <ref type="bibr" target="#b155">(McNatt and Smith, 2001)</ref>. Thus, when trainees do enhance their skills, providing them with a feedback that pinpoints the reason behind the surgical skill improvement becomes problematic since the recorded videos exhibit different duration and are not perfectly aligned.</p><p>Although synchronizing videos has been the center of interest for several computer vision research venues, contributions are generally focused on a special case where multiple simultaneously recorded videos (with different characteristics such as viewing angles and zoom factors) are being processed <ref type="bibr" target="#b248">(Wolf and Zomet, 2002;</ref><ref type="bibr" target="#b244">Wedge, Kovesi, and Huynh, 2005;</ref><ref type="bibr" target="#b173">Padua et al., 2010)</ref>. Another type of multiple video synchronization uses hand-engineered features (such as points of interest trajectories) from the videos <ref type="bibr" target="#b233">(Wang et al., 2014;</ref><ref type="bibr" target="#b55">Evangelidis and Bauckhage, 2011)</ref>, making the approach highly sensitive to the quality of the extracted features. This type of techniques was highly effective since the raw videos were the only source of information available, whereas in our case, the use of robotic surgical systems enables capturing an additional type of data: the kinematic variables such as the x, y, z Cartesian coordinates of the Da Vinci's end effectors <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>.</p><p>In this section, we propose to leverage the sequential aspect of the recorded kinematic data from the Da Vinci surgical system, in order to synchronize their corresponding video frames by aligning the time series data (see <ref type="figure" target="#fig_71">Figure 4</ref>.4 for an example). When aligning two time series, the off-the-shelf algorithm is DTW <ref type="bibr" target="#b196">(Sakoe and Chiba, 1978)</ref> which we indeed used to align two videos. However, when aligning multiple sequences, the latter technique does not generalize in a straightforward and computationally feasible manner <ref type="bibr" target="#b180">(Petitjean et al., 2014)</ref>. Hence, for multiple video synchronization, we propose to align their corresponding time series to the average time series, computed using the DBA algorithm (explained in details in Chapter 2). This process is called Non-Linear Temporal Scaling and has been originally proposed to find the multiple alignment of a set of discretized surgical gestures <ref type="bibr" target="#b58">(Forestier et al., 2014)</ref>, which we extend in this work to continuous numerical kinematic data.  using the NLTS algorithm. Examples of the synchronized videos and the associated code can be found on our GitHub repository 2 , where we used the JHU-ISI Gesture and Skill Assessment Working Set <ref type="bibr" target="#b65">(Gao et al., 2014)</ref> to validate our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Methods</head><p>Here, we detail each step of our video synchronization approach. We start by describing the DTW algorithm which allows us to align two videos. Then, we describe how NLTS enables us to perform multiple video synchronization with respect to the reference average time series computed using the DBA algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Time Warping</head><p>Dynamic Time Warping was first proposed for speech recognition when aligning two audio signals <ref type="bibr" target="#b196">(Sakoe and Chiba, 1978)</ref>. Suppose we want to compute the dissimilarity between two time series, for example two different trials of the same surgical task, A = (a 1 , a 2 , . . . , a m ) and B = (b 1 , b 2 , . . . , b n ). The length of A and B are denoted respectively by m and n, which in our case correspond to the surgical trial's duration. Here, a i is a vector that contains six real values, therefore A and B can be seen as two distinct MTS.</p><p>To compute the DTW dissimilarity between two MTS, several approaches were proposed by the time series data mining community <ref type="bibr" target="#b202">(Shokoohi-Yekta et al., 2017)</ref>, however in order to apply the subsequent algorithm NLTS, we adopted the "dependent" variant of DTW where the Euclidean distance is used to compute the difference between two instants i and j. <ref type="figure">Let M(A, B)</ref> be the m ? n point-wise dissimilarity matrix between A and B, where M i,j = ||a i ? b j || 2 . A warping path P = <ref type="figure" target="#fig_0">((c 1 , d 1 ), (c 2 , d 2 )</ref>, . . . , (c s , d s )) is a series of points that define a crossing of M. The warping path must satisfy three conditions: (1) (c 1 , d 1 ) = (1, 1); (2) (c s , d s ) = (m, n);</p><p>(3) 0 ? c i+1 ? c i ? 1 and 0 ? d j+1 ? d j ? 1 for all i &lt; m and j &lt; n. The DTW measure between two series corresponds to the path through M that minimizes the total distance. In fact, the distance for any path P is equal to D P (A, B) = ? s i=1 P i . Hence if P is the space of all possible paths, the optimal one -whose cost is equal to DTW(A, B) -is denoted by P * and can be computed using: min P?P D P <ref type="figure">(A, B)</ref>.</p><p>The optimal warping path can be obtained efficiently by applying a dynamic programming technique to fill the cost matrix M. Once we find this optimal warping path between A and B, we can deduce how each time series element in A is linked to the elements in B. We propose to exploit this link in order to identify which time stamp should be duplicated in order to align both time series, and by duplicating a time stamp, we are also duplicating its corresponding video frame. Concretely, if elements a i , a i+1 and a i+2 are aligned with the element b j when computing P * , then by duplicating twice the video frame in B for the time stamp j, we are dilating the video of B to have a length that is equal to A's. Thus, re-aligning the video frames based on the aligned Cartesian coordinates: if subject S 1 completed "inserting the needle" gesture in 5 seconds, whereas subject S 2 performed the same gesture within 10 seconds, our algorithm finds the optimal warping path and duplicates the frames for subject S 1 in order to synchronize with subject S 2 the corresponding gesture. <ref type="figure" target="#fig_71">Figure 4</ref>.4 illustrates how the alignment computed by DTW for two time series can be used in order to duplicate the corresponding frames and eventually synchronize the two videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Linear Temporal Scaling</head><p>The previous DTW based algorithm works perfectly when synchronizing only two surgical videos. The problem arises when aligning three or more surgical trials simultaneously, which requires a multiple series alignment. The latter problem has been shown to be NP-Complete <ref type="bibr" target="#b232">(Wang and Jiang, 1994)</ref> with the exact solution requiring O(L N ) operations for N sequences of length L. This is clearly not feasible in our case where L varies between 10 3 and 10 4 and N ? 3, which is why we ought to leverage an approximation of the multiple sequence alignment solution provided by the DBA algorithm which we detailed in the Chapter 2.</p><p>NLTS was originally proposed for aligning discrete sequences of surgical gestures <ref type="bibr" target="#b58">(Forestier et al., 2014)</ref>. In this project, we extend the technique for numerical continuous sequences (time series). The goal of this final step is to compute the approximated multiple alignment of a set of sequences D which will eventually contain the precise information on how much a certain frame from a certain series should be duplicated. We first start by computing the average sequence T (using DBA) for a set of time series D that we want to align simultaneously. Then, by recomputing the Compact Multiple Alignment between the refined average T and the set of time series D, we can extract an alignment between T and each sequence in D. Thus, for each time series in D we will have the necessary information (extracted from the multiple alignment) in order to dilate the time series appropriately to have a length FIGURE 4.6: Snapshots of the three surgical tasks in the JIGSAWS dataset (from left to right): suturing, knot-tying, needle-passing <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>.</p><p>that is equal to T's, which also corresponds to the length of the longest time series in D. <ref type="figure" target="#fig_71">Figure 4</ref>.2, depicts an example of aligning three different time series using the NLTS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experiments</head><p>We start by describing the JIGSAWS dataset we have used for evaluation, before presenting our experimental study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>The JIGSAWS dataset <ref type="bibr" target="#b65">(Gao et al., 2014)</ref> includes data for three basic surgical tasks performed by study subjects (surgeons). The three tasks (or their variants) are usually part of the surgical skills training program. <ref type="figure" target="#fig_71">Figure 4</ref>.6 shows a snapshot example for each one of the three surgical tasks (Suturing, Knot Tying and Needle Passing). The JIGSAWS dataset contains kinematic and video data from eight different subjects with varying surgical experience: two experts (E), two intermediates (I) and four novices (N) with each group having reported respectively more than 100 hours, between 10 and 100 hours and less than 10 hours of training on the Da Vinci. All subjects were reportedly right-handed. The subjects repeated each surgical task five times and for each trial the kinematic and video data were recorded. When performing the alignment, we used the kinematic data which are numeric variables of four manipulators: left and right masters (controlled directly by the subject) and left and right slaves (controlled indirectly by the subject via the master manipulators). These kinematic variables (76 in total) are captured at a frequency equal to 30 frames per second for each trial. Out of these 76 variables, we only consider the Cartesian coordinates (x, y, z) of the left and right slave manipulators, thus each trial will consist of an MTS with 6 temporal variables. We chose to work only with this subset of kinematic variables to make the alignment coherent with what is visible in the recorded scene: the robots' end-effectors which can be seen in <ref type="figure" target="#fig_71">Figure 4</ref>.6. However other choices of kinematic variables are applicable, which we leave the exploration for our future work. Finally we should mention that in addition to the three self-proclaimed skill levels (N,I,E) JIGSAWS contains the modified OSATS score <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>, which corresponds to an expert surgeon observing the surgical trial and annotating the performance of the trainee.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We have created a companion web page 3 to our project where several examples of synchronized videos can be found. <ref type="figure" target="#fig_71">Figure 4</ref>.7 illustrates the multiple videos alignment procedure using our NLTS algorithm, where gray-scale images indicate duplicated frames (paused video) and colored images indicate a surgical motion (unpaused video). In <ref type="figure" target="#fig_71">Figure 4</ref>.7a we can clearly see how the gray-scale surgical trials are perfectly aligned. Indeed, the frozen videos show the surgeon ready to perform "pulling the needle" gesture <ref type="bibr" target="#b65">(Gao et al., 2014)</ref>. On the other hand, the colored trial (bottom right of <ref type="figure" target="#fig_71">Figure 4</ref>.7a) shows a video that is being played, where the surgeon is performing "inserting the needle" gesture in order to catch up with the other paused trials in gray-scale. Finally, the result of aligning simultaneously these four surgical trials is depicted in <ref type="figure" target="#fig_71">Figure 4</ref>.7b. By observing the four trials, one can clearly see that the surgeon is now performing the same surgical gesture "pulling the needle" simultaneously for the four trials. We believe that this type of observation will enable a novice surgeon to locate which surgical gestures still need some improvement in order to eventually become an expert surgeon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suturing</head><p>Needle Passing  Furthermore, in order to validate our intuition that DTW is able to capture characteristics that are in relationship with the motor skill of a surgeon, we plotted the DTW distance as a function of the OSATS <ref type="bibr" target="#b65">(Gao et al., 2014</ref>) score difference. For example, if two surgeons have both an OSATS score of 10 and 16 respectively, the corresponding difference is equal to |10 ? 16| = 6. In <ref type="figure" target="#fig_71">Figure 4</ref>.8, we can clearly see how the DTW score increases whenever the OSATS score difference increases. This observation suggests that the DTW score is low when both surgeons exhibit similar dexterity, and high whenever the trainees show different skill levels. Therefore, we conclude that the DTW score can serve as a heuristic for estimating the quality of the alignment (whenever annotated skill level is not available) -especially since we observed low quality alignments for surgeons with very distinct surgical skill levels.</p><p>Finally, we should note that this work is suitable for many research fields involving motion kinematic data with their corresponding video frames. Examples of such medical applications are assessing mental health from videos <ref type="bibr" target="#b254">(Yamada and Kobayashi, 2017)</ref> where wearable sensor data can be seen as time series kinematic variables and leveraged in order to synchronize a patient's videos and compare how well the patient is responding to a certain treatment. Following the same line of thinking, this idea can be further applied to kinematic data from wearable sensors coupled with the corresponding video frames when evaluating the Parkinson's disease evolution <ref type="bibr" target="#b44">(Criss and McNames, 2011)</ref> as well as infant grasp skills <ref type="bibr" target="#b133">(Li et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Conclusion</head><p>In this section, we showed how kinematic time series data recorded from the Da Vinci's end effectors can be leveraged in order to synchronize the trainee's videos performing a surgical task. With personalized feedback during surgical training becoming a necessity <ref type="bibr" target="#b95">(Ismail Fawaz et al., 2018a;</ref><ref type="bibr" target="#b61">Forestier et al., 2018)</ref>, we believe that replaying synchronized and well aligned videos would benefit the trainees in understanding which surgical gestures did or did not improve after hours of training, thus enabling them to further reach higher skills and eventually become experts. We acknowledge that this work needs an experimental study to quantify how beneficial is replaying synchronized videos for the trainees versus observing non-synchronized trials. Therefore, we leave such exploration and clinical try outs to our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conclusion</head><p>In this chapter, we tackled two different surgical skills related problems. First by designing a one dimensional FCN, we were able to reach state-of-the-art results for the surgical skills evaluation (classification and regression). Doing so, we were able to mitigate the DNNs' black-box effect, using the CAM technique in order to highlight the reason behind a certain surgical skill identification. The second surgical skill related problem was due to surgical training videos being our-of-synch, thus making it hard for trainees to understand and compare videos between different surgeons with various skill levels. We tackled the latter problem by proposing the use of NLTS in order to align and synchronize multiple videos simultaneously. These two projects were orthogonal in the sense that they could also complement each other: perhaps video and time series synchronization could be a pre-processing step that would enhance the performance surgical skills evaluation models. We leave such exploration to our future work when extending these two projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of contributions</head><p>The work presented in this manuscript first examined the current benchmark approaches in the area of deep learning for time series classification. Using the knowledge gained from this study, a number of new methods have been proposed to contribute to the deep learning for TSC literature, leading up to the proposal for the final classifier InceptionTime, which to the best of our knowledge constitutes the first time series classifier to achieve similar results to the famous HIVE-COTE ensemble, while outperforming it significantly in terms of running-time. We should emphasize that in order to foster reproducibility, the corresponding code for each paper is published on the following GitHub profile: https://github.com/hfawaz.</p><p>The main goal of this research was to answer the question outlined in Chapter 1: Is there a current DNN approach that reaches state-of-the-art performance for TSC and is less complex than HIVE-COTE? Most previous work on TSC haven't considered deep learning models as potential strong classifiers, which led to researchers proposing neural networks as baselines with little to no competitive performance in terms of accuracy when compared to other non deep learning based techniques such as HIVE-COTE. Arriving to our benchmark in 2018, we showed how neural networks can compete with state-of-the-art TSC algorithms, while providing interpretability using the CAM method.</p><p>Furthermore, in a series of research projects around regularizing DNNs for TSC, we were able to showcase how much more accuracy we can squeeze from a vanilla neural network architecture designed originally as a baseline. We started by proposing a novel DBA-based method for predicting the best source dataset for a given target dataset when fine-tuning a neural network classifier in a transfer learning setting. Following the ensembles trend of time series classifiers, we showed how we can leverage the variance in a DNN due to the stochastic nature of its optimization process, in order to gain a significant boost in accuracy by ensembling various neural networks with the same or different architectures. We then proposed to further build again upon the DBA algorithm to generate synthetic time series, allowing us to augment the training set and eventually improve the generalization capability of a deep learning classifier. Finally, we turned our attention to a very hot and trending topic in machine learning: adversarial attacks. We showed how vulnerable DNNs are to adversarial examples and highlighted several use case studies where such attacks could be detrimental, while showing how the latter technique can be employed as part of a regularization method called adversarial training.</p><p>Building upon this knowledge gained from studying the field of TSC, we identified a main bottleneck that hinders the practical usage of many published ensembles in real life time series data mining problems. This downside stems from focusing on developing very accurate classifiers, while ignoring the running time of the classifier. We were therefore keen on developing the first neural network ensemble (called InceptionTime) that is able to reach similar results to the current state-of-the-art ensemble HIVE-COTE, while providing scalability in terms of long and large time series dataset. The magnitude of this speed up is consistent across both Big Data TSC repositories as well as longer time series with high sampling rate.</p><p>Motivated originally by the problem of evaluating surgical skills from kinematic time series sensor data, we designed a special CNN network by leveraging our understanding of DNNs gained during the initial study conducted on domain agnostic TSC problems. For this specific surgical data science project, we encountered many challenges that were not present in other TSC problems: such as having very high sampling rates and a huge number of variables to work with. For this task, we provided an FCN based classifier while focusing on its interpretability in order to help surgeons using the system to improve their skill set.</p><p>Having summarized the aforementioned projects, we will present in the next section the different limitations of our approaches as well as the potential research area that could be of interest to many TSC researchers looking deeper into artificial neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of future works</head><p>The deep learning benchmark for TSC published during this thesis has taken a big leap towards introducing time series data mining practitioners to the potential of neural networks when classifying sequential temporal data. While still being one of the largest studies of deep learning for TSC to date, our review focused on a small subset of discriminative end-to-end DNNs. The latter means that we have excluded many types of approaches based on self-supervised learning such as training a neural network on a pre-text task with large unlabeled dataset then using the learned latent representation as input features to an off-the-shelf classifier <ref type="bibr" target="#b62">(Franceschi, Dieuleveut, and Jaggi, 2019)</ref>. Since self-supervised learning has allowed computer vision researchers to leverage efficiently the large amount of unlabeled images and videos on the web <ref type="bibr" target="#b105">(Jing and Tian, 2020)</ref>, we believe that the TSC community would benefit from benchmarking this type of approaches, allowing the exploitation of a large quantity of unlabeled raw time series data.</p><p>Following the review in Chapter 1, we have presented various regularization methods that help in improving the generalization capabilities of a given neural network for TSC. We believe that there exist much more research potential in developing specific time series data augmentation techniques that make use of the temporal aspect of the data. For example we could leverage a recent proposed approach that allows the network to learn the optimal warping by applying continuous piecewise affine transformations <ref type="bibr" target="#b243">(Weber et al., 2019)</ref>, and thus generating an infinite number of warped time series training examples. Furthermore, restricting transfer learning to a single architecture limits the potential of this famous implicit regularization technique, therefore it would be interesting to study how various architectures could benefit / harm the neural network's accuracy when used in a fine-tuning setting. In addition, having showed that ensembling DNNs by averaging the output class provabilities leads to a certain increase in the network's performance, we believe that there is still room for improvement, since averaging the a posteriori probability for each class is a very basic approach and perhaps a more robust meta-ensembling technique would demonstrate further improvements <ref type="bibr" target="#b31">(Boubrahimi, Ma, and Angryk, 2018)</ref>. Finally, with adversarial attacks exposing the vulnerabilities of neural networks, we believe that more and more researchers should focus on developing defenses for TSC models. In fact, the work done in this thesis on crafting adversarial examples has already inspired researchers world wide to propose temporal defense mechanisms <ref type="bibr" target="#b3">(Abdu-Aguye et al., 2020)</ref>.</p><p>In Chapter 3 we focused on the scalability of current state-of-the-art methods and proposed an Inception based network for TSC. We were inspired by the recent neural architecture advances in the field of computer vision. Following the same line of thinking, researchers would further adapt the current state-of-the-art machine learning approaches such as designing algorithms that themselves would design a neural network. <ref type="bibr" target="#b190">Rakhshani et al., 2019</ref> spotted the potential of meta-heuristicsspecifically differential evolution -when building neural networks that are specific to each TSC problem. We therefore believe that InceptionTime is not the ultimate solution to all TSC datasets, but rather a strong and robust starting point to any data mining practitioner that would like to design a DNN for solving their TSC problem. Furthermore, perhaps DNNs are not the sole answer when choosing the best TSC algorithm. In fact, when referring to the "no-free-lunch theorem" developed by <ref type="bibr" target="#b249">Wolpert, Macready, et al., 1995</ref>, and based on our results we believe that perhaps the best approach, is a meta-algorithm that would predict the best classifier based on characteristics extracted from the data.</p><p>Going back to our initial motivation for solving TSC problems in Chapter 4, we developed an FCN based classifier for MTS classification, allowing us to leverage the CAM technique to provide a level of model interpretability. We would like to first highlight the fact that our feedback technique would benefit from an extended real use-case validation process, for example verifying with expert surgeons if indeed the model is able to detect the main reason for classifying a surgical skill. However, this is usually expensive and was not possible during this thesis, but perhaps a research project in collaboration with expert and novice surgeons would be extremely interesting to the community. In addition, the fact that we are performing only a leave-one-super-trial-out setup means that a surgeon should be present in the training set in order to make a prediction. However, since only two experts exist in the dataset, this suggests that performing a leave-one-user-out setup would mean having only one expert in the training set. This constitutes a huge problem originating from the limited dataset size. Therefore, we believe that our approach should be validated on a larger dataset. Nevertheless, another potential solution is to perform adversarial learning in order to force the network to detect patterns that are discriminative in terms of surgical skills but not invariant in terms of subject ID. This would be inspired from the recent success of adversarial learning for speaker invariant speech recognition systems <ref type="bibr" target="#b4">(Adi et al., 2019)</ref>.</p><p>Finally, with deep learning showing successful results in various machine learning fields such text mining, image segmentation and speech recognition, we believe that there is still much more to be explored for the TSC research area, especially since the deep learning discipline is moving very fast. We hope that this thesis, with its accompanied code and published models, could be a cornerstone for future deep learning research on time series classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1. 1 A</head><label>1</label><figDesc>unified deep learning framework for time series classification. . . . . 20 1.2 Multilayer perceptron for time series classification. . . . . . . . . . . . . 22 1.3 The result of a applying a learned discriminative convolution on the GunPoint dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 1.4 Fully Convolutional Neural Network architecture. . . . . . . . . . . . . 25 1.5 An Echo State Network architecture for time series classification. . . . 26 1.6 An overview of the different deep learning approaches for time series classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 1.7 The Residual Network's architecture for time series classification. . . . 32 1.8 Encoder's architecture for time series classification. . . . . . . . . . . . 33 1.9 MCNN's architecture for time series classification. . . . . . . . . . . . . 34 1.10 T-LeNet's architecture for time series classification. . . . . . . . . . . . 36 1.11 MCDCNN's architecture for time series classification. . . . . . . . . . . 36 1.12 Time-CNN's architecture for time series classification. . . . . . . . . . . 37 1.13 Critical difference diagram showing pairwise statistical difference comparison of nine deep learning classifiers on the univariate UCR/UEA time series classification archive. . . . . . . . . . . . . . . . 41 1.14 Critical difference diagram showing pairwise statistical difference comparison of state-of-the-art classifiers on the univariate UCR/UEA time series classification archive. . . . . . . . . . . . . . . . . . . . . . . 43 1.15 Critical difference diagram showing pairwise statistical difference comparison of nine deep learning classifiers on the multivariate time series classification archive. . . . . . . . . . . . . . . . . . . . . . . . . . 45 1.16 Critical difference diagram showing pairwise statistical difference comparison of nine deep learning classifiers on both univariate and multivariate time series classification archives. . . . . . . . . . . . . . . 45 1.17 ResNet's accuracy variation with respect to the amount of training instances in the TwoPatterns dataset. . . . . . . . . . . . . . . . . . . . . 47 1.18 Accuracy of ResNet versus FCN over the UCR/UEA archive when three different aggregations are taken: the minimum, median and maximum. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 1.19 Highlighting with the Class Activation Map the contribution of each time series region for both classes in GunPoint when using the FCN and ResNet classifiers. Red corresponds to high contribution and blue to almost no contribution to the correct class identification (smoothed for visual clarity and best viewed in color). . . . . . . . . . . . . . . . . 50 xiv 1.20 Highlighting with the Class Activation Map the contribution of each time series region for the three classes in Meat when using the FCN and ResNet classifiers. Red corresponds to high contribution and blue to almost no contribution to the correct class identification (smoothed for visual clarity and best viewed in color). . . . . . . . . . . . . . . . . 52 1.21 Multi-Dimensional Scaling (MDS) applied on GunPoint for: (top) the raw input time series; (bottom) the learned features from the Global Average Pooling (GAP) layer for FCN (left) and ResNet (right) -(best viewed in color). This figure shows how the ResNet and FCN are projecting the time series from a non-linearly separable 2D space (when using the raw input), into a linearly separable 2D space (when using the latent representation). . . . . . . . . . . . . . . . . . . . . . . . . . . 55 1.22 Multi-Dimensional Scaling (MDS) applied on Wine for: (top) the raw input time series; (bottom) the learned features from the Global Average Pooling (GAP) layer for FCN (left) and ResNet (right) -(best viewed in color). This figure shows how ResNet, unlike FCN, is able to project the data into an easily separable space when using the learned features from the GAP layer (Color figure online). . . . . . . . 56 2.1 Evolution of model's loss (train and test) with and without the transfer learning method using ElectricDevices as source and OSULeaf as target datasets. (Best viewed in color). . . . . . . . . . . . . . . . . . . . 60 2.2 General deep learning training process with transfer learning for time series classification. In this example, a model is first pre-trained on Car (source dataset) and then the corresponding weights are finetuned on CBF (target dataset). . . . . . . . . . . . . . . . . . . . . . . . . 61 2.3 The variation in percentage over the original accuracy when fine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>unified deep learning framework for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>perceptron for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 1 . 3 :</head><label>13</label><figDesc>The result of a applying a learned discriminative convolution on the GunPoint dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fully Convolutional Neural Network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>An Echo State Network architecture for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 1</head><label>1</label><figDesc>.6: An overview of the different deep learning approaches for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The Residual Network's architecture for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Encoder's architecture for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 1.8 illustrates Encoder's architecture for TSC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>MCNN's architecture for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>11: MCDCNN's architecture for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>12: Time-CNN's architecture for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>14: Critical difference diagram showing pairwise statistical difference comparison of state-of-the-art classifiers on the univariate UCR/UEA time series classification archive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>17: ResNet's accuracy variation with respect to the amount of training instances in the TwoPatterns dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>.18) shows that FCN's performance is less stable compared to ResNet's. In other words, the weight's initial value can easily decrease the accuracy of FCN whereas ResNet maintained a relatively high accuracy when 18: Accuracy of ResNet versus FCN over the UCR/UEA archive when three different aggregations are taken: the minimum, median and maximum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 1 .(</head><label>1</label><figDesc>19 shows the CAM's result when applied on each time series from both classes in the training set while classifying using the FCN model (Figure 1.19a 19: Highlighting with the Class Activation Map the contribution of each time series region for both classes in GunPoint when using the FCN and ResNet classifiers. Red corresponds to high contribution and blue to almost no contribution to the correct class identification (smoothed for visual clarity and best viewed in color). 19b) and the ResNet model (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>20: Highlighting with the Class Activation Map the contribution of each time series region for the three classes in Meat when using the FCN and ResNet classifiers. Red corresponds to high contribution and blue to almost no contribution to the correct class identification (smoothed for visual clarity and best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>21: Multi-Dimensional Scaling (MDS) applied on GunPoint for: (top) the raw input time series; (bottom) the learned features from the Global Average Pooling (GAP) layer for FCN (left) and ResNet (right) -(best viewed in color). This figure shows how the ResNet and FCN are projecting the time series from a non-linearly separable 2D space (when using the raw input), into a linearly separable 2D space (when using the latent representation). 22: Multi-Dimensional Scaling (MDS) applied on Wine for: (top) the raw input time series; (bottom) the learned features from the Global Average Pooling (GAP) layer for FCN (left) and ResNet (right) -(best viewed in color). This figure shows how ResNet, unlike FCN, is able to project the data into an easily separable space when using the learned features from the GAP layer (Color figure online).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>FIGURE 2.1: Evolution of model's loss (train and test) with and without the transfer learning method using ElectricDevices as source andOSULeaf as target datasets. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>General deep learning training process with transfer learning for time series classification. In this example, a model is first pre-trained on Car (source dataset) and then the corresponding weights are fine-tuned on CBF (target dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>FIGURE 2 . 3 :</head><label>23</label><figDesc>The variation in percentage over the original accuracy when fine tuning a pre-trained model. The rows' indexes correspond to the source datasets and the columns' indexes correspond to the target datasets. The red color shows the extreme case where the chosen pair of datasets (source and target) deteriorates the network's performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>4: The three aggregated accuracies (minimum, median and maximum) of the Convolutional Neural Networks with the transfer learning approach against no transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>FIGURE 2 . 5 :</head><label>25</label><figDesc>The accuracy of a fine-tuned model for two cases: (x axis) when the source dataset is selected randomly; (y axis) when the source dataset is selected using our Dynamic Time Warping based solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>fine-tuned model's accuracy variation on the target dataset ShapeletSim with respect to the chosen source dataset neighbor (smoothed for visual clarity -best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>fine-tuned model's accuracy variation on the target dataset HandOutlines with respect to the chosen source dataset neighbor (smoothed for visual clarity -best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>fine-tuned model's accuracy variation on the target dataset Meat with respect to the chosen source dataset neighbor (smoothed for visual clarity -best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>FIGURE 2</head><label>2</label><figDesc>.9: Ensemble of deep convolutional neural networks for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>10: Critical difference diagram showing the pairwise statistical comparison of ten ResNets with random initializations as well as one ResNet ensemble composed of these ten individual neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>13: Critical difference diagram showing the pairwise statistical comparison of current state-of-the-art algorithms with the Neural Network Ensemble (NNE) added to the pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>15: The model's loss with/without data augmentation on the DiatomSizeReduction and Meat datasets (smoothed and clipped for visual clarity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>17: Example of a perturbed time series that is misclassified by a deep network after applying a small perturbation (time series from the Coffee dataset<ref type="bibr" target="#b51">(Dau et al., 2019)</ref> containing spectrographs of coffee beans).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>FIGURE 2 .</head><label>2</label><figDesc>18: Example of perturbing the classification of an input time series from the TwoLeadECG (Dau et al., 2019) dataset by adding an imperceptible noise computed using the Fast Gradient Sign Method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>FIGURE 2.19: Accuracy variation for two attacks (FGSM and BIM)with respect to ResNet's original accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>20: Multi-Dimensional Scaling showing the distribution of perturbed time series on the whole test set of the Ham dataset where the accuracy decreased from 80% to 21% after performing the BIM attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head></head><label></label><figDesc>21: Multi-Dimensional Scaling showing the distribution of perturbed time series on the whole test set of the Coffee dataset where the accuracy decreased from 100% to 50% after performing the FGSM attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>FIGURE 2 .</head><label>2</label><figDesc>22: Accuracy variation with respect to the amount of perturbation for FGSM and BIM attacks on FordA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head></head><label></label><figDesc>23: Accuracy variation for ItalyPowerDemand with respect to the perturbation where FGSM managed to fool the network with this example for ? 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>FIGURE 2</head><label>2</label><figDesc>.25: The effect of adversarial training on the model's accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head></head><label></label><figDesc>Inception network for time series classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head></head><label></label><figDesc>Inside our Inception module for time series classification. For simplicity we illustrate a bottleneck layer of size m = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>FIGURE 3 . 4 :</head><label>34</label><figDesc>Example of a synthetic binary time series classification problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head></head><label></label><figDesc>Critical difference diagram showing the performance of InceptionTime compared to the current state-of-the-art classifiers of time series data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head></head><label></label><figDesc>Figure 3.6 depicts the accuracy plot of InceptionTime against HIVE-COTE for each of the 85 UCR datasets. The results show a Win/Tie/Loss of 40/6/39 in favor of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>FIGURE 3</head><label>3</label><figDesc>.6: Accuracy plot showing how our proposed Inception-Time model is not significantly different than HIVE-COTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>FIGURE 3 . 7 :</head><label>37</label><figDesc>Training time as a function of the series length for the InlineSkate dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>FIGURE 3 . 8 :</head><label>38</label><figDesc>Training time as a function of the training set size for the SITS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>FIGURE 3 . 9 :</head><label>39</label><figDesc>Accuracy as a function of the training set size for the SITS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>FIGURE 3 .</head><label>3</label><figDesc>10: Plot showing how InceptionTime significantly outperforms ResNet(5). 11: Critical difference diagram showing the effect of the number of individual classifiers in the InceptionTime ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>FIGURE 3 .</head><label>3</label><figDesc>13: Accuracy plot for InceptionTime with/without the bottleneck layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>FIGURE 3 .</head><label>3</label><figDesc>14: Critical difference diagram showing how the network's bottleneck size affects InceptionTime' average rank. 15: Accuracy plot for InceptionTime with/without the residual connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head></head><label></label><figDesc>16: Inception network's accuracy over the simulated dataset, with respect to the network's depth as well as the length of the input time series.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>FIGURE 3 .</head><label>3</label><figDesc>17: Critical difference diagram showing how the network's depth affects InceptionTime' average rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head></head><label></label><figDesc>Fully convolutional network for surgical skill evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head></head><label></label><figDesc>2: Using class activation map to provide explainable classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>FIGURE 4 . 3 :</head><label>43</label><figDesc>Feedback using the CAM on subject E's second knottying trial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head></head><label></label><figDesc>Example on how a time series alignment is used to synchronize the videos by duplicating the gray-scale frames. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head></head><label></label><figDesc>Figure 4.5 depicts an example of stretching three different time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><head>FIGURE 4 . 5 :</head><label>45</label><figDesc>Example of aligning coordinate X's time series for subject F, when performing three trials of the suturing surgical task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_70"><head>FIGURE 4 . 7 :</head><label>47</label><figDesc>Video alignment procedure with duplicated (gray-scale) frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head>FIGURE 4</head><label>4</label><figDesc>.8: A polynomial fit (degree 3) of DTW dissimilarity score (y-axis) as a function of the OSATS score difference between two surgeons (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). . . . . . . . . . . 67 2.4 The three aggregated accuracies (minimum, median and maximum) of the Convolutional Neural Networks with the transfer learning approach against no transfer learning. . . . . . . . . . . . . . . . . . . . . 69 2.5 The accuracy of a fine-tuned model for two cases: (x axis) when the source dataset is selected randomly; (y axis) when the source dataset is selected using our Dynamic Time Warping based solution. . . . . . . 70 2.6 The fine-tuned model's accuracy variation on the target dataset ShapeletSim with respect to the chosen source dataset neighbor (smoothed for visual clarity -best viewed in color). . . . . . . . . . . . 71 2.7 The fine-tuned model's accuracy variation on the target dataset HandOutlines with respect to the chosen source dataset neighbor (smoothed for visual clarity -best viewed in color). . . . . . . . . . . . 72 xv 2.8 The fine-tuned model's accuracy variation on the target dataset Meat with respect to the chosen source dataset neighbor (smoothed for visual clarity -best viewed in color). . . . . . . . . . . . . . . . . . . . . . 72 2.9 Ensemble of deep convolutional neural networks for time series classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 2.10 Critical difference diagram showing the pairwise statistical comparison of ten ResNets with random initializations as well as one ResNet ensemble composed of these ten individual neural networks. . . . . . . 78 2.11 Critical difference diagram showing the pairwise statistical comparison of six architectures ensembled with ten different random initializations each, as well as one ensemble containing the six models. . . . 79 2.12 The Neural Network Ensemble (NNE) composed of ResNet, FCN and Encoder is significantly better than an ensemble of pure ResNets. . . . 79 2.13 Critical difference diagram showing the pairwise statistical comparison of current state-of-the-art algorithms with the Neural Network Ensemble (NNE) added to the pool. . . . . . . . . . . . . . . . . . . . . 80 2.14 Ensembling fine-tuned models is significantly better than ensembling randomly initialized FCN models that are trained from scratch. . . . . 81 2.15 The model's loss with/without data augmentation on the DiatomSiz-eReduction and Meat datasets (smoothed and clipped for visual clarity). 82 2.16 Accuracy of ResNet with and/or without data augmentation. . . . . . 86 2.17 Example of a perturbed time series that is misclassified by a deep network after applying a small perturbation (time series from the Coffee dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>dataset by adding an imperceptible noise computed using the Fast Gradient Sign Method. . . . . . . . . 88 2.19 Accuracy variation for two attacks (FGSM and BIM) with respect to ResNet's original accuracy. . . . . . . . . . . . . . . . . . . . . . . . . . . 92 2.20 Multi-Dimensional Scaling showing the distribution of perturbed time series on the whole test set of the Ham dataset where the accuracy decreased from 80% to 21% after performing the BIM attack. . . 93 2.21 Multi-Dimensional Scaling showing the distribution of perturbed time series on the whole test set of the Coffee dataset where the accuracy decreased from 100% to 50% after performing the FGSM attack. 94 2.22 Accuracy variation with respect to the amount of perturbation for FGSM and BIM attacks on FordA. . . . . . . . . . . . . . . . . . . . . . . 95 2.23 Accuracy variation for ItalyPowerDemand with respect to the perturbation where FGSM managed to fool the network with this example for ? 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 2.24 Accuracy of adversarial training with or without AdvProp. . . . . . . 98 2.25 The effect of adversarial training on the model's accuracy. . . . . . . . 98 3.1 Our Inception network for time series classification. . . . . . . . . . . . 103 3.2 Inside our Inception module for time series classification. For simplicity we illustrate a bottleneck layer of size m = 1. . . . . . . . . . . . . . 104 3.3 Receptive field illustration for a two layers CNN. . . . . . . . . . . . . . 105 3.4 Example of a synthetic binary time series classification problem. . . . . 106 3.5 Critical difference diagram showing the performance of Inception-Time compared to the current state-of-the-art classifiers of time series data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 xvi 3.6 Accuracy plot showing how our proposed InceptionTime model is not significantly different than HIVE-COTE. . . . . . . . . . . . . . . . . . . 108 3.7 Training time as a function of the series length for the InlineSkate dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 3.8 Training time as a function of the training set size for the SITS dataset. 109 3.9 Accuracy as a function of the training set size for the SITS dataset. . . . 110 3.10 Plot showing how InceptionTime significantly outperforms ResNet(5). 111 3.11 Critical difference diagram showing the effect of the number of individual classifiers in the InceptionTime ensemble. . . . . . . . . . . . . . 111 3.12 Critical difference diagram showing the effect of the batch size hyperparameter value over InceptionTime's average rank. . . . . . . . . . . . 112 3.13 Accuracy plot for InceptionTime with/without the bottleneck layer. . . 112 3.14 Critical difference diagram showing how the network's bottleneck size affects InceptionTime' average rank. . . . . . . . . . . . . . . . . . . 113 3.15 Accuracy plot for InceptionTime with/without the residual connections.113 3.16 Inception network's accuracy over the simulated dataset, with respect to the network's depth as well as the length of the input time series. . . 114 3.17 Critical difference diagram showing how the network's depth affects InceptionTime' average rank. . . . . . . . . . . . . . . . . . . . . . . . . 115 3.18 Inception network's accuracy over the simulated dataset, with respect to the filter length as well as the input time series length. . . . . . . . . 116 3.19 Inception network's accuracy over the simulated dataset, with respect to the receptive field as well as the input time series length. . . . . . . . 117 3.20 Critical difference diagram showing the effect of the filter length hyperparameter value over InceptionTime' average rank. . . . . . . . . . 117 3.21 Inception network's accuracy over the simulated dataset, with respect to the number of filters as well as the number of classes. . . . . . . . . 118 3.22 Critical difference diagram showing how the network's width affects InceptionTime' average rank. . . . . . . . . . . . . . . . . . . . . . . . . 119 3.23 Critical difference diagram showing how choosing the second best hyperparameters affects InceptionTime's average rank. . . . . . . . . . 120 4.1 Fully convolutional network for surgical skill evaluation. . . . . . . . . 124 4.2 Using class activation map to provide explainable classification. . . . . 128 4.3 Feedback using the CAM on subject E's second knot-tying trial. . . . . 128 4.4 Example on how a time series alignment is used to synchronize the videos by duplicating the gray-scale frames. Best viewed in color. . . . 130 4.5 Example of aligning coordinate X's time series for subject F, when performing three trials of the suturing surgical task. . . . . . . . . . . . . . 131 4.6 Snapshots of the three surgical tasks in the JIGSAWS dataset (from left to right): suturing, knot-tying, needle-passing (Gao et al., 2014). . . . . 133 4.7 Video alignment procedure with duplicated (gray-scale) frames. . . . . 134 4.8 A polynomial fit (degree 3) of DTW dissimilarity score (y-axis) as a function of the OSATS score difference between two surgeons (x-axis). 135 xvii List of Tables 1 List of papers with the corresponding public datasets used as well as the companion GitHub repository. . . . . . . . . . . . . . . . . . . . . . 16 1.1 Architecture's hyperparameters for the deep learning approaches. . . . 38 1.2 Optimization's hyperparameters for the deep learning approaches. . . 38 1.3 The multivariate time series classification archive. . . . . . . . . . . . . 40 1.4 Deep learning algorithms' performance grouped by themes. Each entry is the percentage of dataset themes an algorithm is most accurate for. Bold indicates the best model. . . . . . . . . . . . . . . . . . . . . . 45 1.5 Deep learning algorithms' average ranks grouped by the datasets' length. Bold indicates the best model. . . . . . . . . . . . . . . . . . . . 46 1.6 Deep learning algorithms' average ranks grouped by the training sizes. Bold indicates the best model. . . . . . . . . . . . . . . . . . . . . 46 2.1 Average rank of the six classifiers constituting the Neural Network Ensemble for time series classification over the 85 datasets from the UCR/UEA archive. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 3.1 Filter length variants of InceptionTime with their corresponding average ranks grouped by the datasets' length. Bold indicates the best model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 4.1 Micro, macro and Spearman's coefficient ? for surgical skill evaluation. 126 xix</figDesc><table><row><cell cols="2">xx</cell><cell></cell></row><row><cell></cell><cell>MDS</cell><cell>Multi-Dimensional Scaling</cell></row><row><cell></cell><cell>t-SNE</cell><cell>t-distributed Stochastic Neighbor Embedding</cell></row><row><cell></cell><cell>TEKA</cell><cell>Time Elastic Kernel Averaging</cell></row><row><cell cols="3">NNE List of Abbreviations Neural Network Ensemble FGSM Fast Gradient Sign Method</cell></row><row><cell></cell><cell>KNN</cell><cell>K Nearest Neighbors</cell></row><row><cell></cell><cell>AdvProp</cell><cell>Adversarial Propagation</cell></row><row><cell></cell><cell>BIM</cell><cell>Basic Iterative Method</cell></row><row><cell>CST</cell><cell>RF</cell><cell>Classification de s?ries temporelles Receptive Field</cell></row><row><cell cols="2">ECG SITS</cell><cell>Electrocardiogram Satellite Image Time Series</cell></row><row><cell>TSC</cell><cell>OSATS</cell><cell>Time Series Classification Objective Structured Assessment of Technical Skills</cell></row><row><cell>NN</cell><cell>GMF</cell><cell>Nearest Neighbor Global Movement Features</cell></row><row><cell cols="2">DTW JIGSAWS</cell><cell>Dynamic Time Warping JHU-ISI Gesture and Skill Assessment Working Set</cell></row><row><cell>ED</cell><cell>s-HMM</cell><cell>Euclidean Distance Sparse Hidden Markov Model</cell></row><row><cell cols="2">BOSS ApEn</cell><cell>Bag-of-SFA-Symbols Approximate Entropy</cell></row><row><cell cols="2">DNN NLTS</cell><cell>Deep Neural Networks Non-Linear Temporal Scaling</cell></row><row><cell>NLP</cell><cell></cell><cell>Natural Language Processing</cell></row><row><cell cols="2">ResNet</cell><cell>Residual Network</cell></row><row><cell cols="2">CNN</cell><cell>Convolutional Neural Networks</cell></row><row><cell cols="2">UCR</cell><cell>University of California Riverside</cell></row><row><cell cols="2">UEA</cell><cell>University of East Anglia</cell></row><row><cell cols="2">TWE</cell><cell>Time Warp Edit</cell></row><row><cell cols="2">MSM</cell><cell>Move Split Merge</cell></row><row><cell cols="2">NN-DTW</cell><cell>NN coupled with DTW</cell></row><row><cell cols="2">SVM</cell><cell>Support Vector Machine</cell></row><row><cell cols="2">COTE</cell><cell>Collective Of Transformation-based Ensembles</cell></row><row><cell cols="3">HIVE-COTE Hierarchical Vote Collective of Transformation-Based Ensembles</cell></row><row><cell>ST</cell><cell></cell><cell>Shapelet Transform</cell></row><row><cell cols="2">GPU</cell><cell>Graphical Processing Unit</cell></row><row><cell cols="2">MTS</cell><cell>Multivariate Time Series</cell></row><row><cell cols="2">CAM</cell><cell>Class Activation Map</cell></row><row><cell cols="2">MLP</cell><cell>Multi Layer Perceptron</cell></row><row><cell>ESN</cell><cell></cell><cell>Echo State Network</cell></row><row><cell>FC</cell><cell></cell><cell>Fully Connected</cell></row><row><cell cols="2">ReLU</cell><cell>Rectified Linear Unit</cell></row><row><cell cols="2">RNN</cell><cell>Recurrent Neural Network</cell></row><row><cell cols="2">SDAE</cell><cell>Stacked Denoising Auto-Encoder</cell></row><row><cell cols="2">DBN</cell><cell>Deep Belief Network</cell></row><row><cell cols="2">FCN</cell><cell>Fully Convolutional Network</cell></row><row><cell cols="2">GAP</cell><cell>Global Average Pooling</cell></row><row><cell cols="2">PReLU</cell><cell>Parametric Rectified Linear Unit</cell></row><row><cell cols="2">MCNN</cell><cell>Multi-scale Convolutional Neural Network</cell></row><row><cell>WS</cell><cell></cell><cell>Window Slicing</cell></row><row><cell cols="2">T-LeNet</cell><cell>Time Le-Net</cell></row><row><cell>WW</cell><cell></cell><cell>Window Warping</cell></row><row><cell cols="2">MCDCNN</cell><cell>Multi Channel Deep Convolutional Neural Network</cell></row><row><cell cols="2">MSE</cell><cell>Mean Squared Error</cell></row><row><cell cols="2">TWIESN</cell><cell>Time Warping Invariant Echo State Network</cell></row><row><cell cols="2">SGD</cell><cell>Stochastic Gradient Descent</cell></row><row><cell>EE</cell><cell></cell><cell>Elastic Ensemble</cell></row><row><cell>PF</cell><cell></cell><cell>Proximity Forest</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>List of papers with the corresponding public datasets used as well as the companion GitHub repository.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">.1: Architecture's hyperparameters for the deep learning ap-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">proaches.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Optimization</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>algorithm</cell><cell>valid</cell><cell>loss</cell><cell cols="3">epochs batch learning rate</cell><cell>decay</cell></row><row><cell>MLP</cell><cell>AdaDelta</cell><cell>train</cell><cell>entropy</cell><cell>5000</cell><cell>16</cell><cell>1.0</cell><cell>0.0</cell></row><row><cell>FCN</cell><cell>Adam</cell><cell>train</cell><cell>entropy</cell><cell>2000</cell><cell>16</cell><cell>0.001</cell><cell>0.0</cell></row><row><cell>ResNet</cell><cell>Adam</cell><cell>train</cell><cell>entropy</cell><cell>1500</cell><cell>16</cell><cell>0.001</cell><cell>0.0</cell></row><row><cell>Encoder</cell><cell>Adam</cell><cell>train</cell><cell>entropy</cell><cell>100</cell><cell>12</cell><cell>0.00001</cell><cell>0.0</cell></row><row><cell>MCNN</cell><cell>Adam</cell><cell>split 20%</cell><cell>entropy</cell><cell>200</cell><cell>256</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell>t-LeNet</cell><cell>Adam</cell><cell>train</cell><cell>entropy</cell><cell>1000</cell><cell>256</cell><cell>0.01</cell><cell>0.005</cell></row><row><cell>MCDCNN</cell><cell>SGD</cell><cell>split 33%</cell><cell>entropy</cell><cell>120</cell><cell>16</cell><cell>0.01</cell><cell>0.0005</cell></row><row><cell>Time-CNN</cell><cell>Adam</cell><cell>train</cell><cell>mse</cell><cell>2000</cell><cell>16</cell><cell>0.001</cell><cell>0.0</cell></row><row><cell cols="8">TABLE 1.2: Optimization's hyperparameters for the deep learning ap-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">proaches.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1</head><label>1</label><figDesc>.3 shows the different characteristics of each MTS dataset used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="5">old length new length classes dimensions train</cell><cell>test</cell></row><row><cell>ArabicDigits</cell><cell>4-93</cell><cell>93</cell><cell>10</cell><cell>13</cell><cell>6600</cell><cell>2200</cell></row><row><cell>AUSLAN</cell><cell>45-136</cell><cell>136</cell><cell>95</cell><cell>22</cell><cell>1140</cell><cell>1425</cell></row><row><cell cols="2">CharacterTrajectories 109-205</cell><cell>205</cell><cell>20</cell><cell>3</cell><cell>300</cell><cell>2558</cell></row><row><cell>CMUsubject16</cell><cell>127-580</cell><cell>580</cell><cell>2</cell><cell>62</cell><cell>29</cell><cell>29</cell></row><row><cell>ECG</cell><cell>39-152</cell><cell>152</cell><cell>2</cell><cell>2</cell><cell>100</cell><cell>100</cell></row><row><cell>JapaneseVowels</cell><cell>7-29</cell><cell>29</cell><cell>9</cell><cell>12</cell><cell>270</cell><cell>370</cell></row><row><cell>KickVsPunch</cell><cell>274-841</cell><cell>841</cell><cell>2</cell><cell>62</cell><cell>16</cell><cell>10</cell></row><row><cell>Libras</cell><cell>45-45</cell><cell>45</cell><cell>15</cell><cell>2</cell><cell>180</cell><cell>180</cell></row><row><cell>Outflow</cell><cell>50-997</cell><cell>997</cell><cell>2</cell><cell>4</cell><cell>803</cell><cell>534</cell></row><row><cell>UWave</cell><cell>315-315</cell><cell>315</cell><cell>8</cell><cell>3</cell><cell>200</cell><cell>4278</cell></row><row><cell>Wafer</cell><cell>104-198</cell><cell>198</cell><cell>2</cell><cell>6</cell><cell>298</cell><cell>896</cell></row><row><cell>WalkVsRun</cell><cell>128-1918</cell><cell>1919</cell><cell>2</cell><cell>62</cell><cell>28</cell><cell>16</cell></row><row><cell cols="6">TABLE 1.3: The multivariate time series classification archive.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>4-tsc</cell></row></table><note>4 shows the algorithms' performance with respect to the dataset's theme.2 www.github.com/hfawaz/dl-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 1 .</head><label>1</label><figDesc>Deep learning algorithms' average ranks grouped by the datasets' length. Bold indicates the best model. Deep learning algorithms' average ranks grouped by the training sizes. Bold indicates the best model. accuracy for very long time series since a recurrent model may "forget" a useful information present in the early elements of a long time series. However, TWIESN did reach competitive accuracies on several long time series datasets such as reaching a 96.8% accuracy on Meat whose time series length is equal to 448. This would suggest that ESNs can solve the vanishing gradient problem especially when learning from long time series. A third important characteristic is the training size of datasets and how it affects a DNN's performance. Table 1.6 shows the average rank for each classifier grouped by the train set's size. Again, ResNet and FCN still dominate with not much of a difference. However we found one very interesting dataset: DiatomSizeReduction.</figDesc><table><row><cell>Length</cell><cell></cell><cell>MLP</cell><cell cols="7">FCN ResNet Encoder MCNN t-LeNet MCDCNN Time-CNN TWIESN</cell></row><row><cell>&lt;81</cell><cell></cell><cell cols="2">5.57 3.5</cell><cell cols="2">2.64</cell><cell>2.93</cell><cell>8.93</cell><cell>6.14</cell><cell>3.29</cell><cell>3.86</cell><cell>5.86</cell></row><row><cell>81-250</cell><cell></cell><cell cols="4">4.53 1.74 1.84</cell><cell>3.74</cell><cell>8.74</cell><cell>5.05</cell><cell>5.74</cell><cell>4.89</cell><cell>5.95</cell></row><row><cell>251-450</cell><cell></cell><cell cols="4">4.32 3.05 1.86</cell><cell>3.68</cell><cell>8.82</cell><cell>4.73</cell><cell>6.55</cell><cell>5.14</cell><cell>5.41</cell></row><row><cell>451-700</cell><cell></cell><cell cols="2">5.23 2.92</cell><cell>2.0</cell><cell></cell><cell>4.31</cell><cell>7.77</cell><cell>4.62</cell><cell>6.31</cell><cell>5.38</cell><cell>4.77</cell></row><row><cell>701-1000</cell><cell></cell><cell cols="2">5.4 2.1</cell><cell>1.7</cell><cell></cell><cell>4.6</cell><cell>8.4</cell><cell>2.9</cell><cell>6.1</cell><cell>6.8</cell><cell>5.5</cell></row><row><cell>&gt;1000</cell><cell></cell><cell cols="2">3.71 3.0</cell><cell cols="2">1.57</cell><cell>4.0</cell><cell>8.29</cell><cell>3.29</cell><cell>5.71</cell><cell>6.43</cell><cell>7.0</cell></row><row><cell cols="2">TABLE 1.5: Train size MLP</cell><cell>FCN</cell><cell cols="2">ResNet</cell><cell cols="2">Encoder</cell><cell>MCNN</cell><cell>t-LeNet</cell><cell>MCDCNN</cell><cell>Time-CNN</cell><cell>TWIESN</cell></row><row><cell>&lt;100</cell><cell cols="2">4.64 2.21</cell><cell cols="2">1.93</cell><cell></cell><cell>4.57</cell><cell>8.5</cell><cell>5.0</cell><cell>6.71</cell><cell>4.39</cell><cell>5.32</cell></row><row><cell>100-399</cell><cell cols="2">5.21 2.5</cell><cell cols="2">1.86</cell><cell></cell><cell>3.64</cell><cell>8.29</cell><cell>4.71</cell><cell>5.93</cell><cell>6.25</cell><cell>4.71</cell></row><row><cell>400-799</cell><cell cols="2">4.73 3.07</cell><cell cols="2">2.33</cell><cell></cell><cell>3.67</cell><cell>8.93</cell><cell>6.0</cell><cell>3.67</cell><cell>4.33</cell><cell>6.0</cell></row><row><cell>&gt;799</cell><cell cols="2">4.29 3.64</cell><cell cols="2">1.86</cell><cell></cell><cell>2.71</cell><cell>8.86</cell><cell>2.57</cell><cell>5.21</cell><cell>5.71</cell><cell>7.79</cell></row><row><cell cols="2">TABLE 1.6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4: Deep learning algorithms' performance grouped by themes. Each entry is the percentage of dataset themes an algorithm is most accurate for. Bold indicates the best model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell>.1: Average rank of the six classifiers constituting the Neural</cell></row><row><cell>Network Ensemble for time series classification over the 85 datasets</cell></row><row><cell>from the UCR/UEA archive.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell>.1: Filter length variants of InceptionTime with their corre-</cell></row><row><cell>sponding average ranks grouped by the datasets' length. Bold indi-</cell></row><row><cell>cates the best model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Inception network's accuracy over the simulated dataset, with respect to the number of filters as well as the number of classes. to overfit the datasets when a limited amount of training data is available. Therefore, we decided to train and evaluate InceptionTime versions containing both long and short filters on the UCR/UEA archive. Where the original InceptionTime contained filters of length {10,20,40}, the five models we are testing here contain filters of length {2,4,8}; {4,8,16}; {8,16,32}; {16,32,64}; {32,64,128}.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">number of filters vs number of classes</cell></row><row><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell>4</cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.9 1.0 accuracy 128 0.6</cell><cell cols="10">receptive field vs time series length 256 32</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>0.7 0.8</cell><cell>0.2</cell><cell></cell><cell>512</cell><cell></cell><cell cols="2">1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>number of classes</cell><cell>4 8 16 32</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell>0</cell><cell>20</cell><cell></cell><cell cols="5">40 number of filters 60 80</cell><cell cols="2">100</cell><cell>120</cell></row><row><cell></cell><cell cols="3">0.5 FIGURE 3.21:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">length series time</cell><cell>1024 512 256 128</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>200</cell><cell cols="2">400</cell><cell>600</cell><cell></cell><cell>800</cell><cell>1000</cell><cell cols="2">1200</cell><cell>1400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">receptive field</cell><cell></cell><cell></cell></row><row><cell cols="13">FIGURE 3.19: Inception network's accuracy over the simulated</cell></row><row><cell cols="13">dataset, with respect to the receptive field as well as the input time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">series length.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>12</cell><cell>11</cell><cell>10</cell><cell>9</cell><cell>8</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell cols="2">NN-DTW-WW EE BOSS ST PF InceptionTime.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>InceptionTime.128 InceptionTime.16 InceptionTime.64 InceptionTime.32 HIVE-COTE InceptionTime</cell></row><row><cell cols="13">FIGURE 3.20: Critical difference diagram showing the effect of the</cell></row><row><cell cols="13">filter length hyperparameter value over InceptionTime' average rank.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 3</head><label>3</label><figDesc>.1 depicts the average rank of each variant of InceptionTime over the UCR/UEA archive grouped by the datasets' lengths (with about 15 datasets in each group). Similarly to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Micro Macro ? Micro Macro ?</figDesc><table><row><cell>Method</cell><cell>Suturing Micro Macro ?</cell><cell>Needle passing</cell><cell>Knot tying</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4.1)</cell></row></table><note>S-HMM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell>.1: Micro, macro and Spearman's coefficient ? for surgical</cell></row><row><cell>skill evaluation.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The implementations are available on https://github.com/hfawaz/dl-4-tsc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://germain-forestier.info/src/bigdata2018/ 2 https://github.com/hfawaz/bigdata18</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/hfawaz/ijcnn19ensemble</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our source code is available on https://github.com/hfawaz/aaltd18</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/hfawaz/ijcnn19attacks 6 https://germain-forestier.info/src/ijcnn2019/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hfawaz/InceptionTime</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our source code is available at https://github.com/hfawaz/ijcars19</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/hfawaz/aime19</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://germain-forestier.info/src/aime2019/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015-02-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-scale Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Augmenting The Size of EEG datasets Using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ghodai M Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting Adversarial Attacks In Time-Series Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdu-Aguye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mubarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3092" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">To Reverse the Gradient or Not: an Empirical Comparison of Adversarial and Multi-task Learning in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3742" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Data classification: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Time-series clustering-a decade review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Aghabozorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Seyed</forename><surname>Shirkhorshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teh Ying</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="16" to="38" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Dataset and Benchmarks for Segmentation and Recognition of Gestures in Robotic Surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="18" to="9294" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mid-infrared spectroscopy and authenticity problems in selected meats: a feasibility study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Jowder</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Kemsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Food Chemistry</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="195" to="201" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detection of adulteration in cooked meat products by mid-infrared spectroscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Jowder</forename><surname>Osama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reginald</forename><forename type="middle">H</forename><surname>Kemsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of agricultural and food chemistry</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1325" to="1329" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time Series Trend Detection and Forecasting Using Complex Network Topology Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Anghinoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DA-HOC: Semisupervised Domain Adaptation for Room Occupancy Prediction Using CO2 Sensor Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arief-Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Irvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems for Energy-Efficient Built Environments</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Time Series Classification in Reservoir-and Model-Space: A Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witali</forename><surname>Aswolinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><forename type="middle">Felix</forename><surname>Reinhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="197" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time Series Classification in Reservoir-and Model-Space</title>
	</analytic>
	<monogr>
		<title level="m">Neural Processing Letters 48</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="789" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Time-Series Classification with COTE: The Collective of Transformation-Based Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2522" to="2535" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Time-series classification with COTE: The collective of transformation-based ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1548" to="1549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Run Length Transformation for Discriminating Between Auto Regressive Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Janacek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="154" to="178" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="606" to="660" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the Usage and Performance of The Hierarchical Vote Collective of Transformation-based Ensembles version 1.0 (HIVE-COTE 1.0)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document Image Defect Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structured Document Image Analysis</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Deep Transfer Learning Approach for Improved Post-Traumatic Stress Disorder Diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Bag-of-Features Framework to Classify Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tuv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence 35</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2796" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gokce</surname></persName>
		</author>
		<ptr target="http://www.mustafabaydogan.com.Accessed" />
		<title level="m">Multivariate Time Series Classification Datasets</title>
		<imprint>
			<date type="published" when="2015-02-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Should We Really Use Post-hoc Tests Based on Mean-ranks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Mangili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Learners Benefit More from Out-of-Distribution Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized Denoising Auto-encoders As Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reservoir computing approaches for representation and classification of multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1803.07870</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A review on outlier/anomaly detection in time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">;</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ane</forename><surname>Bl?zquez-Garc?a</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
	<note>Pattern Recognition and Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Binary Shapelet Transform for Multiclass Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data Analytics and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="257" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neuro-ensemble for time series data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soukaina</forename><surname>Boubrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhe</forename><surname>Filali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angryk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond &apos;Dieselgate&apos;: Implications of unaccounted and future air pollutant emissions and energy use for cars in the United Kingdom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Policy 97</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discrimination of Arabica and Robusta in instant coffee by Fourier transform infrared spectroscopy and chemometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Briandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Kemsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reginald</forename><forename type="middle">H</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of agricultural and food chemistry</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="170" to="174" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Surgeon specific mortality in adult cardiac surgery: comparison between crude and risk stratified data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bridgewater</surname></persName>
		</author>
		<idno>BMJ 327.7405</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A CNN adapted to time series for the classification of Supernovae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Brunel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An Automatic Survey System for Paved and Unpaved Road Classification and Road Anomaly Detection using Smartphone Sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Cabral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Service Operations and Logistics, and Informatics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="787" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DECADE: A Deep Metric Learning Model for Multivariate Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Workshop on Mining and Learning from Time Series</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model Metric Co-Learning for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3387" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model-based Kernel for Efficient Time Series Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Genesis of Basic and Multi-Layer Echo State Network Recurrent Autoencoders for Efficient Data Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chouikhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1804.08996</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video assessment of finger tapping for Parkinson&apos;s disease and other movement disorders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Criss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcnames</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="7123" to="7126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep Learning for Time-Series Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Borges Gamboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Domain Adaptation for Visual Applications: A Comprehensive Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In: ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-Scale Convolutional Neural Networks for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In: ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Soft-DTW: a Differentiable Loss Function for Time-Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cutur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Predicting Bucket-Filling Control Actions of a Wheel-Loader Operator Using a Neural Network Ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dadhich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bodin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Judicious setting of Dynamic Time Warping&apos;s window width allows more accurate classification of time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="917" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The UCR time series archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anh</surname></persName>
		</author>
		<idno type="DOI">10.1109/JAS.2019.1911747</idno>
		<idno>1293-1305. ISSN: 2329-9274. DOI: 10 . 1109 / jas . 2019 . 1911747</idno>
		<ptr target="http://dx.doi.org/10.1109/JAS.2019.1911747" />
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Dem?ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A time series forest for classification and feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houtao</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="page" from="142" to="153" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Time-series Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Esling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Agon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient and robust alignment of unsynchronized video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><forename type="middle">D</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Robust Physical-World Attacks on Deep Learning Visual Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1625" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep learning for healthcare applications based on physiological signals: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Faust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Non-linear temporal scaling of surgical processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="143" to="152" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Discovering Discriminative and Interpretable Patterns for Surgical Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="page" from="978" to="981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generating Synthetic Time Series to Augment Sparse Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="865" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Surgical motion analysis using discriminative interpretable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised scalable representation learning for multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Dieuleveut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4650" to="4661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A Comparison of Alternative Tests of Significance for the Problem of m Rankings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milton</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="92" />
			<date type="published" when="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Deep Echo State Network (DeepESN): A Brief Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1712.04323</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">The JHU-ISI Gesture and Skill Assessment Working Set (JIG-SAWS): A Surgical Activity Dataset for Human Motion Modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Modeling and Monitoring of Computer Assisted Interventions -MICCAI Workshop</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An Extension on &quot;Statistical Comparisons of Classifiers over Multiple Data Sets&quot; for all Pairwise Comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning research 9</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2677" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Cost-Sensitive Convolution based Neural Networks for Imbalanced Time-Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1801.04396</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An ultra-fast time series distance measure to allow data mining in more complex real-world deployments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaghayegh</forename><surname>Gharghabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Comparing similarity perception in time series visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gogolou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A Primer on Neural Network Models for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multiobjective Learning in the Model Space for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="2168" to="2267" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Deep neural network ensemble by data augmentation and bagging for skin lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagath</forename><forename type="middle">C</forename><surname>Rajapakse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning Time-series Shapelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josif</forename><surname>Grabocka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Constructing a validity argument for the Objective Structured Assessment of Technical Skills (OSATS): a systematic review of validity evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Hatala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Health Sciences Education</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1149" to="1175" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Classification of time-series images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hatami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gavet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Debayle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The effect of video review of resident laparoscopic surgical skills measured by self-and external assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">E</forename><surname>Herrera-Almario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Surgery</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page" from="315" to="320" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Classification of time series by shapelet transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="851" to="881" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Stochastic neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sam T Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Ridge Regression: Applications to Nonorthogonal Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technometrics 12.1</title>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="page" from="69" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A Simple Sequentially Rejective Multiple Test Procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sture</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Improving time series similarity measures by integrating preprocessing steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>H?ppner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="851" to="878" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Transfer learning for shortterm wind speed prediction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Renewable Energy</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="83" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Forecasting: principles and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Athanasopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>OTexts</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Semi-Supervised Clustering With Multiresolution Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Pensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Real-time human activity recognition from accelerometer data using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="915" to="922" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">The Da Vinci Surgical System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Intuitive Surgical Sunnyvale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Affordable, web-based surgical skill training and evaluation tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gazi</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1532" to="0464" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Evaluating Surgical Skills from Kinematic Data Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ismail Fawaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="214" to="221" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Data augmentation using synthetic data for time series classification with deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
	<note>International Workshop on Advanced Analytics and Learning on Temporal Data, ECML PKDD</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Transfer learning for time series classification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Adversarial Attacks on Deep Neural Networks for Time Series Classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>IEEE International Joint Conference on Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Automatic Alignment of Surgical Videos Using Kinematic Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="page" from="104" to="113" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Deep Neural Network Ensembles for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Deep learning for time series classification: a review</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">InceptionTime: Finding AlexNet for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Science 304</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5667</biblScope>
			<biblScope unit="page" from="78" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Ensemble Deep Learning for Biomedical Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Scalable Classification of Univariate and Multivariate Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karimi-Bidhendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1598" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathan</forename><surname>Kashiparekh</surname></persName>
		</author>
		<title level="m">ConvTimeNet: A Pre-trained Deep Convolutional Neural Network for Time Series Classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>IEEE International Joint Conference on Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Surgical robotics beyond enhanced dexterity instrumentation: a survey of machine learning techniques and their role in intelligent and autonomous surgical actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohannes</forename><surname>Kassahun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1861" to="6429" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Recognizing activities in multiple contexts using transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L M</forename><surname>Kasteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J A</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kr?se</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence -AI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
	<note>in Elder care</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Using dynamic time warping distances as features for improved time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="312" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Intelligent Icons: Integrating Lite-Weight Data Mining and Visualization into GUI Operating Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="912" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Curse of Dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Mueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="314" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Financial time series forecasting using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung-Jae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing 55.1-2</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="307" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">An innovative model for teaching and learning clinical procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Kneebone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="628" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Model-Based Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexios</forename><surname>Kotsifakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Papapetrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Data Analysis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="179" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">OpenImages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Data Augmentation for Brain-Computer Interfaces: Analysis on Event-Related Potentials Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Krell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Multidimensional scaling. Number 07-011 in Sage University Paper series on quantitative applications in the social sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myron</forename><surname>Wish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations -Workshop track</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Predicting mortgage default using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Havard</forename><surname>Kvamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="207" to="217" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A review of unsupervised feature learning and deep learning for time-series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>L?ngkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1710.09220</idno>
		<title level="m">The Heterogeneous Ensembles of Standard Classification Algorithms (HESCA): the Whole is Greater than the Sum of its Parts</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Data Augmentation for Time Series Classification using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Guennec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tavenard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD on Advanced Analytics and Learning on Temporal Data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 86</title>
		<meeting>the IEEE 86</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="18" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nature 521</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Efficient BackProp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Ensemble of binary tree structured deep convolutional network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1448" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Diagnosis Prediction via Medical Context Attention Networks Using Deep Generative Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1104" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Manipulation-skill Assessment from Videos with Spatial Attention Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">GCRNN: Group-Constrained Convolutional Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems 99</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Time series classification with ensembles of elastic distance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="565" to="592" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Time Series Classification with HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Time Series Classification with Multivariate Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsaio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications 77</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="22159" to="22171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Stochastic Multiple Choice Learning for Acoustic Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A 1 TOPS/W Analog Deep Machine-Learning Engine With Floating-Gate Storage in 0.13 ?m CMOS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="270" to="281" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Proximity Forest: An effective and scalable distance-based classifier for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="851" to="881" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Functional echo state network for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Sciences</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page" from="20" to="0255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Health-ATM: A Deep Architecture for Multifaceted Patient Health Record Representation and Risk Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="261" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of machine learning research 9</title>
		<imprint>
			<date type="published" when="2008-11" />
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Surgical data science for next-generation interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2157" to="846" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">TimeNet: Pre-trained deep recurrent neural network for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="607" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Time Warp Edit Distance with Stiffness Adjustment for Time Series Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marteau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence 31</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="306" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Times Series Averaging and Denoising from a Probabilistic Perspective on Time-Elastic Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Fran?ois</forename><surname>Marteau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Mathematics and Computer Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="375" to="392" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning approach for early classification of time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">E-learning as new method of medical education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izet</forename><surname>Masic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acta informatica medica 16</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Knowledgedriven Biometric Authentication in Virtual Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">Ismail</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Khamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">A computer-based laparoscopic skills assessment device differentiates experienced from novice laparoscopic surgeons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mcnatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Endoscopy</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1085" to="1089" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Evaluation of evidence-based practices in online learning: A meta-analysis and review of online learning studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Means</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Time Series Classification using Deep Learning for Process Planning: A Case from the Process Industry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nijat</forename><surname>Mehdiyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="242" to="249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations -Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Time-series modeling with undecimated fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mittelman</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1508.00317</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Video-Based Surgical Learning: Improving Trainee Education and Preparation for Surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of surgical education</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="828" to="835" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Determination of food quality by using spectroscopic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Nawrocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Lamorska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in agrophysical research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Generalized Dynamic Time Warping: Unleashing the Warping Power Hidden in Point-Wise Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodica</forename><surname>Neamtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">How Useful is Self-Supervised Pretraining for Visual Tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In: The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Time Series Classification by Sequence Learning in All-Subsequence Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gsponer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ifrim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="947" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Using the Objective Structured Assessment of Technical Skills (OSATS) global rating scale to evaluate the skills of surgical trainees in the operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Niitsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Surgery Today 43.3</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1436" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Convolutional neural network with multitask learning scheme for acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Nwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Dat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1347" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: State of the art and research challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Nweke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friday</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert Systems with Applications 105</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="233" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Sodium dodecyl sulphate-polyacrylamide gel electrophoresis of proteins in dry-cured hams: Data registration and multivariate analysis across multiple gels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Olias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrophoresis 27</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1288" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ord?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Adversarial Sample Crafting for Time Series Classification with Elastic Similarity Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izaskun</forename><surname>Oregi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent and Distributed Computing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="26" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Powering the nation: Household electricity using habits revealed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foreman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Saving Trust/DECC/DEFRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Linear sequence-to-sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Padua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="304" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A Survey on Transfer Learning</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Transfer Learning to Predict Missing Ratings via Heterogeneous User Feedbacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weike</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2318" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1803.04765</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Technical Report on the CleverHans v2.1.0 Adversarial Examples Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1211.5063</idno>
	</analytic>
	<monogr>
		<title level="m">On the Difficulty of Training Recurrent Neural Networks&quot;. In: International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>III-1310-III-1318</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing 11</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">523</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Dynamic time warping averaging of time series allows faster and more accurate classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="470" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Faster and more accurate classification of time series by exploiting a novel dynamic time warping averaging algorithm</title>
	</analytic>
	<monogr>
		<title level="m">Knowledge and Information Systems 47.1</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Summarizing a set of time series by averaging: From Steiner sequence to compact multiple alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Gan?arski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="page" from="76" to="91" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">A global averaging method for dynamic time warping, with applications to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Ketterlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Gan?arski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="678" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="503" to="519" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">100 years of surgical education: the past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><forename type="middle">V</forename><surname>Polavarapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American College of Surgeons</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Wind power forecasting using time series cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonja</forename><surname>Pravilovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Discovery Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="276" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Multi ROI and Multi Map Networks for Accurate and Efficient Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Anomaly detection in monitoring sensor data for preventive maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Rabatel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Bringay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poncelet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert Systems with Applications 38</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7003" to="7015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">A Generative Modeling Approach to Limited Channel ECG Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thiagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">2571</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Neural Architecture Search for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Rakhshani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">YouTube is the most frequently used educational video source for surgical preparation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><forename type="middle">K</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Surgical Education</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1072" to="1076" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Three Myths about Dynamic Time Warping Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chotirat</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Ann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="506" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Daubechies wavelets and Mathematica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><forename type="middle">C H</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">C</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="635" to="648" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Kern</surname></persName>
		</author>
		<title level="m">International Conference on Knowledge Technologies and Data-driven Business</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>A Literature Survey of Early Time Series Classification and Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Forecasting sales through time series clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Sanwlani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayalakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Mining &amp; Knowledge Management Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Randomness in neural networks: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 7</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1200</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">The BOSS is concerned with time series classification in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sch?fer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1505" to="1530" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Towards a universal neural network encoder for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence Research and Development: Current Challenges, New Trends and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page">120</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Generalizing DTW to the multidimensional case requires an adaptive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shokoohi-Yekta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Speeding up similarity search under dynamic time warping by pruning unpromising alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="988" to="1016" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Digital video in education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scot</forename><surname>Ransbottom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distance learning technologies: Issues, trends and opportunities</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="124" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Transfer Learning for Time Series Classification in Dissimilarity Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Spiegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">The Move-Split-Merge Metric for Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Detecting and interpreting myocardial infarction using fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claas</forename><surname>Strodthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Measurement</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">15001</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Time-Series Classification Methods: Review and Applications to Power Systems Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><surname>Susto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Cenedese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data Application in Power Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="179" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Machine learning for predictive maintenance: A multiple classifier approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><surname>Susto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="812" to="820" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Indexing and classifying gigabytes of time series under time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petitjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="282" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Efficient search of the best warping window for Dynamic Time Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Time Series Classification Using Time Warping Invariant Echo State Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tanisaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="831" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Sparse Hidden Markov Models for Surgical Gesture Classification and Skill Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Computer-Assisted Interventions</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="978" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Wind power density forecasting using ensemble predictions and time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mcsharry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buizza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Energy Conversion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="775" to="782" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Malware Detection with Deep Neural Network Using Process Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tobiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Annual Computer Software and Applications Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Use of features from RR-time series and EEG signals for automated classification of sleep stages in deep neural network framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">Rajendra</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="890" to="902" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">There is no free lunch in adversarial robustness (but there are unexpected benefits)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In: ArXiv 8</note>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Feasibility of an AI-Based Measure of the Hand Motions of Expert and Novice Surgeons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munenori</forename><surname>Uemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Mathematical Methods in Medicine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">Instance Normalization: The Missing Ingredient for Fast Stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Data Augmentation of Wearable Sensor Data for Parkinson&apos;s Disease Monitoring Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">T</forename><surname>Um</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="216" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Analysis of the Structure of Surgical Activity for a Suturing and Knot-Tying Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swaroop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Library of Science One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Transfer Learning for Time Series Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vercruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wannes</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop and Tutorial on Interactive Adaptive Learning co-located with European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2437" to="2446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Deep learning for sensor-based activity recognition: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">An effective multivariate time series classification approach using echo state network and adaptive differential evolution algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="237" to="249" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">On the Complexity of Multiple Sequence Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="337" to="348" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Videosnapping: Interactive synchronization of multiple videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">A Cycle Deep Belief Network Model for Multivariate Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Problems in Engineering</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1611.04578</idno>
		<title level="m">Earliness-Aware Deep Convolutional Networks for Early Time Series Classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Hierarchical Tree Long Short-Term Memory for Sentence Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<monogr>
		<title level="m" type="main">Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1509. 07481</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<title level="m" type="main">Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1610.07258</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Encoding Time Series as Images for Visual Inspection and Classification Using Tiled Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="40" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Imaging Time-series to Improve Classification and Imputation</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3939" to="3945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Deep learning with convolutional neural network for objective skill evaluation in robot-assisted surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Majewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fey</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Diffeomorphic Temporal Alignment Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6574" to="6585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Trajectory based video sequence synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wedge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kovesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Ensemble of Deep Neural Networks with Probability-Based Fusion for Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihua</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Individual Comparisons by Ranking Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence self calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="370" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">No free lunch theorems for search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macready</surname></persName>
		</author>
		<idno>SFI-TR-95-02-010</idno>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Santa Fe Institute</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep. Technical Report</note>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xi</surname></persName>
		</author>
		<title level="m">Deep Dilated Convolution on Multimodality Time Series for Human Activity Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>International Joint Conference on Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Adversarial Examples Improve Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">MNRD: A Merged Neural Model For Rumor Detection In Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Detecting Mental Fatigue from Eye-Tracking Data Gathered While Watching Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatomo</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="page" from="295" to="304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Investigating Deep Reinforcement Learning Techniques in Personalized Dialogue Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="630" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">10 CHALLENGING PROBLEMS IN DATA MINING RESEARCH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Technology &amp; Decision Making 05.04</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Time Series Shapelets: A New Primitive for Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Time series shapelets: a novel technique that allows accurate, interpretable and fast classification</title>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">An Integrated Model for Crime Prediction Using Temporal and Spatial Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1386" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">How Transferable Are Features in Deep Neural Networks?&quot; In: International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
	<note>In: Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b261">
	<monogr>
		<title level="m" type="main">Adversarial Examples: Attacks and Defenses for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Muvan: A multi-view attention network for multivariate temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="717" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>ArXiv. eprint: 1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems Engineering and Electronics 28.1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Exploiting multi-channels deep convolutional neural networks for multivariate time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web-Age Information Management</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2095" to="2236" />
		</imprint>
	</monogr>
	<note>Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Wide and Deep Convolutional Neural Networks for Electricity-Theft Detection to Secure Smart Grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1606" to="1615" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Automated surgical skill assessment in RMIS training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="731" to="739" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ziat</surname></persName>
		</author>
		<title level="m">Spatio-Temporal Neural Networks for Space-Time Series Forecasting and Relations Discovery</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Data Mining</note>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Indexing for interactive exploration of big data series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
