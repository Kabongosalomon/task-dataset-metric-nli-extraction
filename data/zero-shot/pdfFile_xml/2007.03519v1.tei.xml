<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GateNet:Gating-Enhanced Deep Network for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
							<email>towanhuang@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>She</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Corp</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GateNet:Gating-Enhanced Deep Network for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advertising and feed ranking are essential to many Internet companies such as Facebook. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. In recent years, many neural network based CTR models have been proposed and achieved success such as Factorization-Machine Supported Neural Networks, DeepFM and xDeepFM. Many of them contain two commonly used components: embedding layer and MLP hidden layers. On the other side, gating mechanism is also widely applied in many research fields such as computer vision(CV) and natural language processing(NLP). Some research has proved that gating mechanism improves the trainability of non-convex deep neural networks. Inspired by these observations, we propose a novel model named GateNet which introduces either the feature embedding gate or the hidden gate to the embedding layer or hidden layers of DNN CTR models, respectively. The feature embedding gate provides a learnable feature gating module to select salient latent information from the feature-level. The hidden gate helps the model to implicitly capture the high-order interaction more effectively. Extensive experiments conducted on three real-world datasets demonstrate its effectiveness to boost the performance of various state-of-the-art models such as FM, DeepFM and xDeepFM on all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advertising and feed ranking are essential to many Internet companies such as Facebook. The main technique behind these tasks is click-through rate prediction which is known as CTR. Many models have been proposed in this field such as logistic regression (LR) <ref type="bibr" target="#b19">[19]</ref>, polynomial-2 (Poly2) <ref type="bibr" target="#b12">[12]</ref>, tree based models <ref type="bibr" target="#b10">[10]</ref>, tensorbased models <ref type="bibr" target="#b14">[14]</ref>, Bayesian models <ref type="bibr" target="#b8">[8]</ref>, and factorization machines based models <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>With the great success of deep learning in many research fields such as computer vision <ref type="bibr" target="#b15">[15]</ref> and natural language processing <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b20">20]</ref>, many deep learning based CTR models have been proposed in recent years <ref type="bibr">[1, 9, 16, 25?</ref> ]. Many of them contain two commonly used components:embedding layer and MLP hidden layers. On the other side, gating mechanism is also widely applied in many research fields such as computer vision(CV) and natural language processing(NLP). Some research works have proved that gating mechanism improves the trainability of non-convex deep neural networks. Inspired by these observations, a model named GateNet is proposed to select salient latent information from the feature-level and implicitly capture the high-order interaction more effectively for CTR prediction.</p><p>Our main contributions are listed as follows: * Work done at Sina Weibo.</p><p>? We propose the feature embedding gate layer to replace the traditional embedding and enhance the model ability. Inserting the feature embedding gate into the embedding layer of many classical models such as FM, DeepFM, DNN and XDeepFM, we observe a significant performance improvement. ? The MLP layers are an essential component to implicitly capturing the high-order feature interaction in the canonical DNN models, we introduce the hidden gate to the MLP parts of deep models and improve the performance of the the classical models. ? It is simple and effective to enhance the standard DNN model by inserting hidden gate and we can achieve comparable performance with other state-of-the-art model baselines such as DeepFM and XDeepFM. The rest of this paper is organized as follows. In Section 2, we review related works which are relevant with our proposed model, followed by introducing our proposed model in Section 3. We will present experimental explorations on three real-world datasets in Section 4. Finally, we conclude this work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Deep Learning based CTR Models</head><p>Many deep learning based CTR models have also been proposed in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. How to effectively model the feature interactions is the key factor for most of these neural network based models. Factorization-Machine Supported Neural Networks (FNN) <ref type="bibr" target="#b25">[25]</ref> is a forward neural network using FM to pre-train the embedding layer. However, FNN can capture only high-order feature interactions. Wide &amp; Deep model(WDL) <ref type="bibr" target="#b0">[1]</ref> jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommendation systems. However, expertise feature engineering is still needed on the input to the wide part of WDL. To alleviate manual efforts in feature engineering, DeepFM <ref type="bibr" target="#b9">[9]</ref> replaces the wide part of WDL with FM and shares the feature embedding between the FM and deep component.</p><p>In addition, Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b24">[24]</ref> and eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b16">[16]</ref> are recent deep learning methods which explicitly model the feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gating Mechanisms in Deep Learning</head><p>Gating mechanism is widely used in many deep learning fields, such as computer vision(CV), natural language processing(NLP), and recommendation systems.</p><p>The gate mechanism is used in computer vision, such as Highway Network <ref type="bibr" target="#b23">[23]</ref>, they utilize the transform gate and the carry gate to express how much of the output is produced by transforming the input and carrying the output, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2007.03519v1 [cs.</head><p>LG] 6 Jul 2020 <ref type="figure">Figure 1</ref>: The architecture of our proposed GateNet. The left diagram is the standard DNN network, the middle of diagram is the models with feature embedding gate and the right diagram is the deep models with hidden gate.</p><p>The gate mechanism is widely applied to NLP, such as LSTM <ref type="bibr" target="#b6">[6]</ref>, GRU <ref type="bibr" target="#b2">[2]</ref>, language modeling <ref type="bibr" target="#b4">[4]</ref>, sequence to sequence learning <ref type="bibr" target="#b5">[5]</ref> and they utilize the gate to prevent the gradients vanishing and resolve the long-term dependency problem.</p><p>In addition, <ref type="bibr" target="#b18">[18]</ref> uses the gates to automatically adjust parameters between modeling shared information and modeling taskspecific information in recommendation systems. Another recommendation system applying the gate mechanism is hierarchical gating network(HGN) <ref type="bibr" target="#b17">[17]</ref> and they apply feature-level and instancelevel gating modules to adaptively control what item latent features and which relevant item can be passed to the downstream layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED MODEL</head><p>Deep learning models are widely used in industrial recommendation systems, such as WDL, YouTubeNet <ref type="bibr" target="#b3">[3]</ref> and DeepFM. The DNN model is a sub-component in many current DNN ranking systems, and its network structure is shown in the left of <ref type="figure">Figure 1</ref>.</p><p>We can find two commonly used components in most of the current DNN ranking systems: the embedding layer and MLP hidden layer. We aim to enhance the model ability and propose the model named GateNet for CTR prediction tasks. First, we propose the feature embedding gating layer which can convert embedding features into gate-aware embedding features and helps to select salient latent information from the feature-level. Second, we also propose the hidden gate which can adaptively control what latent features and which relevant feature interaction can be passed to the downstream layer. The DNN model with feature embedding gate and DNN model with hidden gate are depicted as the middle and right in <ref type="figure">Figure 1</ref>. In the following subsections, we will describe the feature embedding layer and hidden gate layer in GateNet in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Embedding Gate</head><p>The sparse input layer and embedding layer are widely used in deep learning based CTR models such as DeepFM <ref type="bibr" target="#b9">[9]</ref>. The sparse input layer adopts a sparse representation for raw input features.</p><p>The embedding layer is able to embed the sparse feature into a low dimensional, dense real-value vector. The output of embedding layer is a wide concatenated field embedding vector:</p><formula xml:id="formula_0">E = [e 1 , e 2 , ? ? ? , e i , ? ? ? , e f ]</formula><p>where f denotes the number of fields, e i ? R k denotes the embedding of i-th field, and k is the dimension of embedding layer.</p><p>On the other side, recent research results show that gate can improve the train-ability in training non-convex deep neural networks <ref type="bibr" target="#b7">[7]</ref>. In this work, firstly we propose the feature embedding gate to select salient latent information from the feature-level in the DeepCTR model. The basic steps of the feature embedding gate can be described as followed:</p><p>First, for every field embedding e i , we calculate the gate value which represents the feature-level importance of embedding. We formalize this step as the following formula:</p><formula xml:id="formula_1">? i = ? (W i ? e i )<label>(1)</label></formula><p>where ? is the activation function of gate, e i ? R k is the original embedding, W i is the learned parameters of the i-th gate and the total number of learned parameter matrix W = [W 1 , ? ? ? ,W i , ? ? ? ,W f ], i = 1, ? ? ? , f . Second, we assign the gate value to the corresponding feature embedding and generate a gate-aware embedding.</p><formula xml:id="formula_2">?e i = e i ? ? i<label>(2)</label></formula><p>where ? denotes the Hadamard or element-wise product, e i ? R k is the i-th original embedding, i = 1, ? ? ? , f . Third, we collect all gate-aware embeddings and regard it as gated feature embedding.</p><formula xml:id="formula_3">GE = [?e 1 , ?e 2 , ? ? ? , ?e i , ? ? ? , ?e f ]<label>(3)</label></formula><p>It is a common practice to make gate output a scalar which represents the importance of the whole feature embedding. To learn the bit level salient important information in the feature embedding, we can make this gate output a vector which contains fine-grained information about the feature embedding. And we call this embedding gate 'bit-wise' gate and the common gate 'vectorwise' gate. The vector-wise and bit-wise feature embedding gate can be depicted as <ref type="figure" target="#fig_0">Figure 2</ref>. Seen from the figure, we compare the difference of vector-wise feature gate and bit-wise feature is as follows:</p><formula xml:id="formula_4">Vector-wise: ? i ? R, W i ? R k ?1 , W ? R f ?k ?1 . Bit-wise: ? i ? R k , W i ? R k ?k , W ? R f ?k ?k .</formula><p>We can see that the output of bit-wise gate is a vector which is related to each bit of feature embedding and it can be regarded as using the same value to each bit of feature embedding. The performance comparison of vector-wise and bit-wise feature embedding gate will be discussed in Section 4.2.</p><p>Moreover, as some previous works such as FiBiNet <ref type="bibr" target="#b11">[11]</ref> does, we will explore the parameter sharing mechanism of the feature embedding gate layer. Each gate in the feature embedding gate layer has its own parameters to explicitly learn the salient feature information, we also can make all the gates share parameters in order to reduce the number of parameters. We call this gate 'field sharing' and previous gate 'field private'. From a mathematical perspective, the biggest difference between 'field sharing' and 'field private' is the learned gate parameters W i . W i is shared among all the fields in 'field sharing' while W i is different for each field in 'field private'. The performance of 'field sharing' and 'field private' will be compared in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hidden Gate</head><p>The deep part of many DNN ranking systems usually consists of several full-connected layers, which implicitly captures high-order features interactions. As shown in <ref type="figure">Figure 1</ref>, the input of deep network is the flatten of embedding layer. Let a (0) = [?e 1 , ? ? ? , ?e i , ? ? ? , ?e f ] denotes the outputs of embedding layer, where ?e i ? R k represents the i?th feature embedding. Then, a (0) is fed into multi-layer perceptron network, and the feed forward process is:</p><formula xml:id="formula_5">a (l ) = ? (W (l ) a (l ?1) + b (l ) )<label>(4)</label></formula><p>where l is the depth and ? is the activation function. W (l ) ,b (l ) ,a (l ) are the model weight, bias and output of the l-th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: The Hidden Gate Layer</head><p>Similar to the bit-wise feature embedding gate, we proposed the hidden gate which can be applied to the hidden layer. As depicted as <ref type="figure">Figure 3</ref>, we use this gate as follows:</p><formula xml:id="formula_6">? (l ) = a (l ) ? ? ? (W (l ) ? a (l ) )<label>(5)</label></formula><p>where ? denotes the element-wise product, ? ? is the gate activation function, W</p><p>? is the l-th layer parameter of hidden gate. Likewise, we can stack multiple hidden gate layers like the classic DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Layer</head><p>To summarize, we give the overall formulation of our proposed model'output as:?</p><formula xml:id="formula_8">= ? (W |L | ? |L | + b |L | )<label>(6)</label></formula><p>where? ? (0, 1) is the predicted value of CTR, ? is the sigmoid function, b |L | is the bias and |L| is the depth of DNN. The learning process aims to minimize the following objective function (cross entropy):</p><formula xml:id="formula_9">loss = ? 1 N N i=1 (y i lo?(? i ) + (1 ? y i ) * lo?(1 ?? i ))<label>(7)</label></formula><p>where y i is the ground truth of i-th instance,? i is the predicted CTR, and N is the total size of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to answer the following research questions: (RQ1) Can the feature embedding gate enhance the ability of the baseline models? (RQ2) Can the hidden gate enhance the ability of the baseline models? (RQ3) Can we combine the two gates in one model to achieve further improvements? (RQ4) How do the settings of networks influence the performance of our model?</p><p>We will answer these questions after presenting some fundamental experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Testbeds and Setup</head><p>4.1.1 Data Sets. 1) Criteo. The Criteo 1 dataset is widely used in many CTR model evaluation. It contains click logs with 45 millions data instances. There are 26 anonymous categorical fields and 13 continuous feature fields in Criteo dataset. We split the dataset randomly into two parts: 90% is for training, while the rest is for testing. 2) ICME. The ICME 2 dataset consists of several days of short video click datas. It contains click logs with 19 millions data instances in track2. For each click data, we choose 5 fields(user_id, user_city, item_id,author_id,item_city) to predict the like probability of short video. We split it randomly into two parts: 70% is for training, while the rest is for testing. 3) SafeDriver. The SafeDriver 3 dataset is used to predict the probability that an auto insurance policy holder files a claim. There are 57 anonymous fields in SafeDriver dataset and these features are divided into similar groups:binary features, categorical features, continuous features and ordinal features. It contains 595K data instances. We split the dataset randomly into two parts: 90% is for training, while the rest is for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics.</head><p>In our experiment, we adopt AUC(Area Under ROC) as metric. AUC is a widely used metric in evaluating classification problems. Besides, some work validates AUC as a good measurement in CTR prediction <ref type="bibr" target="#b8">[8]</ref>. AUC is insensitive to the classification threshold and the positive ratio. The upper bound of AUC is 1, and the larger the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baseline Methods.</head><p>To verify the effect of the gate layer added in various mainstream models, we choose some widely used CTR models as our baseline models including FM <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>, DNN, DeepFM <ref type="bibr" target="#b9">[9]</ref>, and XDeepFM <ref type="bibr" target="#b16">[16]</ref>.</p><p>Main goal of this work is not intent to propose a new model instead of enhancing these baseline models via gating mechanism that we proposed. Note that an improvement of 1? in AUC is usually regarded as significant for the CTR prediction because it will bring a large increase in a company's revenue if the company has a very large user base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement all the models with Tensorflow 4 in our experiments. For the embedding layer, the dimension of embedding layer is set to 10. For the optimization method, we use the Adam <ref type="bibr" target="#b13">[13]</ref> with a mini-batch size of 1000, and the learning rate is set to 0.0001. For all deep models, the depth of layers is set to 3, all activation functions are RELU, the number of neurons per layer is 400, and the dropout rate is set to 0.5. The default activation function of feature embedding gate is Sigmoid and activation function of hidden gate is Tanh. We conduct our experiments with 2 Tesla K40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance of Feature Embedding Gate(RQ1)</head><p>In this subsection, we show the performance gains of chosen baseline models after inserting feature embedding gate into a typical embedding layer. The experiments are conducted on Criteo,ICME and SafeDriver datasets and results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Inserting the feature embedding gate into these baseline models, we find our proposed embedding gate mechanisms can consistently boost the baseline model's performance on these three datasets as shown in <ref type="table" target="#tab_0">Table 1</ref>. These results indicate that carefully selecting salient latent information from the feature-level is useful to enhance the model ability and make the baseline models achieve better performance. Among all the baseline models, FM with the feature embedding gate gets a significant improvement which outperforms the classic FM model by almost 2% on ICME dataset. We assume that FM is a shallow model that has only a set of latent vectors to learn, there's no other component in FM to explicitly or implicitly adjust the feature in FM, so the gate mechanism is a good way to adjust the feature weight. Instead of FM, there are many deep models such as DeepFM and XDeepFM, our models with feature embedding gate can enhance these models' ability and make further improvements.</p><p>Moreover, we design some further research about feature embedding gate. First, we conduct some experiments to compare parameter sharing mechanism of gate('field sharing' and 'field private') in <ref type="table" target="#tab_1">Table 2</ref>.  <ref type="table" target="#tab_1">Table 2</ref>, we can find that the performance of 'field private' gate is much better than the 'field sharing' gate for many base models on ICME dataset while it is not significant on Criteo dataset. Although the 'field sharing' can reduce the number of learned parameters, the performance also decreases. These results indicate that the performance of different parameter sharing mechanisms of gate depend on specific task. On the whole, it is a better choice to choose the 'field private' in our experiments.</p><p>Second, we conduct some experiments to explore the vector-wise and bit-wise feature embedding gate. The results in <ref type="table">Table 3</ref> show <ref type="table">Table 3</ref>: Embedding gate mechanism: vector-wise vs bit-wise. The default gate share mechanism is 'field private'. that bit-wise is a little better than vector-wise on Criteo dataset, while we cannot draw an obvious conclusion on the ICME data. The reason behind this needs further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICME</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance of Hidden Gate(RQ2)</head><p>In this subsection, the overall performance gains of chosen baseline models after inserting hidden gate into a typical MLP layer will be reported on these three test sets in <ref type="table" target="#tab_3">Table 4</ref>. Replacing the traditional MLP with the hidden gate layer, our proposed hidden gate mechanisms consistently enhance these baseline models and achieve performance improvements on the ICME, Criteo and SafeDriver dataset as shown in <ref type="table" target="#tab_3">Table 4</ref>. The experimental results indicate that the hidden gate helps the model to implicitly capture the high-order interaction more effectively.</p><p>Although applying hidden gate to MLP layers is simple, it is an effective way to improve the performance of baseline models. Therefore, we conduct experiments to compare hidden gate DNN with some complex base models in the <ref type="table" target="#tab_4">Table 5</ref>.</p><p>From the <ref type="table" target="#tab_4">Table 5</ref>, the standard DNN by inserting hidden gate outperforms some canonical deep learning models such as DeepFM, XDeepFM. It is a simple way to enhance the standard DNN to gain As mentioned previously, we find the feature embedding gate and hidden gate can enhance the model ability and gain good performance, respectively. Can we combine the feature embedding gate and hidden gate in one model to achieve further performance? We conduct some experiments to answer this research question on Criteo and ICME datasets. It can be seen from <ref type="table" target="#tab_5">Table 6</ref> that combining feature embedding gate and hidden gate in one model can not gain further performance improvements. Specifically, there is not much performance improvements on Criteo and some performance decrease on ICME. The feature embedding gate can influence the implicit and explicit feature interaction while the hidden gate can influence the implicit feature interaction, we assume that the implicit feature interactions have been done twice and the implicit feature representations are damaged. The real reason behind this need to conduct further experiments to justify this assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Study(RQ4)</head><p>We conduct some experiments to study the influence of hyperparameter in our proposed gate mechanisms. We test different settings in our proposed GateNet on the SafeDriver dataset and we treat DeepFM, DeepFM e and DeepFM h as the baseline models.</p><p>So we divide the hyper-parameters into the following three parts:</p><p>? Gate activation function. Both embedding and hidden gate include the gate activation functions.</p><p>? Embedding size. We change the embedding size from 10 to 50, and compare the performance of baseline model with embedding gate model. ? Hidden layers. We change the number of layers from 2 to 6, and observe the performance of baseline model and hidden gate model.  <ref type="table" target="#tab_6">Table 7</ref>. We observe that the best activation function in feature embedding gate is linear while the best activation function is Tanh in hidden gate. We change the embedding size from 10 to 50 in feature embedding gate and summarize the range of performances in <ref type="table" target="#tab_7">Table 8</ref>. From the results, we find that embedding size has little influence on the GateNet. Specifically, the standard DeepFM has a good performance with the embedding size 20, while the embedding size of DeepFM e is 10. Therefore, these results show that DeepFM e requires less parameter than DeepFM to train a good model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Number of Layers in Hidden</head><p>Gate. In deep part, we can change the number of neurons per layer, depths of DNN, activation functions and dropout rates. For brevity, we just study the impact of different depths in DNN part. We change the number of layers from 2 to 6 in hidden gate and conclude the performance in <ref type="table" target="#tab_8">Table 9</ref>.  Increasing the number of layers, the performance of DeepFM increases, while DeepFM h decreases. These results indicate that our DeepFM h can learn much better than DeepFM with less parameters on SafeDriver dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Recently, many neural network based CTR models have been proposed and some recent research results found that gating mechanisms can improve the trainability in training non-convex deep neural networks. Inspired by these observations, we proposed a novel model named GateNet which introduces either the feature embedding gate or the hidden gate to the embedding layer or hidden layers of DNN CTR models,respectively. Extensive experiments conducted on three real-world datasets demonstrate its effectiveness to boost the performance of various state-of-the-art models such as FM, DeepFM and xDeepFM on three real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The Feature Embedding Gate. The left diagram represents vector-wise feature embedding gate and the right diagram is bit-wise feature embedding gate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The overall performance improvements of baseline models after inserting feature embedding gate into a typical embedding layer. Unless specially mentioned in our paper, 'field private' and 'vec-wise' model are used as the default setting in the embedding gate model. The suffix with 'e' stands for applying the embedding gate to this model.</figDesc><table><row><cell>Model</cell><cell>ICME Criteo SafeDriver</cell></row><row><cell>FM</cell><cell>0.8696 0.7923 0.6302</cell></row><row><cell>FM e</cell><cell>0.8973 0.7970 0.6327</cell></row><row><cell>?</cell><cell>0.0277 0.0047 0.0025</cell></row><row><cell>DNN</cell><cell>0.8912 0.8067 0.6344</cell></row><row><cell>DNN e</cell><cell>0.9166 0.8096 0.6359</cell></row><row><cell>?</cell><cell>0.0254 0.0029 0.0015</cell></row><row><cell>DeepFM</cell><cell>0.9027 0.8087 0.6276</cell></row><row><cell>DeepFM e</cell><cell>0.9097 0.8097 0.6349</cell></row><row><cell>?</cell><cell>0.0070 0.0010 0.0073</cell></row><row><cell>XDeepFM</cell><cell>0.9052 0.8091 0.6324</cell></row><row><cell cols="2">XDeepFM e 0.9178 0.8098 0.6336</cell></row><row><cell>?</cell><cell>0.0126 0.0007 0.0012</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Parameter sharing mechanism of feature embedding gate: field private vs field sharing.</figDesc><table><row><cell></cell><cell>ICME</cell><cell>Criteo</cell></row><row><cell>Model</cell><cell cols="2">Private Share Private Share</cell></row><row><cell>FM</cell><cell cols="2">0.8973 0.8861 0.7970 0.7957</cell></row><row><cell>DNN</cell><cell cols="2">0.9166 0.9076 0.8096 0.8099</cell></row><row><cell>DeepFM</cell><cell cols="2">0.9097 0.8985 0.8097 0.8098</cell></row><row><cell cols="3">XDeepFM 0.9178 0.9039 0.8098 0.8096</cell></row><row><cell>From the</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The overall performance improvements of baseline models with hidden gates. The suffix with 'h' stands for applying the hidden gate to this model.</figDesc><table><row><cell>Model</cell><cell>ICME Criteo SafeDriver</cell></row><row><cell>DNN</cell><cell>0.8912 0.8067 0.6344</cell></row><row><cell>DNN h</cell><cell>0.9105 0.8093 0.6348</cell></row><row><cell>?</cell><cell>0.0193 0.0026 0.0004</cell></row><row><cell>DeepFM</cell><cell>0.9027 0.8087 0.6276</cell></row><row><cell>DeepFM h</cell><cell>0.9121 0.8090 0.6324</cell></row><row><cell>?</cell><cell>0.0094 0.0003 0.0048</cell></row><row><cell>XDeepFM</cell><cell>0.9052 0.8091 0.6324</cell></row><row><cell cols="2">XDeepFM h 0.9084 0.8092 0.6344</cell></row><row><cell>?</cell><cell>0.0032 0.0001 0.0020</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Compare the performance between standard DNN by inserting hidden gate and other base models. The Safe denotes SafeDrive dataset and XDFM represents XDeepFM.</figDesc><table><row><cell cols="2">Dataset DNN</cell><cell cols="3">DeepFM XDFM FiBiNet DNN h</cell></row><row><cell>Criteo</cell><cell cols="2">0.8063 0.8087</cell><cell>0.8091 0.8102</cell><cell>0.8093</cell></row><row><cell>ICME</cell><cell cols="2">0.8912 0.9027</cell><cell>0.9052 0.9030</cell><cell>0.9105</cell></row><row><cell>Safe</cell><cell cols="2">0.6344 0.6276</cell><cell>0.6324 0.6342</cell><cell>0.6348</cell></row><row><cell cols="5">improvement, which makes the DNN model much more practicable</cell></row><row><cell cols="4">in industrial recommendation systems.</cell></row><row><cell cols="5">4.4 Performance of model Combining FE-Gate</cell></row><row><cell cols="4">and Hidden Gate(RQ3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The overall performance of baseline models with feature embedding gate and hidden gate models.The 'EGate', 'HGate', 'Both' denote the feature embedding gate, hidden gate and feature embedding gate plus hidden gate, respectively.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell>Base</cell><cell>EGate HGate Both</cell></row><row><cell></cell><cell>DNN</cell><cell cols="2">0.8912 0.9166 0.9195 0.9054</cell></row><row><cell>ICME</cell><cell>DeepFM</cell><cell cols="2">0.9027 0.9097 0.9121 0.9114</cell></row><row><cell></cell><cell cols="3">XDeepFM 0.9052 0.9178 0.9084 0.9054</cell></row><row><cell></cell><cell>DNN</cell><cell cols="2">0.8067 0.8096 0.8093 0.8097</cell></row><row><cell>Criteo</cell><cell>DeepFM</cell><cell cols="2">0.8087 0.8097 0.8090 0.8097</cell></row><row><cell></cell><cell cols="3">XDeepFM 0.8091 0.8098 0.8092 0.8098</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The overall performance of different activation functions in the feature embedding gate and hidden gate. Activation function in Gate. The test results on SafeDriver dataset with different activation functions in the feature embedding gate and hidden gate are presented in</figDesc><table><row><cell>Model</cell><cell>Linear Relu</cell><cell cols="2">Sigmoid Tanh</cell></row><row><cell cols="3">DeepFM e 0.6356 0.6343 0.6320</cell><cell>0.6349</cell></row><row><cell cols="3">DeepFM h 0.6321 0.6320 0.6311</cell><cell>0.6324</cell></row><row><cell>4.5.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The performance of different embedding sizes in Feature Embedding gate. DeepFM e 0.6349 0.6322 0.6319 0.6329 0.6307 4.5.2 Embedding Size in Feature Embedding Gate.</figDesc><table><row><cell>Model</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>DeepFM</cell><cell cols="5">0.6276 0.6297 0.6271 0.6284 0.6235</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>The performance of different number of layers in DNN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://labs.criteo.com/downloads/ 2 https://biendata.com/competition/icmechallenge2019 3 https://www.kaggle.com/c/porto-seguro-safe-driver-prediction 4 TensorFlow: https://www.tensorflow.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:cs.CL/1406.1078</idno>
		<title level="m">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><forename type="middle">Quinonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Omnipress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on RecSys</title>
		<meeting>the 13th ACM Conference on RecSys</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05170</idno>
		<title level="m">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09217</idno>
		<title level="m">Hierarchical Gating Networks for Sequential Recommendation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
