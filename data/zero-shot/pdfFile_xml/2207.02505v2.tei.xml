<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pure Transformers are Powerful Graph Learners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Dat Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwoo</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Seunghoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename></persName>
						</author>
						<title level="a" type="main">Pure Transformers are Powerful Graph Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly better results compared to GNN baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt. * Work done during an internship at LG AI Research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Transformer <ref type="bibr" target="#b67">[68]</ref> has served as a versatile architecture in a broad class of machine learning problems, such as natural language processing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref>, computer vision <ref type="bibr" target="#b17">[18]</ref>, and reinforcement learning <ref type="bibr" target="#b8">[9]</ref>, to name a few. It is because the fully-attentional structure of Transformer is general and powerful enough to take, process, and relate inputs and outputs of arbitrary structures, eliminating a need for data-and task-specific inductive bias to be baked into the network architecture. Combined with large-scale training, it opens up a new chapter for building a versatile model that can solve a wide range of problems involving diverse data modalities and even a mixture of modalities <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>In graph learning domain, inspired by the breakthroughs, multiple works tried combining selfattention into graph neural network (GNN) architecture where message passing was previously dominant <ref type="bibr" target="#b49">[50]</ref>. As global self-attention across nodes cannot reflect the graph structure, however, these methods introduce graph-specific architectural modifications. This includes restricting self-attention to local neighborhoods <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b18">19]</ref>, using global self-attention in conjunction with message-passing GNN <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b33">34]</ref>, and injecting edge information into global self-attention via attention bias <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54]</ref>. Despite decent performance, such modifications can be a limiting constraint in terms of versatility, especially considering future integration to multi-task and multi-modal general-purpose attentional architectures <ref type="bibr" target="#b30">[31]</ref>. In addition, deviating from pure self-attention, these methods may inherit the issues of message-passing such as oversmoothing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">52]</ref>, and become incompatible with useful engineering techniques e.g., linear attention <ref type="bibr" target="#b64">[65]</ref> developed for standard self-attention.</p><p>Instead, we explore the opposite direction of applying a standard Transformer directly for graphs. For this, we treat all nodes and edges as independent tokens, augment them with appropriate token-wise embeddings, and feed the tokens as input to the standard Transformer. The model operates identically to Transformers used in language and vision; each node or edge is treated as a token, identical to the words in a sentence or patches of an image <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b17">18]</ref>. Perhaps surprisingly, we show that this simple approach yields a powerful graph learner both in theory and practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Graph</head><p>Node and Edge Tokens with Token-wise Embedding . We treat all nodes and edges of an input graph as independent tokens, augment them with orthonormal node identifiers and trainable type identifiers, and feed them to a standard Transformer encoder. For graph-level prediction, we follow the common practice <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> of using an extra trainable [graph] token.</p><p>As a key theoretical result, we prove that with appropriate token-wise embeddings, self-attention over the node and edge tokens can approximate any permutation equivariant linear operator on a graph <ref type="bibr" target="#b46">[47]</ref>.</p><p>Remarkably, we show that a very simple choice of embedding composed of node identifiers and type identifiers is sufficient for accurate approximation. This provides a solid theoretical guarantee that, with the embeddings and enough attention heads, a Transformer is at least as expressive as a second-order invariant graph network (2-IGN) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b33">34]</ref>, which is already more expressive than all message-passing GNNs <ref type="bibr" target="#b20">[21]</ref>. This also immediately grants the model with the expressive power at least as good as the 2-dimensional Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b45">[46]</ref>, which is often sufficient for real-world graph data <ref type="bibr" target="#b82">[83]</ref>. We further extend our theoretical result to hypergraphs with order-k hyperedges, showing that a Transformer with order-k generalized token embeddings is at least as expressive as k-IGN and, consequently k-WL test.</p><p>We test our model, named Tokenized Graph Transformer (TokenGT), mainly on the PCQM4Mv2 large-scale quantum chemical property prediction dataset containing 3.7M molecular graphs <ref type="bibr" target="#b26">[27]</ref>. Even though TokenGT involves minimal graph-specific architectural modifications, it performs significantly better than all GNN baselines, showing that the advantages of Transformer architecture combined with large-scale training surpass the benefit of hard inductive bias of GNNs. Furthermore, TokenGT achieves competitive performance compared to Transformer variants with strong graphspecific modifications <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54]</ref>. Finally, we demonstrate that TokenGT can naturally utilize efficient approximations in Transformers in contrast to these variants, using kernel attention <ref type="bibr" target="#b10">[11]</ref> that enables linear computation cost without much degradation in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tokenized Graph Transformer (TokenGT)</head><p>In this section, we present the Tokenized Graph Transformer (TokenGT), a pure Transformer architecture for graphs with token-wise embeddings composed of node identifiers and type identifiers ( <ref type="figure" target="#fig_0">Figure 1</ref>). Our goal in this section is to provide a practical overview -for theoretical analysis of the architecture, we guide the readers to Section 3.</p><p>Let G = (V, E) an input graph with n nodes V = {v 1 , ..., v n } and m edges E = {e 1 , ..., e m } ? V 2 , associated with features X V ? R n?C and X E ? R m?C , respectively. We treat each node and edge as an independent token (thus (n + m) tokens in total) and construct their features by X = [X V ; X E ] ? R (n+m)?C . A na?ve way to process a graph is to directly provide the tokens X as input to a Transformer, but it is inappropriate as graph connectivity is discarded. To thoroughly represent graph structure, we augment the tokens X with token-wise embeddings, more specifically orthonormal node identifiers used for representing the connectivity of the tokens and trainable type identifiers that encode whether a token is a node or an edge. Despite the simplicity, we show that a Transformer applied on these embeddings is a theoretically powerful graph learner.</p><p>Node Identifiers The first component of token-wise embedding is the orthonormal node identifier that we use to represent the connectivity structure given in the input graph.</p><p>For a given input graph G = (V, E), we first produce n node-wise orthonormal vectors P ? R n?dp that we refer to as node identifiers. Then, we augment the tokens X with node identifiers as follows.</p><p>? For each node v ? V, we augment the token X v as [X v , P v , P v ].</p><p>? For each edge (u, v) ? E, we augment the token X <ref type="bibr">(u,v)</ref> as [X <ref type="bibr">(u,v)</ref> , P u , P v ].</p><p>Intuitively, a Transformer operating on the augmented tokens can fully recognize the connectivity structure of the graph since comparing the node identifiers between a pair of tokens reveals their incidence information. For instance, we can tell if an edge e = (u, v) is connected with a node k through dot-product (attention) since [P u , P v ][P k , P k ] = 1 if and only if k ? (u, v) and 0 otherwise. This allows the Transformer to identify and exploit the connectivity structure of a graph, for instance by putting more weights on incident pairs when the local operation is important.</p><p>Notably, as the node identifiers P are only required to be orthonormal, we have a large degree of freedom in implementation choices. We outline two practical methods below as examples. Their implementation details can be found in Appendix A.3.1.</p><p>? Orthogonal random features (ORFs), e.g., rows of random orthogonal matrix Q ? R n?n obtained with QR decomposition of random Gaussian matrix G ? R n?n <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b11">12]</ref>. ? Laplacian eigenvectors obtained from eigendecomposition of graph Laplacian matrix, i.e., rows of U from ? = I?D ?1/2 AD ?1/2 = U ?U, where A ? R n?n is adjacency matrix, D is degree matrix, and ?, U correspond to eigenvalues and eigenvectors respectively <ref type="bibr" target="#b19">[20]</ref>.</p><p>Among the two methods, node identifiers generated as ORFs do not encode any information about the graph structure as they are entirely random. This means the Transformer that operates on the ORF-based node identifiers needs to compile and recognize graph structure only from the incidence information provided by the node identifiers. Although this is challenging, perhaps surprisingly, we empirically show in Section 5 that Transformers are strong enough to learn meaningful structural representations out of ORF-based node identifiers and outperform GNNs on large-scale task.</p><p>In contrast to ORFs, Laplacian eigenvectors provide a kind of graph positional embeddings (graph PEs) that describes the distance between nodes on a graph. Due to the positional information, it yields better performance compared to ORFs in our experiments in Section 5. One interesting aspect of Laplacian eigenvectors is that they can be viewed as a generalization of sinusoidal positional embeddings of NLP Transformers to graphs, as the eigenvectors of 1D chain graphs are sine and cosine functions <ref type="bibr" target="#b19">[20]</ref>. Thus, by choosing Laplacian eigenvectors as node identifiers, our approach can be interpreted as a direct extension of the NLP Transformer for inputs involving relational structures.</p><p>Type Identifiers The second component of token-wise embedding is the trainable type identifier that encodes whether a token is node or edge. For a given input graph G = (V, E), we first prepare a trainable parameter matrix E = [E V ; E E ] ? R 2?de that contains two type identifiers E V and E E for nodes and edges respectively. Then, we further augment the tokens with type identifiers as follows.</p><p>? For each node v ? V, we augment the token</p><formula xml:id="formula_0">[X v , P v , P v ] as [X v , P v , P v , E V ]. ? For each edge (u, v) ? E, we augment the token [X (u,v) , P u , P v ] as [X (u,v) , P u , P v , E E ].</formula><p>These embeddings provide information on whether a given token is a node or an edge, which is critical, e.g., when an attention head tries to attend specifically to node tokens and ignore edge tokens.</p><p>Main Transformer With node identifiers and type identifiers, we obtain augmented token features X in ? R (n+m)?(C+2dp+de) , which is further projected by a trainable matrix w in ? R (C+2dp+de)?d to be an input to Transformer. For graph-level prediction, we prepend a special token [graph] with trainable embedding X [graph] ? R d similar to BERT <ref type="bibr" target="#b16">[17]</ref> and ViT <ref type="bibr" target="#b17">[18]</ref>. We utilize the feature of [graph] token at the output of the encoder as the graph representation, on which a linear prediction head is applied to produce the final graph-level prediction. Overall, the tokens Z (0) = [X <ref type="bibr">[graph]</ref> ; X in w in ] ? R (1+n+m)?d are used as the input to the main encoder. As an encoder, we adopt the standard Transformer <ref type="bibr" target="#b67">[68]</ref>, which is an alternating stack of multihead self-attention layers (MSA) and feedforward MLP layers. We provide further details in Appendix A.1.1.</p><p>Inductive Bias Similar to Transformers in language and vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, Tokenized Graph Transformer treats input nodes and edges as independent tokens and applies self-attention to them. This approach leads to much less inductive bias than current GNNs, where the sparse graph structure, or more fundamentally, permutation symmetry of graphs is deliberately baked into each layer <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref>. For TokenGT, such information is provided entirely as a part of input using token-wise embeddings, and the model has to learn how to interpret and utilize the information from data. Although such weak inductive bias might raise questions on the expressiveness of the model, our theoretical analysis in Section 3 shows that TokenGT is a powerful graph learner thanks to the token-wise embeddings and expressive power of self-attention. For example, we show that TokenGT is more expressive than all message-passing GNNs under the framework of Gilmer et al. (2017) <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Analysis</head><p>We now present our theory. Our key result is that TokenGT, a standard Transformer with node and type identifiers presented in Section 2, is provably at least as expressive as the second-order Invariant Graph Network (2-IGN <ref type="bibr" target="#b46">[47]</ref>), which is built upon all possible permutation equivariant linear layers on a graph. This provides solid theoretical guarantees for TokenGT, such as being at least as powerful as the 2-WL graph isomorphism test and more expressive than all message-passing GNNs. Our theory is based on a general framework on hypergraphs represented as higher-order tensors, which leads to the formulation of order-k TokenGT that is at least as expressive as order-k IGN (k-IGN <ref type="bibr" target="#b46">[47]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary: Permutation Symmetry and Invariant Graph Networks</head><p>Representing and Processing Sets and (Hyper)Graphs For a set of n nodes, we often represent their features as X ? R n?d where X i ? R d is the feature of the i-th node. The set is unordered and, therefore, should be treated invariant to the renumbering of the nodes. Let S n the symmetric group or the group of permutations ? on [n] = {1, ..., n}. By ? ? X we denote permuting rows of X with ?, i.e., (? ? X) i = X ? ?1 (i) . Here, X and ? ? X represent the identical set for all ? ? S n .</p><p>Generally, we consider (hyper)graphs represented as order-k tensor X ? R n k ?d with feature X i = X i1,...,i k ? R d attached to (hyper)edge represented as multi-index i = (i 1 , ..., i k ) ? [n] k . Similar to sets, the tensor should be treated invariant to node renumbering by any ? ? S n that acts on X by (? ? X) i = X ? ?1 (i) where ? ?1 (i) = (? ?1 (i 1 ), ..., ? ?1 (i k )). That is, X and ? ? X represent the identical (hyper)graph for all ?. Due to such symmetry, to build a function F (X) ? T for tensor X and target T , a suitable way is to make them invariant F (? ? X) = F (X) when the target is a vector or equivariant F (? ? X) = ? ? F (X) when the target is also a tensor, for all X ? R n k ?d and ? ? S n .</p><p>In our theoretical analysis, we work on order-k dense tensor representation X ? R n k ?d of a graph as they can represent node features (k = 1), edge features (k = 2), or hyperedge features (k &gt; 2) in a unified manner. This is interchangeable but slightly different from the sparse representation of a graph with edge set E used in Section 2. Nevertheless, in Section 5 we empirically verify that our key theoretical findings work equally well for dense and sparse graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant Graph Network</head><p>We mainly develop our theoretical analysis upon Invariant Graph Networks (IGNs) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref>, a family of expressive graph networks derived from the permutation symmetry of tensor representation of graphs. Here we provide a summary. In general, we define:</p><formula xml:id="formula_1">Definition 1.</formula><p>An order-k Invariant Graph Network (k-IGN) is a function F k : R n k ?d0 ? R written as the following:</p><formula xml:id="formula_2">F k = MLP ? L k?0 ? L (T ) k?k ? ? ? ... ? ? ? L (1) k?k ,<label>(1)</label></formula><p>where each L (t) k?k is equivariant linear layer <ref type="bibr" target="#b46">[47]</ref> from R n k ?dt?1 to R n k ?dt , ? is activation function, and L k?0 is a invariant linear layer from R n k ?d T to R.</p><p>A body of previous work have shown appealing theoretical properties of k-IGN, including universal approximation <ref type="bibr" target="#b47">[48]</ref> and alignment to k-Weisfeiler-Lehman (k-WL) graph isomorphism test <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, it is known that k-IGNs are theoretically at least as powerful as the k-WL test <ref type="bibr" target="#b45">[46]</ref>. It is also known that 2-IGNs are already more expressive <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b33">34]</ref> than all message-passing GNNs under the framework of <ref type="bibr" target="#b20">Gilmer et al. (2017)</ref>  <ref type="bibr" target="#b20">[21]</ref>.</p><p>The core building block of IGN is invariant and equivariant linear layers <ref type="bibr" target="#b46">[47]</ref> with maximal expressiveness while respecting node permutation symmetry. The layers are defined as follows: Definition 2. An equivariant linear layer is a function L k?l : R n k ?d ? R n l ?d written as follows for order-k input X ? R n k ?d :</p><formula xml:id="formula_3">L k?l (X) i = ? j B ? i,j X j w ? + ? C ? i b ? ,<label>(2)</label></formula><p>where i ? [n] l , j ? [n] k are multi-indices, w ? ? R d?d , b ? ? R d are weight and bias parameters, and B ? ? R n l+k and C ? ? R n l are binary basis tensors corresponding to order-(l + k) and order-l equivalence classes ? and ?, respectively. Invariant linear layer is a special case of L k?l with l = 0.</p><p>We provide the definition of the equivalence classes and basis tensors in Appendix A.1.1. For now, it is sufficient to know that the basis tensors are binary tensors that form the orthogonal basis of the full space of linear equivariant layers. In general, in Eq. (2) it is known that there exists bell(k + l) number of basis tensors B ? for the weight and bell(l) number of basis tensors C ? for the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Can Self-Attention Approximate Equivariant Basis?</head><p>Now, we present an intuition that connects Transformer (Section 2) and equivariant linear layer (Definition 2). For that, we write out the multihead self-attention layer as follows:</p><formula xml:id="formula_4">MSA(X) i = H h=1 j ? h ij X j w V h w O h where ? h = softmax Xw Q h (Xw K h ) ? d H ,<label>(3)</label></formula><p>where H is number of heads, d H is head size, and</p><formula xml:id="formula_5">w Q h , w K h ? R d?d H , w V h ? R d?dv w O h ? R dv?d .</formula><p>Our intuition is that the weighted sum of values with self-attention matrix ? h in Eq. (3) is analogous to the masked sum with basis tensor B ? in Eq. (2) up to normalization. This naturally leads to the following question: for a given equivariant layer L k?k : R n k ?d ? R n k ?d , can we use a Transformer layer with multihead self-attention MSA : R N ?d ? R N ?d with N = n k to accurately approximate L k?k by having H = bell(2k) attention heads approximate each equivariant basis B ? ?</p><p>We show that this can be possible, but only if we provide appropriate auxiliary information to input. For example, let us consider first-order layer L 1?1 . The layer has bell(2) = 2 basis tensors B ?1 = I and B ?2 = 11 ? I for the weight, and bell(1) = 1 basis tensor C ?1 = 1 for the bias. Given an input set X ? R n?d it computes the following with w 1 , w 2 ? R d?d , b ? R d :</p><formula xml:id="formula_6">L 1?1 (X) = IXw 1 + (11 ? I)Xw 2 + 1b .<label>(4)</label></formula><p>Now consider approximating basis tensor B ?1 = I with an attention matrix ? 1 . The approximation is accurate when i-th query always only attends to i-th key and ignores the rest. To achieve the attention structure consistently, i.e., agnostic to input X, we need to provide auxiliary input that self-attention can "latch onto" to faithfully approximate ? 1 ? I. Without this, attention must entirely rely on the inputs X, which is unreliable and can lead to approximation failure, e.g., when X has repeated rows.</p><p>For the auxiliary information, we prepare n node-wise orthonormal vectors P ? R n?dp (note that this is identical to node identifiers in Section 2), and augment the input to X in = [X, P] ? R n?(d+dp) . Let us assume that the query and key projections in Eq. (3) ignore X and only leave P scaled by ? a with a &gt; 0. Then attention matrix is computed as ? 1 = softmax(S) where S ij = aP i P j . Here, due to the orthonormality of P, we have P i P j = 1 only if i = j and otherwise 0, which leads to S = aI. With a ? ? by scaling up the query and key projection weights, the softmax becomes arbitrarily close to the hardmax operator, and we obtain the following:</p><formula xml:id="formula_7">? 1 = softmax(aI) ? I as a ? ?.<label>(5)</label></formula><p>Thus, self-attention can utilize the auxiliary information P to achieve an input-agnostic approximation of ? 1 to I. Notably, we can achieve a similar approximation for B ?2 = 11 ? I using the same P by flipping the sign of keys, which gives ? 2 = softmax(?aI) due to orthonormality. By sending a ? ?, now attention from the i-th query to the i-th key is suppressed, and we obtain the following:</p><formula xml:id="formula_8">? 2 = softmax (?aI) ? 1 n ? 1 (11 ? I) as a ? ?.<label>(6)</label></formula><p>Note that this approximation is accurate only up to row normalization as rows of ? 2 always sum to one due to softmax, while B ?2 = 11 ? I is binary. In our proofs of the theoretical results, we perform appropriate denormalization with MLP after MSA to achieve an accurate approximation.</p><p>Overall, we see that simple auxiliary input P suffices for two attention heads to approximate the equivariant basis of L 1?1 accurately. We now question the following. Given appropriate auxiliary information as input, can a Transformer layer with bell(2k) attention heads accurately approximate L k?k by having each head approximate each equivariant basis B ? ? What would be the sufficient auxiliary input? We answer the question by showing that, with (order-k generalized) node and type identifiers presented in Section 2, Transformer layers can accurately approximate equivariant layers L k?k via input-agnostic head-wise approximation of each equivariant basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pure Transformers are Powerful Graph Learners</head><p>We now present our main theoretical results that extend the discussions in Section 3.2 to any order k. Note that k = 2 corresponds to TokenGT for graphs presented in Section 2. With k &gt; 2, we naturally extend TokenGT to hypergraphs. All proofs can be found in Appendix A.1.</p><p>We first introduce generalized node and type identifiers (Section 2) for order-k tensors X ? R n k ?d . We define the node identifier P ? R n?dp as an orthonormal matrix with n rows, and the type identifier as a trainable matrix E ? R bell(k)?de that contains bell(k) rows E ?1 , ..., E ? bell(k) , each of which is designated for an order-k equivalence class ?. Then, we augment each entry of input tensor</p><formula xml:id="formula_9">as [X i1,...,i k , P i1 , ..., P i k , E ? ] where (i 1 , ..., i k ) ? ?.</formula><p>Let us exemplify. For k = 1 (sets), each i-th entry is augmented as</p><formula xml:id="formula_10">[X i , P i , E ?1 ], consistent with our discussion in Section 3.2. For k = 2 (graphs), each (i, i)-th entry is augmented as [X ii , P i , P i , E ?1 ] and each (i, j)-th entry (i = j) is augmented as [X ij , P i , P j , E ?2 ]</formula><p>. This is consistent with TokenGT in Section 2, which augments nodes with E V = E ?1 and edges with E E = E ?2 .</p><p>With node and type identifiers, we obtain augmented order-k tensor X in ? R n k ?(d+kdp+de) . We use a trainable projection w in ? R (d+kdp+de)?d T to map them to hidden dimension d T of a Transformer. We now show that self-attention on X in w in can accurately approximate equivariant basis:</p><formula xml:id="formula_11">Lemma 1.</formula><p>For all X ? R n k ?d and their augmentation X in , self-attention coefficients ? h (Eq. <ref type="formula" target="#formula_4">(3)</ref>) computed with X in w in can approximate any basis tensor B ? ? R n 2k of order-k equivariant linear layer L k?k (Definition 2) to arbitrary precision up to normalization.</p><p>Consequently, with the node and type identifiers, a collection of bell(2k) attention heads can approximate the collection of all basis tensors of order-k equivariant layer. This leads to the following:</p><formula xml:id="formula_12">Theorem 1.</formula><p>For all X ? R n k ?d and their augmentation X in , a Transformer layer with bell(2k) self-attention heads that operates on X in w in can approximate an order-k equivariant linear layer L k?k (X) (Definition 2) to arbitrary precision.</p><p>While the approximation in Lemma 1 is only accurate up to normalization over inputs (keys) due to softmax normalization, for the approximation in Theorem 1 we perform appropriate denormalization using MLP after multihead self-attention and can obtain an accurate approximation.</p><p>By extending the result to multiple layers, we arrive at the following:</p><formula xml:id="formula_13">Theorem 2.</formula><p>For all X ? R n k ?d and their augmentation X in , a Transformer composed of T layers that operates on X in w in followed by sum-pooling and MLP can approximate an k-IGN F k (X) (Definition 1) to arbitrary precision.</p><p>This directly leads to the following corollary: Corollary 1. A Transformer on node and type identifiers in Theorem 2 is at least as expressive as k-IGN composed of order-k equivariant linear layers.</p><p>Corollary 1 allows us to draw previous theoretical results on the expressiveness of k-IGN <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34]</ref> and use them to lower-bound the provable expressiveness of a standard Transformer: Corollary 2. A Transformer on node and type identifiers in Theorem 2 is at least as powerful as k-WL graph isomorphism test and is more expressive than all message-passing GNNs within the framework of Gilmer et al. (2017) <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>We outline relevant work including equivariant neural networks, theory on expressive power of Transformers and their connection to modeling equivariance, and Transformers for graphs.</p><p>Equivariant Neural Networks A machine learning task is often invariant or equivariant to specific symmetry of input data, e.g., image classification is invariant to the translation of an input image. A large body of literature advocated baking the invariance or equivariance into a neural network as a type of inductive bias (e.g., translation equivariance of image convolution), showing that it reduces the number of parameters and improves generalization for a wide range of learning tasks involving various geometric structures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>.   <ref type="bibr" target="#b55">[56]</ref> showed that any equivariant layer for discrete group actions is equivalent to a specific parameter sharing structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expressive Power of Transformers and Its Connection to Equivariance Recent work involving</head><p>Transformers often focus on minimizing the domain-and task-specific inductive bias and scaling the model and data so that any useful computation structure can be learned <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref>. The success of this approach is, to some degree, attributed to the high expressive power of Transformers that allows learning diverse functions suited for the data at hand <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41]</ref>. Recent theory has shown that Transformers are expressive enough to even model certain equivariant functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>.  <ref type="bibr" target="#b33">[34]</ref> showed that Transformers can model equivariant linear layers for sets <ref type="bibr" target="#b81">[82]</ref>, which can be viewed as the first-order case of our theory (see Section 3.2). To our knowledge, our work is the first to show that standard Transformers are expressive enough to provably model maximally expressive equivariant layers and k-IGN for (hyper)graphs with k ? 2.</p><p>Transformers for Graphs Unlike in language and vision, developing Transformers for graphs is challenging due to (1) the presence of edge connectivity and (2) the absence of canonical node ordering that prevents adopting simple positional encodings <ref type="bibr" target="#b49">[50]</ref>. To incorporate the connectivity of edges, early methods restricted self-attention to local neighborhoods (thus reducing to messagepassing) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b68">69]</ref> or used global self-attention with auxiliary message-passing modules <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b42">43]</ref>. As message-passing suffers from limited expressive power <ref type="bibr" target="#b76">[77]</ref> and oversmoothing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">52]</ref>, recent works often discard them and use global self-attention on nodes with heuristic modifications to process edges <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. <ref type="bibr">Ying et al. (2021)</ref>  <ref type="bibr" target="#b77">[78]</ref> proposed to inject edge encoding based on shortest paths through self-attention bias. <ref type="bibr">Kreuzer et al. (2021)</ref>  <ref type="bibr" target="#b37">[38]</ref> proposed to incorporate edges into self-attention matrix via elementwise multiplication. On the contrary, we leave the self-attention unmodified and provide both nodes and edges with certain token-wise embeddings (Section 2) as its input. To incorporate graph structure into nodes, on the other hand, some approaches focus on developing graph positional encoding, e.g., based on Laplacian eigenvectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38]</ref>. While these can be directly incorporated into our work via auxiliary node identifiers for better performance, we leave this as future work. We further note that current graph Transformers that utilize Laplacian positional encoding rely heavily on heuristic edge encoding <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> while ours does not. Another closely related approach is the Higher-order Transformer <ref type="bibr" target="#b33">[34]</ref> which generalizes k-IGN with masked self-attention. While it is highly complex to implement due to hard-coded head-wise equivariant masks, our method can be implemented effortlessly using any available implementation of standard Transformer. Furthermore, our method is more flexible as the model can choose to use different attention heads to focus on a specific equivariant operator (e.g., local propagation) if needed. We further discuss the difficulty in applying linear attention to graph Transformers in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first conduct a synthetic experiment that directly confirms our key claims in Lemma 1 (Section 3). Then, we empirically explore the capability of Tokenized Graph Transformer (TokenGT) (Section 2) using the PCQM4Mv2 large-scale quantum chemistry regression dataset <ref type="bibr" target="#b26">[27]</ref>. We further present experiments on transductive node classification datasets involving large graphs in Appendix A.4.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Approximating Second-Order Equivariant Basis</head><p>As in Theorem 1 and 2 (Section 3), our argument on the expressive power of TokenGT relies on its capability to approximate order-k permutation equivariant linear layers L k?k (Definition 2). Specifically, Lemma 1 states that such capability depends on the ability of each self-attention head ? 1 , ..., ? H (Eq. <ref type="formula" target="#formula_4">(3)</ref>) to accurately approximate each equivariant basis B ?1 , ..., B ? bell(2k) (Definition 2) up to normalization.</p><p>We verify this claim for k = 2 (second-order; graphs) in a synthetic setup using Barab?si-Albert random graphs. We use a multihead self-attention layer (Eq. <ref type="formula" target="#formula_4">(3)</ref> We outline the results in <ref type="table" target="#tab_0">Table 1</ref>. Consistent with Lemma 1, self-attention achieves accurate approximation of equivariant basis only when both the orthonormal node identifiers and type identifiers are given. Here, Laplacian eigenvectors (Lap, ) often yield slightly better results than orthogonal random features (ORF, ) presumably due to less stochasticity. Interestingly, we see that self-attention transfers the learned (pseudo-)equivariant self-attention structure to unseen graphs near perfectly.   Non-orthogonal random embeddings lead to inaccurate approximation (Random, ), highlighting the importance of orthogonality of node identifiers. The approximation is also inaccurate when we sample ORF P t independently for each token t (ORF (first-order), ) instead of using concatenated node identifiers [P u , P v ] for token <ref type="bibr">(u, v)</ref>. This supports our argument in Section 2 that the incidence information implicitly provided via node identifiers plays a key role in approximation.</p><p>In <ref type="figure" target="#fig_3">Figure 2</ref>, we provide a visualization of self-attention maps learned under various node and type identifier choices. Additional results can be found in Appendix A.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large-Scale Graph Learning</head><p>An exclusive characteristic of TokenGT is its minimal graph-specific inductive bias, which requires it to learn internal computation structure largely from data. As such models are commonly known to work well with large-scale data <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b17">18]</ref>, we explore the capability of TokenGT on the PCQM4Mv2 quantum chemistry regression dataset <ref type="bibr" target="#b26">[27]</ref>, one of the current largest with 3.7M molecular graphs.</p><p>For TokenGT, we use both node and type identifiers, and use main Transformer encoder configuration based on Graphormer <ref type="bibr" target="#b77">[78]</ref> with 12 layers, 768 hidden dimension, and 32 attention heads. We try both ORF and Laplacian eigenvector as node identifiers, and denote corresponding models as TokenGT (ORF) and TokenGT (Lap) respectively. As an ablation, we also experiment with the same Transformer without node and type identifiers, which we denote as Transformer.</p><p>Finally, we apply the kernel attention <ref type="bibr" target="#b10">[11]</ref> that approximates the attention computation to linear cost (TokenGT (Lap) + Performer). We use AdamW optimizer with (? 1 , ? 2 ) = (0.99, 0.999) and weight decay 0.1, and 60k learning rate warmup steps followed by linear decay over 1M iteration with batch size 1024. For fine-tuning, we use 1k warmup, 0.1M training steps, and cosine learning rate decay. We train the models on 8 RTX 3090 GPUs for 3 days. Further details are in Appendix A.3.3.</p><p>We provide the results in <ref type="table" target="#tab_2">Table 2</ref>. A standard Transformer on the node and edge tokens cannot recognize graph structure and shows low performance (0.2340 valid MAE). Yet, the picture changes as soon as we augment the tokens with node and type identifiers. Notably, TokenGT (ORF) achieves 0.0962 MAE, which is already better than all GNN baselines. This is a somewhat surprising result, as both ORF and the Transformer are not aware of graph structures. This implies Transformer is strong enough to learn to interpret and reason over the incidence structure of tokens provided only implicitly by the node and type identifiers. By further switching to Laplacian eigenvectors that encode position on graphs <ref type="bibr" target="#b19">[20]</ref>, we observe a performance boost to 0.0910 MAE, competitive to Transformers with sophisticated graph-specific modifications (e.g., shortest path-based spatial encoding <ref type="bibr" target="#b77">[78]</ref>). While such methods inject graph structure into attention matrix via bias term and therefore strictly require O(n 2 ) cost, TokenGT enables adopting kernelization for pure self-attention <ref type="bibr" target="#b10">[11]</ref>, resulting in TokenGT (Lap) + Performer with the best performance among O(n + m) models (0.0935 MAE). Further discussion on the empirical performance of TokenGT can be found in Appendix A.5.</p><p>While our theory in Section 3 guarantees that TokenGT can reduce to an equivariant layer by learning fixed equivariant basis at each attention head, in practice, it can freely utilize multihead self-attention to learn less restricted and more useful computation structure from data. To analyze such a structure, we compute the attention distance across heads and network depth by averaging pairwise token distances on a graph weighted by their attention scores <ref type="figure" target="#fig_5">(Figure 3</ref>). This distance is analogous to the number of hops in message-passing. In both TokenGT (ORF) and TokenGT (Lap), in the lowest layers, some heads attend globally over the graph while others consistently have small receptive fields (acting like a local message-passing operator). In deeper layers, the attention distances increase, and most heads attend globally. Interestingly, this behavior is highly consistent with Vision Transformers on image patches <ref type="bibr" target="#b17">[18]</ref>, suggesting that hybrid architectures based on convolution to aid ViT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b79">80]</ref> might also work well for graphs. While TokenGT (ORF) shows relatively consistent attention distance over heads, TokenGT (Lap) shows higher variance, implying that it learns more diverse attention patterns. Judging from the higher performance of TokenGT (Lap), this suggests that the graph structure information of the Laplacian eigenvector facilitates learning useful and diverse attention structures, which calls for future exploration of better node identifiers based on graph PEs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We showed that Transformers directly applied to graphs can work well in both theory and practice. In the theoretical aspect, we proved that with appropriate token-wise embeddings, a Transformer on node and edge tokens is at least as expressive as k-IGN and k-WL test, making it more expressive than all message-passing GNNs. For such token-wise embeddings, we showed that a combination of simple orthonormal node identifiers and trainable type identifiers suffices, which we also verified with a synthetic experiment. In an experiment with PCQM4Mv2 large-scale dataset, we show that Tokenized Graph Transformer (TokenGT) performs significantly better than all GNNs and is competitive with Transformer variants with strong graph-specific architectural components <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>While the results suggest a promising research direction, there are challenges to be addressed in future work. First, treating each node and edge as tokens requires O((n + m) 2 ) asymptotic cost due to the quadratic nature of self-attention. While we address this to some degree with kernelization and achieve O(n + m) cost, other types of efficient Transformers (e.g., sparse) that can deliver better performance are left to be tested. Another issue is slightly lower performance compared to the stateof-the-art. Adopting Transformer engineering techniques from vision and language domains, such as data scaling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>, deepening <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b73">74]</ref>, hybrid architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b79">80]</ref>, and self-supervision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>, are promising. In the societal aspect, to prevent the potential risky behavior in, e.g., decision making from graph-structured inputs, interpretability research regarding self-attention on graphs is desired.</p><p>We finish with interesting research directions that stem from our work. As our approach advocates viewing a graph as (n + m) tokens <ref type="bibr" target="#b36">[37]</ref>, it opens up new paradigms of graph learning, including autoregressive decoding, in-context learning, prompting, and multimodal learning. Another interesting direction is to extend our theory and use self-attention to approximate equivariant basis for general discrete group actions, which might be a viable approach for learning equivariance from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A. Before proceeding to the proofs, we first provide additional preliminary material that supplements Section 3.1. We begin by formally defining multihead self-attention and Transformer. Our definition is equivalent to <ref type="bibr" target="#b67">Vaswani et al. (2017)</ref>  <ref type="bibr" target="#b67">[68]</ref>, except we omit layer normalization for simplicity as in <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. Specifically, a multihead self-attention layer MSA : R n?d ? R n?d is defined as:</p><formula xml:id="formula_14">? h = softmax Xw Q h (Xw K h ) / d H ,<label>(7)</label></formula><p>MSA(</p><formula xml:id="formula_15">X) i = H h=1 n j=1 ? h ij X j w V h w O h ,<label>(8)</label></formula><p>where H is number of heads, d H is head size, and</p><formula xml:id="formula_16">w Q h , w K h ? R d?d H , w V h ? R d?dv w O h ? R dv?d .</formula><p>In our proofs, we use biases for query and key projections as in <ref type="bibr" target="#b80">[81]</ref> but omit them here for brevity. With multihead self-attention, a Transformer layer T : R n?d ? R n?d is defined as:</p><formula xml:id="formula_17">H = X + MSA(X),<label>(9)</label></formula><formula xml:id="formula_18">T (X) = H + MLP(H),<label>(10)</label></formula><p>where MSA : R n?d ? R n?d is a multihead self-attention layer with H heads of size d H and MLP : R n?d ? R n?d is a tokenwise MLP with hidden dimension d F .</p><p>We now provide the complete definition of invariant graph networks (IGNs) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref> and maximally expressive equivariant linear layers <ref type="bibr" target="#b46">[47]</ref> summarized in Section 3.1. We first recall Definition 1 and 2:</p><p>Definition 1. An order-k Invariant Graph Network (k-IGN) is a function F k : R n k ?d0 ? R written as the following:</p><formula xml:id="formula_19">F k = MLP ? L k?0 ? L (T ) k?k ? ? ? ... ? ? ? L (1) k?k ,<label>(1)</label></formula><p>where each L (t) k?k is equivariant linear layer <ref type="bibr" target="#b46">[47]</ref> from R n k ?dt?1 to R n k ?dt , ? is activation function, and L k?0 is a invariant linear layer from R n k ?d T to R. Definition 2. An equivariant linear layer is a function L k?l : R n k ?d ? R n l ?d written as follows for order-k input X ? R n k ?d :</p><formula xml:id="formula_20">L k?l (X) i = ? j B ? i,j X j w ? + ? C ? i b ? ,<label>(2)</label></formula><p>where i ? [n] l , j ? [n] k are multi-indices, w ? ? R d?d , b ? ? R d are weight and bias parameters, and B ? ? R n l+k and C ? ? R n l are binary basis tensors corresponding to order-(l + k) and order-l equivalence classes ? and ?, respectively. Invariant linear layer is a special case of L k?l with l = 0.</p><p>We now define equivalence classes and basis tensors mentioned briefly in Definition 2. The equivalence classes are defined upon a specific equivalence relation ? on the index space of higher-order tensors as follows: Definition 3. An order-l equivalence class ? ? [n] l / ? is an equivalence class of [n] l under the equivalence relation ?, where the equivalence relation ? on multi-index space [n] l relates i ? j if and only if (i 1 , ..., i l ) = (?(j 1 ), ..., ?(j l )) for some node permutation ? ? S n .</p><p>We note that a multi-index i has the same permutation-invariant equality pattern to any j that satisfies i ? j, i.e., i a = i b ? j a = j b for all a, b ? [k]. Consequently, each equivalence class ? in Definition 3 is a distinct set of all order-l multi-indices having a specific equality pattern.</p><p>Now, for each equivalence class, we define the corresponding basis tensor as follows:</p><p>Definition 4. An order-l basis tensor B ? ? R n l corresponding to an order-l equivalence class ? is a binary tensor defined as follows:</p><formula xml:id="formula_21">B ? i = 1 i ? ? 0 otherwise<label>(11)</label></formula><p>For a given l, it is known that there exist bell(l) order-l equivalence classes {? 1 , ..., ? bell(l) } = [n] l / ? regardless of n <ref type="bibr" target="#b46">[47]</ref>. This gives bell(l) order-l basis tensors B ?1 , ..., B ? bell(l) accordingly. Thus, an equivariant linear layer L k?l in Definition 2 has bell(l + k) weights and bell(l) biases.</p><p>Let us consider the first-order equivariant layer L 1?1 as an example. We have bell(2) = 2 secondorder equivalence classes ? 1 and ? 2 for the weight, with ? 1 the set of all (i 1 , i 2 ) with i 1 = i 2 and ? 2 the set of all (i 1 , i 2 ) with i 1 = i 2 . From Definition 4, their corresponding basis tensors are B ?1 = I and B ?2 = 11 ? I. Given a set of features X ? R n?d ,</p><formula xml:id="formula_22">L 1?1 (X) = IXw 1 + (11 ? I)Xw 2 + 1b ,<label>(12)</label></formula><p>with two weights w 1 , w 2 ? R d?d , and a single bias b ? R d . For graphs (k = l = 2), we have bell(4) = 15 weights and bell(2) = 2 biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Proof of Lemma 1 (Section 3.3)</head><p>To prove Lemma 1, we need to show that each basis tensor B ? (Eq. (11)) in weights of equivariant linear layers (Eq. (2)) can be approximated by the self-attention coefficient ? h (Eq. <ref type="formula" target="#formula_14">(7)</ref>) to arbitrary precision up to normalization if its input is augmented by node and type identifiers (Section 3.3).</p><p>From Definition 4, each entry of basis tensor B ? i,j encodes whether (i, j) ? ? or not. Here, our key idea is to break down the inclusion test (i, j) ? ? into equivalent but simpler Boolean tests that can be implemented in self-attention (Eq. (8)) as dot product of i-th query and j-th key followed by softmax.</p><p>To achieve this, we show some supplementary Lemmas. We start with Lemma 2, which comes from Lemma 1 of Kim et al. (2021) <ref type="bibr" target="#b33">[34]</ref> (we repeat their proof here for completeness). Lemma 2. For any order-(l + k) equivalence class ?, the set of all i ? [n] l such that (i, j) ? ? for some j ? [n] k forms an order-l equivalence class. Likewise, the set of all j such that (i, j) ? ? for some i forms an order-k equivalence class.</p><p>Proof. We only prove for i as proof for j is analogous. For some (i 1 , j 1 ) ? ?, let us denote the equivalence class of i 1 as ? l (i.e., i 1 ? ? l ). It is sufficient that we prove i ? ? l ? ?j : (i, j) ? ?.</p><p>(?) For all i ? ? l , as i 1 ? i, there exists some ? ? S n such that i = ?(i 1 ) by definition. As ? acts on multi-indices entry-wise, we have ?(i 1 , j 1 ) = (i, ?(j 1 )). As ?(i 1 , j 1 ) ? (i 1 , j 1 ) holds by definition, we have (i, ?(j 1 )) ? (i 1 , j 1 ), and thus (i, ?(j 1 )) ? ?. Therefore, for all i ? ? l , by setting j = ?(j 1 ) we can always obtain (i, j) ? ?.</p><p>(?) For all (i, j) ? ?, as (i, j) ? (i 1 , j 1 ), there exists some ? ? S n such that (i, j) = ?(i 1 , j 1 ). This gives i = ?(i 1 ) and j = ?(j 1 ), leading to i ? i 1 and therefore i ? ? l . Lemma 2 states that the equivalence classes ? l of i and ? k of j are identical for all (i, j) ? ?. Based on this, we appropriately break down the test (i, j) ? ? into a combination of several simpler tests, in particular including i ? ? l and j ? ? k : Lemma 3. For a given order-(l + k) equivalence class ?, let ? l and ? k be equivalence classes of some i 1 ? [n] l , j 1 ? [n] k respectively that satisfies (i 1 , j 1 ) ? ?. Then, for any i ? [n] l and j ? [n] k , (i, j) ? ? holds if and only if the following conditions both hold:</p><formula xml:id="formula_23">1. i ? ? l and j ? ? k 2. i a = j b ? i 2 a = j 2 b for all a ? [l], b ? [k], and (i 2 , j 2 ) ? ?</formula><p>Proof. (?) If (i, j) ? ?, from Lemma 2 it follows that i ? ? l and j ? ? k . Also, as all (i 2 , j 2 ) ? ? including (i, j) have the same equality pattern, it follows that for all a ? [l], b ? [k], and (i 2 , j 2 ) ? ?, if i 2 a = j 2 b then i a = j b and if i 2 a = j 2 b then i a = j b . (?) We show that the conditions specify that the equivalence class of (i, j) is ?.</p><p>For this, it is convenient to represent an order-l equivalence class ? as an equivalent undirected graph G = (V, E) defined on vertex set V = {v 1 , ..., v l } where the vertices v a and v b are connected, i.e., (v a , v b ) ? E if and only if the equivalence class ? specifies i a = i b ?i ? ?. Then, for some multi-index i ? [n] l , the inclusion i ? ? holds if and only if the equivalence class of i is represented as G.</p><formula xml:id="formula_24">G ? v 1 v 2 u 1 u 2 u 3 v 1 v 2 u 1 u 2 u 3 E C Goal: test (i, j) ? ? Condition 1. Condition 2. i ? ? l and j ? ? k ia = jb ? i 2 a = j 2 b for all G l v 1 v 2 u 1 u 2 u 3 E l E k G k G of (i, j) v 1 v 2 u 1 u 2 u 3 ?</formula><p>?</p><p>?</p><p>?</p><formula xml:id="formula_25">Conditions 1 and 2 hold v 1 v 2 u 1 u 2 u 3 E l E C E k i.e., test G = G ? ? G = G ? (k = 2, l = 3) ? (i, j) ? ? a ? [k ], b ? [l ]</formula><p>, (i 2 , j 2 ) ? ? <ref type="figure">Figure 4</ref>: An exemplary illustration of testing (i, j) ? ? as a combination of simpler tests, based on equivalence classes ?, ? l , and ? k represented as graphs G ? , G l , and G k , respectively.</p><p>Given this, let us represent the equivalence classes ? l , ? k , and ? as graphs G l , G k , and G ? respectively:</p><formula xml:id="formula_26">G l = (V l , E l ) where V l = {v 1 , ..., v l },<label>(13)</label></formula><formula xml:id="formula_27">G k = (V k , E k ) where V k = {u 1 , ..., u k },<label>(14)</label></formula><formula xml:id="formula_28">G ? = (V ? , E ? ) where V ? = V l ? V k = {v 1 , ..., v l , u 1 , ..., u k }.<label>(15)</label></formula><p>From the precondition that ? l and ? k are equivalence classes of</p><formula xml:id="formula_29">i 1 ? [n] l , j 1 ? [n] k that satisfies (i 1 , j 1 ) ? ?, we can see that (v a , v b ) ? E l ? (v a , v b ) ? E ? and (u a , u b ) ? E k ? (u a , u b ) ? E ? .</formula><p>That is, if we consider V l and V k as a graph cut of G ? and write the cut-set (edges between V l and V k ) as</p><formula xml:id="formula_30">E C = {(v a , u b )|(v a , u b ) ? E ? }, we obtain a partition {E l , E k , E C } of the edge set E ? .</formula><p>We now move to the conditions. Let us assume the first condition that i ? ? l and j ? ? k , with the equivalence classes represented as G l and G k , respectively. Now, let us consider the equivalence class of (i, j) represented by (unknown) graph G = (V, E). Considering V k and V l as a graph cut of G, we can see that E is partitioned as {E l , E k , E D } where E D is the cut-set (edges between V l and V k ).</p><p>Let us also assume the second condition i a = j b ? i 2 a = j 2 b for all a ? [l], b ? [k], and (i 2 , j 2 ) ? ?. This directly implies that e ? E C ? e ? E D , meaning that E C = E D . As a result, we see that G and G ? are identical graphs, and therefore the equivalence class of (i, j) is ? and (i, j) ? ? holds.</p><p>In <ref type="figure">Figure 4</ref>, we provide an exemplary illustration of testing (i, j) ? ? following the above discussion.</p><p>With Lemma 3, we have a decomposition of (i, j) ? ? into independent conditions on i and j combined with pairwise conditions between i and j. In the following Definition 5 and Property 1, we encode these tests into a single scoring function that can be later implemented by self-attention. Definition 5. A scoring function ?(i, j; ?, ) is a map that, given an order-(l + k) equivalence class ? and &gt; 0, takes multi-indices i ? [n] l , j ? [n] k and gives the following:</p><formula xml:id="formula_31">?(i, j; ?, ) = 1 i?? l + (1 ? )1 i / ?? l + 1 j?? k + (1 ? )1 j / ?? k + a?[l] b?[k] sgn(a, b)1 ia=j b , (16)</formula><p>where 1 is indicator, ? l and ? k are equivalence classes of i 1 ? [n] l , j 1 ? [n] k such that (i 1 , j 1 ) ? ?, and the sign function sgn(?, ?) is defined as follows:</p><formula xml:id="formula_32">sgn(a, b) = +1 i 2 a = j 2 b ?(i 2 , j 2 ) ? ? ?1 i 2 a = j 2 b ?(i 2 , j 2 ) ? ? .<label>(17)</label></formula><p>An important property of the scoring function ?(i, j; ?) is that it gives the maximum possible value if and only if the input satisfies (i, j) ? ?, as shown in the below Property 1.  Proof. As shown in Lemma 3, (i, j) ? ? holds if and only if the following two conditions are met.</p><formula xml:id="formula_33">1. i ? ? l and j ? ? k 2. i a = j b ? i 2 a = j 2 b for all a ? [l], b ? [k], and (i 2 , j 2 ) ? ?</formula><p>When both conditions are satisfied, in Eq. (16), we always have 1 i?? l + (1 ? )1 i / ?? l = 1 and</p><formula xml:id="formula_34">1 j?? k + (1 ? )1 j / ?? k = 1. We also have 1 ia=j b = 1 for sgn(a, b) = 1 and 1 ia=j b = 0 for sgn(a, b) = ?1 for all a ? [l], b ? [k]</formula><p>. As a result, Eq. (16) gives a constant output for all (i, j) ? ?.</p><p>On the other hand, if given (i, j) violates any of the conditions (thus (i, j) / ? ?), we either have Lemma 1. For all X ? R n k ?d and their augmentation X in , self-attention coefficients ? h <ref type="figure" target="#fig_5">(Eq. (3)</ref>) computed with X in w in can approximate any basis tensor B ? ? R n 2k of order-k equivariant linear layer L k?k (Definition 2) to arbitrary precision up to normalization.</p><formula xml:id="formula_35">1 i?? l + (1 ? )1 i / ?? l = (1 ? ), or 1 j?? k + (1 ? )1 j / ?? k = (1 ? ),</formula><p>Proof. Let us first recall the node and type identifiers (Section 3.3) for order-k tensors X ? R n k ?d . Node identifier P ? R n?dp is an orthonormal matrix with n rows, and type identifier is a trainable matrix E ? R bell(k)?de with bell(k) rows E ?1 , ..., E ? bell(k) , each designated for an order-k equivalence class ?. For each multi-index i = (i 1 , ..., i k ) ? [n] k , we augment the corresponding input tensor entry as [X i , P i1 , ..., P i k , E ? i ] where i ? ? i , obtaining the augmented order-k tensor X in ? R n k ?(d+kdp+de) . We use a trainable projection w in ? R (d+kdp+de)?d T to map them to a hidden dimension d T .</p><p>We now use self-attention on X in w in to perform an accurate approximation of the equivariant basis. Specifically, we use each self-attention matrix ? h (Eq. (7)) to approximate each basis tensor B ? h of L k?k (Eq. (2)) to arbitrary precision up to normalization.</p><p>Let us take d T = (d + kd p + d e ) + bell(2k)d, putting bell(2k)d extra channels on top of channels of the augmented input X in . We now let w in = [I, 0], where I ? R (d+kdp+de)?(d+kdp+de) is an identity matrix and 0 ? R (d+kdp+de)?(d T ?(d+kdp+de)) is a matrix filled with zeros. With this, X = X in w in simply contains X in in the first (d + kd p + d e ) channels and zeros in the rest. Now we pass X to the self-attention layer in Eq. <ref type="formula" target="#formula_14">(7)</ref>, where each self-attention matrix is given as</p><formula xml:id="formula_36">? h = softmax((X w Q h + b Q h )(X w K h + b K h ) / ? d H ).</formula><p>The key idea is to set query and key projection</p><formula xml:id="formula_37">parameters w Q h , w K h ? R d T ?d H and b Q h , b K h ? R d H</formula><p>appropriately so that the self-attention matrix ? h approximates a given basis tensor B ? corresponding to an order-2k equivalence class ?. Let ? Q and ? K be equivalence classes of some i 1 , j 1 ? [n] k respectively that satisfy (i 1 , j 1 ) ? ? (see Lemma 3). We set head dimension d H = k 2 d p + 2d e and set w Q h , w K h , b Q h , b K h as follows:</p><formula xml:id="formula_38">(w Q h ) ij = ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>sgn(s, r) ? aI i?I,j?J I &lt; i ? I + dp for I = d + (s ? 1)dp, J &lt; j ? J + dp for J = (s ? 1)kdp + (r ? 1)dp, for all s, r ? [k] ? aI i?I,j?J</p><formula xml:id="formula_39">I &lt; i ? I + de for I = d + kdp, J &lt; j ? J + de for J = k 2 dp, 0 otherwise ,<label>(18)</label></formula><formula xml:id="formula_40">(w K h ) ij = ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>? aI i?I,j?J I &lt; i ? I + dp for I = d + (r ? 1)dp, J &lt; j ? J + dp for J = (s ? 1)kdp + (r ? 1)dp, for all s, r ? [k] ? aI i?I,j?J</p><formula xml:id="formula_41">I &lt; i ? I + de for I = d + kdp, J &lt; j ? J + de for J = k 2 dp + de, 0 otherwise ,<label>(19)</label></formula><formula xml:id="formula_42">(b Q h ) j = ? aE ? K j?J J &lt; j ? J + de for J = k 2 dp 0 otherwise ,<label>(20)</label></formula><formula xml:id="formula_43">(b K h ) j = ? aE ? Q j?J J &lt; j ? J + de for J = k 2 dp + de 0 otherwise ,<label>(21)</label></formula><p>where a &gt; 0 is a positive real, I is an identity matrix, and sgn(?, ?) is the sign function defined in Eq. (17) (Definition 5). In <ref type="figure">Figure 5</ref> we provide an illustration of the query and key weights w Q h , w K h . With the parameters, i-th query and j-th key entries are computed as follows:</p><formula xml:id="formula_44">X i w Q h + b Q h = ? a[[sgn(1, 1)P i1 , ..., sgn(1, k)P i1 ], ..., [sgn(k, 1)P i k , ..., sgn(k, k)P i k ], E ? i , E ? K ],<label>(22)</label></formula><formula xml:id="formula_45">X j w K h + b K h = ? a[ k repeats [P j1 , ..., P j k ], ..., [P j1 , ..., P j k ], E ? Q , E ? j ].<label>(23)</label></formula><p>Then, scaled pairwise dot product of query and key is given as follows:</p><formula xml:id="formula_46">(X i w Q h + b Q h ) (X j w K h + b K h ) ? d H = a ? d H ? ? (E ? i ) E ? Q + (E ? j ) E ? K + a?[k] b?[k] sgn(a, b)P ia P j b ? ? .<label>(24)</label></formula><p>We refer to the scaled dot product in Eq. (24) as the unnormalized attention coefficient? h i,j . We now let the type identifiers E ?1 , ..., E ? bell(k) be radially equispaced unit vectors on any twodimensional subspace ( <ref type="figure" target="#fig_8">Figure 6</ref>). This guarantees that any pair of type identifiers E ?1 , E ?2 with ? 1 = ? 2 have dot product (E ?1 ) E ?2 ? cos (2?/bell(k)). By setting = 1 ? cos (2?/bell(k)) &gt; 0, this can be equivalently written as (E ?1 ) E ?2 ? 1 ? . We additionally note that (</p><formula xml:id="formula_47">E ? i ) E ? Q = 1 if and only if i ? ? Q because ? i = ? Q ? i ? ? Q .</formula><p>Combining the above, Eq. (24), and Eq. (16), we have the following:</p><formula xml:id="formula_48">? h i,j = a ? d H ?(i, j; ?, ) if (i, j) ? ?,<label>(25)</label></formula><formula xml:id="formula_49">? h i,j ? a ? d H ?(i, j; ?, ) otherwise,<label>(26)</label></formula><p>where = 1 ? cos (2?/bell(k)) and ?(i, j; ?, ) is the scoring function in Eq. (16) (Definition 5). For a given query index i, let us assume there exists at least one key index j such that (i, j) ? ? 3 . From Property 1 and Eq. <ref type="bibr" target="#b24">(25)</ref>, all keys j that give (i, j) ? ? hold the same maximum value? h i,j = a ? d H ?(i, j; ?, ), and any (i, j) / ? ? gives a value smaller at least by min (1, ) &gt; 0. Then, in softmax normalization, we send a ? ? by scaling up the query and key projection parameters. This pushes softmax arbitrarily close to the hardmax operator, leaving only the maximal entries leading to the following:</p><formula xml:id="formula_50">E ? 1 E ? 2 E ? 3 E ? 4 E ? 5 2?/5 de = 3 k = 3 ? bell(k) = 5</formula><formula xml:id="formula_51">? h i,j = exp(? h i,j ) j exp(? h i,j ) ? 1 (i,j??) j 1 (i,j??) = B ? i,j j B ? i,j as a ? ?.<label>(27)</label></formula><p>Thus, as shown in Eq. <ref type="formula" target="#formula_3">(27)</ref>, the attention coefficient ? h can arbitrarily accurately approximate the normalized basis tensor B ? for given equivalence class ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Proof of Theorem 1 (Section 3.3)</head><p>Theorem 1. For all X ? R n k ?d and their augmentation X in , a Transformer layer with bell(2k) self-attention heads that operates on X in w in can approximate an order-k equivariant linear layer L k?k (X) (Definition 2) to arbitrary precision.</p><p>Proof. We continue from the proof of Lemma 1 and assume that each attention matrix ? 1 , ..., ? bell(2k) in Eq. <ref type="formula" target="#formula_14">(7)</ref> head-wise approximates each normalized basis tensor B ?1 , ..., B ? bell(2k) respectively, i.e.,</p><formula xml:id="formula_52">? h i,j = B ? h i,j / j B ? h i,j . 4 Then, in Eq. (8) we use d v = d and set w V h ? R d T ?d to w V h = [I; 0]</formula><p>, where I ? R d?d is an identity matrix and 0 ? R (d T ?d)?d is a matrix filled with zeros. With this, the value projection of each i-th entry simply gives the original input features, X i w V h = X i . Then, we set output projections w O h ? R d?d T as follows:</p><formula xml:id="formula_53">(w O h ) ij = (w ? h ) i,j?J J &lt; j ? J + d for J = (d + kdp + de) + (h ? 1)d 0 otherwise ,<label>(28)</label></formula><p>where w ?1 , ..., w ? bell(2k) ? R d?d are weight matrices of the given equivariant linear layer L k?k in Eq.</p><p>(2) (Definition 2), each corresponding to equivalence classes ? 1 , ..., ? bell(2k) .</p><p>Then, output projection applied after value projection of each i-th input entry gives the following:</p><formula xml:id="formula_54">X i w V h w O h = X i w O h = [0, 0 L , X i w ? h , 0 R ],<label>(29)</label></formula><p>where 0 ? R (d+kdp+de) , 0 L ? R (h?1)d , 0 R ? R d T ?(d+kdp+de)?hd are zero vectors.</p><p>Based on the results, we compute the MSA with skip connection H = X + MSA(X ) (Eq. <ref type="formula" target="#formula_17">(9)</ref>):</p><formula xml:id="formula_55">H i = X i + MSA(X ) i (30) = X i , P i1 , ..., P i k , E ? i , 0 1 + ? ? 0 2 , j B ?1 i,j j B ?1 i,j X j w ?1 , ..., j B ? bell(2k) i,j j B ? bell(2k) i,j X j w ? bell(2k) ? ? (31) = ? ? X i , P i1 , ..., P i k , E ? i , j B ?1 i,j j B ?1 i,j X j w ?1 , ..., j B ? bell(2k) i,j j B ? bell(2k) i,j X j w ? bell(2k) ? ? ,<label>(32)</label></formula><p>where</p><formula xml:id="formula_56">0 1 ? R d T ?(d+kdp+de) , 0 2 ? R (d+kdp+de) are zero vectors.</formula><p>We use feedforward MLP (Eq. (10)) to denormalize and combine the result. Specifically, we make the elementwise MLP approximate following f : R d T ? R d T based on universal approximation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>:</p><formula xml:id="formula_57">f (H i ) j = ? ? ? ?H i,j + h?[bell(2k)] g(H i ) h H i,j+J + b(H i ) j j ? d 0 d &lt; j ? (d + kdp + de) ?H i,j j &gt; (d + kdp + de) ,<label>(33)</label></formula><formula xml:id="formula_58">g(H i ) h = j B ? h i,j ,<label>(34)</label></formula><formula xml:id="formula_59">b(H i ) j = (b ? i ) j = ( ? C ? i b ? ) j ,<label>(35)</label></formula><p>where J = (d + kd p + d e ) + (h ? 1)d, and b ?1 , ..., b ? bell(k) are biases of the given equivariant linear layer L k?k with corresponding basis tensors C ?1 , ..., C ? bell(k) (Eq. <ref type="formula" target="#formula_3">(2)</ref>).</p><p>Within the function f , the auxiliary function g : R d T ? R bell(2k) computes head-wise attention denormalization factor 5 and b : R d T ? R d computes bias. As n and k are fixed constants, the outputs g(H i ) and b(H i ) only depend on the equivalence class ? i of i. We note that the functions g and b can deduce the equivalence class from the input H i , by extracting the type identifier E ? i = H i [0 3 , I, 0 4 ] with I ? R de?de an identity matrix and 0 3 ? R d+kdp , 0 4 ? R bell(2k)d zero matrices.</p><p>Based on the results, we compute the feedforward MLP with skip connection T (X ) = H + MLP(H) (Eq. <ref type="figure" target="#fig_0">(10)</ref>), which is the output of Transformer layer T :</p><formula xml:id="formula_60">T (X ) i = H i + MLP(H) i (36) = H i + f (H i )<label>(37)</label></formula><formula xml:id="formula_61">= X i , P i1 , ..., P i k , E ? i , S 1 i , ..., S bell(2k) i + ? ? ?X i + h?[bell(2k)] j B ? h i,j X j w ? h + ? C ? i b ? , 0 5 , ?S 1 i , ..., ?S bell(2k) i ? ? ,<label>(38)</label></formula><formula xml:id="formula_62">= ? ? h?[bell(2k)] j B ? h i,j X j w ? h + ? C ? i b ? , P i1 , ..., P i k , E ? i , 0 6 ? ? ,<label>(39)</label></formula><p>where we write</p><formula xml:id="formula_63">S h i = j B ? h i,j j B ? h i,j X j w ? h and 0 5 ? R kdp+de , 0 6 ? R (d T ?(d+kdp+de)) are zeros.</formula><p>In Eq. (39), note that the Transformer layer T (X ) i only updates the first d channels of X i from X i to ? j B ? i,j X j w ? + ? C ? i b ? . Therefore, with a simple projection w out = [I; 0] ? R d T ?d where I ? R d?d is an identity matrix and 0 ? R (d T ?d)?d is a matrix filled with zeros, we can select the first d channels of the output and finally obtain T (X )w out = L k?k (X).</p><p>In conclusion, a Transformer layer with bell(2k) self-attention heads that operates on augmented X can approximate any given L k?k (X) to arbitrary precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Proof of Theorem 2 (Section 3.3)</head><p>Theorem 2. For all X ? R n k ?d and their augmentation X in , a Transformer composed of T layers that operates on X in w in followed by sum-pooling and MLP can approximate an k-IGN F k (X) (Definition 1) to arbitrary precision.</p><p>Proof. We continue from the proof of Theorem 1, and assume that each Transformer layer T can approximate a given L k?k by only updating the first d channels.</p><p>Then, based on Theorem 1 we assume the following for each t &lt; T :</p><formula xml:id="formula_64">T (t) (X ) i = ?(L (t) k?k (X)) i , P i1 , ..., P i k , E ? i , 0 6<label>(40)</label></formula><p>where</p><formula xml:id="formula_65">X i = [X i , P i1 , ..., P i k , E ? i , 0 6 ]. While Theorem 1 gives L (t)</formula><p>k?k (X) in the first d channels, we add elementwise activation ?(?) by absorbing it into the elementwise MLP in Eq. <ref type="bibr" target="#b32">(33)</ref>. Then, leveraging the property that each Transformer layer T (t) only updates the first d channels, we stack T ? 1 Transformer layers T <ref type="bibr" target="#b0">(1)</ref> , ..., T (T ?1) and obtain the following:</p><formula xml:id="formula_66">T (T ?1) ? ... ? T (1) (X ) i = ? ? L (T ?1) k?k ? ? ? ... ? ? ? L (1) k?k (X) i , P i1 , ..., P i k , E ? i , 0 6 . (41)</formula><p>For the last layer T (T ) , we follow the procedure in the proof of Theorem 1 to approximate L (T ) k?k , but slightly tweak Eq. (33) so that elementwise MLP copies each output entry L k?k (X) (T ) i in appropriate reserved channels. Specifically, we let the elementwise MLP approximate following f :</p><formula xml:id="formula_67">f (H i ) j = ?H i,j j ? D ?H i,j + C ?a i F i,j?(D+(a?1)d) D + (a ? 1)d &lt; j ? D + ad for all a ? [bell(k)] ?H i,j D + bell(k)d &lt; j ,<label>(42)</label></formula><p>where D = (d + kd p + d e ) and we abbreviate F i,j = h?[bell(2k)] g(H i ) h H i,j+J + b(H i ) j with J, g, b defined as same as in Eq. <ref type="bibr" target="#b32">(33)</ref>. Recall that C ?a i = 1 if and only if i ? ? a . Therefore, with Eq. (42), we are simply duplicating each output entry F i = L (T ) k?k (X) i to spare channel indices reserved for the equivalence class of i (? a that i ? ? a ).</p><p>With the choice of T (T ) , the layer output T (T ) (X ) = H + MLP(H) (Eq. (10)) is computed as:</p><formula xml:id="formula_68">T (T ) (X ) i = H i + MLP(H) i (43) = H i + f (H i ) (44) = 0 7 , C ?1 i L (T ) k?k (X) i , ..., C ?bell(k) i L (T ) k?k (X) i , 0 8 ,<label>(45)</label></formula><p>where 0 7 ? R (d+kdp+de) , 0 8 ? R d T ?(d+kdp+de)?bell(k)d are zero vectors.</p><p>Then, by applying T (T ) (Eq. (45)) on top of T (T ?1) ? ... ? T <ref type="bibr" target="#b0">(1)</ref>  <ref type="figure" target="#fig_0">(Eq. (41)</ref>), we obtain the following:</p><formula xml:id="formula_69">T (T ) ? ... ? T (1) (X ) i = 0 7 , C ?1 i Y i , ..., C ?bell(k) i Y i , 0 8 .<label>(46)</label></formula><p>where we abbreviate Y = L</p><formula xml:id="formula_70">(T ) k?k ? ? ? ... ? ? ? L<label>(1)</label></formula><p>k?k (X). The remaining step is to utilize MLP ? sumpool to approximate MLP k ? L k?0 that tops F k . By sum-pooling over all indices i, we obtain the following:</p><formula xml:id="formula_71">sumpool ? T (T ) ? ... ? T (1) (X ) = 0 7 , i C ?1 i Y i , ..., i C ?bell(k) i Y i , 0 8 .<label>(47)</label></formula><p>Now, we let the final MLP approximate the following function f :</p><formula xml:id="formula_72">R d T ? R d : f (X) = MLP k ? ? a?[bell(k)] X a w ?a + b f ? ? where X a j = X D+(a?1)d+j for j ? [d],<label>(48)</label></formula><p>where w ?1 , ..., w ? bell(k) ? R d?d and b f ? R d are the weights and bias of the given invariant linear layer L k?0 , and each X a ? R d is a chunk that coincides with reserved channels in Eq. <ref type="bibr" target="#b41">(42)</ref>. By plugging in the sum-pooled representation in Eq. (47), we finally obtain the following:</p><formula xml:id="formula_73">MLP ? sumpool ? T (T ) ? ... ? T (1) (X ) = f ? sumpool ? T (T ) ? ... ? T (1) (X ) (49) = MLP k ? ? a?[bell(k)] i C ?a i Y i w ?a + b f ? ? (50) = MLP k ? L k?0 (Y)<label>(51)</label></formula><formula xml:id="formula_74">= MLP k ? L k?0 ? L (T ) k?k ? ? ? ... ? ? ? L (1) k?k (X) (52) = F k (X),<label>(53)</label></formula><p>where the last equality comes from Definition 1.</p><p>Taken together, we arrive at the conclusion that MLP?sumpool?T (T ) ?...?T (1) (X ) can approximate F k (X) to arbitrary precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional Discussion on Linear Attention for Graph Transformers (Section 4)</head><p>We provide an additional discussion on related work, specifically on why Graphormer <ref type="bibr" target="#b77">[78]</ref>, based on fully-connected self-attention on nodes, is not compatible with many linear attention methods that reduce the memory complexity from O(n 2 ) to O(n). A range of prior graph Transformers including EGT <ref type="bibr" target="#b28">[29]</ref>, GRPE <ref type="bibr" target="#b53">[54]</ref>, and SAN <ref type="bibr" target="#b37">[38]</ref> can be analyzed analogously. Let us first remind self-attention with query, key, value Q, K, V ? R n?d and self-attention matrix ? ? R n?n :</p><formula xml:id="formula_75">Att(Q, K, V) i = j ? ij V j where ? ij = exp(Q i K j / ? d) k exp(Q i K k / ? d) .<label>(54)</label></formula><p>For graphs, as self-attention on nodes alone cannot recognize the edge connectivity, Graphormer incorporates the structural information of an input graph G into the self-attention matrix ? G ? R n?n via attention bias matrix b G ? R n?n (referred to as the edge and spatial encoding) as the following:</p><formula xml:id="formula_76">? G ij = exp(Q i K j / ? d + b G ij ) k exp(Q i K k / ? d + b G ij ) .<label>(55)</label></formula><p>Unfortunately, this modification immediately precludes the adaptation of many efficient attention techniques developed for pure self-attention.</p><p>As representative examples, we take Performer <ref type="bibr" target="#b10">[11]</ref>, Linear Transformer <ref type="bibr" target="#b31">[32]</ref>, Efficient Transformer <ref type="bibr" target="#b61">[62]</ref>, and Random Feature Attention <ref type="bibr" target="#b54">[55]</ref>. The methods are based on kernelization of the Att(?) operator as the following:</p><formula xml:id="formula_77">Att ? (Q, K, V) i = j ?(Q i ) ?(K j ) k ?(Q i ) ?(K k ) V j = ?(Q i ) j ?(K j )V j ?(Q i ) ( k ?(K k ))</formula><p>.</p><p>As the above factorization of exp(?) into a pairwise dot product eliminates the need to explicitly compute the attention matrix, it reduces both time and memory cost of self-attention to O(n). Yet, in Eq. (55), since the bias b G ij is added to the dot product before exp(?), it is required that the full attention matrix ? G ? R n?n is always explicitly computed. Thus, Graphormer and related variations are unable to utilize the method and are bound to O(n 2 ).</p><p>While above discussion regards kernelization, a wide range of other efficient Transformers, including Set Transformer <ref type="bibr" target="#b38">[39]</ref>, LUNA <ref type="bibr" target="#b44">[45]</ref>, Linformer <ref type="bibr" target="#b70">[71]</ref>, Nystr?mformer <ref type="bibr" target="#b75">[76]</ref>, Perceiver <ref type="bibr" target="#b30">[31]</ref>, and Perceiver-IO <ref type="bibr" target="#b29">[30]</ref> are not applicable to Graphormer due to similar reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental Details (Section 5)</head><p>We provide detailed information on the datasets and models used in our experiments in Section 5. Dataset statistics can be found in <ref type="table" target="#tab_5">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Implementation Details of Node and Type Identifiers</head><p>In most of our experiments on graph data (k = 2), we fix the Transformer encoder configuration and experiment with choices of node identifiers P ? R n?dp and type identifiers E ? R 2?de (Section 2).</p><p>For type identifiers E, we set d e equal to the hidden dimension d of the main encoder d e = d and initialize and train them jointly with the model.</p><p>For orthonormal node identifiers P, we use normalized orthogonal random features (ORFs) or Laplacian eigenvectors obtained as follows:</p><p>? For orthogonal random features (ORFs), we use rows of random orthogonal matrix Q ? R n?n obtained with QR decomposition of random Gaussian matrix G ? R n?n <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b11">12]</ref>. ? For Laplacian eigenvectors, we perform eigendecomposition of graph Laplacian matrix, i.e., rows of U from ? = I?D ?1/2 AD ?1/2 = U ?U, where A ? R n?n is adjacency matrix, D is degree matrix, and ?, U correspond to eigenvalues and eigenvectors respectively <ref type="bibr" target="#b19">[20]</ref>.</p><p>The model expects d p -dimensional node identifiers P ? R n?dp , while ORF and Laplacian eigenvectors are n-dimensional. To resolve this, if n &lt; d p , we zero-pad the channels. If n &gt; d p , for ORF we randomly sample d p channels and discard the rest, and for Laplacian eigenvectors we use d p eigenvectors with the smallest eigenvalues following common practice <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>As the Laplacian eigenvectors are defined up to the factor ?1 after normalized to unit length <ref type="bibr" target="#b19">[20]</ref>, we randomly flip their signs during training. For PCQM4Mv2 (Section 5.2), we apply random dropout on eigenvectors during training, similar to 2D channel dropout in ConvNets <ref type="bibr" target="#b66">[67]</ref>. In our experiments with PCQM4Mv2, we find that both sign flip and eigenvector dropout work as effective regularizers and improves performance on validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Second-Order Equivariant Basis Approximation (Section 5.1)</head><p>Dataset For the equivariant basis approximation experiment, we use a synthetic dataset containing Barab?si-Albert (BA) random graphs <ref type="bibr" target="#b1">[2]</ref>. With U denoting discrete uniform distribution, each graph is generated by first sampling the number of nodes n ? U <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b19">20)</ref> and the number for preferential attachment k ? U(2, 3), then iteratively adding n nodes by linking each new node to k random previous nodes. We do not utilize node or edge attributes and only use edge connectivity. We generate 1152 graphs for training and 128 for testing. Further dataset statistics is provided in <ref type="table" target="#tab_5">Table 3a</ref>.</p><p>Architecture Each model tested in <ref type="table" target="#tab_0">Table 1</ref> is a single multihead self-attention layer (Eq. (9)) with hidden dimension d = 1024, heads H = bell(2 + 2) = 15, and head dimension d H = 128. As for the node identifier dimension, we use d p = 24 for ORF and d p = 20 for Laplacian eigenvectors.</p><p>Experimental Setup We experiment with sparse or dense input graph representations. For sparse input, we embed each graph with n nodes and m edges into X in ? R (n+m)?(2dp+de) . For dense input, we use all n 2 pairwise edges and obtain X in ? R n 2 ?(2dp+de) , so that sparse edge connectivity is only used for obtaining Laplacian node identifiers.</p><p>For both sparse and dense inputs, we follow the standard procedure in Section 2 to use node and type identifiers to obtain X in ? R N ?(2dp+de) where N = (n + m) or n 2 , and project it to dimension d with trainable projection w in . We also utilize a special token We train all models with AdamW optimizer <ref type="bibr" target="#b43">[44]</ref> on 4 RTX 3090 GPUs each with 24GB. For sparse inputs we use batch size 512, and for dense inputs we use batch size 256 due to increased memory cost. We train all models for 3k steps (which takes about ?1.5 hours) and apply linear learning rate warmup for 1k steps up to 1e-4 followed by linear decay to 0. For all models, we use dropout rate of 0.1 on the input [X <ref type="bibr">[null]</ref> ; X in w in ] to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Large-Scale Graph Learning (Section 5.2)</head><p>Dataset For large-scale learning, we use the PCQM4Mv2 quantum chemistry regression dataset from the OGB-LSC benchmark <ref type="bibr" target="#b26">[27]</ref> that contains 3.7M molecular graphs. Along with graph structure, we utilize both node and edge features e.g., atom and bond types following our standard procedure in Section 2. Dataset statistics is provided in <ref type="table" target="#tab_5">Table 3b</ref>.</p><p>Architecture All our models in <ref type="table" target="#tab_2">Table 2</ref> (under Pure Transformers) have the same encoder configuration following Graphormer <ref type="bibr" target="#b77">[78]</ref>, with 12 layers, hidden dimension d = 768, heads H = 32, and head dimension d H = 24. We adopt PreLN <ref type="bibr" target="#b74">[75]</ref> that places layer normalization before MSA layer (Eq. (9)), MLP layer (Eq. (10)), and the final output projection after the last encoder layer. We implement MLP (Eq. (10)) as a stack of two linear layers with GeLU nonlinearity <ref type="bibr" target="#b24">[25]</ref> in between. As for node identifier dimension, we use d p = 64 for ORF and d p = 16 for Laplacian eigenvectors.</p><p>As an additional GNN baseline, we run Graph Attention Network (GATv2) <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b4">5]</ref> under several configurations. For GAT and GAT-VN in <ref type="table" target="#tab_2">Table 2</ref>, we use 5-layer GATv2 with hidden dimension 600 and a single attention head, having 6.7M parameters in total. For GAT-VN (large), we use a 10-layer GATv2 with hidden dimension 1200 and a single attention head, having 55.2M parameters in total. For GAT-VN and GAT-VN (large), we use virtual node that helps modeling global interaction <ref type="bibr" target="#b26">[27]</ref>.</p><p>Training and Evaluation We mainly report and compare the Mean Absolute Error (MAE) on the validation data, and report MAE on the hidden test data if possible. We train all models with L1 loss using AdamW optimizer <ref type="bibr" target="#b43">[44]</ref> with gradient clipping at global norm 5.0. We use batch size 1024 and train the models on 8 RTX 3090 GPUs with 24GB for ?3 days. We train our models for 1M iterations, and apply linear lr warmup for 60k iterations up to 2e-4 followed by linear decay to 0. For our models in <ref type="table" target="#tab_2">Table 2</ref> except TokenGT (Lap) + Performer, we use the following regularizers:</p><p>?  <ref type="table" target="#tab_2">Table 2</ref>, we use batch size 256 and train the models for 100 epochs with initial learning rate 0.001 decayed with a factor of 0.25 every 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Experimental Results (Section 5)</head><p>We report additional experimental results and discussions that could not be included in the main text due to space restriction.  <ref type="figure">Figure 7</ref>: Self-attention maps learned under various node and type identifier configurations for two target equivariant basis tensors (out of 15), for both dense and sparse inputs. For better visualization, we clamp the entries by 0.01. Self-attention learns acute pattern coherent to equivariant basis when orthonormal node identifiers and type identifiers are provided both as input.    In addition to the <ref type="figure" target="#fig_3">Figure 2</ref> in the main text that shows learned self-attention maps for dense input, in <ref type="figure">Figure 7</ref>, we provide an extended visualization of self-attention maps for both dense and sparse inputs. Consistent to Lemma 1 and <ref type="table" target="#tab_0">Table 1</ref>, self-attention achieves accurate approximation of equivariant basis only when both the orthonormal node identifiers (ORF or Lap) and type identifiers are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Large-Scale Graph Learning (Section 5.2)</head><p>In addition to the <ref type="figure" target="#fig_5">Figure 3</ref> in the main text that shows attention distance measured for the PCQM4Mv2 validation data, in <ref type="figure" target="#fig_10">Figure 8</ref>, we provide an extended figure of attention distance measured for the entire training set that contains ?3M graphs. Overall we find similar trends as analyzed in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Transductive Node Classification on Large Graphs (Section 5)</head><p>While our main experiment in Section 5.2 focuses on graph-level predictions, TokenGT can in principle be applied to a more broad class of node-level or edge-level graph understanding tasks by putting prediction head on appropriate output tokens. To demonstrate this, we conduct additional experiments on a variety of transductive node classification datasets. In contrast to PCQM4Mv2, they involve large graphs with up to tens of thousands of nodes, posing a challenge to O(n 2 ) complexity methods such as graph Transformers that rely on dense attention bias.</p><p>Dataset We use transductive node classification datasets, where each data is represented as a node in a large-scale graph, including co-authorship (CS, Physics) <ref type="bibr" target="#b60">[61]</ref>, co-purchase (Photo, Computers) <ref type="bibr" target="#b60">[61]</ref>, and Wikipedia page networks (Chameleon, Crocodile) <ref type="bibr" target="#b58">[59]</ref>. We randomly split the dataset into train, validation, and test sets by randomly reserving 30 random nodes per class for validation and test respectively, and use the rest of the nodes for training. Dataset statistics is provided in <ref type="table" target="#tab_9">Table 4</ref>.</p><p>Approach We utilize simple variants of TokenGT with Performer kernel attention of O(n + m) complexity. Due to the large number of nodes n, an immediate challenge for TokenGT is dealing with the orthonormality assumption on the node identifiers (Lemma 1) as the maximal number of orthonormal node identifiers is bounded by dimension d p . In this case, it is reasonable to introduce near-orthonormal vectors as node identifiers, as it is theoretically guaranteed that we can draw an exponential number O(e ?(dp) ) of d p -dimensional near-orthonormal vectors <ref type="bibr" target="#b21">[22]</ref>. For TokenGT (Near-ORF), we use d p = 64-dimensional random node identifiers where each entry is sampled from {?1/d p , +1/d p } with coin toss <ref type="bibr" target="#b21">[22]</ref>. For TokenGT (Lap), we use a subset of the Laplacian eigenvectors as node identifiers, specifically d p /2 eigenvectors with lowest eigenvalues and d p /2 eigenvectors with highest eigenvalues, and choose d p among 64-100 based on validation performance.</p><p>While Near-ORF and Lap can theoretically serve as an efficient low-rank approximation for orthonormal node identifiers, their approximation can affect the quality of modeled equivariant basis (Section 3). In particular, equivariant basis (?) represented as sparse basis tensor (B ? ; Definition 4) are expected to be affected more, as they require most entries to be zero. To remedy this, we take a simple approach of residually adding one of such sparse equivariant operators X ii ? X ii + j =i X ij explicitly after each Transformer layer. We denote this variant as TokenGT (Lap) + Performer + SEB, where SEB abbreviates sparse equivariant basis. This fix is minimal, easy to implement, and highly efficient as it only requires a single torch.coalesce() call, and also empirically effective.</p><p>Architecture All our models in <ref type="table" target="#tab_10">Table 5</ref> utilize a linear prediction head on the node tokens obtained at the final Transformer layer to perform node-level classification. We perform an exploratory hyperparameter search over the number of layers from 2-4, heads H from 1-4, hidden dimension d from 128-1024, and dropout rate from {0.1, 0.5}, based on validation performance.</p><p>We employ strong message-passing GNN and graph Transformer baselines, including GCN <ref type="bibr" target="#b35">[36]</ref>, GAT <ref type="bibr" target="#b68">[69]</ref>, GIN <ref type="bibr" target="#b76">[77]</ref> which has 2-WL expressiveness similar to ours, and Graphormer <ref type="bibr" target="#b77">[78]</ref> based on fully-connected node self-attention. For message-passing GNNs, we use a 4-layer architecture and search hidden dimension d from {64, 1024} based on validation performance. For Graphormer, we perform an exploratory search on the number of layers from 1-4, heads H from 1-4, and hidden dimension d from 128-1024 based on validation performance. We apply 0.5 dropout for all baselines.</p><p>Training and Evaluation We report and compare classification accuracy on the test nodes at best validation accuracy aggregated over 7 randomized runs. We train all models with node-level categorical cross-entropy loss using Adam optimizer <ref type="bibr" target="#b34">[35]</ref> on a single RTX 3090 GPU with 24GB. We train all models with a learning rate of 1e-3 for 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are in <ref type="table" target="#tab_10">Table 5</ref>. Graphormer <ref type="bibr" target="#b77">[78]</ref> suffers out-of-memory in the Physics dataset mainly due to the spatial encoding that requires O(n 2 ) memory. By constraining the model capacity appropriately, we were able to run Graphormer on other datasets. However, we observe a low performance, presumably due to the memory cost that prevents depth and head scaling. As the spatial encoding is incorporated into the model via attention bias, the model strictly requires O(n 2 ) memory and cannot be easily made more efficient. On the other hand, TokenGT variants are able to utilize Performer attention with O(m + n) cost, which allows using larger models to achieve the best performance in all but one dataset (Computers, where the performance is on par with the best model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Additional Discussion on Performance on PCQM4Mv2 (Section 5.2)</head><p>As in the <ref type="table" target="#tab_2">Table 2</ref> in the main text, TokenGT currently shows a slightly lower performance compared to the Graphormer and its successors in the PCQM4Mv2 benchmark. We conjecture this is partly because we intentionally keep its components simple to faithfully adhere to the equivariance theory. We discuss some engineering approaches that may enhance the performance of TokenGT at the cost of differentiating from the theory. We consider engineering TokenGT to match or outperform sophisticated graph Transformers as a promising and important next research direction.</p><p>Node Identifiers Our best performing TokenGT (Lap) currently uses Laplacian eigenvectors <ref type="bibr" target="#b19">[20]</ref> as the node identifiers, which has been criticized for issues such as loss of structural information <ref type="bibr" target="#b37">[38]</ref> and sign ambiguity <ref type="bibr" target="#b41">[42]</ref>. Thus, one could try to relax the theoretical requirement for orthonormality of node identifiers and incorporate more powerful node positional encodings <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref> as node identifiers, which could potentially yield better performance in practice.</p><p>(Hyper)Edge Tokens TokenGT currently treats an undirected input edge (u, v) as if both directions (u, v) and (v, u) are present, leading to a pair of edge tokens [X <ref type="bibr">(u,v)</ref> , P u , P v ] and [X <ref type="bibr">(v,u)</ref> , P v , P u ]. Similarly, an undirected order-k input hyperedge (v 1 , ..., v k ) of an higher-order hypergraph is parsed to all possible orderings of node identifiers. While this is a common characteristic of tensor-based permutation equivariant neural networks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref>, they can lead to memory overhead and redundancy since multiple tokens represent an identical undirected edge. To avoid this, one can use a single token for each undirected (hyper)edge and pool the node identifiers as k i=1 ?(P vi ). Combined with powerful node identifiers, this approach could potentially enhance the model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of Tokenized Graph Transformer (TokenGT)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b81">Zaheer et al. (2017)</ref> <ref type="bibr" target="#b81">[82]</ref> and <ref type="bibr" target="#b46">[47]</ref> derived the parameter sharing for node permutation-symmetric data (sets and (hyper)graphs), which gives the maximally expressive equivariant linear layers and k-IGN in Section 3.1. The work on equivariant neural networks underlie our theory of how a standard Transformer can be a powerful learner for sets and (hyper)graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Andreoli et al. (2019) [1] cast self-attention and convolution into a unified framework using basis tensors similar to ones in Section 3.1. Cordonnier et al. (2020) [15] advanced the idea and showed that Transformers with relative positional encodings can approximate any image convolution layers. Lee et al. (2019) [39] and Kim et al. (2021)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Self-attention maps learned under various node and type identifier configurations for two target equivariant basis tensors (out of 15). For better visualization, we clamp the entries by 0.01. Self-attention learns acute patterns coherent to equivariant basis when orthonormal node identifiers and type identifiers are both provided as input. More images can be found in Appendix A.4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Attention distance by head and network depth. Each dot shows mean attention distance in hops across graphs of a head at a layer. The visualization is inspired by Dosovitskiy et al. (2020) [18]. More images can be found in Appendix A.4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Property 1 .</head><label>51</label><figDesc>Query and key projection matrices w Q h , w K h (Eq. (18), Eq. (19)). Uncolored cells are zeros. For given order-(l + k) equivalence class ? and positive real number &gt; 0, for any i ? [n] l and j ? [n] k , (i, j) ? ? holds if and only if the scoring function ?(i, j; ?, ) (Eq. (16)) outputs the maximum possible value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>or 1 ia=j b = 0 for sgn(a, b) = 1 or 1 ia=j b = 1 for sgn(a, b) = ?1 for some a ? [l], b ? [k]. Any of these violations decrements the output of Eq. (16) by a positive (1 or ), resulting in a non-maximum output. Thus, the scoring function ?(i, j; ?, ) gives the maximum possible output if and only if (i, j) ? ?. Now, we prove Lemma 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>k = 3 case example of bell(k) = 5 type identifiers embedded in d e = 3 dimensional space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Attention distance by head and network depth, measured for entire PCQM4Mv2 training set. Each dot shows mean attention distance in hops across graphs of a head at a layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Second-order equivariant basis approximation. We report average and standard deviation of L2 error averaged over heads over 3 runs. For Random/ORF (first-order), we sample random embeddings independently for each token. ? 0.600 53.93 ? 1.426 29.88 ? 0.450 34.70 ? 1.167 ? 32.38 ? 0.448 40.06 ? 1.202 15.92 ? 0.275 20.39 ? 0.765 Random (first-order) 32.19 ? 0.476 32.49 ? 3.687 15.87 ? 0.247 16.56 ? 0.904 ORF (first-order) 32.35 ? 0.369 39.87 ? 1.263 15.87 ? 0.247 16.56 ? 0.908 Random ? 5.909 ? 0.019 5.548 ? 0.090 8.152 ? 0.042 8.270 ? 0.285 ORF ? 5.472 ? 0.035 5.143 ? 0.078 7.167 ? 0.025 7.190 ? 0.217</figDesc><table><row><cell></cell><cell></cell><cell cols="2">dense input</cell><cell></cell><cell cols="2">sparse input</cell></row><row><cell>node id.</cell><cell>type id.</cell><cell>train L2 ?</cell><cell cols="2">test L2 ?</cell><cell>train L2 ?</cell><cell>test L2 ?</cell></row><row><cell cols="7">? 47.95 Laplacian eigenvector ? ? 1.899 ? 3.050 1.702 ? 2.912 0.288 ? 0.019 0.064 ? 0.010</cell></row><row><cell>Random</cell><cell></cell><cell cols="5">0.375 ? 0.009 0.234 ? 0.011 0.990 ? 0.108 0.875 ? 0.042</cell></row><row><cell>ORF</cell><cell></cell><cell>0.080 ? 0.001</cell><cell cols="4">0.009 ? 5e-5 0.129 ? 0.002 0.011 ? 0.002</cell></row><row><cell cols="2">Laplacian eigenvector</cell><cell cols="5">0.053 ? 1.5e-5 0.005 ? 1e-4 0.101 ? 0.003 0.019 ? 0.007</cell></row><row><cell>No embedding</cell><cell>Type id. only</cell><cell cols="2">Node id. only</cell><cell cols="2">Node id. + type id.</cell><cell>Equivariant basis</cell></row><row><cell></cell><cell cols="2">Self-attention matrix ?</cell><cell></cell><cell></cell><cell></cell></row></table><note>h Basis tensor B ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>) with bell(2 + 2) = 15 heads and explicitly supervise head-wise attention scores ? h to approximate each (normalized) equivariant basis tensor B ? h by minimizing L2 loss. Having the layer hyperparameters fixed, we provide different combinations of node and type identifiers, and test if multihead self-attention can jointly approximate all 15 equivariant basis on unseen graphs. We experiment with both dense and sparse graph representations; for graphs with n nodes and m edges, the dense graph considers all n 2 pairwise edges as input as in Section 3, whereas the sparse graph considers only the present m edges as in Section 2. Further details can be found in Appendix A.3.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on PCQM4Mv2 large-scale graph regression benchmark. We report the Mean Absolute Error (MAE) on the validation set, and report MAE on the unavailable test set if possible. method # parameters valid MAE ? test-dev MAE ? asymptotics</figDesc><table><row><cell>Message-passing GNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN [27]</cell><cell>2.0M</cell><cell>0.1379</cell><cell>0.1398</cell><cell>O(n + m)</cell></row><row><cell>GIN [27]</cell><cell>3.8M</cell><cell>0.1195</cell><cell>0.1218</cell><cell>O(n + m)</cell></row><row><cell>GAT</cell><cell>6.7M</cell><cell>0.1302</cell><cell>N/A</cell><cell>O(n + m)</cell></row><row><cell>GCN-VN [27]</cell><cell>4.9M</cell><cell>0.1153</cell><cell>0.1152</cell><cell>O(n + m)</cell></row><row><cell>GIN-VN [27]</cell><cell>6.7M</cell><cell>0.1083</cell><cell>0.1084</cell><cell>O(n + m)</cell></row><row><cell>GAT-VN</cell><cell>6.7M</cell><cell>0.1192</cell><cell>N/A</cell><cell>O(n + m)</cell></row><row><cell>GAT-VN (large)</cell><cell>55.2M</cell><cell>0.1361</cell><cell>N/A</cell><cell>O(n + m)</cell></row><row><cell cols="3">Transformers with strong graph-specific modifications</cell><cell></cell><cell></cell></row><row><cell>Graphormer [63]</cell><cell>48.3M</cell><cell>0.0864</cell><cell>N/A</cell><cell>O(n 2 )</cell></row><row><cell>EGT [29]</cell><cell>89.3M</cell><cell>0.0869</cell><cell>0.0872</cell><cell>O(n 2 )</cell></row><row><cell>GRPE [54]</cell><cell>46.2M</cell><cell>0.0890</cell><cell>0.0898</cell><cell>O(n 2 )</cell></row><row><cell>Pure Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>48.5M</cell><cell>0.2340</cell><cell>N/A</cell><cell>O((n + m) 2 )</cell></row><row><cell>TokenGT (ORF)</cell><cell>48.6M</cell><cell>0.0962</cell><cell>N/A</cell><cell>O((n + m) 2 )</cell></row><row><cell>TokenGT (Lap)</cell><cell>48.5M</cell><cell>0.0910</cell><cell>0.0919</cell><cell>O((n + m) 2 )</cell></row><row><cell>TokenGT (Lap) + Performer</cell><cell>48.5M</cell><cell>0.0935</cell><cell>N/A</cell><cell>O(n + m)</cell></row><row><cell cols="2">TokenGT (ORF)</cell><cell></cell><cell cols="2">TokenGT (Lap)</cell></row><row><cell>Mean attention distance (hops)</cell><cell></cell><cell></cell><cell></cell><cell>Head 1 Head 32 ...</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Network depth (layer)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the datasets. (a) Statistics of Barab?si-Albert random graph dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(b) Statistics of PCQM4Mv2 dataset.</cell></row><row><cell>Dataset</cell><cell>Barab?si-Albert</cell><cell>Dataset</cell><cell>PCQM4Mv2</cell></row><row><cell>Size</cell><cell>1280</cell><cell>Size</cell><cell>3.7M</cell></row><row><cell cols="2">Average # node 14.9</cell><cell cols="2">Average # node 14.1</cell></row><row><cell cols="2">Average # edge 47.8</cell><cell cols="2">Average # edge 14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>For an input [X[null]  ; X in w in ], the goal is to supervise each of the H = 15 self-attention heads with attention matrices ? 1 , ..., ? 15 to explicitly approximate row-normalized version of each equivariant basis B ?1 , ..., B ?15 ? R N ?N (Eq. (2)) on the N = (n + m) or n 2 input tokens (except[null]). An issue in supervision is that for rows of B ? that only contains zeros, normalization is not defined. To sidestep this, for such rows we simply supervise to attend to the special [null] token only. For all other rows of B ? that contains nonzero entry, we supervise the model to ignore [null] token.Training and EvaluationWe train and evaluate all models with L2 loss between attention matrix ? h and normalized basis tensor B ? h (involving [null] token) averaged over heads h = 1, ..., 15.</figDesc><table /><note>[null] with trainable embedding X [null] ? R d (we shortly explain its use) to obtain the final input [X [null] ; X in w in ] ? R (1+N )?d .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Performer inTable 2, we load a trained model checkpoint of TokenGT (Lap), change its self-attention to FAVOR+ kernelized attention of Performer<ref type="bibr" target="#b10">[11]</ref> that can provably accurately approximate softmax attention, and fine-tune it with AdamW optimizer for 0.1M training steps with 1k warmup iterations and cosine learning rate decay. With batch size 1024 on 8 RTX 3090 GPUs, fine-tuning takes ? 12 hours. We do not use stochastic depth and eigenvector dropout for fine-tuning. For GAT baselines in</figDesc><table><row><cell>Attention and MLP dropout rate 0.1</cell></row><row><cell>? Weight decay 0.1</cell></row><row><cell>? Stochastic depth [28, 64] with linearly increasing layer drop rate, reaching 0.1 at last layer</cell></row><row><cell>? Eigenvector dropout rate 0.2 for TokenGT (Lap) (see Appendix A.3.1)</cell></row><row><cell>For TokenGT (Lap) +</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the transductive node classification datasets.</figDesc><table><row><cell>Dataset</cell><cell>CS</cell><cell cols="2">Physics Photo</cell><cell cols="3">Computers Chameleon Crocodile</cell></row><row><cell># nodes</cell><cell cols="2">18,333 34,493</cell><cell>7,650</cell><cell>13,752</cell><cell>2,277</cell><cell>11,631</cell></row><row><cell># edges</cell><cell cols="4">81,894 247,962 119,081 245,861</cell><cell>36,101</cell><cell>180,020</cell></row><row><cell cols="2"># classes 15</cell><cell>5</cell><cell>8</cell><cell>10</cell><cell>6</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Transductive node classification. OOM denotes out-of-memory error on a 24GB RTX 3080 GPU. We report aggregated test accuracy at best validation accuracy over 7 randomized runs.? 0.005 0.937 ? 0.01 0.947 ? 0.006 0.914 ? 0.002 0.632 ? 0.011 0.692 ? 0.017 GIN 0.895 ? 0.005 0.886 ? 0.046 0.886 ? 0.017 0.362 ? 0.051 0.479 ? 0.027 0.515 ? 0.041 Graphormer 0.791 ? 0.015 OOM 0.894 ? 0.004 0.814 ? 0.013 0.457 ? 0.011 0.489 ? 0.014 TokenGT (Near-ORF) + Performer 0.882 ? 0.007 0.931 ? 0.009 0.872 ? 0.011 0.82 ? 0.019 0.568 ? 0.019 0.583 ? 0.024 TokenGT (Lap) + Performer 0.902 ? 0.004 0.941 ? 0.007 0.919 ? 0.009 0.86 ? 0.012 0.637 ? 0.032 0.638 ? 0.025 TokenGT (Lap) + Performer + SEB 0.903 ? 0.004 0.950 ? 0.003 0.949 ? 0.007 0.912 ? 0.006 0.653 ? 0.029 0.718 ? 0.012 A.4.1 Second-Order Equivariant Basis Approximation (Section 5.1)</figDesc><table><row><cell></cell><cell>CS</cell><cell>Physics</cell><cell>Photo</cell><cell>Computers</cell><cell>Chameleon</cell><cell>Crocodile</cell></row><row><cell>GCN</cell><cell cols="5">0.895 ? 0.004 0.932 ? 0.004 0.926 ? 0.008 0.873 ? 0.004 0.593 ? 0.01</cell><cell>0.660 ? 0.01</cell></row><row><cell>GAT</cell><cell>0.893</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">If such key index j does not exist, corresponding basis tensor entries are B ? i,j = 0?j, and approximation target cannot be defined as normalizing denominator j B ? i,j is 0. Thus we do not approximate for such i, let attention row ? i,? have any finite values, and later silence their attention output by multiplying zero at MLP.<ref type="bibr" target="#b3">4</ref> we handle the case j B ? h i,j = 0 later separately as mentioned in footnote 3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that the g(H i ) h gives 0 for all i that j B ? h i,j = 0, which automatically handles the corner case as discussed at footnote 3 and footnote 4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Andreoli</surname></persName>
		</author>
		<title level="m">Convolution, attention and structure embedding. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<title level="m">Emergence of scaling in random networks. arXiv</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the computational power of transformers and its implications in sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-rank bottleneck in multi-head attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2022</title>
		<imprint/>
	</monogr>
	<note>Cited on 24</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2020. (Cited on 1, 7, 10</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A note on over-smoothing for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 4</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 2, 9, 10, 22, 24</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of structured random orthogonal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 3, 23</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Steerable cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 3, 4, 7, 10</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 3, 4, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<title level="m">Benchmarking graph neural networks. arXiv, 2020</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 3, 7, 10, 23</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 2, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximation with random bases: Pro et contra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Tyukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Sofeikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Approximating continuous functions by relu nets of minimal width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sellke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 14, 20</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Masked autoencoders are scalable vision learners. arXiv, 2021. (Cited on 10</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus). arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 24</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Cited on 2, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Cited on 24</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Edge-augmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<title level="m">Perceiver IO: A general architecture for structured inputs &amp; outputs. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Cited on 1, 7, 22</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 7, 22</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 22</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 28</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformers generalize deepsets and can be extended to graphs and hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 2, 4, 6, 7, 14, 15, 28</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">First place solution of 2019 champs predicting molecular properties challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mailoa</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/champs-scalar-coupling/discussion/106575" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 10</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention. NeurIPS, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 7, 10, 22, 23</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 7, 22</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On the expressive power of self-attention matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Sign and basis invariant networks for spectral graph representation learning. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 7, 10, 23</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mesh graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 24</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luna</surname></persName>
		</author>
		<title level="m">Linear unified nested attention. NeurIPS, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 22</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 2, 4, 6, 14, 28</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 2, 4, 5, 6, 7, 14, 15, 28</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 4, 28</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On learning sets of symmetric elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transformer for graphs: An overview from architecture perspective. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Universal graph transformer self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW, 2022</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Permutation equivariant layers for higher order interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS, 2022</title>
		<imprint/>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<title level="m">Grpe: Relative positional encoding for graph transformer. arXiv, 2022</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 22</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>?olna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bordbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>A generalist agent. arXiv, 2022. (Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 1, 7)</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complex Networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Cited on 26</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Set2graph: Learning graphs from sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 7, 28</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 26</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 22</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Benchmarking graphormer on large-scale molecular modeling datasets. arXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Cited on 9</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Efficient transformers: A survey. arXiv, 2020. (Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1, 3, 9</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note>Cited on 1, 7, 24</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Deepnet: Scaling transformers to 1, 000 layers. arXiv, 2022. (Cited on 10</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>with linear complexity. arXiv, 2020. (Cited on 22</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Self-attention with structural position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cited on 1</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Which transformer architecture fits my data? A vocabulary bottleneck in self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Cited on 24</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint/>
	</monogr>
	<note>Cited on 22</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Do transformers really perform bad for graph representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
		</imprint>
	</monogr>
	<note>Cited on 1, 2, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Orthogonal random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Cited on 3, 23</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Are transformers universal approximators of sequence-to-sequence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
	<note>Cited on 7, 14</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cited on 7</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">1-wl expressiveness is (almost) all you need. arXiv, 2022</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zopf</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Cited on 2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
