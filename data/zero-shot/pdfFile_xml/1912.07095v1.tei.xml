<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Named Entity Recognition with Truecasing Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
							<email>mayhew@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
							<email>nitishg@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<email>danroth@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Named Entity Recognition with Truecasing Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although modern named entity recognition (NER) systems show impressive performance on standard datasets, they perform poorly when presented with noisy data. In particular, capitalization is a strong signal for entities in many languages, and even state of the art models overfit to this feature, with drastically lower performance on uncapitalized text. In this work, we address the problem of robustness of NER systems in data with noisy or uncertain casing, using a pretraining objective that predicts casing in text, or a truecaser, leveraging unlabeled data. The pretrained truecaser is combined with a standard BiLSTM-CRF model for NER by appending output distributions to character embeddings. In experiments over several datasets of varying domain and casing quality, we show that our new model improves performance in uncased text, even adding value to uncased BERT embeddings. Our method achieves a new state of the art on the WNUT17 shared task dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Modern named entity recognition (NER) models perform remarkably well on standard English datasets, with F1 scores over 90% <ref type="bibr" target="#b8">(Lample et al. 2016)</ref>. But performance on these standard datasets drops by over 40 points F1 when casing information is missing, showing that these models rely strongly on the convention of marking proper nouns with capitals, rather than on contextual clues. Since text in the wild is not guaranteed to have conventional casing, it is important to build models that are robust to test data case.</p><p>One possible solution to keep NER systems from overfitting to capitalization is to omit casing information. This would potentially lead to better performance on uncased text, but fails to take advantage of an important signal.</p><p>We build on prior work in truecasing <ref type="bibr" target="#b13">(Susanto, Chieu, and Lu 2016)</ref> by proposing to use a truecaser to predict missing case labels for words or characters (as shown in <ref type="figure">Figure  1</ref>). Intuitively, since named entities are marked with capitals, a trained truecaser should improve an NER system. We design an architecture in which the truecaser output is fed into a standard BiLSTM-CRF model for NER. We experiment with pretraining the truecaser, and fine-tuning on the NER train set. When designing the truecaser, we suggest a Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. eliud kipchoge has won in london again.</p><p>Eliud Kipchoge has won in London again.</p><p>Truecaser <ref type="figure">Figure 1</ref>: This figure shows the same sentence in two casing scenarios: on the top, uncased, on the bottom, truecased. Notice that the words marked as uppercase are also named entities. This paper explores how truecasing can be used in named entity recognition.</p><p>preprocessing regimen that biases the data towards named entity mentions.</p><p>In our experiments, we show that truecasing remains a difficult task, and further that although a perfect truecaser gives high NER scores, the quality of the truecaser does not necessarily correlate with NER performance. Even so, incorporating predicted casing information from the right truecaser improves performance in both cased and uncased data, even when used in conjunction with uncased BERT <ref type="bibr" target="#b6">(Devlin et al. 2019)</ref>. We evaluate on three NER datasets (CoNLL, Ontonotes, WNUT17) in both cased and uncased scenarios, and achieve a new state of the art on WNUT17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truecaser Model</head><p>A truecaser model takes a sentence as input and predicts case values (upper vs. lower) for all characters in the sentence. We model truecasing as a character-level binary classification task, similar to <ref type="bibr" target="#b13">Susanto, Chieu, and Lu (2016)</ref>. The labels, L for lower and U for upper, are predicted by a feedforward network using the hidden representations of a bidirectional LSTM (BiLSTM) that operates over the characters in a sentence. For example, in <ref type="figure">Figure 2</ref>, the truecaser predicts "name was Alan" for the input "name was alan". Formally, the output of this model is a categorical distribution d c over two values (true/false), for each character c, where h c ? R H is the hidden representation for the c-th character from the BiLSTM, and W ? R 2?H represents a learnable feed-forward network. In addition to the parameters of the LSTM and the feedforward network, the model also learns character embeddings. Generating training data for this task is trivial and does not need any manual labeling; we lower-case text to generate input instances for the truecaser, and use the original case values as the gold labels to be predicted by the model. The model also uses whitespace characters as input for which the gold label is always lowercase (L) (not shown in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed NER Model</head><p>In NER literature, the standard model is the BiLSTM-CRF. In this model, token embeddings are given as input to a BiLSTM, and the output hidden representations are in turn passed as features to a Conditional Random Field (CRF). The BiLSTM input embeddings are typically a concatena-tion of pretrained word embeddings (such as GloVe; <ref type="bibr" target="#b9">(Pennington, Socher, and Manning 2014)</ref>) and encoded character embeddings. Given the popularity of this model, we omit a detailed description, and refer interested readers to prior work <ref type="bibr" target="#b3">(Chiu and Nichols 2016;</ref><ref type="bibr" target="#b8">Lample et al. 2016</ref>).</p><p>Our proposed model augments the standard BiLSTM at the character level, appending truecaser predictions (d c , from above) to each character embedding. Formally, let the input embedding for token t be x t .</p><formula xml:id="formula_0">x t = w x f (c 1 , c 2 , ..., c |xt| )<label>(1)</label></formula><p>Here, word embeddings w x are concatenated with an embedding generated by f (.), which is some encoder function over character embeddings c 1:|xt| representing characters in x t . This is usually either a BiLSTM, or, in our case, a convolutional neural network (CNN). Even if the word vectors are uncased, casing information may be encoded on the character level through this mechanism.</p><p>For any x t , our model extends this framework to include predictions from a truecaser for all characters, as a concatenation of character embedding c i with the corresponding truecaser prediction</p><formula xml:id="formula_1">d i . v i = c i d i<label>(2)</label></formula><p>Now, the input vector to the BiLSTM is:</p><formula xml:id="formula_2">x t = w x f (v 1 , v 2 , ..., v |xt| )<label>(3)</label></formula><p>In this way, each character is now represented by a vector which encodes the value of the character (such as 'a', or 'p'), and also a distribution over its predicted casing. This separation of character and casing value has also been used in <ref type="bibr" target="#b3">Chiu and Nichols (2016)</ref> and <ref type="bibr" target="#b5">Collobert et al. (2011)</ref>. The distinction of our model is that we explicitly use the predicted casing value (and not the true casing in the data). The reason for using predicted casing is to make the NER model robust to noisy casing during test time. Also, an NER system using gold-casing while training might overfit to quirks of the training data, whereas using predicted casing will help the model to learn from its own mistakes and to better predict the correct NER tag.</p><p>A diagram of our model is shown in <ref type="figure">Figure 3</ref>. We refer to the two-dimensional addition to each character vector as "case vectors." Notice that there are two separate character vectors present in the model: those that are learned by the truecaser and those that are learned in the NER model (char vectors). We chose to keep these separate because it's not clear that the truecaser character vectors encode what the NER model needs, and they need to stay fixed in order to work properly in the truecaser model.</p><p>In our experiments, we found that it is best to detach the truecaser predictions from the overall computation graph. This means that the gradients propagated back from the NER tag loss do not flow into the truecaser. Instead, we allow parameters in the truecaser to be modified through two avenues: pretrained initialization, and truecaser fine-tuning.  <ref type="table">Table 2</ref>: Example sentences from the Common Crawl truecasing training data. Notice that the first word is capitalized only for names, "thursday" is lowercased, and "Seventies" is a non-named entity which was not caught by the lowercasing rules.</p><p>These avenues are orthogonal and can operate independently of each other or together. In one of our experiments, we try all combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Having described our models, now we move to the experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truecaser data</head><p>Supervision for a truecaser comes from raw text, with the only requirement being that the use of capitals follow certain desirable standards. We train on two different datasets: the Wikipedia dataset (Wiki) introduced in Coster and Kauchak (2011) and used in <ref type="bibr" target="#b13">Susanto, Chieu, and Lu (2016)</ref>, and a specially preprocessed large dataset from English Common Crawl (CC). 1 Statistics for each dataset are found in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We used the Wiki dataset as is, but modified the CC dataset in a few important ways. Since our ultimate goal is to train NER models, we applied rules with the aim of leaving only named entities capitalized. We did this in two ways: 1) convert the first word of each sentence to its most common form, and 2) lowercase certain words which are commonly capitalized, but rarely named entities.</p><p>To accomplish the first, we used scripts from the moses package <ref type="bibr" target="#b7">(Koehn et al. 2007</ref>) to collect casing statistics for each word in a large corpus, then replace the first word of each sentence with the most common form. 2 For example, this sentence begins with 'For', which is more likely to take the form 'for' in a large corpus.</p><p>To accomplish the second preprocessing step, we briefly examined the training data, and applied rules that lowercased a small number of words. For example, this list in-1 commoncrawl.org 2 In a naming clash, the moses script is called a 'truecaser', even though it only touches sentence-initial words.   <ref type="table" target="#tab_2">Table 3</ref>.</p><p>CoNLL 2003 English, a widely used dataset constructed from newswire, uses 4 tags: Person, Organization, Location, and Miscellaneous.</p><p>Ontonotes is the largest of the three datasets, and is composed of 6 diverse sub-genres, with a named entity annotation layer with 17 tags. We use the v5 split from <ref type="bibr" target="#b10">Pradhan et al. (2012)</ref>.</p><p>WNUT17 is the dataset from the shared task at the Workshop for Noisy User-generated Text 2017 (WNUT17), with 6 named entity tags. Since the focus of the shared task was on emerging entities, the dev and test sets are considerably different from the train set, and have a distinctly low entity overlap with train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>When setting up experiments, we always assume that we have access to cased training data, and the uncertainty arises in the casing of the test data. We suggest that there are only two casing scenarios for test data: 1) test data is cased correctly with high probability, (this may be the situation if the text comes from a reputable news source, for example) and 2) test data in which there is some doubt about the quality of the casing. In such a case, we argue that the text should be all lowercased as a preprocessing step.</p><p>These two scenarios will guide our experiments. In the first scenario, high-likelihood cased test data, we will train models on cased text, and evaluate on cased text, as well as uncased text. This simulates the situation in which lowercased text is given to the model. Since there is a casing mismatch between train and test, we would expect these numbers to be low. In the second scenario, we target known lowercased text and all training data is lowercased.</p><p>In all experiments, we used a truecaser trained on the Common Crawl data, with character embedding of size 50, hidden dimension size 100, and dropout of 0.25. One trick during training was to leave 20% of the sentences in their original case, effectively teaching the truecaser to retain casing if it exists, but add casing if it doesn't. We refer to this as pass-through truecasing.</p><p>For the NER model, we used character embeddings of size 16, hidden dimension size 256, and GloVe 100dimensional uncased embeddings (Pennington, Socher, and Manning 2014). 3 Even though the word embeddings are uncased, the character embeddings retain case.</p><p>For the WNUT17 experiments, we used 100-dimensional GloVe twitter embeddings, and 300-dimensional embeddings from FastText, 4 trained on English Common Crawl.</p><p>For BERT, we used the model called bert-base-uncased as provided by HuggingFace. <ref type="bibr">5</ref> All experiments used AllenNLP <ref type="bibr" target="#b7">(Gardner et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>This section describes our experiments and results, first for the truecaser, then for the NER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truecaser</head><p>We train our truecaser on each of the two datasets (Wiki and CC), and evaluate on the corresponding test sets, as well as on the test sets of NER datasets. Further, we train the truecaser on NER training sets (without using the NER labels), and evaluate respectively. Performance is shown in <ref type="table" target="#tab_4">Table 4</ref>, using character-level F1, with U as the positive label.</p><p>The highest scores for each test set are obtained with models trained on the corresponding training set. Given the difference in training data preprocessing, the performance of Wiki and CC are not comparable. All the same, a pattern emerges between the two truecasers: the highest performance is on Ontonotes, then CoNLL, then WNUT17. In fact, this pattern holds even in the bottom section, where the NER training set was used for training.</p><p>Given that the pattern holds not only across truecasers from separate data, but also when training on in-domain training sets, this suggests that the explanation lies not in the power of the truecaser used to predict, but in the consistency (or inconsistency) of the data.</p><p>Since capitalization is just a convention, truecasing is not well-defined. For example, if in this sentence, THESE WORDS were capitalized, it would not be so much "wrong" as unconventional. As such, the rankings of scores should be understood as a measure of how much a particular dataset 3 nlp.stanford.edu/projects/glove/ 4 fasttext.cc 5 github.com/huggingface/pytorch-pretrained-BERT/  No truecaser could be reasonably expected to correctly mark capitals here.</p><p>Finally, WNUT17, the twitter dataset, has the lowest scores, suggesting the lowest attention to convention. Intuitively, this makes sense, and examples from the training set confirm it:</p><p>? HAPPY B . DAY TORA *OOOOOOOOO* ? ThAnK gOd ItS fRiDaY !! ? im thinking jalepeno poppers tonight :] ] ? Don Mattingly will replace Joe Torre as LA Dodgers manager after this season Ultimately, it's important to understand that truecasers are far from perfect. It may seem like a simple task, but the many edge cases and inconsistencies can lead to poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truecaser Initialization for NER</head><p>We would like to know which truecaser training data performs best for NER. We would also like to understand if there is a correlation between truecaser and NER performance.</p><p>To measure this, we train a standard BiLSTM-CRF with GloVe embeddings on uncased text, using several different truecaser pretraining initializations, shown in <ref type="table" target="#tab_6">Table 5</ref>. As baseline and ceiling respectively, we use No initialization (None) and perfect predictions (Gold). Further, we compare  With no initialization, the model is poor, and with perfect initialization (gold predictions), the model has performance comparable to models trained on cased text. However, the story with learned initializations shows that higher Char F1 does not lead to higher NER F1.</p><p>This may seem counter-intuitive, but consider the following. There exists a truecaser that capitalizes the first word of each sentence, and every occurrence of 'Mrs.' and 'I'. Such a truecaser would have reasonably high Char F1, but poor NER performance. Conversely, there also exists a truecaser that capitalizes every named entity and nothing else, it would clearly be useful for NER, but would perform poorly on truecaser metrics.</p><p>In this light, if you can't build a perfect truecaser, then targeting entities is the next best option. This validates our decision to preprocess the CC truecaser training data to more closely track with named entities. With this intuition, and the results from this experiment, we chose to use CC in all further experiments.</p><p>The CoNLL-trained truecaser achieves high Char F1, but hurts the NER performance. We suspect that the truecaser learns to capitalize certain easy words (first word of sentence, day names) and to memorize names in the training data, and the model learns to rely on this signal, as in regular cased data. But at test time, the remaining 19% F1 consists of names that the truecaser fails to correctly capitalize, thus misleading the NER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Using the CC truecaser, we experiment in several settings on the three NER datasets. All results are shown in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>Cased Training Data In the first setting (top section of the table), we train on cased NER data, resulting in standard performance on the cased data, but a severe drop in performance on the uncased data (from 90.2 F1 to 24.3 F1). This is the performance one might see when using a typical NER model in the wild. Adding our truecaser representations, we maintain scores on cased test data, and significantly improve performance on uncased data; by an average of 12.5 F1.</p><p>The middle section of <ref type="table" target="#tab_8">Table 6</ref> shows results from a data augmentation approach proposed in <ref type="bibr" target="#b9">Mayhew, Tsygankova, and Roth (2019)</ref>. In this approach, we simply concatenate cased training data with a lowercased copy of itself. This leads to strong performance on the average of cased and uncased outputs, although the outcome in the WNUT17 is somewhat degraded.</p><p>Uncased Training Data In the second experiment (lower half of <ref type="table" target="#tab_8">Table 6</ref>), we address the scenario in which the casing of the test data cannot be trusted, leading us to lower-case all test data as a pre-processing step.</p><p>In theory, appending gold case labels to the character vectors (as in <ref type="bibr" target="#b3">Chiu and Nichols (2016)</ref>, for example) should result in performance comparable to training on cased data. This is the case for CoNLL and Ontonotes, as shown in the GloVe + Gold case vectors row.</p><p>We experiment with two different embeddings: using GloVe uncased embeddings as before, or using BERT uncased embeddings. In each setting, we train models with and without the truecaser representations.</p><p>The first experiment with GloVe is equivalent to training on lower-cased text only, which is about 3 points lower than the cased version. This drop is much lower than the performance gap seen when training on cased data, but tested on uncased data. When we use our truecaser representations in this model, we see a consistent improvement across datasets, with an average of 1.4 points F1. This goes to show that even when training on lower-cased data, using the predicted truecasing labels helps the model perform better on uncased text.</p><p>Although the data augmentation approach is effective for average performance on cased and uncased data, if the target data is known to be lowercased (or if one decides to lowercase it because of uncertain casing), then the approach in this section has the best performance.</p><p>We gathered scores for each sub-genre in Ontonotes, displayed in <ref type="table" target="#tab_10">Table 7</ref>. Performance increases in every subsection, with the greatest gains in the broadcast conversation (bc) subsection (1.9) and the smallest in the telephone conversation (tc) subsection (0.3).</p><p>When using BERT uncased embeddings, even without the truecaser, the performance is better than a cased model without BERT, which shows that BERT is a strong contextual token embedder as compared to a BiLSTM with GloVe embeddings. Using the truecaser along with BERT, the model performance still improves (average of 0.3 points F1), showing that the truecaser is able to provide the model with complementary information that BERT does not capture.</p><p>Fine-Tuning and Pretraining So far, we have used a pretrained truecaser with no fine-tuning. However, we can finetune the truecaser on training data with case labels, when available. <ref type="table" target="#tab_11">Table 8</ref> on WNUT17 shows that fine-tuning a pretrained truecaser substantially increases truecasing performance from 30.6 F1 to 52.3 F1. Though, this increase does not translate to an increase in NER performance, perhaps because of the domain mismatch in the pretrained truecaser's training data and WNUT17.  When we do not initialize the truecaser with pretrained parameters, but train the truecaser from scratch in combination with the NER objective, although the truecasing performance is not the highest, the NER performance improves greatly, achieving state of the art performance on WNUT17. In this case, during the initial iterations, the NER model receives random truecaser predictions, encouraging it to discriminate using the context. As the truecaser improves, the NER model now receives casing predictions helpful for learning, leading to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our work uses truecasing as a pretraining objective, with the specific goal of improving NER. In this section, we discuss prior work in each area.</p><p>Truecasing Early work in truecasing (Brown and Coden 2001) was motivated by the prevalence of tasks that produced "case deficient" outputs, such as closed-caption TV, and automatic speech recognition (ASR) <ref type="bibr" target="#b8">(Kubala et al. 1998</ref>). The proposed solution was a mix of heuristics (capitalizing single letters followed by a period), learned rules (frequency tables of commonly capitalized words), and contextual clues learned from running an NER system over well-cased data.</p><p>Further innovations include lattice-based decoding with language models <ref type="bibr" target="#b8">(Lita et al. 2003)</ref>, and truecasing as sequence tagging (Chelba and Acero 2006), with applications to machine translation <ref type="bibr" target="#b14">(Wang, Knight, and Marcu 2006)</ref> and social media <ref type="bibr" target="#b9">(Nebhi, Bontcheva, and Gorrell 2015)</ref>. Later, <ref type="bibr" target="#b13">Susanto, Chieu, and Lu (2016)</ref>    Pretraining Objectives In recent years, several works have shown how models trained over large amounts of raw text can produce powerful contextual representations <ref type="bibr" target="#b6">(Devlin et al. 2019;</ref><ref type="bibr" target="#b9">Peters et al. 2018)</ref>. In most works, the training objective is language modeling, or predicting a masked word given context. Our work can be seen as pretraining with a truecasing objective.</p><p>Named Entity Recognition Named Entity Recognition (NER) is the task of identifying and classifying named entity mentions, such as persons, organizations, and locations. NER is typically modeled as a sequence tagging problem, in which each word is given a named entity tag, often using BIO encoding (Ramshaw and Marcus 1996) to mark phrases. Early models of NER used linear methods, including classifiers with Conditional Random Fields (CRF) (Finkel and Manning 2009), and weighted averaged perceptron (Ratinov and Roth 2009). Neural models based on BiLSTM-CRF have since shown strong performance, and have consequently become ubiquitous <ref type="bibr" target="#b3">(Chiu and Nichols 2016;</ref><ref type="bibr" target="#b8">Lample et al. 2016)</ref>.</p><p>When using the BiLSTM-CRF model, it is common to include character embeddings, either encoded with a convolutional neural network (CNN) <ref type="bibr" target="#b3">(Chiu and</ref><ref type="bibr">Nichols 2016), or an LSTM (Lample et al. 2016)</ref>. In some cases, notably <ref type="bibr" target="#b3">(Chiu and Nichols 2016)</ref> and <ref type="bibr" target="#b5">(Collobert et al. 2011</ref>), casing is included as an explicit feature in the character and word embeddings. Our work is similar to this, except we predict the casing.</p><p>In recent years, contextual representations such as ELMO (Peters et al. 2018) and BERT <ref type="bibr" target="#b6">(Devlin et al. 2019</ref>) have proven to be remarkably successful for NER.</p><p>While there is little work that targets robustness of general NER models, there has been work on NER in noisy domains like twitter <ref type="bibr" target="#b11">(Ritter et al. 2011)</ref>, and several Workshops on Noisy User-generated Text (WNUT) <ref type="bibr" target="#b12">(Strauss et al. 2016;</ref><ref type="bibr" target="#b5">Derczynski et al. 2017)</ref>. In particular, (Aguilar et al. 2018) target the WNUT17 task, achieving strong results using a phonetic representation to model their text (with no capitalization, incidentally), and including multitask objectives. Recent work has suggested data augmentation as a solution <ref type="bibr" target="#b9">(Mayhew, Tsygankova, and Roth 2019;</ref><ref type="bibr" target="#b2">Bodapati, Yun, and Al-Onaizan 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We have shown how pretraining with a truecasing objective can improve the robustness of a named entity recognition system to in both cased and uncased test scenarios. Our experiments with varied types of truecasing training data give insights into best practices for preprocessing. Finally, we have demonstrated that BERT uncased representations are helpful for lowercased NER, but can also be extended with our techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>d c = softmax(Wh c ) arXiv:1912.07095v1 [cs.CL] 15 Dec 2019 Diagram of our truecasing model. The input is characters, and the output hidden states from the BiLSTM are used to predict binary labels, 'U' for upper case, and 'L' for lower case. A sentence fragment is shown, but the B-LSTM takes entire sentences as input. Diagram of our NER model. The left side shows a standard BiLSTM CRF model with word vectors (red) concatenated with character vectors (purple). The right side shows how the character vectors are created. The gray shaded area is our truecaser, with parameters frozen. This produces 2-dimensional case predictions (in blue) for each character. These are concatenated with learned character embeddings (in green) before going to a CNN for encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of tokens in the train/dev/test splits for Wikipedia (Wiki) and Common Crawl (CC). Texas genius . Rothbart visits at 6 p.m. next thursday for a talk . has Smith made a decision ? in the Seventies , you would have written Britain off .</figDesc><table><row><cell>Dataset</cell><cell>Train Dev</cell><cell>Test</cell></row><row><cell>Wikipedia</cell><cell cols="2">2.9M 294K 32K</cell></row><row><cell cols="3">Common Crawl 24M 247K 494K</cell></row><row><cell>Example sentences</cell><cell></cell><cell></cell></row><row><cell cols="3">" the investigation is still ongoing , " McMahon said .</cell></row><row><cell>he 's a</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Data sizes of the NER datasets. In each group, the top row shows number of tokens, the bottom row shows number of entity phrases. order to avoid keeping titles or other upper cased sentences, we removed all sentences in which the ratio of capitalized words to total words exceeded 20%. This removed such misleading sentences as "Man Bites Dog In Pajamas."Table 2shows some example sentences from the CC training data.</figDesc><table><row><cell>cludes titles (Mr., Senator, Archbishop), month/day names</cell></row><row><cell>(January, Thursday, although not April, or May, as these</cell></row><row><cell>may well be person names), and other conventionally capi-</cell></row><row><cell>talized words which are not entities (GMT, PM, AM). These</cell></row><row><cell>rules covered many commonly capitalized non-entities, but</cell></row><row><cell>are not comprehensive.</cell></row><row><cell>Further, in NER Data</cell></row><row><cell>We experiment on 3 English datasets: CoNLL 2003 (Sang</cell></row><row><cell>and Meulder 2003), Ontonotes v5 (Hovy et al. 2006),</cell></row><row><cell>WNUT17 (Derczynski et al. 2017). Data statistics are shown</cell></row><row><cell>in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Truecasing performance on the character level</cell></row><row><cell>(precision, recall, F-measure). In each of the three bottom</cell></row><row><cell>sections, there is a descending pattern of F1 scores from</cell></row><row><cell>Ontonotes, CoNLL, WNUT17. This suggests decreasing ob-</cell></row><row><cell>servance of capitalization conventions.</cell></row><row><cell>follows convention. These observations ring true with in-</cell></row><row><cell>spection of the data. Ontonotes, with some exceptions, fol-</cell></row><row><cell>lows relatively standard capitalization conventions. CoNLL</cell></row><row><cell>is slightly less standard, with a combination of headlines</cell></row><row><cell>and bylines, such as "RUGBY UNION -CUTTITTA BACK</cell></row><row><cell>FOR ITALY", and tables summarizing sporting events. In a</cell></row><row><cell>few memorable cases, capitalization is repurposed, as in:</cell></row><row><cell>League games on Thursday ( home team in CAPS ) :</cell></row><row><cell>Hartford 4 BOSTON 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: CoNLL2003 testb performance of BiLSTM-</cell></row><row><cell>CRF+GloVe uncased with different pretraining initializa-</cell></row><row><cell>tions, no auxiliary loss. NER F1 does not correlate with true-</cell></row><row><cell>caser character-level F1.</cell></row><row><cell>using a truecaser trained on Wiki, trained on CC, and trained</cell></row><row><cell>on CoNLL training data directly.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>F1 scores on cased (C) and uncased (U) test data. Models are trained on cased text in the upper section, and uncased text in the lower section. +truecaser means we include our pretrained two-dimensional character embeddings without finetuning. ? uses additional gazetteer features. * our run using their code, training only on train data. The ? columns shows average performance improvements of adding the truecaser.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>study the task with characterbased representations and recurrent neural networks.</figDesc><table><row><cell>Test Set</cell><cell cols="2">F1 F1 +truecaser</cell><cell>?</cell></row><row><cell cols="2">All (micro) 83.4</cell><cell cols="2">84.4 1.0</cell></row><row><cell>bc</cell><cell>81.8</cell><cell cols="2">83.7 1.9</cell></row><row><cell>bn</cell><cell>87.5</cell><cell cols="2">88.6 1.1</cell></row><row><cell>mz</cell><cell>79.5</cell><cell cols="2">81.0 1.5</cell></row><row><cell>nw</cell><cell>85.8</cell><cell cols="2">86.6 0.8</cell></row><row><cell>tc</cell><cell>69.6</cell><cell cols="2">69.9 0.3</cell></row><row><cell>wb</cell><cell>76.1</cell><cell cols="2">76.6 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Scores on uncased Ontonotes test set from an uncased model, broken down by sub-genre. The leftmost column is a standard BiLSTM-CRF, the middle column is our proposed approach, and the rightmost column is the difference. In all cases, the truecaser outperforms the original model, with the greatest improvements in bc (broadcast news) and mz (magazine).</figDesc><table><row><cell>TC Train</cell><cell cols="2">NER F1 Char F1</cell></row><row><cell>Fixed pretrained</cell><cell>46.9</cell><cell>30.6</cell></row><row><cell>Fine-tuned pretrained</cell><cell>46.3</cell><cell>52.3</cell></row><row><cell>From scratch</cell><cell>47.7</cell><cell>36.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results on WNUT17 with BERT uncased, trained and tested on uncased (U) data, varying the Truecaser (TC) training paradigm.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by Contracts HR0011-15-C-0113 and HR0011-18-2-0052 with the US Defense Advanced Re-search Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling noisiness to recognize named entities using multitask neural networks on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguilar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1401" to="1412" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blythe</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vollgraf ; Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptation of maximum entropy capitalizer: Little data can help a lot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Bodapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Onaizan ;</forename><surname>Bodapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acero</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Information Retrieval Techniques for Speech Applications</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="382" to="399" />
		</imprint>
	</monogr>
	<note>Robustness to capitalization errors in named entity recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Named Entity Recognition with Bidirectional LSTM-CNNs. TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>and Nichols</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on monolingual text-to-text generation</title>
		<meeting>the workshop on monolingual text-to-text generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-08" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
	<note>Proceedings of the 3rd Workshop on Noisy User-generated Text</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Proc. of the Conference on Empirical Methods for Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</title>
		<meeting>the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Proceedings of HLT/NAACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kubala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsygankova</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth ;</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsygankova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nebhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bontcheva</forename><surname>Nebhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1111" to="1115" />
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Balancing Act: Combining Symbolic and Statistical Approaches to Language</title>
		<editor>Klavans, J., and Resnik, P.</editor>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>Proc. of the Conference on Computational Natural Language Learning (CoNLL)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno>Strauss et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)</title>
		<meeting>the 2nd Workshop on Noisy User-generated Text (WNUT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to capitalize with character-level recurrent neural networks: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieu</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu ;</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2090" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Capitalizing machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
