<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-14">14 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kunitsyn</surname></persName>
							<email>kunitsyn.alexnder@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kalashnikov</surname></persName>
							<email>maxim.kalashnikov@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
							<email>dzabraev.maksim1@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">Ivaniuta</forename><surname>Huawei</surname></persName>
						</author>
						<title level="a" type="main">MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-14">14 Mar 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video</term>
					<term>language</term>
					<term>retrieval</term>
					<term>multi-modal</term>
					<term>cross-modal</term>
					<term>tem- porality</term>
					<term>transformer</term>
					<term>attention</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we present a new State-of-The-Art on the textto-video retrieval task on MSR-VTT, LSMDC, MSVD, YouCook2 and TGIF obtained by a single model. Three different data sources are combined: weakly-supervised videos, crowd-labeled text-image pairs and textvideo pairs. A careful analysis of available pre-trained networks helps to choose the best prior-knowledge ones. We introduce three-stage training procedure that provides high transfer knowledge efficiency and allows to use noisy datasets during training without prior knowledge degradation. Additionally, double positional encoding is used for better fusion of different modalities and a simple method for non-square inputs processing is suggested.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The text-to-video retrieval task is defined as search of most relevant video segments for an arbitrary natural language text query. A search query may contain description of arbitrary actions, objects, sounds or a combinations of them. Note that an arbitrary search query means zero-shot mode of search. A specific search query might not occur in the training database. Despite this, the model should successfully perform the search operation.</p><p>The text-to-video retrieval technology can be used for semantic search within a single long video. For example, inside a full-length movie or a stream video. After describing the event, the user can easily find the appropriate video segment. A more general task is the search for a relevant video segment within a large gallery, for example, the entire video hosting like YouTube or Vimeo.</p><p>Another application is the search for a specific event in a surveillance cameras dataset or real time video stream. This can be useful to identify illegal actions, accidents or any other important events.</p><p>An important requirement for a text-to-video retrieval system is scaling to a large video gallery. A good example of an efficient architecture is the twostream models. Within this approach the video segment and the text query are encoded independently by the video model and text model respectively. Separate processing allows to compute embeddings for the entire video gallery beforehand. During the inference time, the system calculates the embedding for the search query. Next it calculates the similarity function between query embedding and each embedding from the gallery. Most common choice for similarity function is the cosine similarity.</p><p>The data required for the training consists of pairs of video segment and text description. Noise Contrastive Estimation (NCE) is currently the most common framework for this task <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>. Within the framework, the model learns to distinguish a positive pair from a set of negative pairs. The most popular losses used in NCE are bi-directional max-margin ranking loss <ref type="bibr" target="#b20">[21]</ref> and symmetric cross entropy loss <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Since a search query may describe a sound or a visual component, it is important to capture information from both visual stream and audio stream of input video. In this work we fuse information from three modalities: RGB modality (processes each frame independently), motion modality (processes multiple consecutive frames) and audio modality.</p><p>Transfer learning-based methods are getting more and more popular to be applied for this task. One of the first successful applications of transfer learning for text-to-video retrieval task can be attributed to <ref type="bibr" target="#b30">[31]</ref>, where several pre-trained networks are used to extract features from video. In <ref type="bibr" target="#b13">[14]</ref> the authors additionally adopted BERT model <ref type="bibr" target="#b7">[8]</ref> as initialization for text encoder. Later works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref> use CLIP model as initialization for both text and vision encoders.</p><p>Pre-trained models, suitable for the text-to-video retrieval task can be divided into two classes. The first class is trained using crowd-labeled datasets such as Imagenet <ref type="bibr" target="#b6">[7]</ref> or Kinetics <ref type="bibr" target="#b21">[22]</ref> datasets. Usually such models produce task-specific embeddings, which does not allow to achieve high quality in the text-to-video retrieval task. The second class is trained with a large amount of weakly-supervised data collected from the Internet. The most popular are CLIP, BERT and irCSN152, which are trained with the IG65M dataset (irCSN152-IG65M) <ref type="bibr" target="#b15">[16]</ref>.</p><p>The analysis of pre-trained models in <ref type="bibr" target="#b10">[11]</ref> and our experience show that models trained with a large amount of web-crawled data are able to produce embeddings for general application and allow to reach better quality in the textto-video retrieval task.</p><p>Using CLIP as an initialization or a feature extractor significantly improves the results in the text-to-video retrieval task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>. The CLIP model family has several different architectures. All of them have independent text encoder and visual encoder.</p><p>In this work we manage to use text-video, text-image and text-video weaklysupervised (HT100M) datasets together in the same training. In addition, we use the best pre-trained models. This allows us to achieve State-of-The-Art results with a single model on a number of benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our model follows the idea of MDMMT <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>. However, we suggest an advanced multistage training approach, as well as perform analysis of existing prior knowledge and choose optimal backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The architecture consists of four parts: pre-trained experts, aggregator, text encoder and text embedding projection.</p><p>Pre-trained expert is a frozen pre-trained network that produces sequence of features for input video. In this work we use three experts, each for different modality. The first one is for image (RGB modality), processes video frames independently. The second one is for motion. It deals with several continuous frames together. The third one is for audio. See pseudocode example in Lst. 1.1.</p><p>The aggregator accepts embeddings made by experts and produce single embedding for the video. See pseudocode example in Lst. 1.2.</p><p>The text encoder accepts arbitrary English natural language text and produces embedding. The text embedding projection part maps text embedding to distinct space for each modality. See example in Lst 1.3. GEU* means Gated Embedding Unit <ref type="bibr" target="#b27">[28]</ref>. a1 , a2 , a3 = softmax ( FC_512_to_3 ( temb )) temb_rgb = a1 * GEU1 ( temb ) temb_motion = a2 * GEU2 ( temb ) temb_audio = a3 * GEU3 ( temb ) return concatenate ([ temb_rgb , temb_motion , temb_audio ]) # (512*3 , )</p><p>Note that this architecture is flexible. It is possible to remove or add additional modalities. Also it is possible to replace a given pre-trained text encoder with another one. For example, it is possible to use CLIP ViT-B/32 as RGB expert and text part of CLIP ViT-B/16 as text encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Double positional encoding</head><p>Each expert takes different type and shape of data as input. For example, CLIP takes a single image frame to produce an embedding. irCSN152-IG65M produces a single embedding from a sequence of 32 consecutive frames. Slow-Fast (SF) <ref type="bibr" target="#b22">[23]</ref> takes a melspectrogram of a 5 seconds long audio frame to produce an embedding.</p><p>Positional encoding is used in the transformer encoder architecture to provide information about the order of tokens in input sequence. In our case, positional (temporal) encoding has to provide information not only about the order of tokens, but also about the time length of each individual token.</p><p>We introduce double positional encoding. For each embedding we add two biases: the first stands for the timestamp of beginning of video segment and the second represents timestamp of end of video segment, see pseudocode in Lst. 1.4. This way we make sure that different time lengths per expert embedding are processed correctly. The results in Tab. 1 support this novelty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets</head><p>A list of datasets used in this work is provided in Tab. 2. Only training splits of listed datasets are used in training dataset. Note that we use both text-video and text-image datasets. In Sec. 4.4 we show results for video only datasets and image plus video datasets. Since each dataset has different amount of videos and captions, it is important to combine datasets properly <ref type="bibr" target="#b10">[11]</ref>.</p><p>In the following experiments MSR-VTT full clean split is used. This split is introduced in <ref type="bibr" target="#b10">[11]</ref>. The test part of full clean split is the same as test part of full split. The training part of full clean split mostly similar to full split, but some videos are removed. All removed videos have corresponding duplicate in test part.  <ref type="bibr" target="#b11">[12]</ref> 14k 70k 69k LSMDC <ref type="bibr" target="#b40">[41]</ref> 101k 101k 101k TwitterVines <ref type="bibr" target="#b0">[1]</ref> 6.5k 23k 23k YouCook2 <ref type="bibr" target="#b51">[52]</ref> 1.5k 12k 12k MSVD <ref type="bibr" target="#b3">[4]</ref> 2k 80k 64k TGIF <ref type="bibr" target="#b26">[27]</ref> 102k 125k 125k SomethingV2 <ref type="bibr" target="#b17">[18]</ref> 193k 193k 124k VATEX <ref type="bibr" target="#b44">[45]</ref> 28k 278k 278k TVQA <ref type="bibr" target="#b23">[24]</ref> 20k 179k 178k Sum above 477k 1261k</p><p>Flicker30k <ref type="bibr" target="#b47">[48]</ref> 32k 159k 158k COCO <ref type="bibr" target="#b4">[5]</ref> 123k 617k 592k Conceptual Captions <ref type="bibr" target="#b42">[43]</ref> 3M 3M 2M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss</head><p>The MDMMT-2 is trained with the bi-directional max-margin ranking loss <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_0">1 B B i=1 j =i max(0, s ij ? s ii + m) + max(0, s ji ? s ii + m)<label>(1)</label></formula><p>where B, s ij , m denote the batch size, the similarity score between the i-th query and the j-th video of the given batch, and some predefined margin correspondingly. We set m = 0.05 and B = 256 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In Results are reported as mean ?std or just mean over 3 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLIP</head><p>In <ref type="bibr" target="#b10">[11]</ref> it is shown that CLIP works as a strong visual feature extractor and outperforms other available models by large margin. We found out that CLIP text backbone also works better than other available text models, such as BERT <ref type="bibr" target="#b7">[8]</ref>, which was originally used in <ref type="bibr" target="#b13">[14]</ref>, or GPT <ref type="bibr" target="#b2">[3]</ref>. Currently there are several publicly available CLIP models. In this section we compare their performance to make sure that we use the best possible combination. Results are presented in Tab. 3.</p><p>Our observations:</p><p>-Suppose we have pre-trained CLIP: text backbone and corresponding visual backbone. We observe that if we replace original visual backbone with a bigger/deeper one, we obtain better video retrieval system. -If we use the same visual backbone with different text backbones, a text backbone of a bigger/deeper model not necessarily shows better results.</p><p>In fact, if we take a look at (Tab. 3) RN50(xN) models, the best result is achieved by a combination of the deepest visual backbone (RN50x64) and the text backbone from the most shallow model (RN50). -CLIP ViT-L/14 shows the best performance both as visual and text backbone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experts combination</head><p>Using combination of different experts allows to achieve better performance. In Tab. 5 various combinations of experts are presented. Using three modalities gives the best result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dealing with non-square videos</head><p>Both irCSN152-IG65M and CLIP take videos (images) of square shape as input. Therefore it is not possible to use information from the whole video directly. It may happen that some object or action is taking place in the corner (out of the center crop) of the video. So if we use center crop to compute embeddings, the information from the corners will be lost. There are several possible solutions to this problem:</p><p>-Squeeze a video to a square without saving the aspect ratio (squeeze) -Pad a video to a square with blackbars (padding) -Take several crops from the video, average the embeddings of these crops, and use this average as embedding (mean)</p><p>For the mean technique we take three crops: left or bottom, center, right or top (depending on video orientation) and then average embeddings of these crops.</p><p>Experiments in Tab. 6 show that squeeze works worse than center crop, padding works slightly better than center crop, and mean works the best.</p><p>We want to emphasize that using mean during test improves video-retrieval performance even if other methods were used during train. In <ref type="bibr" target="#b10">[11]</ref> it is shown that the proper combination of datasets allows to train a single model that can capture the knowledge from all used datasets and in most cases the model trained on the combination of datasets is better than the model trained on a single dataset. In Tab. 8 we show that proper combination of text-video and text-image datasets allows to improve video-retrieval performance. Hyperparameters are specified in Sec. 4.5, stage S 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adding images</head><p>Weights for combining all datasets are specified in Tab. 7. First 10 rows are video datasets (denoted as 10V) and last 3 are image datasets (denoted as 3I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pre-training and fine-tuning</head><p>Note that in our work aggregator is initialised form scratch, while text backbone is pre-trained. If we simultaneously train randomly initialised aggregator and pre-trained text backbone, then at the time when aggregator will be trained, the text backbone might degrade. That is why for final result we introduce training procedure that consists of three stages (denoted as S 0 , S 1 , S 2 ).</p><p>During stage S 0 we use noisy HT100M dataset. Text backbone is frozen, only aggregator and text embedding projection part are trained.</p><p>During stage S 1 we use crowd-labeled datasets 10V+3I . Same as in S 0 , text backbone is frozen, only aggregator and text embedding projection part are trained.</p><p>During stage S 2 , same as in S 1 , we use crowd-labeled datasets 10V+3I. Now, however, we unfreeze text backbone and train all three main components: aggregator, text backbone and text embedding projection.</p><p>Hyperparameters for these stages are listed in Tab. 9. Results for different combinations of stages are listed in Tab. 10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We performed a refined study of each conceptual part of transformer application for the text-to-video retrieval task. The analysis of the prior knowledge allows to choose optimal existing backbone experts. Combining different types of data sources allows to significantly increase the overall training data amount. Also we suggest a multi-stage training procedure without experts fine-tuning, which prevents their overfitting to a particular domain. Usage of the expanded data and optimal experts leads to a great increase in the generalization ability. It allows to obtain a model, which simultaneously performs well in multiple domains and benefits with the domains diversity increasing. We demonstrate an incredible novelty -possibility to obtain SOTA results in different domains by a same model, instead of preparing a domain-specific model for each. In particular, we obtained new SOTA results in MSR-VTT, LSMDC, MSVD, YouCook2 and TGIF with a single model trained only once. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Listing 1 . 1 .Listing 1 . 2 .</head><label>1112</label><figDesc>Example of pre-trained expert usage def encode_rgb ( V ): # V : input video embs = [] frames_lst = r e a d _ 1 _ f r a m e _ p e r _ s e c o n d( V ) for frame in frames_lst : emb = image_network ( frame ) embs . append ( emb ) rgb_embs = concatenate ( embs , dim =0) return rgb_embs Example of aggregator def aggregator ( rgb_embs , motion_embs , audio_embs ): rgb_embs = FC_768_to_512 ( rgb_embs ) rgb_cls = rgb_embs . max ( dim =0) + rgb_bias # (1 , 512) rgb_input = rgb_embs + positional + rgb_bias # do the same for other modalities x = concatenate ([ rgb_cls , motion_cls , audio_cls , rgb_input , motion_input , audio_input ] , dim =0) x = transforme r _e n co d er ( x ) x = normalize ( x ) video_emb = x [:3]. reshape ( -1) # (512*3 , ) return video_emb</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Listing 1 . 3 .</head><label>13</label><figDesc>Text embedding projection def t e x t _ e m b e d d i n g _ p r o j e c t i o n( temb ):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Listing 1 . 4 .</head><label>14</label><figDesc>Pseudocode for double positional encoding # nsec : video duration in seconds positions_be g = nn . Parameter (32 , 512) positions_en d = nn . Parameter (32 , 512) audio_embs = audio_embs + positions_beg [0::5][: nsec //5] + positions_end [5::5][: nsec //5] rgb_embs = rgb_embs + positions_beg [: nsec ] + positions_end [1:][: nsec ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of standard positional encoding with proposed double positional encoding. Dataset: MSR-VTT full clean split (see Sec. 3.3); Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-L/14, irCSN152-IG65M, SF Temporal Text ? Video Embedding R@1? R@5? R@10? MdR? Single 22.1?0.1 48.2?0.0 60.0?0.1 6.0?0.0 Double 22.2?0.1 48.5?0.2 60.3?0.2 6.0?0.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The "Num videos" column represents the number of video clips (images) in the dataset, the "Num pairs" column represents the total number of video-caption (image-caption) pairs, the "Num unique captions" column represents the number of unique captions in the dataset</figDesc><table><row><cell></cell><cell>Num</cell><cell cols="2">Num Num</cell></row><row><cell>Dataset</cell><cell cols="3">videos pairs unique</cell></row><row><cell></cell><cell>(images)</cell><cell></cell><cell>captions</cell></row><row><cell>MSR-VTT [46]</cell><cell>10k</cell><cell>200k</cell><cell>167k</cell></row><row><cell>ActivityNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of CLIP visual and text backbones combinations. RN50x16 43.9 43.5 43.6 43.0 44.4 44.5 44.4 RN50x64 44.6 43.9 44.1 44.2 44.8 45.2 45.4 ViT-B/32 42.0 41.2 40.9 40.9 42.5 42.4 42.2 ViT-B/16 44.4 43.8 43.4 43.3 44.8 45.4 44.9 ViT-L/14 46.2 45.7 45.3 45.3 46.5 46.8 47.2</figDesc><table><row><cell>Experts: CLIP;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Experiments on different audio experts. Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-B/32, irCSN152-IG65M, audio</figDesc><table><row><cell cols="2">Audio</cell><cell></cell><cell cols="2">Text ? Video</cell></row><row><cell cols="2">expert</cell><cell cols="3">R@1? R@5? R@10? MdR?</cell></row><row><cell cols="5">vggish [19] 19.3?0.2 44.3?0.0 56.3?0.2 7.0?0.0</cell></row><row><cell cols="5">Slow-Fast [23] 19.6?0.3 44.9?0.3 57.0?0.2 7.0?0.0</cell></row><row><cell cols="5">Table 5. Experts combinations. Text backbone: CLIP ViT-B/32</cell></row><row><cell></cell><cell>Experts</cell><cell></cell><cell></cell><cell>Text ? Video</cell></row><row><cell cols="4">CLIP irCSN152-IG65M SF R@1?</cell><cell>R@5?</cell><cell>MdR?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">10.2?0.0 29.3?0.1 17.3?0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">11.2?0.1 31.5?0.2 15.0?0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">21.3?0.1 46.5?0.2 7.0?0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">21.5?0.1 46.7?0.1 7.0?0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">22.0?0.1 47.8?0.1 6.0?0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">22.2?0.1 48.5?0.2 6.0?0.0</cell></row><row><cell cols="5">Table 6. Comparison of different techniques for extracting features from non-square</cell></row><row><cell cols="5">videos. Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-L/14; Metric: R@5</cell></row><row><cell>Train</cell><cell cols="4">Test Squeeze Center crop Padding Mean</cell></row><row><cell cols="2">Squeeze</cell><cell>46.3</cell><cell>46.0</cell><cell>46.0 47.1</cell></row><row><cell cols="3">Center Crop 46.0</cell><cell>46.5</cell><cell>46.0 47.3</cell></row><row><cell cols="2">Padding</cell><cell>46.0</cell><cell>46.2</cell><cell>46.7 47.0</cell></row><row><cell cols="2">Mean</cell><cell>45.9</cell><cell>46.4</cell><cell>45.9 47.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Datasets used in train procedure. The "Weight" column describes how often we sample examples from the dataset. The probability of obtaining an example from the dataset with the weight w equals to w divided by a sum of all weights</figDesc><table><row><cell>Dataset</cell><cell>Weight</cell><cell>Type</cell></row><row><cell>MSR-VTT</cell><cell>140</cell><cell></cell></row><row><cell>ActivityNet</cell><cell>100</cell><cell></cell></row><row><cell>LSMDC</cell><cell>70</cell><cell></cell></row><row><cell>Twitter Vines YouCook2 MSVD TGIF</cell><cell>60 20 20 102</cell><cell>Text-video datasets (10V)</cell></row><row><cell>SomethingV2</cell><cell>169</cell><cell></cell></row><row><cell>VATEX</cell><cell>260</cell><cell></cell></row><row><cell>TVQA</cell><cell>150</cell><cell></cell></row><row><cell>COCO</cell><cell cols="2">280 Text-image</cell></row><row><cell>Flicker30k</cell><cell>200</cell><cell>datasets</cell></row><row><cell cols="2">Conceptual Captions 160</cell><cell>(3I)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Test results on MSR-VTT full clean split.</figDesc><table><row><cell>Text backbone: CLIP ViT-B/32;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Hyperparameters for different stages</figDesc><table><row><cell cols="4">Train Examples Num. Learning stage per epoch epochs rate</cell><cell>? Datasets</cell></row><row><cell>S0</cell><cell>60k</cell><cell>200</cell><cell cols="2">5e-5 0.98 HT100M</cell></row><row><cell>S1</cell><cell>380k</cell><cell>45</cell><cell cols="2">5e-5 0.95 10V+3I</cell></row><row><cell>S2</cell><cell>200k</cell><cell>20</cell><cell>2e-5</cell><cell>0.8 10V+3I</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Test results for train stages on MSR-VTT full clean split. Training procedure is described in Sec. 4.5. Results are shown in Tab. 11 -Tab.<ref type="bibr" target="#b15">16</ref>.Center crop is used for visual features extraction during training and testing for all datasets except MSR-VTT (see Tab. 12), where we report two results on testing set: center crop and mean method (see Sec. 4.3).</figDesc><table><row><cell>Text backbone:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .Table 12 .Table 13 .Table 14 .Table 15 .Table 16 .</head><label>111213141516</label><figDesc>Test results on MSR-VTT-1k-A dataset. 5?0.3 75.4?0.3 83.9?0.5 13.8?0.3 2.0?0.0 Test results on MSR-VTT dataset. Results are collected from articles and https://paperswithcode.com/sota/video-retrieval-on-msr-vtt Test results on LSMDC dataset. Results are collected from articles and https://paperswithcode.com/sota/video-retrieval-on-lsmdc Ours) 26.9?0.6 46.7?0.5 55.9?0.4 48.0?0.5 6.7?0.5 Test results on MSVD dataset. Results are collected from articles and https://paperswithcode.com/sota/video-retrieval-on-msvd Test results on YouCook2 dataset. Results are collected from articles and https://paperswithcode.com/sota/video-retrieval-on-youcook2 Ours) 32.0?0.7 64.0?0.3 74.8?0.2 12.7?0.3 3.0?0.0 Test results on TGIF dataset. Results are collected from articles and https://paperswithcode.com/sota/video-retrieval-on-tgif Ours) 25.5?0.1 46.1?0.0 55.7?0.1 94.1?0.3 7.0?0.0</figDesc><table><row><cell>Results that were dual softmax [6, 15] on inference) are shown. Results are collected from articles and obtained using original testing protocol (without https://paperswithcode.com/sota/video-retrieval-on-msr-vtt-1ka Model MSR-VTT-1k-A text ? video R@1? R@5? R@10? MnR? MdR? JSFusion [49] 10.2 31.2 43.2 -13.0 E2E [33] 9.9 24.0 32.4 -29.5 HT [34] 14.9 40.2 52.8 -9.0 CE [28] 20.9 48.8 62.4 28.2 6.0 CLIP [39] 22.5 44.3 53.7 61.7 8.0 MMT [14] 26.6 57.1 69.6 24.0 4.0 AVLnet[42] 27.1 55.6 66.6 -4.0 SSB [37] 30.1 58.5 69.3 -3.0 CLIP agg [38] 31.2 53.7 64.2 -4.0 MDMMT [11] 38.9 69.0 79.7 16.5 2.0 CLIP4Clip [29] 44.5 71.4 81.6 15.3 2.0 CLIP2Video [13] 45.6 72.6 81.7 14.6 2.0 LAFF [20] 45.8 71.5 82.0 --CAMoE [6] 44.6 72.6 81.8 13.3 2.0 MDMMT-2 full (Ours) 46.5?0.8 74.3?0.6 83.3?0.2 14.1?0.1 2.0?0.0 QB-Norm+CLIP2Video [2] 47.2 73.0 83.0 -2.0 CLIP2TV [15] 48.3 74.6 82.8 14.9 2.0 MDMMT-2 1k-A (Ours) 48.Model MSR-VTT text ? video Split R@1? R@5? R@10? MnR? MdR? VSE [35] full 5.0 16.4 24.6 -47.0 VSE++ [35] 5.7 17.1 24.8 -65.0 Multi Cues [35] 7.0 20.9 29.7 -38.0 W2VV [9] 6.1 18.7 27.5 -45.0 Dual Enc. [10] 7.7 22.0 31.8 -32.0 CE [28] 10.0 29.0 41.2 86.8 16.0 MMT [14] 10.7 31.1 43.4 88.2 15.0 CLIP [39] 15.1 31.8 40.4 184.2 21.0 CLIP agg [38] 21.5 41.1 50.4 -4.0 MDMMT [11] 23.1 49.8 61.8 52.8 6.0 TACo [47] 24.8 52.1 64.0 -5.0 LAFF [20] 29.1 54.9 65.8 --CLIP2Video [13] 29.8 55.5 66.2 45.4 4.0 CAMoE [6] 32.9 58.3 68.4 42.6 3.0 CLIP2TV [15] 33.1 58.9 68.9 44.7 3.0 MDMMT-2 (Ours) 33.4?0.1 60.1?0.1 70.5?0.1 39.2?0.2 3.0?0.0 MDMMT-2 test mean (Ours) 33.7?0.1 60.5?0.0 70.8?0.1 37.8?0.3 3.0?0.0 MMT [14] full clean 10.4 30.2 42.3 89.4 16.0 MDMMT [11] 22.8 49.5 61.5 53.8 6.0 MDMMT-2 (Ours) 33.3 59.8 70.2 38.7 3.0 Model LSMDC text ? video R@1? R@5? R@10? MnR? MdR? CT-SAN [50] 5.1 16.3 25.2 -46.0 JSFusion [49] 9.1 21.2 34.1 -36.0 MEE [32] 9.3 25.1 33.4 -27.0 MEE-COCO [32] 10.1 25.6 34.6 -27.0 CE [28] 11.2 26.9 34.8 96.8 25.3 CLIP agg [38] 11.3 22.7 29.2 -56.5 CLIP [39] 12.4 23.7 31.0 142.5 45.0 MMT [14] 12.9 29.9 40.1 75.0 19.3 MDMMT [11] 18.8 38.5 47.9 58.0 12.3 CLIP4Clip [29] 21.6 41.8 49.8 58.0 -QB-Norm+CLIP4Clip [2] 22.4 40.1 49.5 -11.0 CAMoE [6] 25.9 46.1 53.7 54.4 -MSVD text ? video R@1? R@5? R@10? MnR? MdR? LAFF [20] 45.4 76.0 84.6 --CLIP4Clip [29] 46.2 76.1 84.6 10.0 2.0 CLIP2Video [13] 47.0 76.8 85.9 9.6 2.0 QB-Norm+CLIP2Video [2] 48.0 77.9 86.2 -2.0 CAMoE [6] 49.8 79.2 87.0 9.4 -MDMMT-2 (Ours) 56.8?0.2 83.1?0.2 89.2?0.1 8.8?0.0 1.0?0.0 Model YouCook2 text ? video R@1? R@5? R@10? MnR? MdR? Text-Video Embedding [34] 8.2 24.5 35.3 -24.0 COOT [17] 16.7 -52.3 --UniVL [30] 28.9 57.6 70.0 -4.0 TACo [47] 29.6 59.7 72.7 -4.0 MDMMT-2 (Model TGIF text ? video R@1? R@5? R@10? MnR? MdR? W2VV++ [26] 9.4 22.3 29.8 --SEA [25] 11.1 25.2 32.8 --LAFF [20] 24.5 45.0 54.5 --MDMMT-2 (Model MDMMT-2 (</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The text-to-video retrieval task originates in 2016 from work <ref type="bibr" target="#b40">[41]</ref>.</p><p>Nowadays there is large a number of high-quality crowd-labeled datasets suitable for text-to-video retrieval task <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b17">18]</ref> and numerous works using these datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">47]</ref>. In <ref type="bibr" target="#b32">[33]</ref> the authors leverage large amount of weakly-supervised data (HT100M dataset) from YouTube to train a model. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref> both weakly-supervised data for pre-training and crowd-labeled datasets for fine-tuning are used.</p><p>The task requires a large amount of data and looking for alternative data sources is quite reasonable. Since the visual stream of a video is a sequence of frames (images), any individual image can be considered as a one-frame video. In <ref type="bibr" target="#b31">[32]</ref> the authors successfully use both image-text and video-text datasets.</p><p>Impressive results are achieved in text-to-image retrieval by CLIP model, which is trained with a large amount of web-crawled data <ref type="bibr" target="#b39">[40]</ref>.</p><p>To create a text-to-video retrieval model for general application (without specialization for a particular domain), a large amount of data is required. The authors of CLIP use hundreds of millions of data units to train text-to-image retrieval model for general application. Most probably text-to-video retrieval task requires no less and rather more data.</p><p>Unfortunately, combining all crowd-labeled text-video and text-image datasets do not allow to approach to the high quality general application model. In <ref type="bibr" target="#b32">[33]</ref> the authors attempt to use large amount of weakly-supervised data but the result is still far from high quality model.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TRECVID 2020: comprehensive campaign for evaluating video retrieval tasks across multiple application domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<idno>Pro- ceedings of TRECVID 2020. NIST, USA. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cross Modal Retrieval with Querybank Normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12777</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language Models are Few-Shot Learners. 2020</title>
		<imprint/>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting Highly Parallel Data for Paraphrase Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting Visual Features From Text for Image and Video Caption Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmm.2018.2832602</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dual Encoding for Zero-Example Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06181</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MDMMT: Multidomain Multimodal Transformer for Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw53098.2021.00374</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<editor>Bernard Ghanem Fabian Caba Heilbron Victor Escorcia and Juan Carlos Niebles</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mastering Video-Text Retrieval via Image CLIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-modal Transformer for Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10639</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00561</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ging</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00597</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04261</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CNN Architectures for Large-Scale Audio Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09430</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lightweight Attentional Feature Fusion for Video Retrieval by Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01832</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.MM</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems -Volume 2. NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems -Volume 2. NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Kinetics Human Action Video Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Slow-Fast Auditory Streams For Audio Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03516</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TVQA: Localized, Compositional Video Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.3042067</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4351" to="4362" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">W2VV++: Fully Deep Learning for Ad-hoc Video Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350906</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TGIF: A New Dataset and Benchmark on Animated GIF Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06430</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Straightforward Framework For Video Retrieval Using CLIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s Andr?s</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Marin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12443</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>In: Image 2 (</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03705</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Learning Audio-Visual Language Representations from Instructional Videos. 2020</title>
		<imprint/>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03493</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09980</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00166</idno>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A Joint Sequence Fusion Model for Video Question Answering and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02947</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Making Convolutional Networks Shift-Invariant Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11486</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards Automatic Learning of Procedures From Web Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7590" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
