<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxu</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Birkbeck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balu</forename><surname>Adsumilli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
						</author>
						<title level="a" type="main">RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video quality assessment</term>
					<term>natural scene statistics</term>
					<term>temporal</term>
					<term>video compression</term>
					<term>perceptual quality</term>
					<term>user-generated content</term>
					<term>image quality assessment</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Blind or no-reference video quality assessment of user-generated content (UGC) has become a trending, challenging, heretofore unsolved problem. Accurate and efficient video quality predictors suitable for this content are thus in great demand to achieve more intelligent analysis and processing of UGC videos. Previous studies have shown that natural scene statistics and deep learning features are both sufficient to capture spatial distortions, which contribute to a significant aspect of UGC video quality issues. However, these models are either incapable or inefficient for predicting the quality of complex and diverse UGC videos in practical applications. Here we introduce an effective and efficient video quality model for UGC content, which we dub the Rapid and Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime. RAPIQUE combines and leverages the advantages of both quality-aware scene statistics features and semantics-aware deep convolutional features, allowing us to design the first general and efficient spatial and temporal (space-time) bandpass statistics model for video quality modeling. Our experimental results on recent large-scale UGC video quality databases show that RAPIQUE delivers top performances on all the datasets at a considerably lower computational expense. We hope this work promotes and inspires further efforts towards practical modeling of video quality problems for potential real-time and low-latency applications. To promote public usage, an implementation of RAPIQUE has been made freely available online: https://github.com/vztu/RAPIQUE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent years have witnessed an explosion of user-generated content (UGC) captured and streamed over social media platforms such as YouTube, Facebook, TikTok, and Twitter. Thus, there is a great need to understand and analyze billions of these shared contents to optimize video pipelines of efficient UGC data storage, processing, and streaming. UGC videos, which are typically created by amateur videographers, often suffer from unsatisfactory perceptual quality, arising from imperfect capture devices, uncertain shooting skills, and a variety of possible content processes, as well as compression and streaming distortions. In this regard, predicting UGC video quality is much more challenging than assessing the quality of synthetically distorted videos in traditional video databases. UGC distortions are more diverse, complicated, commingled, and no "pristine" reference is available.</p><p>Traditional video quality assessment (VQA) models have been widely studied <ref type="bibr" target="#b0">[1]</ref> as an increasingly important toolset used by the streaming and social media industries. While full-reference (FR) VQA research is gradually maturing and several algorithms <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> are quite widely deployed, recent attention has shifted more towards creating better no-reference (NR) VQA models that can be used to predict and monitor the quality of authentically distorted UGC videos. One intriguing property of UGC videos, from the data compression aspect, is that the original videos to be compressed often already suffer from artifacts or distortions, making it difficult to decide the compression settings <ref type="bibr" target="#b3">[4]</ref>. Similarly, it is of great interest Z. <ref type="bibr">Tu</ref> to be able to deploy flexible video transcoding profiles in industry-level applications based on measurements of input video quality to achieve even better rate-quality tradeoffs relative to traditional encoding paradigms <ref type="bibr" target="#b4">[5]</ref>. The decision tuning strategy of such an adaptive encoding scheme, however, would require the guidance of an accurate and efficient NR or blind video quality (BVQA) model suitable for UGC <ref type="bibr" target="#b5">[6]</ref>.</p><p>Many blind video quality models have been proposed to solve the UGC-VQA problem <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Among these, BRISQUE <ref type="bibr" target="#b7">[8]</ref>, GM-LOG <ref type="bibr" target="#b10">[11]</ref>, FRIQUEE <ref type="bibr" target="#b11">[12]</ref>, V-BLIINDS <ref type="bibr" target="#b8">[9]</ref>, and VIDEVAL <ref type="bibr" target="#b5">[6]</ref> have leveraged different sets of natural scene statistics (NSS)-based quality-aware features, using them to train shallow regressors to predict subjective quality scores. Another well-founded approach is to design a large number of distortion-specific features, whether individually <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, or combined, as is done in TLVQM <ref type="bibr" target="#b12">[13]</ref> to achieve a final quality prediction score. Recently, convolutional neural networks (CNN) have been shown to deliver remarkable performance on a wide range of computer vision tasks <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Several deep CNN-based BVQA models have also been proposed <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> by training them on recently created large-scale psychometric databases <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. These methods have yielded promising results on synthetic distortion datasets <ref type="bibr" target="#b0">[1]</ref>, but still struggled on UGC quality assessment databases <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>.</p><p>Prior work has mainly focused on spatial distortions, which have been shown to indeed play a critical role in UGC video quality prediction <ref type="bibr" target="#b5">[6]</ref>. The exploration of the temporal statistics of natural videos, however, has been relatively limited. The authors of <ref type="bibr" target="#b5">[6]</ref> have shown that temporal-or motionrelated features are essential components when analyzing the quality of mobile captured videos, as exemplified by those in the LIVE-VQC database <ref type="bibr" target="#b29">[30]</ref>. Yet, previous BVQA models that account for temporal distortions, such as V-BLIINDS and TLVQM, generally involve expensive motion estimation mod-els, which are not practical in many scenarios. Furthermore, while compute-efficient VQA models exist, simple BVQA models like BRISQUE <ref type="bibr" target="#b7">[8]</ref>, NIQE <ref type="bibr" target="#b32">[33]</ref>, GM-LOG <ref type="bibr" target="#b10">[11]</ref> are incapable of capturing complex distortions that arise in UGC videos. Complex models like V-BLIINDS <ref type="bibr" target="#b8">[9]</ref>, TLVQM <ref type="bibr" target="#b12">[13]</ref>, and VIDEVAL <ref type="bibr" target="#b5">[6]</ref>, on the contrary, perform well on existing UGC video databases, but are much less efficient, since they either involve intensive motion-estimation algorithms or complicated scene statistics features. A recent deep learning model, VSFA <ref type="bibr" target="#b18">[19]</ref>, which extracts frame-level ResNet-50 <ref type="bibr" target="#b33">[34]</ref> features followed by training a GRU layer, is also less practical due to the use of full-size, frame-wise image inputs and the recurrent layers.</p><p>We have made recent progress towards efficient modeling of temporal statistics relevant to the video quality problem, by exploiting and combining spatial and temporal scene statistics, as well as deep spatial features of natural videos. We summarize our contributions as follows:</p><p>? We created a rapid and accurate video quality predictor, called RAPIQUE, in an efficient manner, achieving superior performance that is comparable or better than stateof-the-art (SOTA) models, but with a relative 20x speedup on 1080p videos. The runtime of RAPIQUE also scales well as a function of video resolution, and is 60x faster than the SOTA model VIDEVAL on 4k videos. ? We built a first-of-its-kind BVQA model that combines novel, effective, and easily computed low-level scene statistics features with high-level deep learning features. Aggressive spatial and temporal sampling strategies are used, exploiting content and distortion redundancies, to increase efficiency without sacrificing performance. ? We created a new spatial NSS feature extraction module within RAPIQUE, which is a highly efficient and effective alternative to the popular but expensive featurebased BIQA model, FRIQUEE. The spatial NSS features used in RAPIQUE are suitable for inclusion as basic elements of a variety of perceptual transforms, leading to significant efficiencies which might also be useful when developing future BVQA models. ? We designed the first general, effective and efficient temporal statistics model (beyond frame-differences) that is based on bandpass regularities of natural videos, and which can also be used as a standalone module to boost existing BVQA methods on temporally-distorted or motion-intensive videos.</p><p>The rest of this paper is organized as follows. Section II reviews previous literature relating to video quality assessment models, while Section III unfolds the details of the RAPIQUE model. Experimental results and concluding remarks are given in Section IV and Section V, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traditional BVQA Models</head><p>Many early BVQA/BIQA models have been 'distortion specific' in that they were designed to quantify a specific type of distortion such as blockiness <ref type="bibr" target="#b34">[35]</ref>, blur <ref type="bibr" target="#b35">[36]</ref>, ringing <ref type="bibr" target="#b19">[20]</ref>, banding <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, or noise <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b38">[39]</ref> in compressed images and videos. Recent high-performing BIQA/BVQA models are almost exclusively learning-based, operating by training sets of generic quality-aware features, which are combined to conduct quality predictions <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>. Learningbased BVQA models are more versatile and generalizable than 'distortion specific' models, in that the selected features are broadly perceptually relevant, while powerful regression models can adaptively map the features onto quality scores learned from the data in the context of a specific application.</p><p>The most popular BVQA algorithms deploy perceptually relevant, low-level features based on simple, yet highly regular parametric bandpass models of scene statistics <ref type="bibr" target="#b39">[40]</ref>. These natural scene statistics (NSS) models often are predictably altered by the presence of distortions <ref type="bibr" target="#b40">[41]</ref>, although they have more limited power to characterize complex, commingled distortions. Successful picture quality models of this type have been explored in the wavelet (BIQI <ref type="bibr" target="#b41">[42]</ref>, DIIVINE <ref type="bibr" target="#b6">[7]</ref>, C-DIIVINE <ref type="bibr" target="#b42">[43]</ref>), DCT (BLIINDS <ref type="bibr" target="#b43">[44]</ref>, BLIINDS-II <ref type="bibr" target="#b44">[45]</ref>) and spatial domains (NIQE <ref type="bibr" target="#b32">[33]</ref>, BRISQUE <ref type="bibr" target="#b7">[8]</ref>), and have been further extended to encompass natural bandpass space-time video statistics models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>, among which the most well-known model is Video-BLIINDS <ref type="bibr" target="#b8">[9]</ref>. Other extensions of empirical NSS include models of the joint statistics of the gradient magnitude and Laplacian of Gaussian (GM-LOG <ref type="bibr" target="#b10">[11]</ref>), in log-derivative and log-Gabor spaces (DESIQUE <ref type="bibr" target="#b48">[49]</ref>), as well as in the gradient domain of LAB color transforms (HI-GRADE <ref type="bibr" target="#b9">[10]</ref>). The FRIQUEE model <ref type="bibr" target="#b11">[12]</ref> achieves excellent performance on UGC/consumer video/picture databases <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> by leveraging a bag of NSS features drawn from diverse color spaces and perceptually motivated transform domains.</p><p>Time-domain behavior is the key attribute that differentiates videos from still pictures. The perception of video correlates highly with motion and temporal change <ref type="bibr" target="#b51">[52]</ref>. Amongst BVQA models, Video-BLIINDS <ref type="bibr" target="#b8">[9]</ref> was the first to explore the use of (spatio-) temporal scene statistics of video using DCT coefficient statistics in the time-differenced domain. V-BLIINDS also involves calculating motion coherence and global motion features, which requires expensive motion estimation, to account for temporal masking effects.</p><p>Instead of using DCT transforms, Mittal et al. proposed a completely blind model called VIIDEO <ref type="bibr" target="#b46">[47]</ref>, which inspects the divisively normalized spatial statistics of frame differences. Bandpass filtering followed by divisive normalization was applied to frame differences, after which the inter-subband correlations are modeled over the temporal variation of the extracted generalized Gaussian parameters. As a highly experimental temporal-only model, VIIDEO includes no spatial features, hence does not perform well on natural UGC video datasets <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>Regarding the joint modeling of spatiotemporal statistics, Li et al. proposed to adopt 3D-DCT transforms of local spacetime regions from videos to extract quality-aware features <ref type="bibr" target="#b45">[46]</ref>. More recently, the authors of <ref type="bibr" target="#b52">[53]</ref> leveraged 3D divisive normalization transformed (DNT) and spatiotemporal Gaborfiltered responses of 3D-DNT coefficients of natural videos. The 3D transforms adopted therein, however, are too expensive for practical use; neither have these models been observed to perform well on UGC datasets <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Another intriguing and more practical approach to integrating temporal features into BVQA models is to design separable spatial-temporal statistics <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Spatial features can be modified to capture temporal effects within BIQA models like BRISQUE, whereby simple frame-differences or spatially displaced frame-differences are deployed <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>.</p><p>A very recent feature-based BVQA model called TLVQM <ref type="bibr" target="#b12">[13]</ref> uses a two-level feature extraction mechanism to achieve efficient computation of a set of impairment/distortion-relevant features. Unlike NSS-based models, TLVQM relies on a comprehensive set of highly crafted features that measure motion, specific distortion artifacts, and aesthetics. TLVQM requires that a large number of parameters be specified by the user, which may affect its general performance on datasets or scenarios it has not been exposed to. The model currently achieves very good performance on natural video quality databases at a reasonable complexity.</p><p>VIDEVAL <ref type="bibr" target="#b5">[6]</ref> is currently the SOTA feature-based BVQA model on recent large-scale video dataset like KoNViD-1k <ref type="bibr" target="#b30">[31]</ref> and YouTube-UGC <ref type="bibr" target="#b31">[32]</ref>. It employs feature selection and fusion on top of efficacious NSS-based models as well as distortion-based features. It is also a very compact model as it only utilizes 60 features. However, it has not been observed to efficiently scale to high-resolution and high-framerate videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning-Based BVQA Models</head><p>Deep convolutional neural networks (CNNs) have been shown to deliver standout performance in a wide variety of low-level computer vision applications <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Recently, the release of several large-scale psychometric visual quality databases <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b50">[51]</ref> have sped the application of deep CNNs to perceptual video and image quality modeling. To conquer the limits of small data size, researchers have either proposed to conduct patch-wise dataaugmentation during training <ref type="bibr" target="#b58">[59]</ref>- <ref type="bibr" target="#b60">[61]</ref>, or to pretrain deep nets on larger visual sets like ImageNet <ref type="bibr" target="#b61">[62]</ref>, then fine tune on target quality databases. Several authors report remarkable performance on synthetic distortion databases <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref> or on naturally distorted databases <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b50">[51]</ref>.</p><p>Deep CNN models have also been employed for natural video quality prediction. Kim et al. <ref type="bibr" target="#b25">[26]</ref> proposed a deep video quality assessor (DeepVQA) to learn spatio-temporal visual sensitivity maps via a deep CNN and a convolutional aggregation network. The V-MEON model [65] leveraged a multi-task CNN framework which jointly optimizes a 3D-CNN for feature extraction and a codec classifier, and using fully-connected layers to predict video quality. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> leveraged transfer learning to develop a general-purpose BVQA framework based on weakly supervised learning and a resampling strategy. In the VSFA model <ref type="bibr" target="#b18">[19]</ref>, the authors applied a pre-trained image classification CNN as a deep feature extractor, then integrated the frame-wise deep features using a gated recurrent unit and a subjectively-inspired temporal pooling layer, reporting leading performance on several natural video databases <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b66">[66]</ref>. The authors then built an enhanced version of VSFA, dubbed MDVSFA <ref type="bibr" target="#b67">[67]</ref>, by employing a mixed datasets training strategy on top, training a single VQA model on multiple datasets, and reporting superior performance on publicly available datasets. Several other popular CNN-based BVQA models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b67">[67]</ref> produce accurate quality predictions on legacy (single  synthetic distortion) video datasets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b68">[68]</ref>, but struggle on recent in-the-wild UGC databases <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b66">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RAPID AND ACCURATE VIDEO QUALITY EVALUATOR (RAPIQUE)</head><p>Prior statistics-based video quality models have been shown to be capable of capturing complex UGC distortions, such as FRIQUEE <ref type="bibr" target="#b11">[12]</ref>, TLVQM <ref type="bibr" target="#b12">[13]</ref>, and VIDEVAL <ref type="bibr" target="#b5">[6]</ref>. However, these models are subject to time-consuming executions since either expensive motion estimation or high-order statistical features are required. CNN models are able to efficiently capture high-level features, which have also been observed to be useful quality indicators <ref type="bibr" target="#b18">[19]</ref>, albeit directly applying a CNN on high-resolution video frames is expensive. Here we propose an efficient two-branch framework, as depicted in <ref type="figure" target="#fig_0">Fig.  1</ref>, which combines quality-aware, low-level NSS features with high-level, semantics-aware CNN features. The NSS features operate on higher-resolution spatial and temporal bandpass feature maps, while the CNN feature extractor is applied on a resized low-resolution frames for practical considerations. We also adopt a sparse frame sampling strategy when extracting features, which further accelerates the runtime. We present the details of RAPIQUE in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Natural Scene Statistics</head><p>It has been observed that the spatial wavelet/subband coefficients of natural images exhibit strong regularities (Gaussianity) following a divisive normalization transform (DNT) <ref type="bibr" target="#b6">[7]</ref>. A simple but effective form of divisive normalization, called mean subtraction and contrast normalization (MSCN), has been observed to accurately characterize image naturalness in multiple feature transforms <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b48">[49]</ref>. We develop NSS-based features following the methodology of FRIQUEE <ref type="bibr" target="#b11">[12]</ref>, which leverages multiple perceptually-relevant feature transforms to extract a large number of statistical features. RAPIQUE uses simple yet effective low-order bandpass statistics, achieving comparable performance as the complex and time-consuming high-order features used in FRIQUEE. We were inspired by the successful and efficient basic features developed as products of spatially-adjacent MSCN responses, and log-derivative statistics in BRISQUE <ref type="bibr" target="#b7">[8]</ref> and DESIQUE <ref type="bibr" target="#b48">[49]</ref>, respectively. Specifically, let Y (i, j) be a given intensity image or a transformed feature map. The MSCN operator is applied on Y (i, j) to further decorrelate and Gaussianize the local pixels:?</p><formula xml:id="formula_0">(i, j) = Y (i, j) ? ?(i, j) ?(i, j) + C ,<label>(1)</label></formula><p>where, (i, j) are spatial indices and C = 1 is a constant that prevents instabilities caused by having a small variance in the denominators. The factors ?(i, j) and ?(i, j) are the weighted local mean and standard deviation within a spatial window centered at location (i, j) calculated by: where w = {w k, |k = ?K, ..., K, = ?L, ..., L} is a 2D isotropic, truncated, unit-volume Gaussian weighting function. We used K = L = 3 in our implementations. It has been empirically observed that the MSCN coefficients of images or video frames have characteristic statistical properties that are altered by the presence of distortion, and therefore, quantifying these deviations can help enable the prediction of perceived quality <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b39">[40]</ref>. A well-known model is the generalized Gaussian distribution (GGD) with zero mean <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_1">?(i, j) = K k=?K L =?L w k,l Y (i ? k, j ? ) (2) ?(i, j) = K k=?K L =?L w k, [Y (i ? k, j ? ) ? ?(i, j)] 2 ,<label>(3)</label></formula><formula xml:id="formula_2">f (x; ?, ? 2 ) = ? 2??(1/?) exp ? |x| ? ? ,<label>(4)</label></formula><p>where ? = ? ?(1/?) ?(3/?) and ?(?) is the gamma function:</p><formula xml:id="formula_3">?(a) = ? 0 t a?1 e ?t dt.</formula><p>The two parameters are the shape ? and the spread ?, of the zero-mean symmetric GGD, which are estimated using a popular moment-matching based method <ref type="bibr" target="#b69">[69]</ref>. These are used as features to predict perceptual quality.</p><p>Another statistical observation is that the sample distributions of products of pairs of neighboring pixels in the MSCN coefficient map along four directions -horizontal (H)</p><formula xml:id="formula_4">(? (i, j)? (i, j + 1)), vertical (V) (? (i, j)? (i + 1, j)), main- diagonal (D1) (? (i, j)? (i+1, j +1))</formula><p>, and secondary-diagonal (D2) (? (i, j)? (i + 1, j ? 1)) also exhibit a regular statistical structure, which are well-modeled as following a zero mode asymmetric generalized Gaussian distribution (AGGD) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b46">[47]</ref>:</p><formula xml:id="formula_5">f (x; ?, ? 2 l , ? 2 r ) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? (? l +? r )?( 1 ? ) exp ? ?x ? l ? x &lt; 0 ? (? l +? r )?( 1 ? ) exp ? x ? r ? x &gt; 0,<label>(5)</label></formula><formula xml:id="formula_6">where ? l = ? l ?(1/?) ?(3/?) and ? r = ? r ?(1/?) ?(3/?) .<label>(6)</label></formula><p>An AGGD model has four parameters: ? controls the shape of the distribution, while (? l , ? r ) are scale parameters that control the spread along each side of the mode; and ? is the mean of the distribution, given by ? = (? r ? ? l ) ?(2/?) ?(1/?) . Apart from the second-order pair-product statistics, we also extract another supplementary set of features by modeling the log-derivative statistics of neighboring MSCN coefficient pairs as introduced in <ref type="bibr" target="#b48">[49]</ref>. Specifically, the absolute pixel values of Y (i, j) are first logarithmically transformed:</p><formula xml:id="formula_7">Z(i, j) = log[|? (i, j)| + 0.1],<label>(7)</label></formula><p>then seven types of log-derivative statistics (Eqs. <ref type="formula" target="#formula_9">(8)</ref>) along six paired orientations -horizontal (H: ? x Z(i, j)), vertical (V: ? y Z(i, j)), main-diagonal (MD: ? xy Z(i, j)), secondary-diagonal (SD: ? yx Z(i, j)), horizontal-vertical (HV: ? x ? y Z(i, j)), and two combined-diagonals (CDs:</p><formula xml:id="formula_8">? cx ? cy Z(i, j) 1 , ? cx ? cy Z(i, j) 2 )</formula><p>, are modeled as GGD, respectively, after which the estimated GGD parameters are gathered as additional statistical features for learning the eventual quality predictor.</p><formula xml:id="formula_9">D 1 :? x Z(i, j) = Z(i, j + 1) ? Z(i, j) D 2 :? y Z(i, j) = Z(i + 1, j) ? Z(i, j) D 3 :? xy Z(i, j) = Z(i + 1, j + 1) ? Z(i, j) D 4 :? yx Z(i, j) = Z(i + 1, j ? 1) ? Z(i, j) D 5 :? x ? y Z(i, j) = Z(i ? 1, j) + Z(i + 1, j) ? Z(i, j ? 1) ? Z(i, j + 1) D 6 :? cx ? cy Z(i, j) 1 = Z(i, j) + Z(i + 1, j + 1) ? Z(i, j + 1) ? Z(i + 1, j) D 7 :? cx ? cy Z(i, j) 2 = Z(i?1, j ?1) + Z(i+1, j +1) ? Z(i ? 1, j + 1) ? Z(i + 1, j ? 1)<label>(8)</label></formula><p>For each pair log-derivative feature map, a single scale NSS model is used to derive two parameters (?, ?) by fitting a GGD distribution using the same moment-matching procedure, yielding a total of 14 additional features.</p><p>The variance field (or 'sigma' field) in Eq. (3) has been previously shown to provide effective quality-aware features deriving from the same NSS/retinal model <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We extract two additional quantities from the variance field (Eq.  </p><formula xml:id="formula_10">? ? = 1 M N M ?1 i=0 N ?1 j=0 ?(i, j)<label>(9)</label></formula><p>where the CoV is ? = (? ? /? ? ) 2 and where </p><formula xml:id="formula_11">? ? = 1 M N M ?1 i=0 N ?1 j=0 [?(i, j) ? ? ? ] 2 .<label>(10)</label></formula><formula xml:id="formula_12">INDEX DESCRIPTION COMPUTATION PROCEDURE f 1 ? f 2 (?, ?) Fit GGD to MSCN coefficients f 3 ? f 4 (??, ??) Compute statistics on 'sigma' map f 5 ? f 8 (?, ?, ? l , ?r) Fit AGGD to H pairwise products f 9 ? f 12 (?, ?, ? l , ?r) Fit AGGD to V pairwise products f 13 ? f 16 (?, ?, ? l , ?r) Fit AGGD to D1 pairwise products f 17 ? f 20 (?, ?, ? l , ?r) Fit AGGD to D2 pairwise products f 21 ? f 22 (?, ?) Fit GGD to D 1 pairwise log-derivative f 23 ? f 24 (?, ?) Fit GGD to D 2 pairwise log-derivative f 25 ? f 26 (?, ?) Fit GGD to D 3 pairwise log-derivative f 27 ? f 28 (?, ?) Fit GGD to D 4 pairwise log-derivative f 29 ? f 30 (?, ?) Fit GGD to D 5 pairwise log-derivative f 31 ? f 32 (?, ?) Fit GGD to D 6 pairwise log-derivative f 33 ? f 34 (?, ?) Fit GGD to D 7 pairwise log-derivative</formula><p>informative to include all these statistical features in the final prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Features</head><p>We built a basic statistical feature extraction module using the NSS features mentioned in the previous section, as summarized in <ref type="table" target="#tab_1">Table I</ref>. Given an input image or feature map, it extracts two features (?, ?) from the MSCN transforms, two features (? ? , ? ? ) from the variance field, 16 features 4 ? (?, ?, ? l , ? r ) from the AGGD fit of MSCN adjacent pair products along 4 directions, and 14 features 7 ? (?, ?) from GGD fits of paired log-derivatives along 7 directions, yielding a total of 34 features, which we dub NSS-34 for simplicity.</p><p>We also hypothesized that the NSS-34 operator is able to extract valuable color quality information if applied in a chromatic space, which we deem to be a more unified and efficient approach than crafting specific, complex features in different feature spaces, as is done in FRIQUEE <ref type="bibr" target="#b11">[12]</ref>. The proposed NSS-34 feature set is based on fast, low-level statistics derived from GGD and AGGD models only, which we will show to deliver superior efficiency in the experimental section.</p><p>The gradient magnitude (GM) of a video frame is defined as the root mean square of directional gradients along two orthogonal spatial directions. GM is computed by convolving with a linear filter such as the Roberts, Sobel, or Prewitt. We utilized the Sobel kernels:</p><formula xml:id="formula_13">h x = ? ? +1 0 ?1 +2 0 ?2 +1 0 ?1 ? ? and h y = ? ? +1 +2 +1 0 0 0 ?1 ?2 ?1 ? ? (11)</formula><p>whereby the GM of an image or frame I(i, j) is calculated by:</p><formula xml:id="formula_14">GM = (I * h x ) 2 + (I * h y ) 2 ,<label>(12)</label></formula><p>where * denotes the convolution operator. It has been observed that two dimensional difference-of-Gaussian (DoG) or Laplacian-of-Gaussian (LoG) operators well-characterize the multiscale receptive fields of retinal ganglion cells <ref type="bibr" target="#b70">[70]</ref>. We also extract two bandpass maps, using LoG and DoG, and extract their corresponding NSS-34 features, respectively. The LoG of image I is:  where the LoG kernel is defined as:</p><formula xml:id="formula_15">LoG = I * h LoG ,<label>(13)</label></formula><formula xml:id="formula_16">h LoG = ? 2 ?x 2 + ? 2 ?y 2 g ? (x, y) = x 2 + y 2 ? 2? 2 2?? 6 exp ? x 2 + y 2 2? 2 ,<label>(14)</label></formula><p>where g ? (x, y) is an isotropic Gaussian function with scale parameter ?. We used a window size of 9?9 for LoG filtering. While the GM and LoG are used by RAPIQUE to amplify high-frequency responses relating to local frame structures, the DoG is configured to capture mid-frequencies, expressive of structure at larger bandpass scales. The DoG response is defined as the difference of the responses of two Gaussian filters with different standard deviations</p><formula xml:id="formula_17">DoG = I * g ?1 ? I * g ?2 = I * (g ?1 ? g ?2 ).<label>(15)</label></formula><p>To avoid redundant information between the LoG and DoG, only the first level of an N -level DoG decomposition with k = 1.6, ? i = k i?1 , i = 1, ..., N ? 1 is utilized. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the differences between the GM, LoG, and DoG responses on a sample video frame. Overall, the four luma channel feature maps (Y, GM, LoG, DoG) (where Y = 0.299R + 0.587G + 0.114B) are fed into the NSS-34 module to obtain useful statistical features.</p><p>Most previous BVQA models have overlooked the importance of chromatic features, whereas recent work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b66">[66]</ref> has shown the efficacy of color components for UGC video quality prediction. Previous efforts on the chromatic statistics of quality models involve opponent color spaces such as YIQ/YUV <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b72">[72]</ref>, LMS <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b72">[72]</ref>, perceptual color spaces like CIELAB <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b73">[73]</ref>, HSI <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b74">[74]</ref>, Yellow color <ref type="bibr" target="#b11">[12]</ref>, and "colorfulness" features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b75">[75]</ref>. Here we deploy perceptually relevant color transforms from RGB frames (where R(i, j), G(i, j), B(i, j) are red, green, and blue channels) to O 1 O 2 O 3 , red-green (RG), and blue-yellow (BY) as follows:</p><formula xml:id="formula_18">O 1 O 2 O 3</formula><formula xml:id="formula_19">? ? O 1 O 2 O 3 ? ? = ? ? 0.06 0.63 0.27 0.30 0.04 ?0.35 0.34 ?0.60 0.17 ? ? ? ? R G B ? ?<label>(16)</label></formula><p>and </p><formula xml:id="formula_20">R(i, j) = log[R(i, j) + 0.1] ? ? R G(i, j) = log[G(i, j) + 0.1] ? ? G B(i, j) = log[B(i, j) + 0.1] ? ? B ,<label>(17)</label></formula><formula xml:id="formula_21">= (R + G + B)/ ? 3 BY = (R + G ? 2B)/ ? 6 RG = (R ? G)/ ? 2 .<label>(18)</label></formula><p>We also included chroma maps A, B from the most widely used CIELAB perceptual color space <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, which can be converted from RGB via CIEXYZ <ref type="bibr" target="#b76">[76]</ref>. Note that we extract both chroma maps as well as their corresponding gradient maps (via Eq. (12)) following the suggestions in <ref type="bibr" target="#b9">[10]</ref>. The above defined luma and chroma feature transforms are visualized in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>Images are naturally multiscale, and distortions affect image structures across scales. Incorporating multiscale information in quality models provides significant performance improvements <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b77">[77]</ref>. Hence, we extract NSS-34 features from the four luma feature maps (Y, GM, LoG, DoG) at two scales: the original image scale and a reduced (by a factor two) resolution. However, we only extract NSS-34 features at the half scale on the twelve chroma feature maps, since frames are often compressed in YUV420 format, which already contain chroma information in reduced scale; additionally, it has also been observed that humans are more sensitive to luma distortions than chroma distortions <ref type="bibr" target="#b54">[55]</ref>. To sum up, the entire collection of spatial features is collected by applying two-scales NSS-34 to the luma feature maps and single-scale NSS-34 to the chromatic maps, yielding a total of 34 ? 2 ? 4 + 34 ? 1 ? 12 = 680 features. It is worth noting that this 680-dim spatial model is an improved alternative to the SOTA 560-dim FRIQUEE model <ref type="bibr" target="#b11">[12]</ref> since it achieves comparable performance as FRIQUEE, but is 20x faster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal Features</head><p>Prior BVQA methods accounting for temporal distortions, however, either rely on expensive motion estimation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or underperform on UGC videos by only accounting for simple frame-difference statistics <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, even including complex CNN models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Here we attempt to exploit more general temporal scene statistics of natural videos to develop and improve BVQA models. To the best of our knowledge, we propose the first general, effective and efficient temporal statistics model based on bandpass regularities of natural videos along the time dimension, going beyond simpler frame-difference models <ref type="bibr" target="#b78">[78]</ref>.</p><p>Inspired by the efficacy of temporal bandpass statistics in the prediction of frame rate-dependent video quality <ref type="bibr" target="#b53">[54]</ref>, our proposed temporal model utilizes 1D temporal bandpass representations. Specifically, consider a bank of K temporal bandpass filters denoted h k , k ? {0, ..., K ? 1}, where k denotes the subband index. The temporal bandpass responses of a video F (x, t) (where x = (x, y) and t represents spatial and temporal co-ordinates, respectively) is</p><formula xml:id="formula_22">Y k (x, t) = F (x, t) * h k (t) k = 0, ..., K ? 1,<label>(19)</label></formula><p>where * and Y k are 1D temporal convolution operations and the bandpass response of the k th filter, respectively. Note that frame differences are a special case of Eq. (19) (the high-pass component of a 2-tap Haar wavelets). <ref type="figure" target="#fig_7">Fig. 7</ref> visually illustrates the bandpass responses of a natural video from the LIVE-VQA dataset [1] using 3-level Haar wavelet filters. Attempting to generalize the spatial NSS as mentioned in Sec. III-A to the temporal domain, we instead analyze the statistics of the temporal bandpass coefficients Y k (x, t), k = 1, ..., 7 (ignoring the lowest band k = 0) by again applying MSCN transforms, as in Eq. (1), to further decorrelate the subband representations, over a set of frame time samples t ? {t 0 , t 1 , ..., t N } (note that t does not need to be densely sampled). Note that in Eq. (1), ? k (x, t) and ? k (x, t) are replaced by the local mean and standard deviation within a spatial window centered at location (x, t), for each subband k.</p><p>We have found that the MSCN coefficients of the temporal bandpass coefficients of natural videos also exhibit a Gaussianlike appearance, as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, while the regularities are modified by the presence of distortion, strongly suggesting the possibility of quantifying deviations to predict perceived video quality. We model the distributions of subband MSCN 4k-360p 20 MOS+? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> coefficients again using the pre-defined GGD and AGGD distributions, by merely passing them into the NSS-34 feature extractor (Sec. III-B). Similar to the spatial feature processing, we also extract the temporal statistical features over two scales (original and half scale), yielding a 476-dim feature vector ((34 features/band)?(7 subbands)?(2 scales) = 476). Inspired by the efficacy of standard deviation pooling as first introduced in GMSD <ref type="bibr" target="#b79">[79]</ref> and later also shown effective when utilized for temporal pooling in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, we calculate the 680 spatial features at two frames per second within each non-overlapping one-second chunk, then enrich the feature set by applying average and absolute difference pooling <ref type="bibr" target="#b80">[80]</ref> of the frame features within each chunk, based on the hypothesis that the variation of spatial features also correlates with the temporal properties of the video. Finally, all of the chunkwise feature vectors are average pooled across all chunks to derive the final set of features over the entire video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deep Learning Features</head><p>CNN-based solutions have been observed to generally perform well on UGC picture quality problems <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b50">[51]</ref> thanks to several recently released large-scale picture quality datasets <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b82">[82]</ref>. Still, none of them have proven effective on UGC video quality databases <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>. However, the authors of <ref type="bibr" target="#b5">[6]</ref> have shown that the simple feature vector from an FC-layer, without fine-tuning, to be a useful quality indicator if training a shallow regressor on top. Therefore, we, for the first time, propose to leverage the best of both worlds, by combining powerful quality-aware NSS features as described in Sec. III-A, III-B, III-C, with pre-trained deep learning features, by jointly training a regressor on them to predict the final quality score.</p><p>One issue encountered when dealing with quality prediction problems is the mismatch of picture sizes between the standard inputs of CNN models such as VGG-16 <ref type="bibr" target="#b83">[83]</ref>, ResNet-50 <ref type="bibr" target="#b33">[34]</ref>, and IQA-valid high-resolution images. Two possible solutions have been attempted to solve this. The authors of <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b60">[61]</ref> suggested applying a CNN on spatially sampled small patches, then aggregating the locally predicted scores to obtain global quality scores. The authors of <ref type="bibr" target="#b50">[51]</ref> presented a CNN operating on full-sized images, but with either global average pooling (GAP) or spatial pyramid pooling (SPP) <ref type="bibr" target="#b84">[84]</ref>, feeding FC layers. These two schemes, however, increase the computation load of the CNN models. Since our proposed model is already armed with powerful spatial and temporal quality-aware features, we added CNN features only to exploit its ability to capture high-level semantic information, supplementing the low-level NSS features. In this regard, we aggressively downscaled the frames to fit the CNN model  <ref type="figure">Fig. 9</ref>. Scatter plots and nonlinear logistic fitted curves of (c) RAPIQUE versus MOS, compared against (a) TLVQM <ref type="bibr" target="#b12">[13]</ref> and (b) VIDEVAL <ref type="bibr" target="#b5">[6]</ref>, using a grid-search SVR using k-fold cross-validation on KoNViD-1k <ref type="bibr" target="#b30">[31]</ref>, LIVE-VQC <ref type="bibr" target="#b29">[30]</ref>, YouTube-UGC <ref type="bibr" target="#b31">[32]</ref>, and the All-Combined set (Sec. IV-A), respectively.</p><p>inputs when extracting these semantic-aware features, yielding greater efficiency than previous CNN VQA models. Another reason to use a pre-trained CNN without fine-tuning is to prevent overfitting, since existing video quality datasets are of limited sizes. In our implementation, we used a ResNet-50 (2,048-dim) as a semantic feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Learning a Video Quality Predictor</head><p>We summarize the feature extraction process as follows. Since our goal is to build an efficient BVQA model, we devised spatial and temporal sampling strategies to further improve its speed. Specifically, given an input video F (x, t), RAPIQUE uniformly samples 2 frames per second, based on which the 680-dim spatial NSS features (in Sec. III-B) are extracted, then average and absolute-difference pools these to obtain 680 spatial and 680 temporal variation features, respectively. RAPIQUE also uniformly samples 8 consecutive frames each second, then applies temporal Haar filter (Eq. <ref type="bibr" target="#b6">(7)</ref>) to extract 7 bandpass responses, from which 476 features are calculated at each time sample. The above features are calculated at a higher resized resolution while maintaining the aspect ratio (we used 512p in our experiments). However, the CNN backbone (ResNet-50) operates on resized frames at a sparse temporal sampling of 1 frame/sec to attain an additional 2,048 features.</p><p>After obtaining all the spatial, temporal, and CNN features within each one-second chunk, we adopt a simple approach to concatenate them all into a totally 3884-dimensional feature vector for each video chunk, then average-pool each to obtain a single 3884 feature vector over the entire video. A shallow or deep regressor head can then be trained on the aggregated feature vector to predict the final video quality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Settings</head><p>Datasets and Baselines: We used three recent BVQA datasets as testbeds for the performance evaluations: KoNViD-1k <ref type="bibr" target="#b30">[31]</ref>, LIVE-VQC <ref type="bibr" target="#b29">[30]</ref>, and YouTube-UGC <ref type="bibr" target="#b31">[32]</ref>, as summarized in <ref type="table" target="#tab_1">Table II</ref>. We also used the combined set (denoted All-Combined) as introduced in <ref type="bibr">[</ref> where equations <ref type="bibr" target="#b19">(20)</ref> and <ref type="formula" target="#formula_0">(21)</ref> are used to calibrate KoNViD-1k and LIVE-VQC, respectively (YouTube-UGC does not need to be changed). Here y adj denotes the adjusted scores, while y org is the original MOS. We refer the reader to <ref type="bibr" target="#b5">[6]</ref> for details regarding assumptions and derivations of the calibration process.</p><p>The baseline models used for comparison are BRISQUE <ref type="bibr" target="#b7">[8]</ref>, GM-LOG <ref type="bibr" target="#b10">[11]</ref>, HIGRADE <ref type="bibr" target="#b9">[10]</ref>, FRIQUEE <ref type="bibr" target="#b11">[12]</ref>, the codebook-based models CORNIA <ref type="bibr" target="#b13">[14]</ref> and HOSA <ref type="bibr" target="#b81">[81]</ref>, and the deep learning models, KonCept512 <ref type="bibr" target="#b50">[51]</ref>, PaQ-2-PiQ <ref type="bibr" target="#b16">[17]</ref> which are all spatial-only models. All the spatial models extract features at 1 fps, which were average-pooled to obtain the final video-level feature vector used for training. We also compared against three feature-based BVQA models, V-BLIINDS <ref type="bibr" target="#b8">[9]</ref>, TLVQM <ref type="bibr" target="#b12">[13]</ref>, and VIDEVAL <ref type="bibr" target="#b5">[6]</ref>, and the deep learning-based models, V-MEON <ref type="bibr" target="#b65">[65]</ref> and VSFA <ref type="bibr" target="#b18">[19]</ref> as well as its enhanced version, MDVSFA <ref type="bibr" target="#b67">[67]</ref>. Since 'completely blind' models such as NIQE <ref type="bibr" target="#b32">[33]</ref> and VIIDEO <ref type="bibr" target="#b46">[47]</ref> were not observed to perform reasonably well on these natural video datasets <ref type="bibr" target="#b5">[6]</ref>, we did not include them.</p><p>Evaluation Method: We used a support vector regressor (SVR) as the back-end regression model to learn the featureto-score mappings <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b60">[61]</ref>. We optimized <ref type="figure" target="#fig_0">Fig. 10</ref>. Performance comparison of SRCC/PLCC as a function of the percentage of the content used to train the compared blind VQA models on the composite All-Combined set. Note that this result is self-explanatory as we used a slightly different evaluation method (20 iterations, SVR with randomized search cross-validation) compared to previous experiments. the SVR parameters (C, ?) via a randomized grid-search on the training set. Following convention, we randomly split the dataset into training and test sets (80%/20%) over 20 iterations, and the overall median test performance was reported. All of the evaluated methods were implemented using the original release by the respective authors. Four performance metrics were used: the Spearman Rank-Order Correlation Coefficient (SRCC) and the Kendall Rank-Order Correlation Coefficient (KRCC) are non-parametric measures of prediction monotonicity, while the Pearson Linear Correlation Coefficient (PLCC) with corresponding Root Mean Square Error (RMSE) were computed to assess prediction accuracy. Note that PLCC and RMSE are computed after performing a nonlinear fourparametric logistic regression to linearize the objective predictions to be on the same scale as MOS <ref type="bibr" target="#b0">[1]</ref>: <ref type="table" target="#tab_1">Table III</ref> shows the main comparison results on the four evaluated datasets. It may be observed that RAPIQUE achieved the best performance on KoNViD-1k, even outperforming the most recent, dense deep learning models such as VSFA and MDVSFA. On LIVE-VQC, which contains many mobile videos exhibiting large camera motions <ref type="bibr" target="#b5">[6]</ref>, TLVQM, which contains numerous heavily crafted motion-relevant features, was the best performer. However, RAPIQUE ranked a clear second, indicating that the temporal NSS features in RAPIQUE are powerful indications of temporal and motionrelated distortions.</p><formula xml:id="formula_23">f (x) = ? 2 + ? 1 ? ? 2 1 + exp (?x + ? 3 /|? 4 |) .<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Main Evaluation Results</head><p>The most recent deep still picture quality models, Kon-Cept512 and PaQ-2-PiQ, have been observed to perform poorly on UGC-VQA datasets. One reason for this is that these models were trained on picture quality datasets <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[51]</ref>, containing strictly spatial content and distortions. A leading blind deep video quality model, V-MEON, also does not perform well, likely because it was trained on compression artifacts rather than on complex combinations of UGC distortions.</p><p>On the larger datasets, RAPIQUE delivered the second-best correlation against the subjective data on YouTube-UGC, only slightly worse than the current SOTA model VIDEVAL, while RAPIQUE ranked the best on the 3165-video composite set, All-Combined. Since VIDEVAL was created by a supervised feature selection process (using subjective labels) on the composite combined set, wherein YouTube-UGC accounts for a large portion, it would be expected to outperform on these two sets. The RAPIQUE model, on the contrary, is databaseagnostic, and also exhibited uniformly well performance on all four test sets. In this regard, RAPIQUE has the potential to perform better on future larger-scale datasets and in realworld application scenarios it has not been exposed to. The scatter plots and fitted curves of RAPIQUE predictions versus MOS in <ref type="figure">Fig. 9</ref> visually demonstrate that the performance of RAPIQUE remains stable on video sequences from different databases, achieving smaller RMSE on larger databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effects of Training Data Size</head><p>To study the degree of performance variation by the compared algorithms, we vary the training-test splits from 10% to 90% of the content used for training, using the rest for testing on the composite combined set. As might be seen in <ref type="figure" target="#fig_0">Fig. 10</ref>, the RAPIQUE model was able to already achieve better than 0.8 in PLCC provided only 50% of the data for training. When compared to SOTA methods, although RAPIQUE was not observed to outperform VIDEVAL when the fraction of training data was less than 40%, it delivered improved performances relative to VIDEVAL and TLVQM as the proportion of training data was increased, as shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. This suggests that RAPIQUE is very data-efficient, with the potential to achieve even better results when largerscale datasets become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>To analyze the importance of each module in RAPIQUE, we conducted an ablation study. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the incremental performance attained when adding each module sequentially. It is worth mentioning the dataset biases of the evaluated benchmarks. For example, the authors of <ref type="bibr" target="#b5">[6]</ref> observed that the LIVE-VQC videos generally contain more (camera) motions and temporal distortions than other databases, while spatial distortions predominate on KoNViD-1k and YouTube-UGC. It may be observed in <ref type="figure" target="#fig_0">Fig. 11</ref>(a) that the spatial NSS module (Sec. III-B) performs quite well on the UGC databases that mainly present spatial distortions, like KoNViD-1k and YouTube-UGC, indicating its efficacy in capturing authentic spatial distortions. LIVE-VQC, which mainly contains videos with large motions, challenges the spatial NSS module, aligning with the empirical observations made above <ref type="bibr" target="#b5">[6]</ref>. Adding spatial variation and temporal NSS features (Sec. III-C) improves the performance of RAPIQUE on LIVE-VQC, indicating that these two types of temporal features capture important attributes of motion-intensive videos. Interestingly, we also noticed that including the SpatialNSS-Var features degraded performance on YouTube-UGC. It is possible that the SpatialNSS-Var features are redundant with SpatialNSS features on YouTube-UGC, causing the training algorithm to underperform. We also observed that temporal statistics did not contribute much to the assessment of Internet UGC videos from YouTube and KoNViD-1k (Flickr).</p><p>It is also important to note that including deep learning features (Sec. III-D) significantly boosts the performance over only using NSS features on all these UGC datasets, further validating our assumptions expressed in Sec. II-B, that high-level semantic features are also informative when conducting UGC video quality prediction. To better understand which types of videos are advantageously analyzed by the CNN features, we divided the combined set into three subsets of differing contents: 2,667 natural videos, 163 screen contents, and 209 gaming videos, as shown in <ref type="figure" target="#fig_0">Fig. 11(b)</ref>. Notably, we observed that the CNN features provided more benefits on screen content and gaming videos than on natural videos. The new temporal statistical features yielded noticeable improvements relative to using only spatial features. Lastly, our deployment of CNN modules is essentially different from other methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[51]</ref> in that RAPIQUE only requires a single pass of the resized frames (224x224), making it highly advantageous in application scenarios having high-speed requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance on different deep features</head><p>To determine which kinds of deep features most effectively complement the proposed NSS features, we conducted another ablation study. We compared the performance of RAPIQUE variants that use features from different backbones: VGG-19, ResNet-50, PaQ-2-PiQ (trained on LIVE-FB <ref type="bibr" target="#b16">[17]</ref>), and KonCept512 (trained on KonIQ-10k <ref type="bibr" target="#b50">[51]</ref>). Since PaQ-2-PiQ was designed for local quality prediction, we included the predicted 3 ? 5 local quality scores along with the single global score. For KonCept512, the 256-dim feature vector immediately before the last linear layer in the fully connected head was included. We also included VGG-19 and ResNet-50, except for they were pre-trained on ImageNet classification.</p><p>The overall performance results are tabulated in <ref type="table" target="#tab_1">Table IV</ref>. It may be observed that combining NSS features with ResNet-50 yielded the best or top performances on all benchmarks,  <ref type="figure" target="#fig_0">Fig. 13</ref>. Our proposed RAPIQUE model enables high-resolution video quality prediction at significantly lower runtimes than existing BVQA methods. Particularly, as seen in the plot our model is 2-150x faster than baselines, depending on resolution, and the higher, the faster. slightly better than KonCept512, suggesting that features pretrained on classification tasks provide valuable high-level semantic information to the quality assessment process. Moreover, using features pre-trained on a specific IQA dataset may limit model generalizability to future, unseen distortions. Another important reason why we prefer ResNet-50 over Kon-Cept512 is the gigantic model size of the InceptionResNetV2 <ref type="bibr" target="#b85">[85]</ref>, used as the backbone of KonCept512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complexity and Runtime Comparison</head><p>Apart from performance analysis, computational efficiency is also of great importance for BVQA models. Thus, we also study the model (feature) dimension and runtime comparisons in <ref type="table" target="#tab_7">Table V</ref>. For a fair comparison, all the experiments were carried out in the same desktop computer, a Dell OptiPlex 7080 Desktop with Intel Core i7-8700 CPU@3.2GHz, 32G RAM, and GeForce GTX 1050 Graphics Cards. The models were implemented using their original releases on MATLAB R2018b and Python 3.6.7 under Ubuntu 18.04.3 LTS system. It should be noted that our comparison of computing complexity involves methods that use different sampling rate (or FPS), which is critical to model efficiency. However, we regard the design of FPS itself as an important aspect of BVQA algorithms, and thus our comparison still provides insights on developing more efficient BVQA models.</p><p>It may be seen that RAPIQUE is extremely efficient as compared to other complex top-performing BVQA models like TLVQM and VIDEVAL. Specifically, RAPIQUE is 10x faster than TLVQM, which also aims to efficiency. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the scatter plots of SRCC versus runtime, which indicates that RAPIQUE achieves comparable prediction accuracy, but with 20x less computational expense as compared to VIDEVAL, the current SOTA model on the UGC-VQA problem <ref type="bibr" target="#b5">[6]</ref>. We observe however that, CNN models that benefit from optimized low-level implementations are generally faster than NSS models executed in MATLAB; we have observed a ?10x speedup by switching from CPU to GPU on the CNNbased models, KonCept512, PaQ-2-PiQ, V-MEON, VSFA, and MDVSFA.</p><p>Predicting the quality of videos having multiple diverse resolutions is also a pressing problem, but has barely been discussed, since most video datasets only contain single-resolution contents. Thanks to the large-scale dataset, YouTube-UGC <ref type="bibr" target="#b31">[32]</ref>, which contains videos at five different resolutions, we were able to extend the complexity analysis to videos ranging from 540p to 4k, to study computational scalability with respect to video size. <ref type="figure" target="#fig_0">Fig. 13</ref> compares computation time as a function of video resolution. We may observe that RAPIQUE has superior computational scalability in terms of data sizes, making it attractive and preferable for potential real-time, low-latency, and light-weight applications requiring high-resolution video inputs. Particularly, as seen in the plot our model is 2-150x faster than baselines, depending on resolution, and the higher, the faster. In <ref type="table" target="#tab_1">Table VI</ref> we list the partial compute time of each sub-module in RAPIQUE on 1080p videos. Since all of the NSS-based features are implemented in MATLAB, a high-level prototyping tool, we would expect further accelerations to be possible (by orders-ofmagnitude) if implemented in low-level languages like C/C++, or GPU-friendly frameworks such as Tensorflow or PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have proposed an effective and efficient model for predicting the subjective quality of user-generated videos, which we call the Rapid and Accurate Video Quality Evaluator (RAPIQUE). The model, for the first time, leverages a composite of spatio-temporal scene statistics features and deep CNN-based high-level features in a two-branch framework, then jointly learns a regressor head for video quality prediction. Within the model, we developed new spatial scene statistics models in an efficient way and further extended the overall model to include normalized temporal bandpass responses, yielding the first general efficacious temporal NSS model for UGC video quality problems. Experiments on recent large-scale UGC video databases show the superior accuracy and efficiency of the proposed model in that it achieves competitive or substantially higher accuracy than both SOTA conventional as well as deep learning video quality models. RAPIQUE is computationally less expensive by orders-ofmagnitude than the most accurate benchmark methods and scales remarkably well with video resolution. To support reproducible research, an implementation of RAPIQUE is available on https://github.com/vztu/RAPIQUE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Schematic overview of the proposed RAPIQUE model. Top block shows the spatial and temporal NSS feature extraction branch, while bottom block depicts the CNN feature extraction flow. The final feature vector is simply concatenated from the extracted spatial and temporal NSS and the CNN features, which is further used to train a regressor head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) img1 (MOS=3.82, best quality) (b) img2 (MOS=2.93, good quality) (c) img3 (MOS=35.8, bad quality) (d) img4 (MOS=15.9, worst quality) Exemplar test images exhibiting four categories of quality: (a) img1 (best) and (b) img2 (good) are two good-quality images from KonIQ-10k (MOS range:<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>)<ref type="bibr" target="#b50">[51]</ref>, while (c) img3 (bad) and (d) img4 (worst) are two bad-quality pictures from CLIVE (MOS range: [0, 100])<ref type="bibr" target="#b28">[29]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Histograms of MSCN (left) and variance map (right) of the four images shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Histograms of four-orientation (H, V, D1, D2) MSCN pair-production of the four images shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Histograms of seven types of MSCN paired log-derivative (Eqs. (8)) of the four images shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Extracted feature maps defined in Sec. III-B. 1st row: Y , GM , LoG, DoG; 2nd row: O 2 , O 3 , GM O 2 , GM O 3 ; 3rd row: BY , RG, GM BY , GM RG; 4th row: A, B, GM A, GM B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>): the mean ? ? and square of the reciprocal of the coefficient of variation (CoV):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Top row: eight exemplar consecutive frames sampled from sequence Tractor in LIVE-VQA [1]. Middle row: temporal bandpass-filtered responses by convolving with the filters in an 8-subband Haar-wavelet filter bank, which are shown in the bottom row. The subband frequency increases from left to right: k = 0, ..., 7, for both the responses and wavelet functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Histograms of raw subband (left) and the corresponding MSCN normalized coefficients (right) of a natural video Tractor, where the normalized coefficients exhibit homogeneous regularities across bands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>6] as an additional composite benchmark. The All-Combined dataset is simply the union of KoNViD-1k (1,200), LIVE-VQC (575), and YouTube-UGC (1,380) after MOS calibration: y adj = 5 ? 4 ? [(5 ? y org )/4 ? 1.1241 ? 0.0993] (20) y adj = 5 ? 4 ? [(100 ? y org )/100 ? 0.7132 + 0.0253] (21)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Ablation study of RAPIQUE on (a) four benchmarks and (b) different content types -Spat denotes the spatial model (Sec. III-B), Var is the temporal difference-pooled spatial models (Sec. III-C), CNN represents deep features (Sec. III-D), and Temp presents the Haar bandpass-filtered features introduced in Sec. III-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Scatter plots of SRCC (on All-Combined) of selected BVQA algorithms versus CPU runtime (per 1080p video on average). Purple indicates the proposed RAPIQUE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, X. Yu, and A. C. Bovik are with Laboratory for Image and Video Engineering (LIVE), Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 78712, USA (emails: zhengzhong.tu@utexas.edu, yuxiangxu@utexas.edu, bovik@utexas.edu).</figDesc><table /><note>Y. Wang, N. Birkbeck, and B. Adsumilli are with YouTube Media Al- gorithms Team, Google LLC, Mountain View, CA, 94043, USA. (emails: yilin@google.com, birkbeck@google.com, badsumilli@google.com)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF THE PROPOSED NSS-34 FEATURE EXTRACTION MODULE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II SUMMARY</head><label>II</label><figDesc>OF THE TESTED BVQA DATASETS.</figDesc><table><row><cell>Database</cell><cell># Vid</cell><cell cols="2">Reso Sec Label</cell><cell>Range</cell></row><row><cell>KoNViD-1k'17 [31]</cell><cell>1,200</cell><cell>540p</cell><cell>8 MOS+?</cell><cell>[1,5]</cell></row><row><cell>LIVE-VQC'18 [30]</cell><cell cols="3">585 1080p-240p 10 MOS</cell><cell>[0,100]</cell></row><row><cell cols="2">YouTube-UGC'20 [32] 1,380</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON OF THE EVALUATED BVQA MODELS ON THE FOUR BVQA DATASETS. THE UNDERLINED AND BOLDFACED ENTRIES INDICATE THE BEST AND TOP THREE PERFORMERS ON EACH DATABASE FOR EACH PERFORMANCE METRIC, RESPECTIVELY.</figDesc><table><row><cell>DATASET</cell><cell cols="3">KoNViD-1k [31]</cell><cell cols="3">LIVE-VQC [30]</cell><cell cols="3">YouTube-UGC [32]</cell><cell cols="3">All-Combined [6]</cell></row><row><cell>MODEL</cell><cell cols="3">SRCC? PLCC? RMSE?</cell><cell cols="3">SRCC? PLCC? RMSE?</cell><cell cols="3">SRCC? PLCC? RMSE?</cell><cell cols="3">SRCC? PLCC? RMSE?</cell></row><row><cell>BRISQUE [8]</cell><cell>0.6567</cell><cell>0.6576</cell><cell>0.4813</cell><cell>0.5925</cell><cell>0.6380</cell><cell>13.100</cell><cell>0.3820</cell><cell>0.3952</cell><cell>0.5919</cell><cell>0.5695</cell><cell>0.5861</cell><cell>0.5617</cell></row><row><cell>GM-LOG [11]</cell><cell>0.6578</cell><cell>0.6636</cell><cell>0.4818</cell><cell>0.5881</cell><cell>0.6212</cell><cell>13.223</cell><cell>0.3678</cell><cell>0.3920</cell><cell>0.5896</cell><cell>0.5650</cell><cell>0.5942</cell><cell>0.5588</cell></row><row><cell>HIGRADE [10]</cell><cell>0.7206</cell><cell>0.7269</cell><cell>0.4391</cell><cell>0.6103</cell><cell>0.6332</cell><cell>13.027</cell><cell>0.7376</cell><cell>0.7216</cell><cell>0.4471</cell><cell>0.7398</cell><cell>0.7368</cell><cell>0.4674</cell></row><row><cell>FRIQUEE [12]</cell><cell>0.7472</cell><cell>0.7482</cell><cell>0.4252</cell><cell>0.6579</cell><cell>0.7000</cell><cell>12.198</cell><cell>0.7652</cell><cell>0.7571</cell><cell>0.4169</cell><cell>0.7568</cell><cell>0.7550</cell><cell>0.4549</cell></row><row><cell>CORNIA [14]</cell><cell>0.7169</cell><cell>0.7135</cell><cell>0.4486</cell><cell>0.6719</cell><cell>0.7183</cell><cell>11.832</cell><cell>0.5972</cell><cell>0.6057</cell><cell>0.5136</cell><cell>0.6764</cell><cell>0.6974</cell><cell>0.4946</cell></row><row><cell>HOSA [81]</cell><cell>0.7654</cell><cell>0.7664</cell><cell>0.4142</cell><cell>0.6873</cell><cell>0.7414</cell><cell>11.353</cell><cell>0.6025</cell><cell>0.6047</cell><cell>0.5132</cell><cell>0.6957</cell><cell>0.7082</cell><cell>0.4893</cell></row><row><cell>KonCept512 [51]</cell><cell>0.7349</cell><cell>0.7489</cell><cell>0.4260</cell><cell>0.6645</cell><cell>0.7278</cell><cell>11.626</cell><cell>0.5872</cell><cell>0.5940</cell><cell>0.5135</cell><cell>0.6608</cell><cell>0.6763</cell><cell>0.5091</cell></row><row><cell>PaQ-2-PiQ [17]</cell><cell>0.6130</cell><cell>0.6014</cell><cell>0.5148</cell><cell>0.6436</cell><cell>0.6683</cell><cell>12.619</cell><cell>0.2658</cell><cell>0.2935</cell><cell>0.6153</cell><cell>0.4727</cell><cell>0.4828</cell><cell>0.6081</cell></row><row><cell>V-BLIINDS [9]</cell><cell>0.7101</cell><cell>0.7037</cell><cell>0.4595</cell><cell>0.6939</cell><cell>0.7178</cell><cell>11.765</cell><cell>0.5590</cell><cell>0.5551</cell><cell>0.5356</cell><cell>0.6545</cell><cell>0.6599</cell><cell>0.5200</cell></row><row><cell>TLVQM [13]</cell><cell>0.7729</cell><cell>0.7688</cell><cell>0.4102</cell><cell>0.7988</cell><cell>0.8025</cell><cell>10.145</cell><cell>0.6693</cell><cell>0.6590</cell><cell>0.4849</cell><cell>0.7271</cell><cell>0.7342</cell><cell>0.4705</cell></row><row><cell>VMEON [65]</cell><cell>0.1118</cell><cell>0.1958</cell><cell>0.6322</cell><cell>0.4024</cell><cell>0.4088</cell><cell>15.524</cell><cell>0.0634</cell><cell>0.1100</cell><cell>0.6304</cell><cell>0.2578</cell><cell>0.2594</cell><cell>0.6657</cell></row><row><cell>VSFA [19]</cell><cell cols="7">0.7728  -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MDVSFA [67]</cell><cell cols="2">0.7812  *  0.7856  *</cell><cell>-</cell><cell cols="2">0.7382  *  0.7728  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VIDEVAL [6]</cell><cell>0.7832</cell><cell>0.7803</cell><cell>0.4026</cell><cell>0.7522</cell><cell>0.7514</cell><cell>11.100</cell><cell>0.7787</cell><cell>0.7733</cell><cell>0.4049</cell><cell>0.7960</cell><cell>0.7939</cell><cell>0.4268</cell></row><row><cell>RAPIQUE</cell><cell>0.8031</cell><cell>0.8175</cell><cell>0.3623</cell><cell>0.7548</cell><cell>0.7863</cell><cell>10.518</cell><cell>0.7591</cell><cell>0.7684</cell><cell>0.4060</cell><cell>0.8070</cell><cell>0.8229</cell><cell>0.3968</cell></row><row><cell cols="6">*  The results are cited from experiments reported in their original papers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) TLVQM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) VIDEVAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) RAPIQUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 0.7754* 0.4205* 0.6978* 0.7426* 11.649*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF RAPIQUE COMBINED WITH DIFFERENT DEEP LEARNING FEATURES. RAPIQUE ((W/ RESNET-50) IS THE DEFAULT VERSION PROPOSED IN THIS PAPER.</figDesc><table><row><cell>DATASET</cell><cell>MODEL / METRIC</cell><cell>SRCC? PLCC? RMSE?</cell></row><row><cell></cell><cell>RAPIQUE (w/ ResNet-50)</cell><cell>0.8031 0.8175 0.3623</cell></row><row><cell>KoNViD-1k</cell><cell cols="2">RAPIQUE (w/ VGG-19) RAPIQUE (w/ KonCept512) 0.7802 0.7793 0.3975 0.7554 0.7389 0.4238</cell></row><row><cell></cell><cell>RAPIQUE (w/ PaQ-2-PiQ)</cell><cell>0.7726 0.7672 0.4026</cell></row><row><cell></cell><cell>RAPIQUE (w/ ResNet-50)</cell><cell>0.7548 0.7863 10.518</cell></row><row><cell>LIVE-VQC</cell><cell cols="2">RAPIQUE (w/ VGG-19) RAPIQUE (w/ KonCept512) 0.7497 0.7611 11.231 0.6888 0.7048 12.228</cell></row><row><cell></cell><cell>RAPIQUE (w/ PaQ-2-PiQ)</cell><cell>0.7147 0.7308 11.599</cell></row><row><cell></cell><cell>RAPIQUE (w/ ResNet-50)</cell><cell>0.7591 0.7684 0.4060</cell></row><row><cell>YouTube-UGC</cell><cell cols="2">RAPIQUE (w/ VGG-19) RAPIQUE (w/ KonCept512) 0.7668 0.7678 0.4190 0.7379 0.7398 0.4365</cell></row><row><cell></cell><cell>RAPIQUE (w/ PaQ-2-PiQ)</cell><cell>0.7596 0.7606 0.4200</cell></row><row><cell></cell><cell>RAPIQUE (w/ ResNet-50)</cell><cell>0.8070 0.8229 0.3968</cell></row><row><cell>All-Combined</cell><cell cols="2">RAPIQUE (w/ VGG-19) RAPIQUE (w/ KonCept512) 0.7924 0.7976 0.4169 0.6888 0.7048 12.228</cell></row><row><cell></cell><cell>RAPIQUE (w/ PaQ-2-PiQ)</cell><cell>0.7742 0.7809 0.4312</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V FEATURE</head><label>V</label><figDesc>DIMENSIONALITY AND AVERAGE (CPU/GPU) RUNTIME COMPARISON (IN SECONDS) EVALUATED ON 1080p VIDEOS.</figDesc><table><row><cell>MODEL</cell><cell>DIM</cell><cell></cell><cell cols="2">RUNTIME</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CPU</cell><cell>GPU</cell></row><row><cell>BRISQUE (1 fr/sec)</cell><cell>36</cell><cell></cell><cell>1.7</cell><cell>-</cell></row><row><cell>GM-LOG (1 fr/sec)</cell><cell>40</cell><cell></cell><cell>2.1</cell><cell>-</cell></row><row><cell>HIGRADE (1 fr/sec)</cell><cell>216</cell><cell></cell><cell>11.6</cell><cell>-</cell></row><row><cell>FRIQUEE (1 fr/sec)</cell><cell>560</cell><cell></cell><cell>701.2</cell><cell>-</cell></row><row><cell>CORNIA (1 fr/sec)</cell><cell>10k</cell><cell></cell><cell>14.3</cell><cell>-</cell></row><row><cell>HOSA (1 fr/sec)</cell><cell>14.7k</cell><cell></cell><cell>1.2</cell><cell>-</cell></row><row><cell>KonCept512 (1 fr/sec)</cell><cell>-</cell><cell></cell><cell>2.8</cell><cell>0.3</cell></row><row><cell>PaQ-2-PiQ (1 fr/sec)</cell><cell>-</cell><cell></cell><cell>6.9</cell><cell>0.8</cell></row><row><cell>V-BLIINDS</cell><cell>47</cell><cell cols="2">1989.9</cell><cell>-</cell></row><row><cell>V-MEON</cell><cell>-</cell><cell></cell><cell>16.4</cell><cell>2.6</cell></row><row><cell>TLVQM</cell><cell>75</cell><cell></cell><cell>183.8</cell><cell>-</cell></row><row><cell>VSFA</cell><cell>-</cell><cell cols="2">1288.7</cell><cell>157.9</cell></row><row><cell>MDVSFA</cell><cell>-</cell><cell cols="2">1319.4</cell><cell>162.5</cell></row><row><cell>VIDEVAL</cell><cell>60</cell><cell></cell><cell>305.8</cell><cell>-</cell></row><row><cell>RAPIQUE (proposed)</cell><cell>3.8k</cell><cell></cell><cell>17.3</cell><cell>-</cell></row><row><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="5">COMPLEXITY ANALYSIS OF RAPIQUE. TABULATED VALUES REFLECT</cell></row><row><cell cols="5">THE PARTIAL TIME DEVOTED TO EACH SUB-COMPONENT IN RAPIQUE.</cell></row><row><cell>MODULE</cell><cell></cell><cell>DIM</cell><cell cols="2">RUNTIME</cell></row><row><cell cols="2">SpatialNSS (Sec. III-B) SpatialNSS-Var (Sec. III-C)</cell><cell>680 680</cell><cell></cell><cell>11.1</cell></row><row><cell>TemporalNSS (Sec. III-C)</cell><cell></cell><cell>476</cell><cell></cell><cell>5.8</cell></row><row><cell>CNN (Sec. III-D)</cell><cell></cell><cell>2.0k</cell><cell></cell><cell>0.4</cell></row><row><cell>RAPIQUE (Full model)</cell><cell></cell><cell>3.8k</cell><cell></cell><cell>17.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward a practical perceptual video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Netflix Tech Blog</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Predicting the quality of compressed videos with pre-existing distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02943</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Video transcoding optimization based on input perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<editor>Appl. Digital Image Process. XLIII, A. G. Tescher and T. Ebrahimi</editor>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UGC-VQA: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4449" to="4464" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of tone-mapped HDR pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and laplacian features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4850" to="4862" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image quality assessment using human visual DOG model fused with random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3282" to="3292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Noreference video quality assessment using space-time chips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ebenezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00031</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Patch-VQ:&apos;patching up&apos;the video quality problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13544</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf. (MM)</title>
		<meeting>ACM Multimedia Conf. (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measurement of ringing artifacts in JPEG images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digit. Pub</title>
		<imprint>
			<biblScope unit="volume">6076</biblScope>
			<biblScope unit="page">60760</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bband index: a no-reference banding artifact predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2712" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Film grain synthesis for AV1 video codec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compress. Conf. (DCC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ProxIQA: A proxy approach to perceptual optimization of learned image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="360" to="373" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">I am going mad: Maximum discrepancy competition for comparing classifiers adaptively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent. (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to distort images using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blind video quality assessment with weakly supervised learning and resampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2244" to="2255" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">KonIQ-10K: Towards an ecologically valid and large-scale IQA database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08489</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Konstanz natural video database (KoNViD-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Qual. Multimedia Exper</title>
		<meeting>9th Int. Conf. Qual. Multimedia Exper</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">YouTube UGC dataset for video compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06457</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blind measurement of blocking artifacts in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A no-reference perceptual blur metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>III-III</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A perceptual visibility metric for banding artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-U</forename><surname>Kum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2067" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive debanding filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1715" to="1719" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast and reliable structure-oriented video noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="118" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">C-DIIVINE: No-reference image quality assessment based on local magnitude and phase statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="725" to="747" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A DCT statistics-based blind image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="586" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatio-temporal measures of naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1750" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on log-derivative statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43025</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CVD2014-a database for evaluating no-reference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structure and function of visual area MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="157" to="189" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment using natural spatiotemporal scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V R</forename><surname>Dendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5612" to="5624" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">ST-GREED: Space-time generalized entropic differences for frame rate dependent video quality prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13715</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Perceptual video quality prediction emphasizing chroma distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video quality model for space-time resolution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the space-time statistics of motion pictures</title>
	</analytic>
	<monogr>
		<title level="j">J. Optical Society America A</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">EnlightenGAN: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A deep neural network for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3773" to="3777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Color image database TID2013: Peculiarities and preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th</title>
		<meeting>4th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Eur. Workshop Vis. Inf. Process. (EUVIP)</title>
		<imprint>
			<biblScope unit="page" from="106" to="111" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf. (MM)</title>
		<meeting>ACM Multimedia Conf. (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unified quality assessment of in-the-wild videos with mixed datasets training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">ViS3: An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13016</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Estimation of shape parameter for generalized gaussian distributions in subband decompositions of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="56" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Application of fourier analysis to the visibility of gratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiology</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">551</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of color images using adaptive signal representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vis. Electron. Imaging XV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7527</biblScope>
			<biblScope unit="page">75271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Toward a no-reference image quality assessment using statistics of perceptual color descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3875" to="3889" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Suesstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vis</title>
		<meeting>SPIE Human Vis</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5007</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Cielab color space -Wikipedia, the free encyclopedia</title>
		<ptr target="https://en.wikipedia.org/wiki/CIELABcolorspace" />
		<imprint>
			<date type="published" when="2020-10-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Asilomar Conf. Signals, Syst. Comput</title>
		<meeting>IEEE Asilomar Conf. Signals, Syst. Comput</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Video quality assessment by reduced reference spatio-temporal entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="694" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A comparative evaluation of temporal pooling methods for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of smartphone photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3677" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intelligence</title>
		<meeting>AAAI Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
