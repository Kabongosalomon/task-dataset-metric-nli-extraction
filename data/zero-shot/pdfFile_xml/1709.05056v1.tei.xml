<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Compact Geometric Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Khoury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Compact Geometric Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to learning features that represent the local geometry around a point in an unstructured point cloud. Such features play a central role in geometric registration, which supports diverse applications in robotics and 3D vision. Current state-of-the-art local features for unstructured point clouds have been manually crafted and none combines the desirable properties of precision, compactness, and robustness. We show that features with these properties can be learned from data, by optimizing deep networks that map high-dimensional histograms into low-dimensional Euclidean spaces. The presented approach yields a family of features, parameterized by dimension, that are both more compact and more accurate than existing descriptors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Local geometric descriptors represent the local geometry around a point in a point cloud. They play a central role in geometric registration, which supports diverse applications in robotics and 3D vision <ref type="bibr" target="#b15">[16]</ref> and underpins modern 3D reconstruction pipelines <ref type="bibr" target="#b41">[42]</ref>. To enable accurate and efficient registration, the descriptor must possess a number of properties <ref type="bibr" target="#b11">[12]</ref>. First, it should map the local geometry to a vector in a Euclidean space R n ; such Euclidean representations support efficient geometric search structures and nearest-neighbor queries. Second, the descriptor should be discriminative: nearest neighbors in feature space should correspond to points with genuinely similar local neighborhoods. Third, the representation should be compact, with a small dimensionality n: this supports fast spatial search. Finally, the representation should be robust to artifacts that are commonly encountered in real data, such as noise and missing regions.</p><p>The design of local geometric descriptors has been the subject of intensive study for the past two decades. Many hand-crafted descriptors have been designed and evaluated <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. Nevertheless, no existing descriptor jointly satisfies the desiderata of high discriminative ability, compactness, and robustness <ref type="bibr" target="#b11">[12]</ref>. Part of the challenge is the difficulty of optimizing the parameters of a highdimensional feature representation by hand.</p><p>In this paper, we present an approach to learning local geometric features from data. Our descriptor applies directly to unstructured point clouds and does not require a clean and consistent surface parameterization <ref type="bibr" target="#b4">[5]</ref>, a volumetric representation <ref type="bibr" target="#b40">[41]</ref>, or the synthesis of auxiliary depth images <ref type="bibr" target="#b34">[35]</ref>. Our features support nearest-neighbor queries in a Euclidean space, which allows establishing dense correspondences across point sets in near-linear time, in contrast to the quadratic complexity required by pairwise matching networks. We thus obtain the first learned geometric feature that can serve as a drop-in replacement for state-of-the-art hand-crafted features in existing pipelines <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>We show that the presented approach yields descriptors that are both more discriminative and more compact than state-of-the-art hand-crafted features. An illustration is provided in <ref type="figure" target="#fig_0">Figure 1</ref>. Experiments demonstrate that our Compact Geometric Features (CGF) yield more accurate matches at lower query times. When CGF is used on the standard Redwood benchmark for geometric registration, with no training or fine-tuning on that dataset, it yields the highest recall reported on the benchmark to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>The development of geometric descriptors for rigid alignment of unstructured point clouds dates back to the 90s. Classic descriptors include Spin Images <ref type="bibr" target="#b18">[19]</ref> and 3D Shape Context <ref type="bibr" target="#b10">[11]</ref>. More recent work introduced Point Feature Histograms (PFH) <ref type="bibr" target="#b25">[26]</ref>, Fast Point Feature Histograms (FPFH) <ref type="bibr" target="#b24">[25]</ref>, Signature of Histogram Orientations (SHOT) <ref type="bibr" target="#b26">[27]</ref>, and Unique Shape Contexts (USC) <ref type="bibr" target="#b32">[33]</ref>. A comprehensive evaluation of existing local geometric descriptors is reported by Guo et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>Significant work has also been conducted on descriptors for nonrigid registration of deformable surfaces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32]</ref>. These descriptors tend to make stronger assumptions, such as the existence of a reasonably clean meshed surface, and are designed to be invariant to isometric deformations. In contrast, rigid registration requires sensitivity to isometric deformations -the opposite of invariance. And applications in robotics require handling noisy unstructured point sets. Our work is devoted to rigid registration of unstructured point clouds.</p><p>A number of recent works applied learning to the problem of matching corresponding points based on local geometry. Wei et al. <ref type="bibr" target="#b34">[35]</ref> describe an approach that matches points on human body scans and operates on ensembles of depth images. Boscani et al. <ref type="bibr" target="#b4">[5]</ref> extend convolutional networks to Riemannian manifolds and apply them to establish correspondences across compatible manifolds. The contemporaneous work of Zeng et al. <ref type="bibr" target="#b40">[41]</ref> uses volumetric signed distance fields and develops learned descriptors that use such volumetric representations as input. Cosmo et al. <ref type="bibr" target="#b7">[8]</ref> learn descriptors for isometry-invariant nonrigid matching. In contrast, our work is devoted to learning compact descriptors for local geometry in unstructured point clouds, which can be used as highly efficient drop-in replacements for prior such descriptors in existing rigid registration pipelines <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Deep networks have been applied to matching image patches and learning local image descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Our research is informed by this work and applies related techniques to a different domain: point cloud registration. In particular, we investigate the effect of output dimensionality on accuracy and show that extremely lowdimensional descriptors can effectively represent the local geometry in an unstructured point cloud, significantly accelerating correspondence search in point cloud registration.</p><p>Learning has also been applied to shape classification and retrieval. Researchers have considered volumetric <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b21">22]</ref> and multi-view representations <ref type="bibr" target="#b30">[31]</ref>. These works do not deal with local geometric features and do not address the challenge of obtaining a local feature that is both accurate and compact. The difference between learning local geometric features and shape classification/retrieval is analogous to the difference between learning local image fea-tures <ref type="bibr" target="#b29">[30]</ref> and image classification/retrieval <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Parameterization. We parameterize the input to our model using spherical histograms centered at each point. These spherical histograms capture the local geometry in a neighborhood around each point. To incorporate rotational invariance, each spherical histogram is oriented to the normal and tangent spaces at each point. The interior of these spheres is subdivided along the radial, elevation, and azimuth directions. All neighboring points in the sphere are accumulated into the bins of the subdivision. The input parameterization is described in Section 4.</p><p>Feature embedding. We train a deep network to map from the high-dimensional space of spherical histograms to a very low-dimensional Euclidean space. The network learns an embedding into a low-dimensional feature space that maps similar geometric neighborhoods to nearby points. The model is trained using the triplet embedding loss. This is described in Sections 5 and 6.</p><p>Correspondences. Given a mapping f from a point into our learned feature space, computing correspondences between two point clouds P i and P j reduces to performing nearestneighbor queries. We compute the set of features f (P i ) and f (P j ), and construct a k-d tree T on the point set f (P j ).</p><p>For each point in f (P i ), we compute its nearest neighbor in f (P j ) using T . As demonstrated in Section 7, correspondences computed using our feature space are much more accurate than correspondences computed using prior geometric feature descriptors. The low dimensionality of our features enables nearest-neighbor queries that are much faster than the second most accurate feature descriptor on realworld data.</p><p>Applications. Our features can serve as drop-in replacements for existing descriptors. We demonstrate this by replacing widely used Fast Point Feature Histograms (FPFH) <ref type="bibr" target="#b24">[25]</ref> in existing geometric registration pipelines. This yields higher registration accuracy with no other modifications. These experiments are reported in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Input Parameterization</head><p>Our basic approach is to start with a very highdimensional representation of the raw local geometry around a point and train a deep network to embed this initial representation into a compact Euclidean space. Forming the initial representation is not trivial. Unlike images, which are laid out on a regular grid with a clear parameterization, a point cloud constitutes a set of unorganized points in R 3 . Even the cardinality of the set of points within a given neighborhood is not fixed. One possibility is to discretize the input into a uniform voxel grid, but such representa-S S n p n p r min r min <ref type="figure">Figure 2</ref>. Our input parameterization, illustrated in two dimensions for clarity. The sphere S is centered at the point p. In this twodimensional illustration, the interior of S is subdivided into three bins along the radial direction and eight bins along the polar direction. This yields a 24-bin histogram into which the points in S are accumulated. The subdivision is aligned to the normal np. In the real three-dimensional setting, the histogram has approximately two thousand bins. tions are wasteful. For a 3-dimensional grid with C 3 cells, a smooth 2-dimensional surface will only intersect O(C 2 ) cells: the rest are empty <ref type="bibr" target="#b17">[18]</ref>. An alternative is to assume a clean parameterization of the underlying surface <ref type="bibr" target="#b4">[5]</ref>, but such a parameterization is not available in general.</p><p>Our initial representation is a histogram of the distribution of points in a local neighborhood, binned along a non-uniform radial grid <ref type="bibr" target="#b10">[11]</ref>. Consider p ? P and let S be a sphere centered at p with radius r. For rotational invariance, we estimate the normal n p and a local reference frame based on this normal <ref type="bibr" target="#b26">[27]</ref>. Consider the third vector z p of the estimated local reference frame. If the dot product n p , z p &lt; 0, we flip the signs of all three vectors in the local reference frame.</p><p>The volume bounded by S can be subdivided into bins along the radial, elevation, and azimuth directions. These directions are defined in terms of the local reference frame. We subdivide the azimuth direction into A bins, each of extent 2?/A. The elevation direction is subdivided into E bins, each of extent ?/E. The radial direction, which has total span r, is logarithmically subdivided into R bins with the following thresholds:</p><formula xml:id="formula_0">r i = exp ln r min + i R ln r r min .<label>(1)</label></formula><p>The first threshold r 0 evaluates to r min , which avoids excessive binning near the center. The thresholds grow exponentially, yielding an initial representation of multiscale context. The result is a spherical histogram with N = R ? E ? A bins. This is illustrated in <ref type="figure">Figure 2</ref>. Let N ? P be the set of neighboring points that lie inside the sphere S. The set N can be found efficiently using a k-d tree. For each point q ? N , we locate the histogram bin that contains q in constant time and increment the corresponding histogram value. After binning all the points in N , we normalize the histogram by dividing each entry by |N |. This yields a normalized N -dimensional feature vector that is used as input for a nonlinear embedding into a lower-dimensional Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Feature Embedding</head><p>We train a deep network f : R N ? R n to map the space of input histograms into a lower-dimensional Euclidean space R n . This mapping serves two purposes. First, Euclidean distances between input histograms in R N are to a significant extent arbitrary and do not appropriately reflect the similarity or dissimilarity of the geometric contexts represented by the histograms. Second, nearest-neighbor search in the lower-dimensional space R n is much faster, which is important because nearest-neighbor search dominates the runtime of geometric registration pipelines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The mapping is trained to pull similar features together while pushing dissimilar features apart. To this end, we use the triplet loss <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>. This objective has been used to optimize feature embeddings for a number of applications in computer vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Consider a set of triplets of input histograms</p><formula xml:id="formula_1">T = {(x a i , x p i , x n i )} i .</formula><p>Vector x a i is referred to as the anchor of triplet i, vector x p i is a positive example that is known to be similar to the anchor, and vector x n i is a negative example that is known to be dissimilar. Given such a set of triplets, we optimize the following objective:</p><formula xml:id="formula_2">L(?) = 1 |T | |T | i=1 f (x a i ; ?) ? f (x p i ; ?) 2 ? f (x a i ; ?) ? f (x n i ; ?) 2 + 1 + ,<label>(2)</label></formula><p>where ? are the parameters of the mapping f and [?] + denotes max(?, 0). Intuitively, f is optimized such that x a i is embedded closer to x p i than to x n i , with a margin separating the distances.</p><p>We use a fully-connected network f with 5 hidden layers. Each hidden layer contains 512 nodes and is followed by an elementwise truncation max(?, 0). We validated our model architecture with a controlled experiment reported in the supplement. At test time, computing the n-dimensional descriptor corresponding to an input histogram amounts to a sequence of matrix multiplications and elementwise operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training</head><p>Consider a set of point clouds {P i } i that depict overlapping fragments of a scene. Let {T i } i be a set of rigid transformations that align the point clouds</p><formula xml:id="formula_3">{P i } i . Thus {T i P i } i ? ? 2? 2? Figure 3</formula><p>. Two overlapping points clouds, shown here in red and black, are sampled from an underlying surface. We consider two concentric spheres of radius ? and 2? around a black point p. The red points in the innermost sphere, which form the set Np,? , are good correspondences for p. The red points in the outermost sphere form the set Np,2? . We generate triplets for p by sampling x p ? Np,? and x n ? Np,2? \ Np,? .</p><p>is a set of point clouds aligned to a common coordinate frame, in which distances between points p ? T i P i and q ? T j P j that depict nearby points in the latent scene are small. In this section we assume that the point clouds {P i } i and transformations {T i } i are given. Data in this form can be obtained from a variety of sources including scene reconstruction pipelines.</p><p>Consider a single point cloud P i and a point p ? P i . Let nn(p, P i ) denote the nearest neighbor of p in P i \ p. Let ? i be the median of the set of distances { p ? nn(p, P i ) : p ? P i } and define ? = max i ? i . Now consider a pair of point clouds (P i , P j ). For each point p ? T i P i we can compute the nearest neighbor nn(p, T j P j ) of p in T j P j . Consider the fraction of such pairs that are within distance ?. Specifically, define</p><formula xml:id="formula_4">? i,j = |{p ? T i P i : p ? nn(p, T j P j ) ? ?}| |P i |<label>(3)</label></formula><p>and similarly for ? j,i . We say that P i and P j overlap if min(? i,j , ? j,i ) ? 0.3. This implies that the underlying surfaces from which P i and P j were sampled overlap by at least 30%.</p><p>Consider the set O of overlapping pairs of point clouds from {P i } i . For each pair (P i , P j ) ? O we examine each point p ? P i . We compute the set of neighbors N j p,? in P j that are at distance at most ? from p. When ? is sufficiently small, the points in N j p,? are good correspondences for p in P j . Similarly consider N j p,2? , the set of points in P j that are at distance at most 2? from p. The set N j p,2? \N j p,? contains difficult negative examples for p. These points have local geometries that are in general more similar to that of p than a randomly chosen point, but are not as close as those in N j p,? . This is illustrated in <ref type="figure">Figure 3</ref>. We generate training triplets (x a , x p , x n ) by sampling a pair of point clouds (P i , P j ) ? O, sampling a point x a ? P i from the overlap region of P i and P j , sampling x p from the set N j x a ,? , and sampling x n from the set N j</p><p>x a ,2? \ N j x a ,? . This procedure is used to generate a large number of training triplets. The triplets are then permuted randomly and partitioned into minibatches.</p><p>We use minibatches of size 512 and train the mapping f using Adam <ref type="bibr" target="#b19">[20]</ref>. The initial weights of the hidden nodes are drawn from a normal distribution with mean 0 and standard deviation 0.1. The learning rate is set to 10 ?4 . The parameters for the exponential decay of the first and second moment estimates are set to ? 1 = 0.9 and ? 2 = 0.999. The network is trained for three epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Setup</head><p>Input parameterization. For the input parameterization, we use R = 17 subdivisions in the radial direction, E = 11 in the elevation direction, and A = 12 in the azimuth direction. The dimensionality of the input histogram is thus N = 2,244. We validate these choices via controlled experiments that are reported in the supplement.</p><p>Our approach yields a family of features parameterized by dimension. The primary setting of our feature space dimensionality is n = 32; the corresponding feature is referred to as CGF-32. The experimental results will demonstrate that this low dimensionality significantly outperforms prior, much larger descriptors.</p><p>On laser scan data, the radius r of the sphere S is set to 17% of the diameter of each model and r min is set to 1.5% of the diameter. The value of the search radius is validated via controlled experiments reported in the supplement. The local reference frame at each point is computed using a search radius of 2% of the diameter.</p><p>On data from SceneNN <ref type="bibr" target="#b16">[17]</ref>, which has absolute metric scale, the radius r is set to 1.2 meters, which is approximately 17% of the diameter of the grid of each fragment, and r min is set to 0.1 meters. The local reference frame is computed using a search radius of 0.25 meters.</p><p>Laser scan data. For experiments with laser scan data, we use a number of public-domain 3D models that are commonly used for this purpose. We use three models from the AIM@SHAPE repository (Bimba, Dancing Children, and Chinese Dragon), four models from the Stanford 3D Scanning Repository (Armadillo, Buddha, Bunny, and Stanford Dragon), and the Berkeley Angel <ref type="bibr" target="#b20">[21]</ref>. Four of these models -Angel, Bimba, Bunny, and Chinese Dragon -were used as the training set, the Dancing Children model was used for validation, and the remaining three models -Armadillo, Buddha, and Stanford Dragon -were used as the test set.</p><p>For each model in the training and validation sets -Angel, Bimba, Bunny, Chinese Dragon, and Dancing Children -we synthesize depth images from 14 views uniformly distributed along the surface of an enclosing sphere. For each depth image we construct a point cloud that lies on the model. We compute the set of pairs of point clouds O that overlap in world space by at least 30%. Since some of these models do not have absolute scale, we set parameters and measure precision in relation to the diameter of the model. Synthesizing depth images allows us to automatically generate as much training data as we need and provides a controlled training environment in which we can validate our design choices. We found that descriptors trained on such synthetically scanned models successfully generalize to raw laser scans.</p><p>For testing we use the original raw laser scans of the models in our test set -Armadillo, Buddha, and Stanford Dragon. All three models were scanned with a Cyberware 3030 MS scanner. Armadillo has 114 scans, Buddha has 58 scans, and the Stanford Dragon has 71 scans. Using the provided alignments we compute a set of pairs of scans O that overlap in world space by at least 30%. These models demonstrate the ability of CGF to generalize to new domains, handle symmetric objects, and cope with noise encountered in laser scanned models.</p><p>SceneNN data. For experiments on real indoor scenes, we use SceneNN <ref type="bibr" target="#b16">[17]</ref>, a comprehensive recent dataset of indoor scenes scanned with consumer depth cameras. Starting from the raw SceneNN scans, we create fragments and register them using the pipeline of Choi et al. <ref type="bibr" target="#b6">[7]</ref>. Each fragment is fused from 100 consecutive frames.</p><p>50% of the scenes are used for training, 25% for validation, and 25% as the test set, split randomly. We will publish our train/val/test split so that others can replicate our experiments. Let O be the set of pairs of overlapping fragments in the training scenes. For maximally precise alignment during training, we refined the registration of each pair (P i , P j ) ? O using ICP <ref type="bibr" target="#b3">[4]</ref>. We use the implementation of ICP provided in the Point Cloud Library <ref type="bibr" target="#b15">[16]</ref>.</p><p>Training. For each point cloud in the training set (synthetic depth image in the case of laser scan data, scene fragment in the case of SceneNN), we sample 40 triplets per point. Of these 40 triplets, 15 are constructed by sampling negatives from N p,2? \ N p,? , as described in Section 6. The remaining 25 are constructed by sampling negatives from the entire model. The threshold ? is set to 1% of the model's diameter in the case of laser scans and 7.5 cm in the case of SceneNN.</p><p>Baselines. We compare CGF to six well-known local descriptors: Point Feature Histograms (PFH) <ref type="bibr" target="#b25">[26]</ref> (dimensionality 125), Fast Point Feature Histograms (FPFH) <ref type="bibr" target="#b24">[25]</ref> (dimensionality 33), Rotational Projection Statistics (RoPS) <ref type="bibr" target="#b12">[13]</ref> (dimensionality 135), Signature of Histogram Orientations (SHOT) <ref type="bibr" target="#b26">[27]</ref> (dimensionality 352), Spin Images (SI) <ref type="bibr" target="#b18">[19]</ref> (dimensionality 153), and Unique Shape Contexts (USC) <ref type="bibr" target="#b32">[33]</ref> (dimensionality 1,980). For RoPS we use the implementation provided by the authors <ref type="bibr" target="#b12">[13]</ref>. For all other baselines we use the implementations provided in the Point Cloud Library <ref type="bibr" target="#b15">[16]</ref>. Each of these existing geometric feature descriptors has several parameters that need to be tuned to ensure good performance. We performed extensive hyperparameter sweeps to ensure that each baseline performed as well as possible in our experiments.</p><p>We have also applied Principal Components Analysis (PCA) to embed our input 2,244-dimensional histograms into R n , using our primary dimensionality n = 32. This evaluates the advantage of the presented nonlinear feature embedding over a linear embedding of the same input into the same space.</p><p>Additional baselines and controlled experiments are reported in the supplement. </p><formula xml:id="formula_5">Accuracy measure. Let {P i } i , {T i } i ,</formula><formula xml:id="formula_6">P i , P j ) ? O.</formula><p>Given a function f that maps points to geometric features, a set of correspondences between P i and P j can be found by first computing the sets of geometric features f (P i ) and f (P j ). Then we build a k-d tree T on the set f (P j ). For each point p ? P i , we compute nn(f (p), f (P j )) by performing a nearest neighbor query in T . Define C f = {(p, q) : p ? P i , q ? P j , f (q) = nn(f (p), f (P j ))} (4) as the set of matches yielded by the feature f .</p><p>Since P i only partially overlaps with P j , we first discard all correspondences (p, q) such that</p><formula xml:id="formula_7">T i p ? nn(T i p, T j P j ) &gt; ?.<label>(5)</label></formula><p>These points have no ground-truth correspondence in P j . Let C f denote the remaining set of correspondences.</p><p>For any distance threshold x, we can compute the fraction of matches that are within distance x of the ground truth:</p><formula xml:id="formula_8">precision f (x) = |{ T i p ? T j q ? x : (p, q) ? C f }| |C f | .</formula><p>(6) This will be our primary measure for evaluating the accuracy of different features f .</p><p>Timings. Average correspondence search times for different descriptors were benchmarked using a single thread on an Intel Xeon E7-8890 2.5 GHz CPU. We use FLANN <ref type="bibr" target="#b23">[24]</ref> to perform nearest-neighbor queries.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Laser scan data</head><p>Precision of different features on the test set is shown in <ref type="figure">Figure 4(a)</ref>. CGF-32 is much more accurate than existing descriptors. For example, 41.4% of the correspondences produced by CGF-32 lie within 1% of the model's diameter of the true match, whereas the most precise prior feature, SI, yields only 32.2% precision at this distance. CGF-32 improves over SI by 28.5% in relative precision while being 4.7 times more compact.</p><p>Timings. Query times for different features are presented in <ref type="figure">Figure 5</ref>. CGF-32 has an average query time of 0.42 ms, which is 3.9 times faster than the second most accurate feature (SI, 1.62 ms) and 75 times faster than USC (31.6 ms). CGF-12 has an average query time of 0.05 ms, slightly slower than the fastest feature (FPFH, 0.04 ms) while being more precise than all baselines (33.2% at 1% of the diameter).</p><p>Visualization. <ref type="figure">Figure 6</ref> (top) shows error distributions of correspondences established in different feature spaces over two laser scans of the Buddha statue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">SceneNN data</head><p>Precision of different feature descriptors on real-world scene fragments from the SceneNN test set is shown in <ref type="figure">Figure 4(b)</ref>. 50.6% of the matches established with CGF-32 are within 10 cm of the true match, much more than USC (29.8%), RoPS (22.7%), PFH (22.1%), FPFH (20.7%), SHOT (20.2%), and SI (8.2%). The baseline constructed by applying PCA to our 2,244-dimensional input parameterization yielded precision of 13.4%, far lower than the precision of the learned nonlinear embedding into the same space. Note that the second highest performing feature on laser scan data, SI, performed poorest on SceneNN.  <ref type="figure">Figure 5</ref>. CGF-32 has an average query time of 0.1 ms, 67 times faster than the second most accurate feature (USC, 6.75 ms). CGF-12 has an average query time of 0.025 ms, matching the speed of FPFH. In addition to its speed, CGF-12 is more precise than all other features, with 31.5% of correspondences within 10 cm of the true match.</p><p>Visualization. <ref type="figure">Figure 6</ref> (bottom) shows error distributions of correspondences established in different feature spaces over two fragments in the SceneNN test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Visualization</head><p>To get a qualitative sense of the learned representation, we can visualize the variation of the learned features over the surface of any model. Specifically, we can use PCA to project from the learned feature space into the 3-dimensional RGB color space. Given a point set, we can evaluate the learned feature for every point, use the learned linear mapping to obtain the corresponding color, and assign this color to the point. <ref type="figure" target="#fig_5">Figure 7</ref> shows the result of this procedure for two synthesized views of the Dancing Children model. Note that the feature mapping appears stable, coherent, and discriminative. Corresponding points on the two views of the model tend to have similar color. Color varies more rapidly in regions of high-frequency geometric variation and is more stable in regions that are geometrically more uniform. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Geometric registration</head><p>We now evaluate the utility of CGF in geometric registration. For this purpose, we use Fast Global Registration (FGR) <ref type="bibr" target="#b41">[42]</ref>, a state-of-the-art global registration algorithm that relies on feature matching. Since feature matching is the computational bottleneck of the algorithm, using a compact feature is important. The authors' implementation of FGR uses FPFH <ref type="bibr" target="#b41">[42]</ref>. We use the published FGR pipeline as the baseline. To evaluate the utility of the learned CGF descriptor, we simply replace FPFH by CGF in the FGR pipeline.</p><p>To evaluate geometric registration accuracy, we follow the evaluation protocol of Choi et al. <ref type="bibr" target="#b6">[7]</ref>, which was also OpenCV <ref type="bibr" target="#b8">[9]</ref> Super 4PCS <ref type="bibr" target="#b22">[23]</ref> PCL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> FGR <ref type="bibr" target="#b41">[42]</ref> CZK <ref type="bibr" target="#b6">[7]</ref> 3DMatch <ref type="bibr">[</ref> Cross-dataset generalization: Redwood benchmark. We now evaluate on the global registration benchmark of Choi et al. <ref type="bibr" target="#b6">[7]</ref>. This benchmark has four datasets, each containing tens of scene fragments. Geometric registration is performed on every pair of fragments, with no initialization. For this experiment, we use the feature embedding that was trained on the SceneNN dataset. We did not retrain or fine-tune the descriptor in any way. This demonstrates the learned descriptor's ability to generalize to new datasets, as well as its ability to serve as a drop-in replacement in preexisting pipelines that depend upon discriminative geometric features.</p><p>The results are reported in <ref type="table" target="#tab_0">Table 1</ref>. We report all the baselines from the evaluation conducted on this dataset by Zhou et al. <ref type="bibr" target="#b41">[42]</ref>. We plug CGF-32 into FGR <ref type="bibr" target="#b41">[42]</ref> and CZK <ref type="bibr" target="#b6">[7]</ref>, the existing implementations of which use the FPFH feature. This yields the corresponding "FGR with CGF-32" and "CZK with CGF-32" conditions. CGF improves the recall of each method by more than 9 percentage points. With CGF-32, the CZK pipeline achieves a recall of 72%, by far the highest reported on the benchmark. Note that this is 6.9 percentage points higher than the contemporaneous results of Zeng et al. <ref type="bibr" target="#b40">[41]</ref>.</p><p>Choi et al. <ref type="bibr" target="#b6">[7]</ref> defined two evaluation measures: recall and precision. Recall is the primary measure. The impor-tance of recall is driven by two factors. First, the maximal level of precision that can be achieved by pairwise registration methods is low due to symmetric structures and other sources of geometric aliasing. Second, there are known ways to raise precision. Given a set of pairwise alignments, robust optimization of all fragments can prune false positives, retaining a given level of recall but increasing precision dramatically <ref type="bibr" target="#b6">[7]</ref>.</p><p>The effect of robust optimization is demonstrated in Table 2. Given pairwise alignments produced by CZK with CGF-32 features, robust optimization removes false positives and yields a set of pairwise alignments with 71.1% recall and 95.1% precision. The accuracy of this final result is limited not by the precision of the input set of pairwise alignments -as the results demonstrate, the overall pipeline is robust to low precision -but by the level of recall. Similar precision can be achieved by applying the framework of Choi et al. <ref type="bibr" target="#b6">[7]</ref> to any of the prior works in <ref type="table" target="#tab_0">Table 1</ref>  <ref type="table">Table 2</ref>. After post-processing with robust global optimization <ref type="bibr" target="#b6">[7]</ref>, CZK with CGF-32 achieves 71.1% recall and 95.1% precision on the Redwood benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We presented an approach to obtaining discriminative features for local geometry in unstructured point clouds. We have shown that state-of-the-art accuracy can be achieved with a low-dimensional feature space. The learned descriptor is both more precise and more compact than handcrafted features. Due to its Euclidean structure, the learned descriptor can be used as a drop-in replacement for existing features in robotics, 3D vision, and computer graphics applications. We expect future work to further improve precision, compactness, and robustness, possibly using new approaches to optimizing feature embeddings <ref type="bibr" target="#b33">[34]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our approach yields a family of Compact Geometric Features (CGF), parameterized by dimension. This figure illustrates the performance of CGF on the SceneNN test set. Our features are both more compact and more precise than the baselines. The horizontal axis (dimensionality) is on a logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and O be defined as in Section 6 and consider an overlapping pair (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Precision on laser scan data test set (b) Precision on the SceneNN test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>(a) The precision of several geometric feature descriptors on laser scan data in the test set. For correspondences provided by CGF-32, 41.4% are precise to within 1% of the diameter. Prior feature descriptors are less accurate. (b) Precision of local geometric features on pairs of fragments from the SceneNN test set. CGF-32 yields the highest precision: 50.6% of the matches computed in the learned feature space are within 10 cm of the ground truth. USC (a 1,980-dimensional descriptor) comes in second at 29.8%. Query time and precision on laser scan data test set (b) Query time and precision on the SceneNN test set (a) The query time and precision of several geometric feature descriptors on laser scan data in the test set. CGF-32 has an average query time of 0.42 ms, 3.9 times faster than the second most accurate feature (SI, 1.62 ms). (b) The query time and precision of local geometric features on pairs of fragments from the SceneNN test set. CGF-32 has an average query time of 0.1 ms, 67 times faster than the second most accurate feature (USC, 6.75 ms). The horizontal axis (time) is on a logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Timings. 32 Figure 6 .</head><label>326</label><figDesc>Query times for different features are presented in Laser scans Top. (a,b) Two laser scans of the Buddha statue. (c-e) Error magnitudes of matches established across the two scans in different feature spaces. CGF provides broad coverage of the surface with accurate matches. Units are in percentage of the model's diameter: black corresponds to error of 3% of the diameter or higher. Bottom. (a,b) Two fragments in the SceneNN test set. (c-e) Error magnitudes of correspondences established across these fragments in different feature spaces. Black corresponds to errors of 25 cm or higher. Correspondences established via CGF are more precise on average. Note the thin structure above the large hole in the middle of the fragment, along which all other feature spaces fail to establish good correspondences. Points shown in grey do not have a ground-truth correspondence on the other point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of local features over two views of the Dancing Children model. Features were projected from the learned feature space into the RGB color space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation on the Redwood benchmark. Plugging our learned descriptor into a pre-existing registration pipeline (CZK) yields the highest recall reported on the benchmark to date, with no training or fine-tuning on this dataset. used by Zhou et al. [42] and Zeng et al. [41] . Laser scans. We compare the accuracy of FGR when FPFH is used to the accuracy of FGR when CGF-32 is used. On the laser scan test set, FGR with FPFH correctly aligns 82.96% of the pairs while FGR with CGF-32 correctly aligns 92.27%. The average RMSE of FGR with FPFH is 13.8% of the diameter, while the average RMSE of FGR with CGF-32 is 9.2% of the diameter. SceneNN. On the SceneNN test set, FGR with FPFH correctly aligns 88.54% of the fragment pairs, while FGR with CGF-32 correctly aligns 91.19%. The average RMSE of FGR with FPFH is 14.86 cm, while the average RMSE of FGR with CGF-32 is 11.83 cm.If we focus on the correctly aligned pairs and evaluate the average RMSE only across those, FGR with FPFH yields an RMSE of 4.68 cm and FGR with CGF-32 yields an average RMSE of 4.07 cm. This in effect evaluates the tightness of the alignment produced by global registration. CGF-32 provides more precise correspondence pairs, which yield tighter alignment.</figDesc><table><row><cell>41]</cell><cell>FGR with CGF-32</cell><cell>CZK with CGF-32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell cols="2">Before pruning</cell><cell cols="2">After pruning</cell></row><row><cell></cell><cell>FGR with</cell><cell>CZK with</cell><cell>FGR with</cell><cell>CZK with</cell></row><row><cell></cell><cell>CGF-32</cell><cell>CGF-32</cell><cell>CGF-32</cell><cell>CGF-32</cell></row><row><cell>Recall (%)</cell><cell>60.7</cell><cell>72.0</cell><cell>60.7</cell><cell>71.1</cell></row><row><cell>Precision (%)</cell><cell>9.4</cell><cell>14.6</cell><cell>86.8</cell><cell>95.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The wave kernel signature: A quantum mechanical approach to shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schlickewei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">PN-Net: Conjoined triple deep network for learning local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05030</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<idno>1992. 5</idno>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bronstein. Matching deformable objects in clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparison of nearest-neighbor-search strategies and implementations for efficient shape registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elseberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magnenat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>N?chter</surname></persName>
		</author>
		<idno>2012. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Software Engineering for Robotics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing objects in range data using regional point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>B?low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comprehensive performance evaluation of 3D local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rotational projection statistics for 3D local surface description and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Registration with the point cloud library: A modular framework for aligning in 3-D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SceneNN: A scene meshes dataset with annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic isosurface propagation using an extrema graph and sorted boundary cell lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koyamada</surname></persName>
		</author>
		<idno>1995. 3</idno>
		<imprint>
			<biblScope unit="volume">TVCG</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Using spin images for efficient object recognition in cluttered 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectral surface reconstruction from noisy point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>O&amp;apos;brien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super 4PCS: Fast global pointcloud registration via smart indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable nearest neighbor algorithms for high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SHOT: Unique signatures of histograms for surface and texture description. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A concise and provably informative multi-scale signature based on heat diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dense human body correspondences using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vouga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LIFT: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?bontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning the matching of local 3D geometry in range scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
