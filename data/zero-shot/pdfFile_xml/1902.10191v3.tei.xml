<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
							<email>giacomo.domeniconi1@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
							<email>chenjie@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
							<email>tengfei.ma1@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
							<email>hirokik@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kaler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><forename type="middle">B</forename><surname>Schardl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are ubiquitous data structures that model the pairwise interactions between entities. Learning with graphs encounters unique challenges, including their combinatorial nature and the scalability bottleneck, compared with Euclidean data (e.g., images, videos, speech signals, and natural languages). With the remarkable success of deep learning for the latter data types, there exist renewed interests in the learning of graph representations <ref type="bibr" target="#b15">(Perozzi, Al-Rfou, and Skiena 2014;</ref><ref type="bibr" target="#b16">Tang et al. 2015;</ref><ref type="bibr" target="#b2">Cao, Lu, and Xu 2015;</ref><ref type="bibr">Ou et al. 2016;</ref><ref type="bibr" target="#b8">Grover and Leskovec 2016)</ref> on both the node and the graph level, now parameterized by deep neural networks <ref type="bibr" target="#b1">(Bruna et al. 2014;</ref><ref type="bibr" target="#b4">Duvenaud et al. 2015;</ref><ref type="bibr" target="#b4">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b12">Li et al. 2016;</ref><ref type="bibr" target="#b6">Gilmer et al. 2017;</ref><ref type="bibr" target="#b11">Kipf and Welling 2017;</ref><ref type="bibr" target="#b9">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b10">Jin et al. 2017;</ref><ref type="bibr" target="#b3">Chen, Ma, and Xiao 2018;</ref><ref type="bibr" target="#b17">Velickovi? et al. 2018;</ref><ref type="bibr" target="#b5">Gao and Ji 2019)</ref>.</p><p>These neural network models generally focus on a given, static graph. In real-life applications, however, often one encounters a dynamically evolving graph. For example, users of a social network develop friendship over time; hence, the vectorial representation of the users should be updated accordingly to reflect the temporal evolution of their social relationship. Similarly, a citation network of scientific articles is constantly enriched due to frequent publications of new work citing prior art. Thus, the influence, and even sometimes the categorization, of an article varies along time. Update of the node embeddings to reflect this variation is desired. In financial networks, transactions naturally come with time stamps. The nature of a user account may change owing to the characteristics of the involved transactions (e.g., an account participates money laundering or a user becomes a victim of credit card fraud). Early detection of the change is crucial to the effectiveness of law enforcement and the minimization of loss to a financial institute. These examples urge the development of dynamic graph methods that encode the temporal evolution of relational data. Built on the recent success of graph neural networks (GNN) for static graphs, in this work we extend them to the dynamic setting through introducing a recurrent mechanism to update the network parameters, for capturing the dynamism of the graphs. A plethora of GNNs perform information fusion through aggregating node embeddings from one-hop neighborhoods recursively. A majority of the parameters of the networks is the linear transformation of the node embeddings in each layer. We specifically focus on the graph convolutional network (GCN) <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref> because of its simplicity and effectiveness. Then, we propose to use a recurrent neural network (RNN) to inject the dynamism into the parameters of the GCN, which forms an evolving sequence.</p><p>Work along a similar direction includes <ref type="bibr" target="#b16">(Seo et al. 2016;</ref><ref type="bibr" target="#b13">Manessia, Rozza, and Manzo 2017;</ref><ref type="bibr" target="#b13">Narayan and Roe 2018)</ref>, among others, which are based on a combination of GNNs (typically GCN) and RNNs (typically LSTM). These meth-ods use GNNs as a feature extractor and RNNs for sequence learning from the extracted features (node embeddings). As a result, one single GNN model is learned for all graphs on the temporal axis. A limitation of these methods is that they require the knowledge of the nodes over the whole time span and can hardly promise the performance on new nodes in the future.</p><p>In practice, in addition to the likelihood that new nodes may emerge after training, nodes may also frequently appear and disappear, which renders the node embedding approaches questionable, because it is challenging for RNNs to learn these irregular behaviors. To resolve these challenges, we propose instead to use the RNN to regulate the GCN model (i.e., network parameters) at every time step. This approach effectively performs model adaptation, which focuses on the model itself rather than the node embeddings. Hence, change of nodes poses no restriction. Further, for future graphs with new nodes without historical information, the evolved GCN is still sensible for them.</p><p>Note that in the proposed method, the GCN parameters are not trained anymore. They are computed from the RNN and hence only the RNN parameters are trained. In this manner, the number of parameters (model size) does not grow with the number of time steps and the model is as manageable as a typical RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Methods for dynamic graphs are often extensions of those for a static one, with an additional focus on the temporal dimension and update schemes. For example, in matrix factorization-based approaches (Roweis and Saul 2000; <ref type="bibr" target="#b0">Belkin and Niyogi 2002)</ref>, node embeddings come from the (generalized) eigenvectors of the graph Laplacian matrix. Hence, DANE <ref type="bibr" target="#b13">(Li et al. 2017)</ref> updates the eigenvectors efficiently based on the prior ones, rather than computing them from scratch for each new graph. The dominant advantage of such methods is the computational efficiency.</p><p>For random walk-based approaches <ref type="bibr" target="#b15">(Perozzi, Al-Rfou, and Skiena 2014;</ref><ref type="bibr" target="#b8">Grover and Leskovec 2016)</ref>, transition probabilities conditioned on history are modeled as the normalized inner products of the corresponding node embeddings. These approaches maximize the probabilities of the sampled random walks. CTDANE <ref type="bibr" target="#b14">(Nguyen et al. 2018</ref>) extends this idea by requiring the walks to obey the temporal order. Another work, NetWalk , does not use the probability as the objective function; rather, it observes that if the graph does not undergo substantial changes, one only needs to resample a few walks in the successive time step. Hence, this approach incrementally retrains the model with warm starts, substantially reducing the computational cost.</p><p>The wave of deep learning introduces a flourish of unsupervised and supervised approaches for parameterizing the quantities of interest with neural networks. DynGEM <ref type="bibr" target="#b6">(Goyal et al. 2017</ref>) is an autoencoding approach that minimizes the reconstruction loss, together with the distance between connected nodes in the embedding space. A feature of DynGEM is that the depth of the architecture is adaptive to the size of the graph; and the autoencoder learned from the past time step is used to initialize the training of the one in the following time.</p><p>A popular category of approaches for dynamic graphs is point processes that are continuous in time. Know-Evolve <ref type="bibr" target="#b17">(Trivedi et al. 2017)</ref> and DyRep <ref type="bibr" target="#b17">(Trivedi et al. 2018</ref>) model the occurrence of an edge as a point process and parameterize the intensity function by using a neural network, taking node embeddings as the input. <ref type="bibr">Dynamic-Triad (Zhou et al. 2018</ref>) uses a point process to model a more complex phenomenon-triadic closure-where a triad with three nodes is developed from an open one (a pair of nodes are not connected) to a closed one (all three pairs are connected). HTNE <ref type="bibr" target="#b19">(Zuo et al. 2018</ref>) similarly models the dynamism by using the Hawkes process, with additionally an attention mechanism to determine the influence of historical neighbors on the current neighbors of a node. These methods are advantageous for event time prediction because of the continuous nature of the process.</p><p>A set of approaches most relevant to this work is combinations of GNNs and recurrent architectures (e.g., LSTM), whereby the former digest graph information and the latter handle dynamism. The most explored GNNs in this context are of the convolutional style and we call them graph convolutional networks (GCN), following the terminology of the related work, although in other settings GCN specifically refers to the architecture proposed by <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref>. GCRN <ref type="bibr" target="#b16">(Seo et al. 2016</ref>) offers two combinations. The first one uses a GCN to obtain node embeddings, which are then fed into the LSTM that learns the dynamism. The second one is a modified LSTM that takes node features as input but replaces the fully connected layers therein by graph convolutions. The first idea is similarly explored in WD-GCN/CD-GCN (Manessia, Rozza, and Manzo 2017) and RgCNN (Narayan and Roe 2018). WD-GCN/CD-GCN modifies the graph convolution layers, most notably by adding a skip connection. In addition to such simple combinations, STGCN <ref type="bibr" target="#b19">(Yu, Yin, and Zhu 2018)</ref> proposes a complex architecture that consists of so-called ST-Conv blocks. In this model, the node features must be evolving over time, since inside each ST-Conv block, a 1D convolution of the node features is first performed along the temporal dimension, followed by a graph convolution and another 1D convolution. This architecture was demonstrated for spatiotemporal traffic data (hence the names STGCN and ST-Conv), where the spatial information is handled by using graph convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we present a novel method, coined evolving graph convolutional network (EvolveGCN), that captures the dynamism underlying a graph sequence by using a recurrent model to evolve the GCN parameters. Throughout we will use subscript t to denote the time index and superscript l to denote the GCN layer index. To avoid notational cluttering, we assume that all graphs have n nodes; although we reiterate that the node sets, as well as the cardinality, may change over time. Then, at time step t, the input data consists of the pair (A t ? R n?n , X t ? R n?d ), where the former is the graph (weighted) adjacency matrix and the latter is the GCN 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Convolutional Network (GCN)</head><p>A GCN <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref> consists of multiple layers of graph convolution, which is similar to a perceptron but additionally has a neighborhood aggregation step motivated by spectral convolution. At time t, the l-th layer takes the adjacency matrix A t and the node embedding matrix H (l) t as input, and uses a weight matrix W (l) t to update the node embedding matrix to H (l+1) t as output. Mathematically, we write</p><formula xml:id="formula_0">H (l+1) t = GCONV(A t , H (l) t , W (l) t ) := ?( A t H (l) t W (l) t ),<label>(1)</label></formula><p>where A t is a normalization of A t defined as (omitting time index for clarity):</p><formula xml:id="formula_1">A = D ? 1 2 A D ? 1 2 , A = A + I, D = diag j A ij ,</formula><p>and ? is the activation function (typically ReLU) for all but the output layer. The initial embedding matrix comes from the node features; i.e., H (0) t = X t . Let there be L layers of graph convolutions. For the output layer, the function ? may be considered the identity, in which case H (L) t contains high-level representations of the graph nodes transformed from the initial features; or it may be the softmax for node classification, in which case H (L) t consists of prediction probabilities. <ref type="figure" target="#fig_0">Figure 1</ref> is a schematic illustration of the proposed EvolveGCN, wherein each time step contains one GCN indexed by time. The parameters of the GCN are the weight matrices W (l) t , for different time steps t and layers l. Graph convolutions occur for a particular time but generate new information along the layers. <ref type="figure">Figure 2</ref> illustrates the computation at each layer. The relationship between H</p><formula xml:id="formula_2">(l) t , W (l) t , and H (l+1) t</formula><p>is depicted in the middle part of the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weight Evolution</head><p>At the heart of the proposed method is the update of the weight matrix W (l) t at time t based on current, as well as historical, information. This requirement can be naturally fulfilled by using a recurrent architecture, with two options.</p><p>The first option is to treat W (l) t as the hidden state of the dynamical system. We use a gated recurrent unit (GRU) to update the hidden state upon time-t input to the system. The input information naturally is the node embeddings H</p><formula xml:id="formula_3">(l) t . Abstractly, we write GCN weights W (l) t hidden state = GRU( node embeddings H (l) t input , GCN weights W (l) t?1 hidden state ),</formula><p>with details deferred to a later subsection. The GRU may be replaced by other recurrent architectures, as long as the roles of W</p><formula xml:id="formula_4">(l) t , H (l) t , and W (l)</formula><p>t?1 are clear. We use "-H" to denote this version; see the left part of <ref type="figure">Figure 2(a)</ref>.</p><p>The second option is to treat W (l) t as the output of the dynamical system (which becomes the input at the subsequent time step). We use a long short-term memory (LSTM) cell to model this input-output relationship. The LSTM itself maintains the system information by using a cell context, which acts like the hidden state of a GRU. In this version, node embeddings are not used at all. Abstractly, we write GCN weights</p><formula xml:id="formula_5">W (l) t output = LSTM( GCN weights W (l) t?1 input ), W (l) t?1 H (l) t W (l) t GRU H (l) t W (l) t H (l+1) t GCONV W (l) t?1 H (l) t W (l) t H (l+1) t EGCU-H + = (a) EvolveGCN-H,</formula><p>where the GCN parameters are hidden states of a recurrent architecture that takes node embeddings as input.</p><formula xml:id="formula_6">W (l) t?1 W (l) t LSTM H (l) t W (l) t H (l+1) t GCONV W (l) t?1 H (l) t W (l) t H (l+1) t EGCU-O + = (b) EvolveGCN-O,</formula><p>where the GCN parameters are input/outputs of a recurrent architecture. <ref type="figure">Figure 2</ref>: Two versions of EvolveGCN. In each version, the left is a recurrent architecture; the middle is the graph convolution unit; and the right is the evolving graph convolution unit. Red region denotes information input to the unit and blue region denotes output information. The mathematical notation W means GCN parameters and H means node embeddings. Time t progresses from left to right, whereas neural network layers l are built up from bottom to top.</p><p>with details deferred to a later subsection. The LSTM may be replaced by other recurrent architectures, as long as the roles of W </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evolving Graph Convolution Unit (EGCU)</head><p>Combining the graph convolution unit GCONV presented in Section 3.1 and a recurrent architecture presented in Section 3.2, we reach the evolving graph convolution unit (EGCU). Depending on the way that GCN weights are evolved, we have two versions:</p><formula xml:id="formula_7">1: function [H (l+1) t , W (l) t ] = EGCU-H(A t , H (l) t , W (l) t?1 ) 2: W (l) t = GRU(H (l) t , W (l) t?1 ) 3: H (l+1) t = GCONV(A t , H (l) t , W (l) t ) 4: end function 1: function [H (l+1) t , W (l) t ] = EGCU-O(A t , H (l) t , W (l) t?1 ) 2: W (l) t = LSTM(W (l) t?1 ) 3: H (l+1) t = GCONV(A t , H (l) t , W (l) t ) 4: end function</formula><p>In the -H version, the GCN weights are treated as hidden states of the recurrent architecture; whereas in the -O version, these weights are treated as input/outputs. In both versions, the EGCU performs graph convolutions along layers and meanwhile evolves the weight matrices over time.</p><p>Chaining the units bottom-up, we obtain a GCN with multiple layers for one time step. Then, unrolling over time horizontally, the units form a lattice on which information (H (l) t and W (l) t ) flows. We call the overall model evolving graph convolutional network (EvolveGCN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation of the -H Version</head><p>The -H version can be implemented by using a standard GRU, with two extensions: (a) extending the inputs and hidden states from vectors to matrices (because the hidden state is now the GCN weight matrices); and (b) matching the column dimension of the input with that of the hidden state.</p><p>The matrix extension is straightforward: One simply places the column vectors side by side to form a matrix. In other words, one uses the same GRU to process each column of the GCN weight matrix. For completeness, we write the matrix version of GRU in the following, by noting that all named variables (such as X t and H t ) are only local variables; they are not to be confused with the mathematical notations we have been using so far. We use these local variable names so that the reader easily recognizes the GRU functionality.</p><p>1: function H t = g(X t , H t?1 ) 2:</p><formula xml:id="formula_8">Z t = sigmoid(W Z X t + U Z H t?1 + B Z ) 3: R t = sigmoid(W R X t + U R H t?1 + B R ) 4: H t = tanh(W H X t + U H (R t ? H t?1 ) + B H ) 5: H t = (1 ? Z t ) ? H t?1 + Z t ? H t 6: end function</formula><p>The second requirement is that the number of columns of the GRU input must match that of the hidden state. Let the latter number be k. Our strategy is to summarize all the node embedding vectors into k representative ones (each used as a column vector). The following pseudocode gives one popular approach for this summarization. By convention, it takes a matrix X t with many rows as input and produces a matrix Z t with only k rows (see, e.g., <ref type="bibr">(Cangea et al. 2018;</ref><ref type="bibr" target="#b5">Gao and Ji 2019)</ref>). The summarization requires a parameter vector p that is independent of the time index t (but may vary for different graph convolution layers). This vector is used to compute weights for the rows, among which the ones corresponding to the top k weights are selected and are weighted for output.</p><formula xml:id="formula_9">1: function Z t = summarize(X t , k) 2: y t = X t p/ p 3: i t = top-indices(y t , k) 4: Z t = [X t ? tanh(y t )] it</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5: end function</head><p>With the above functions g and summarize, we now completely specify the recurrent architecture:</p><formula xml:id="formula_10">W (l) t = GRU(H (l) t , W (l) t?1 ) := g(summarize(H (l) t , #col(W (l) t?1 )) T , W (l) t?1 ),</formula><p>where #col denotes the number of columns of a matrix and the superscript T denotes matrix transpose. Effectively, it summarizes the node embedding matrix H (l) t into one with appropriate dimensions and then evolves the weight matrix W (l) t?1 in the past time step to W (l) t for the current time. Note again that the recurrent hidden state may be realized by not only GRU, but also other RNN architectures as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation of the -O Version</head><p>Implementing the -O version requires only a straightforward extension of the standard LSTM from the vector version to the matrix version. The following is the pseudocode, where note again that all named variables are only local variables and they are not to be confused with the mathematical notations we have been using so far. We use these local variable names so that the reader easily recognizes the LSTM functionality. Current input X t is the same as the past output H t?1 3:</p><formula xml:id="formula_11">F t = sigmoid(W F X t + U F H t?1 + B F ) 4: I t = sigmoid(W I X t + U I H t?1 + B I ) 5: O t = sigmoid(W O X t + U O H t?1 + B O ) 6: C t = tanh(W C X t + U C H t?1 + B C ) 7: C t = F t ? C t?1 + I t ? C t 8: H t = O t ? tanh(C t ) 9: end function</formula><p>With the above function f , we now completely specify the recurrent architecture: W</p><formula xml:id="formula_12">(l) t = LSTM(W (l) t?1 ) := f (W (l)</formula><p>t?1 ). Note again that the recurrent input-output relationship may be realized by not only LSTM, but also other RNN architectures as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Which Version to Use</head><p>Choosing the right version is data set dependent. When node features are informative, the -H version may be more effective, because it incorporates additionally node embedding in the recurrent network. On the other hand, if the node features are not much informative but the graph structure plays a more vital role, the -O version focuses on the change of the structure and may be more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present a comprehensive set of experiments to demonstrate the effectiveness of EvolveGCN. The setting includes a variety of data sets, tasks, compared methods, and evaluation metrics. Hyperparameters are tuned by using the validation set and test results are reported at the best validation epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>We use a combination of synthetic and publicly available benchmark data sets for experiments.</p><p>Stochastic Block Model. (SBM for short) SBM is a popularly used random graph model for simulating community structures and evolutions. We follow <ref type="bibr" target="#b6">(Goyal et al. 2017)</ref> to generate synthetic data from the model. Bitcoin OTC. 1 (BC-OTC for short) BC-OTC is a whotrusts-whom network of bitcoin users trading on the platform http://www.bitcoin-otc.com. The data set may be used for predicting the polarity of each rating and forecasting whether a user will rate another one in the next time step.</p><p>Bitcoin Alpha. 2 (BC-Alpha for short) BC-Alpha is created in the same manner as is BC-OTC, except that the users and ratings come from a different trading platform, http://www. btc-alpha.com. UC Irvine messages. 3 (UCI for short) UCI is an online com-munity of students from the University of California, Irvine, wherein the links of this social network indicate sent messages between users. Link prediction is a standard task for this data set.</p><p>Autonomous systems. 4 (AS for short) AS is a communication network of routers that exchange traffic flows with peers. This data set may be used to forecast message exchanges in the future.</p><p>Reddit Hyperlink Network. 5 (Reddit for short) Reddit is a subreddit-to-subreddit hyperlink network, where each hyperlink originates from a post in the source community and links to a post in the target community. The hyperlinks are annotated with sentiment. The data set may be used for sentiment classification.</p><p>Elliptic. 6 Elliptic is a network of bitcoin transactions, wherein each node represents one transaction and the edges indicate payment flows. Approximately 20% of the transactions have been mapped to real entities belonging to licit categories versus illicit ones. The aim is to categorize the unlabeled transactions.</p><p>These data sets are summarized in <ref type="table" target="#tab_2">Table 1</ref>. Training/validation/test splits are done along the temporal dimension. The temporal granularity is case dependent but we use all available information of the data sets, except AS for which we use only the first 100 days following <ref type="bibr" target="#b6">(Goyal et al. 2017</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tasks</head><p>The proposed EvolveGCN supports three predictive tasks elaborated below. The model for producing the embeddings and the predictive model are trained end to end. The output embedding of a node u by GCN at time t is denoted by h u t . Link Prediction. The task of link prediction is to leverage information up to time t and predict the existence of an edge (u, v) at time t + 1. Since historical information has been encoded in the GCN parameters, we base the prediction on h u t and h v t . To achieve so, we concatenate these two vectors and apply an MLP to obtain the link probability. As a standard practice, we perform negative sampling and optimize the cross-entropy loss function.</p><p>Five data sets are used for experimentation for this task. See the header of <ref type="table" target="#tab_3">Table 2</ref>. Evaluation metrics include mean average precision (MAP) and mean reciprocal rank (MRR).</p><p>Edge Classification. Predicting the label of an edge (u, v) at time t is done in almost the same manner as link prediction: We concatenate h u t and h v t and apply an MLP to obtain the class probability.</p><p>Three data sets are used for experimentation for this task: BC-OTC, BC-Alpha, and Reddit. Evaluation metrics are precision, recall, and F1.</p><p>Node Classification. Predicting the label of a node u at time t follows the same practice of a standard GCN: The activation function of the last graph convolution layer is the softmax, so that h u t is a probability vector. Publicly available data sets for node classification in the dynamic setting are rare. We use only one data set (Elliptic) for demonstration. This data set is the largest one in node count in <ref type="table" target="#tab_2">Table 1</ref>. The evaluation metrics are the same as those for edge classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>We compare the two versions of the proposed method, EvolveGCN-H and EvolveGCN-O, with the following four baselines (two supervised and two unsupervised).</p><p>GCN. The first one is GCN without any temporal modeling. We use one single GCN model for all time steps and the loss is accumulated along the time axis.</p><p>GCN-GRU. The second one is also a single GCN model, but it is co-trained with a recurrent model (GRU) on node embeddings. We call this approach GCN-GRU, which is conceptually the same as Method 1 of <ref type="bibr" target="#b16">(Seo et al. 2016)</ref>, except that their GNN is the ChebNet  and their recurrent model is the LSTM.</p><p>DynGEM. <ref type="bibr" target="#b6">(Goyal et al. 2017)</ref> The third one is an unsupervised node embedding approach, based on the use of graph autoencoders. The autoencoder parameters learned at the past time step is used to initialize the ones of the current time for faster learning.</p><p>dyngraph2vec. <ref type="bibr" target="#b7">(Goyal, Chhetri, and Canedo 2019)</ref> This method is also unsupervised. It has several variants: dyngraph2vecAE, dyngraph2vecRNN, and dyn-graph2vecAERNN. The first one is similar to DynGEM, but additionally incorporates the past node information for autoencoding. The others use RNN to maintain the past node information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Details</head><p>The data set Elliptic is equipped with handcrafted node features; and Reddit contains computed feature vectors. For all other data sets, we use one-hot node-degree as the input feature. Following convention, GCN has two layers and MLP has one layer. The embedding size of both GCN layers is set the same, to reduce the effort of hyperparameter tuning. The time window for sequence learning is 10 time steps, except for SBM and Elliptic, where it is 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results for Link Prediction</head><p>The MAP and MRR are reported in <ref type="table" target="#tab_3">Table 2</ref>. At least one version of EvolveGCN achieves the best result for each of the data sets SBM, UCI, and AS. For BC-OTC and BC-Alpha, EvolveGCN also outperforms the two GCN related baselines, but it is inferior to DynGEM and dyngraph2vec. These latter methods differ from others in that node embeddings are obtained in an unsupervised manner. It is surprising that unsupervised approaches are particularly good on certain data sets, given that the link prediction model is trained separately from graph autoencoding. In such a case, graph convolution does not seem to be sufficiently powerful in capturing the intrinsic similarity of the nodes, rendering a much inferior starting point for dynamic models to catch up. Although EvolveGCN improves over GCN substantially, it still does not reach the bar set by graph autoencoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results for Edge Classification</head><p>The F1 scores across different methods are compared in <ref type="figure">Figure</ref> 3, for the data sets BC-OTC, BC-Alpha, and Reddit. In all cases, the two EvolveGCN versions outperform GCN and GCN-GRU. Moreover, similar observations are made for the precision and the recall, which are omitted due to space limitation. These appealing results corroborate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Results for Node Classification</head><p>The F1 scores for the data set Elliptic are plotted also in <ref type="figure" target="#fig_2">Figure 3</ref>. In this data set, the classes correspond to licit and illicit transactions respectively and they are highly skewed. For financial crime forensic, the illicit class (minority) is the main interest. Hence, we plot the minority F1. The micro averages are all higher than 0.95 and not as informative.</p><p>One sees that EvolveGCN-O performs better than the static GCN, but not so much as GCN-GRU. Indeed, dynamic models are more effective.</p><p>For an interesting phenomenon, we plot the history of the F1 scores along time in <ref type="figure" target="#fig_3">Figure 4</ref>. All methods perform poorly starting at step 43. This time is when the dark market shutdown occurred. Such an emerging event causes performance degrade for all methods, with non-dynamic models suffering the most. Even dynamic models are not able to perform reliably, because the emerging event has not been learned.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>A plethora of neural network architectures were proposed recently for graph structured data and their effectiveness have been widely confirmed. In practical scenarios, however, we are often faced with graphs that are constantly evolving, rather than being conveniently static for a once-for-all investigation. The question is how neural networks handle such a dynamism. Combining GNN with RNN is a natural idea. Typical approaches use the GNN as a feature extractor and use an RNN to learn the dynamics from the extracted node features. We instead use the RNN to evolve the GNN, so that the dynamism is captured in the evolving network parameters. One advantage is that it handles more flexibly dynamic data, because a node does not need to be present all time around. Experimental results confirm that the proposed approach generally outperforms related ones for a variety of tasks, including link prediction, edge classification, and node classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic illustration of EvolveGCN. The RNN means a recurrent architecture in general (e.g., GRU, LSTM). We suggest two options to evolve the GCN weights, treating them with different roles in the RNN. See the EvolveGCN-H version and EvolveGCN-O version inFigure 2. matrix of input node features. Specifically, each row of X t is a d-dimensional feature vector of the corresponding node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are clear. We use "-O" to denote this version; see the left part ofFigure 2(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance of edge classification and node classification. For edge classification (BC-OTC, BC-Alpha, and Reddit), the F1 score is the micro average. For node classification (Elliptic), because of the exceedingly high class imbalance and strong interest in the minority class (illicit transactions), the minority F1 is plotted instead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance of node classification over time. The F1 score is for the minority (illicit) class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1: function H t = f (X t )</figDesc><table /><note>2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Data sets.</figDesc><table><row><cell></cell><cell># Nodes</cell><cell># Edges</cell><cell># Time Steps</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(Train / Val / Test)</cell></row><row><cell>SBM</cell><cell>1,000</cell><cell>4,870,863</cell><cell>35 / 5 / 10</cell></row><row><cell>BC-OTC</cell><cell>5,881</cell><cell>35,588</cell><cell>95 / 14 / 28</cell></row><row><cell>BC-Alpha</cell><cell>3,777</cell><cell>24,173</cell><cell>95 / 13 / 28</cell></row><row><cell>UCI</cell><cell>1,899</cell><cell>59,835</cell><cell>62 / 9 / 17</cell></row><row><cell>AS</cell><cell>6,474</cell><cell>13,895</cell><cell>70 / 10 / 20</cell></row><row><cell>Reddit</cell><cell>55,863</cell><cell>858,490</cell><cell>122 / 18 / 34</cell></row><row><cell>Elliptic</cell><cell>203,769</cell><cell>234,355</cell><cell>31 / 5 / 13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of link prediction. Each column is one data set.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">mean average precision</cell><cell></cell><cell></cell><cell cols="2">mean reciprocal rank</cell></row><row><cell></cell><cell>SBM</cell><cell cols="2">BC-OTC BC-Alpha</cell><cell>UCI</cell><cell>AS</cell><cell>SBM</cell><cell cols="2">BC-OTC BC-Alpha</cell><cell>UCI</cell><cell>AS</cell></row><row><cell>GCN</cell><cell>0.1987</cell><cell>0.0003</cell><cell>0.0003</cell><cell cols="3">0.0251 0.0003 0.0138</cell><cell>0.0025</cell><cell>0.0031</cell><cell>0.1141 0.0555</cell></row><row><cell>GCN-GRU</cell><cell>0.1898</cell><cell>0.0001</cell><cell>0.0001</cell><cell cols="3">0.0114 0.0713 0.0119</cell><cell>0.0003</cell><cell>0.0004</cell><cell>0.0985 0.3388</cell></row><row><cell>DynGEM</cell><cell>0.1680</cell><cell>0.0134</cell><cell>0.0525</cell><cell cols="3">0.0209 0.0529 0.0139</cell><cell>0.0921</cell><cell>0.1287</cell><cell>0.1055 0.1028</cell></row><row><cell>dyngraph2vecAE</cell><cell>0.0983</cell><cell>0.0090</cell><cell>0.0507</cell><cell cols="3">0.0044 0.0331 0.0079</cell><cell>0.0916</cell><cell>0.1478</cell><cell>0.0540 0.0698</cell></row><row><cell cols="2">dyngraph2vecAERNN 0.1593</cell><cell>0.0220</cell><cell>0.1100</cell><cell cols="3">0.0205 0.0711 0.0120</cell><cell>0.1268</cell><cell>0.1945</cell><cell>0.0713 0.0493</cell></row><row><cell>EvolveGCN-H</cell><cell>0.1947</cell><cell>0.0026</cell><cell>0.0049</cell><cell cols="3">0.0126 0.1534 0.0141</cell><cell>0.0690</cell><cell>0.1104</cell><cell>0.0899 0.3632</cell></row><row><cell>EvolveGCN-O</cell><cell>0.1989</cell><cell>0.0028</cell><cell>0.0036</cell><cell cols="3">0.0270 0.1139 0.0138</cell><cell>0.0968</cell><cell>0.1185</cell><cell>0.1379 0.2746</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://snap.stanford.edu/data/soc-sign-bitcoin-otc.html 2 http://snap.stanford.edu/data/soc-sign-bitcoin-alpha.html 3 http://konect.uni-koblenz.de/networks/opsahl-ucsocial</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://snap.stanford.edu/data/as-733.html 5 http://snap.stanford.edu/data/soc-RedditHyperlinks.html 6 https://www.kaggle.com/ellipticco/elliptic-data-set</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Relational Representation Learning</title>
		<editor>ICLR. [Cangea et al. 2018] Cangea, C.</editor>
		<editor>Veli?kovi?, P.</editor>
		<editor>Jovanovi?, N.</editor>
		<editor>and Thomas Kipf, P. L</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Towards sparse hierarchical graph classifiers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GraRep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu ; Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bresson</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vandergheynst ; Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>and Ji 2019</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DynGEM: Deep embedding method for dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Workshop on Representation Learning for Graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">dyngraph2vec: Capturing network dynamics using dynamic graph representation learning. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chhetri</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Chhetri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canedo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<meeting><address><addrLine>Leskovec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting organic reaction outcomes with Weisfeiler-Lehman network</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attributed network embedding for learning in a dynamic environment</title>
		<idno type="arXiv">arXiv:1704.06199</idno>
	</analytic>
	<monogr>
		<title level="m">Dynamic graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="433" to="438" />
		</imprint>
	</monogr>
	<note>Learning graph dynamics using deep neural networks. IFAC-PapersOnLine</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
		</imprint>
	</monogr>
	<note>DeepWalk: Online learning of social representations</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<idno>Tang et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>LINE: Large-scale information network embedding</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Know-Evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04051</idno>
	</analytic>
	<monogr>
		<title level="m">Representation learning over dynamic graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">NetWalk: A flexible deep embedding approach for anomaly detection in dynamic networks</title>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
